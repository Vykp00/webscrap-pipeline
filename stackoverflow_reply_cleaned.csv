"id","question_id","domain","like_num","created_date","scrap_date","content"
77032391,77024572,"stackoverflow.com",1,"2023-09-03 16:14:07+03","2024-05-17 04:30:19.036017+03","Lambda is the only service afaik that can If the longrunning task can complete within the Lambda 15 minute max timeout then use Lambda Note that with increased RAM Lambda functions get more allocated CPU and network bandwidth so they can accomplish more in 15 minutes In this scenario note that API Gateway has an integration timeout of 29 seconds for Lambda function so the Lambda function initially invoked by API Gateway would have to asynchronously invoke a 2nd Lambda function to perform the longrunning task If the task takes longer than 15 minutes can it be split into multiple subtasks each of which completes in under 15 minutes If so then look at Step Functions with multiple concurrent or serial Lambda functions Otherwise it sounds like you may need a different compute solution for example ECS or EC2 Both EC2 and ECSbased persistent solutions will have some ongoing cost however Another option for tasks that do not fit in Lambdas 15 minute timeout may be to consider creating some orchestration that can run your task on a newlylaunched EC2 instance which autoterminates when the task is complete thus you are only paying for EC2 when needed This option is a little more complex because you have to orchestrate it e g by triggering a Lambda function to launch an EC2 instance pass the task specification to that EC2 instance e g in launchtime userdata script or via SMS Run Command monitor for success collect the output autoterminate the instance when complete and handle retriesfailures "
77819457,77819298,"stackoverflow.com",0,"2024-01-15 13:25:35+02","2024-05-17 04:30:22.00376+03","Solved Using include parameter"
77959424,77959246,"stackoverflow.com",1,"2024-02-08 07:23:51+02","2024-05-17 04:30:23.867868+03","The certificate is for global CloudFront infrastructure which resides not in a single region but in all regions useast1 is AWS main region and is where you need to create most stuff thats used globally "
67879769,67877704,"stackoverflow.com",0,"2021-06-08 02:33:18+03","2024-05-17 04:30:26.251894+03","I simple destroyed the table using the zappadjangoutils command The tricky part was to create a new admin user Since I replaced the auth model with the accounts model I had to use the raw python command Typically the custom user model should be implemented before any migrations as many things are related to the User model So dropping all the tables were inevitable though I tried to avoid it "
68010358,68007065,"stackoverflow.com",2,"2021-06-17 00:42:43+03","2024-05-17 04:30:27.248274+03","There is an open pull request that is ready to merge but needs additional user testing The older project has a pull request that claims layer support has been merged Feel free to try it out and let the maintainers know so documentation can be updated "
68155239,68137613,"stackoverflow.com",1,"2021-06-28 00:06:45+03","2024-05-17 04:30:28.197507+03","One of the primary uses of Zappa is to allow the migration of an existing Django project into AWS Lambda So moving your REST API to be serverless is an excellent use case for Zappa In addition Zappa generally is compatible with older versions of Django The issue you are encountering is that AWS Lambda functions must be under 50MB compressed and 250MB uncompressed Zappa offers a workaround using the slim_handler option that allows your project to approach 512MB uncompressed To determine how much disk space your current project requires uncompressed you may run the following Zappa command And Zappa will generate a zip file of your Django project Then you can uncompress the zip file and determine the disk space requirements If you require more than 512MB of disk space then your only option is to use the AWS Lambda Container Image Support feature that was introduced recently at least when this answer was written Zappa version 0 53 0 supports this feature Instructions on how to leverage this are not yet in the Zappa documentation so the best guide right now can be found at Ian Whitestones blog On the topic of performance for AWS Lambda and heaviness of an existing Django application Putting your Django app in AWS Lambda environment is like installing a bigger engine in a car You may find that your Django applications perform better in AWS "
68191556,68176733,"stackoverflow.com",8,"2021-06-30 11:56:06+03","2024-05-17 04:30:29.919479+03","There is already an issue in the zappa github httpsgithub comzappaZappaissues995 The problem is that zappa adds setuptools to its dependencies but the latest release of piptools 6 2 0 does the same now As a quick fix you can pin piptools to version 6 1 0 in your requirements txt"
68391622,68391621,"stackoverflow.com",18,"2021-07-15 12:53:11+03","2024-05-17 04:30:31.952529+03","Since version 3 0 0 the package troposphere removed the deprecated Template methods see the changelog Breaking changes Python 3 6 Python 2 x and earlier Python 3 x support is now deprecated due to Python EOL Remove previously deprecated Template methods The above issue can be fixed by adding troposphere3 in the requirements file "
69122216,68391621,"stackoverflow.com",1,"2021-09-09 20:06:06+03","2024-05-17 04:30:31.95453+03","The answer by nbeuchat might not work for everyone an alternative solution that might help would be to change the line in venvlibpython3 6 or 78 etc sitepackageszappacore py from add_description to set_description Troposphere updated add_description to set_description along with other previously deprecated template methods which is why you see the problem Source"
66512824,65550786,"stackoverflow.com",0,"2021-03-07 05:06:51+02","2024-05-17 04:30:35.910573+03","you can rerun arkade install again with new parameters and it will upgrade If you want to test in a safe space use ark get kind and then use kind to build a test cluster on your local That is what I did to get the output below Background Under the hood arkade uses Helm to manage applications installed into kubernetes clusters and Helm can do in place upgrades Below is an example Before with 1 gateway replica Upgrading to 2 gateway replicas The output for arkade install will show you the actual Helm command used In this case helm upgrade install which will install if the app does not exist and upgrade if it does Here you will see two gateway pods"
66098915,66079490,"stackoverflow.com",1,"2021-02-08 11:27:10+02","2024-05-17 04:30:37.728995+03","The OpenFaas is equiped with autoscaler itself with its own alert manager OpenFaaS ships with a single autoscaling rule defined in the mounted configuration file for AlertManager AlertManager reads usage requests per second metrics from Prometheus in order to know when to fire an alert to the API Gateway After some reading I found out that OpenFaas autoscaleralertmanger is more focused on API hit rates whereas Kubernetes HPA is more focused on CPU and memory usage so it all depends on what you exactly need So you have two different annotations for two different tools for scaling The deployment kubernetes iomaxreplicas2 is for Kubernetes HPA and com openfaas scale max 1 is for the OpenFaas autoscaler The OpenFaas has a great example of how you can use HPA instead built in scaler You can also use custom Prometheus metrics with HPA as described here "
66289029,66131012,"stackoverflow.com",0,"2021-02-20 09:21:43+02","2024-05-17 04:30:38.84803+03","Go http clients usually return EOF in case of request timeouts OpenFaas watchdog gateway queue workers all use GO http clients internally Most probably your OpenFaas installation is not configured properly for long running functions You can double check this by making your Springboot API return immediately and see if that works You can refer to this to configure your OpenFaas to run long running functions This is an excellent sample function for long running functions For your second question if your faas build is succeeding and i am assuming you are working with standard OpenFaas Java template it packages the complete output of gradle build inside the docker container which should carry all your dependency files as well "
68578577,66581554,"stackoverflow.com",0,"2021-07-29 18:22:06+03","2024-05-17 04:30:39.450356+03","I believe you can find your answer here To summarize"
61088449,60694250,"stackoverflow.com",2,"2020-04-07 22:59:46+03","2024-05-17 04:30:41.618259+03","I had the exact same error trying to deploy a Flask app using Zappa and then realized that I was using an old botocore package version I changed all the package versions in my requirements txt file to the ones on zappas github page and it fixed the issue for me "
62102881,60714279,"stackoverflow.com",0,"2020-06-01 09:15:44+03","2024-05-17 04:30:42.346511+03","Try this"
62148496,60929286,"stackoverflow.com",8,"2020-06-02 12:05:31+03","2024-05-17 04:30:43.373484+03","python m pip install upgrade pythondateutil Upgrading pythondateutil solve the problem for me On my machine 2 5 3 to 2 8 1 Skimming the source code I found out that UTC variable is added in 2 7 0 So I think any version 2 7 0 will solve the issue "
60948130,60929286,"stackoverflow.com",1,"2020-03-31 13:11:43+03","2024-05-17 04:30:43.375484+03","Got a similar issue while trying to copy files from AWS S3 using aws cli Running pip install pythondateutil updated the pythondateutil to 2 8 1 and it works well now "
62701755,60929286,"stackoverflow.com",1,"2020-07-02 20:17:31+03","2024-05-17 04:30:43.376484+03","Try to specify the pythondateutil version and install it before the package which has a error in your case zappa exempla my case the package which i was getting a erro was freezegun so i ran and it worked"
62851257,60929286,"stackoverflow.com",1,"2020-07-11 18:42:53+03","2024-05-17 04:30:43.377484+03","I had this issue where trying to run eb deploy for AWS Elastic Beanstalk What worked for me was sudo pip3 install pythondateutil upgrade note running this without sudo did not fix the issue For reference this was the error I was getting prior to this"
71579549,61288483,"stackoverflow.com",1,"2022-03-23 00:12:10+02","2024-05-17 04:30:44.455115+03","From Zappa official repo Running the Initial Setup Settings Zappa can automatically set up your deployment settings for you with the init command This will automatically detect your application type FlaskDjango Pyramid users see here and help you define your deployment configuration settings Once you finish initialization you will have a file named zappa_settings json in your project directory defining your basic deployment settings It will probably look something like this for most WSGI apps or for Django Be sure to run zappa init at the root folder of your project with a virtual environnement activated "
61449201,61440878,"stackoverflow.com",1,"2020-04-27 01:55:43+03","2024-05-17 04:30:46.572979+03","From Selecting a Stack Template AWS CloudFormation Amazon S3 URL The URL must point to a template with a maximum size of 460800 bytes that is stored in an S3 bucket that you have read permissions to and that is located in the same region as the stack I suspect that your stack is failing because the template is in an Amazon S3 bucket that is in a different region to where the stack is being launched You will need to copy the template into a bucket in the same region then provide it in the create_stack command You can test this by using the AWS Console to launch the template rather than having to go via boto3 "
66507059,61445245,"stackoverflow.com",0,"2021-03-06 17:00:42+02","2024-05-17 04:30:47.557918+03","I had the exactly same error as you In my case the problem was that I simply forgot to update the deployment by running zappa update dev again after I updated the settings "
54676949,50155268,"stackoverflow.com",0,"2019-02-13 21:43:23+02","2024-05-17 04:30:49.511515+03","The likely reason why the install is not working is the name of the chart To install it using the chart your command should be So with a between the two occurrences of openfaas at the end"
70107523,50155268,"stackoverflow.com",0,"2021-11-25 09:53:55+02","2024-05-17 04:30:49.51271+03","Deploy the Chart with arkade fastest option The arkade install command installs OpenFaaS using its official helm chart but without using tiller a component which is insecure by default arkade can also install other important software for OpenFaaS users such as certmanager and nginxingress It is the easiest and quickest way to get up and running You can use arkade to install OpenFaaS to a regular cloud cluster your laptop a VM a Raspberry Pi or a 64bit ARM machine Install the OpenFaaS app If you are using a managed cloud Kubernetes service which supplies LoadBalancers then run the following Note the loadbalancer flag has a default of false so by passing the flag the installation will request one from your cloud provider If you are using a local Kubernetes cluster or a VM then run After the installation you will receive a command to retrieve your OpenFaaS URL and password Other options for installation are available with arkade install openfaas help For cloud users run and look for EXTERNALIP This is your gateway address "
51257593,51256488,"stackoverflow.com",0,"2018-07-10 08:06:48+03","2024-05-17 04:30:50.022876+03","I believe I made a rookie mistake In stead of using handler js as the main file I used index js I probably had a leftover handler js from running the tutorial but mv index js handler js did the trick after rebuilding and deploying of course "
51325738,51325211,"stackoverflow.com",1,"2018-07-13 16:00:02+03","2024-05-17 04:30:50.883225+03","I have seen this error because the docker version was not supported by Kubernetes As of Kubernetes version 1 11 the supported versions are 1 11 2 to 1 13 1 and 17 03 x I could not test the solution with OpenFaaS "
53491923,53453517,"stackoverflow.com",4,"2018-11-27 04:32:36+02","2024-05-17 04:30:53.53687+03","You cant as stated here httpsgithub comopenfaasfaasnetesissues147 The solution suggested is to return a payload with the status code and parse the payload on the receiving end "
58314600,53453517,"stackoverflow.com",0,"2019-10-10 05:25:22+03","2024-05-17 04:30:53.53787+03","The default python template appears to return 200 even when you system exit 1 or throw an exception I assume other older templates behave similarly and that this is expected of the classic watchdog However if the language template you use supports running the new ofwatchdog I think it can return non200 codes like 400 404 etc You can download these templates using the faascli they are from the openfaasincubator project and the community I just confirmed using the python3http languagetemplate from openfaasincubator and csharphttprequest from distantcam that non200 codes can be returned like 400 404 etc This will list available templates To install the csharp template I tested with The OOTB handler for csharphttprequest can be easily modified like this to return a 400"
72437085,53453517,"stackoverflow.com",0,"2022-05-30 18:59:53+03","2024-05-17 04:30:53.539871+03","After a lot of research about how to do it in python dont use the casual template you should use the pythonflask template It works like a charm "
53958526,53957422,"stackoverflow.com",0,"2020-06-20 12:12:55+03","2024-05-17 04:30:55.088639+03","I am not an expert but as of now it seems not possible to specify the http method inside the trigger Check latest trigger spec as you can see there is no notion of http method here However handling different HTTP methods can be done inside the function itself For example in Java with fdkjava v1 0 80 you can use com fnproject fn api httpgateway HTTPGatewayContext as the first parameter of the function as described in the section Accessing HTTP Information From Functions of the documentation In Fn for Java when your function is being served by an HTTP trigger or another compatible HTTP gateway you can get access to both the incoming request headers for your function by adding a com fnproject fn api httpgateway HTTPGatewayContext parameter to your functions parameters Using this allows you to You can then retrieve the HTTP method by calling getMethod on the HTTPGatewayContext passed as parameter In other languages with others fdk it is possible to do the same From this different contexts you will then be able to get method parameter passed when fn invoke method[GET POST ] via fnhttpmethod header The main drawback here is that all HTTP methods should be handled in the same function Nonetheless you can structure your code to have only one class per method "
70081218,70075290,"stackoverflow.com",1,"2021-11-23 14:48:59+02","2024-05-17 04:34:57.956145+03","Add to your zappa_settings json Zappa issue about it"
53969797,53957422,"stackoverflow.com",0,"2018-12-29 15:05:35+02","2024-05-17 04:30:55.09164+03","After some further thought it seems fairly clear now what my actual misunderstanding was When I have built Serverless framework services in the past or built and deployed Lambda functions using terraform I have been deploying to AWS and so have been using AWSs API Gateway offering their product is actually called API gateway but its important to recognise that API Gateway is a distributed systems microsevices design pattern API gateway makes it possible to route specific http request types including the method GETPOSTPUTDELETE to the desired functions Platforms such as Fn project and OpenFaaS do not provide an out of the box api gateway solution and it seems we would need to take care of this ourselves These above mentioned platforms are about deployment of functions We find the other bits via our product of choice "
76445633,53957422,"stackoverflow.com",0,"2023-06-10 12:36:47+03","2024-05-17 04:30:55.09364+03","The concept of fn project is to create functions with inputoutput independent of lower level protocols we would be able to bind the function to several protocols on different systems In practice however only http is currently supported as far as I know You can always break the abstractions by reaching into the lower levels of the API see norbdj answer and finding HTTP related stuff But is not present in higher level API because it was meant too be independent of it "
54840808,54827583,"stackoverflow.com",1,"2019-02-23 12:53:45+02","2024-05-17 04:30:57.243063+03","Are you using Firefox If so this answer might help you httpsstackoverflow coma182511291014508 TLDR Mixed Active Content is blocked by default in Firefox 23"
54944589,54933044,"stackoverflow.com",1,"2019-03-01 14:22:07+02","2024-05-17 04:30:57.984325+03","Since you said that you are going to have multiple instances I am guessing that that means one clientone lambda You can use Zappas feature to deploy to several environments with one main zappa config that you are going to extend In each deplyment environment you can set specific django settings file to be used "
54933461,54933044,"stackoverflow.com",0,"2019-02-28 22:23:04+02","2024-05-17 04:30:57.985325+03","You can convert settings py to settings folder and create as many domains setting you would like to add I would create base py for common setting and then each domain which has a specific setting should import base py and additional domain specific settings Directory structure would be settings When you load domain_1 then try to load using python manage py runserver settingssettings domain_1 py and request meta[HTTP_HOST] can give you domain name "
55031350,55030817,"stackoverflow.com",1,"2019-03-06 22:11:57+02","2024-05-17 04:30:59.880324+03","I do not know off the top of my head how long APNs certificates are but if they are less than 4 KB then you could add it as a Lambda environment variable and read it into a temporary file on startup in settings py If they are larger than 4 KB you could store the certificate in S3 and then download it to a temporary file on startup Disclaimer I have not tested this exact code This could get expensive though because you are fetching the file from S3 every time the Lambda is invoked Zappas keep_warm feature might help with this but I am not completely sure how it works so take that with a grain of salt You will probably also want to disable this unless DEBUG or another indicator that you are developing locally is set so you do not try to fetch your production APNs cert during development "
55048048,55030817,"stackoverflow.com",1,"2019-03-08 11:07:55+02","2024-05-17 04:30:59.882325+03","Certificates need to be handled in useast1 region N Virginia through the AWS console under Certificate Manager service Other regions will not work There you have the option to Import a certificate After you have the certificate there you can use boto3 to read it and use it in your code For code example please see httpsdocs aws amazon comcodesampleslatestcatalogpythonacmget_certificate py html"
55200781,55200708,"stackoverflow.com",3,"2019-03-16 21:41:50+02","2024-05-17 04:31:01.911663+03","As mentioned in Zappa README Please note that Zappa must be installed into your projects virtual environment You should use something like virtualenv to create a virtual environment which makes it easy to switch Python version If you use virtualenv you can try create an environment by Then pip install zappa in this virtual environment "
55200790,55200708,"stackoverflow.com",0,"2019-03-16 21:42:38+02","2024-05-17 04:31:01.913664+03","I do not know about Zappa but if you want use a specific version of python can do and if whant use the command python with a specific version permanently in linux modify the file home[user_name] bashrc and add the next line"
55200797,55200708,"stackoverflow.com",0,"2019-03-16 21:50:01+02","2024-05-17 04:31:01.914664+03","You can use virtualenv to setup an environment using a specific Python version by You can also use an absolute path to the Python executable if it is named the same but located in different folders Then switch to use the environment This environment uses Python 3 6 so install Zappa with pip like normal and you are good to go You can read more about usage of virtualenv here "
55540031,55381811,"stackoverflow.com",0,"2019-04-05 19:44:50+03","2024-05-17 04:31:03.001517+03","The answer ended up being probably that I was using Django 1 9 x when Zappa requires 1 10"
56633559,55433118,"stackoverflow.com",0,"2019-06-17 17:37:31+03","2024-05-17 04:31:03.622978+03","You could use the python pip package from here httpspypi orgprojectzbar Add zbar to your requirements txt Then it will be installed and uploaded in the python virtualenv active your virtualenv in the docker container install the python libraries defined in the requirements txt"
55608449,55474765,"stackoverflow.com",5,"2019-04-10 11:52:43+03","2024-05-17 04:31:04.34919+03","zappa init is just to get you started with the zappa_settings json file you do not need to run this each time to edit it for example however this could be better written as "
55717505,55683640,"stackoverflow.com",1,"2019-08-12 13:36:43+03","2024-05-17 04:31:05.439492+03","The nodes in the service map correspond to segments so in order for you to see faults or errors reflected youll need to add the error flag to the segment Unfortunately the LambdaFunction segment is created by Lambda and cannot be modified so if you handle the exception yourself there isnt a way to set the LambdaFunction segment error flag You can however make the imxmppprod node reflect errors or faults in the service map by mapping the status code returned in the Lambda payload to a segment fault or error in your application code You can do this assuming this is also Python by calling segment xray_recorder current_segment to get the current segment and then calling segment add_error_flag or segment add_fault_flag to set the segment error or fault attribute equal to true "
64814240,55742741,"stackoverflow.com",3,"2020-11-13 03:33:24+02","2024-05-17 04:31:06.559822+03","If you need to keep warm a Django application you are using AWS Lambda functions the wrong way for start trust me personal experience AWS Lambda functions were created to deploy lightweight functions to the world Your Django app is a heavy iest function ever AWS Lambda was designed for apps that have lifetime of fractions of second 25 MB for itself is a huge load to Lambda functions Consider using a lightweight framework as Flask instead Do not overcome Lambda functions They were not meant for that Use AWS ECS instead "
55744063,55742741,"stackoverflow.com",0,"2019-04-18 13:25:34+03","2024-05-17 04:31:06.561829+03","Can you include your Zappa configuration Heres an example of how keep_warm should be used in the context of the settings file with more settings included Good luck "
53012580,45829958,"stackoverflow.com",0,"2018-10-26 19:08:24+03","2024-05-17 04:31:57.876445+03","Remove the dependencies and Recreate the Virtualenv It should work Ref httpsgithub comMiserlouZappaissues1222"
70148688,70125389,"stackoverflow.com",1,"2021-11-29 09:23:34+02","2024-05-17 04:34:59.066685+03","Upgrade to Zappa 0 54 1 fixes this issue as a wait function has been included to collect the correct key params from AWS "
56966279,55742741,"stackoverflow.com",0,"2019-07-10 11:24:45+03","2024-05-17 04:31:06.562822+03","Technically this is not an issue if we can accept the fact that this is one of the AWS lambda function limitation The main issue here is that we are forcing ourselves to use lambda which is obviously not fit to the requirements due to that limitation delay The problem if we use lambda for this case is this Instead of by passing the way lambda normally works and cost you a lot of money I would like to suggest to use an EC2 as your web serer API with auto scaling and load balancing on top of it In this approach the response back from the API will be way faster because your api is awake and waiting for any request more cheaper than you having it on lambda because lambda charged 0 00001667 per GBsecond of compute time your Lambda performs imagine your lambda awake for 10 mins Hope this helps Cheers Monkeys"
58701577,55742741,"stackoverflow.com",0,"2019-11-04 23:49:41+02","2024-05-17 04:31:06.565823+03","The periodic hits is simple techniques yet it would not help with the case of simultaneous requests One lamdba instance can handle only one request at time I am not sure how exactly to do it with zappa Yet the promising solution is using docker checkpoints httpswww imperial ac ukmediaimperialcollegefacultyofengineeringcomputingpublic1819ugprojectsStenbomORefunctionEliminatingServerlessColdStartsThroughContainerReuse pdf Of course ou can double memory It would not double to bill because requests would be processed faster I also a bit fantasized of adding to zappa fancy strategies like running cheep 1M instances by default but detect cold start and redirect to 3M instances or beans but checkpoints seems more promising for oversized frameworks like django "
56018109,56017721,"stackoverflow.com",1,"2019-05-07 11:05:42+03","2024-05-17 04:31:07.617623+03","It looks like the deployment is succeeding but when Zappa checks to see if the code is working the return code is 502 which suggests that the lambda function is failing to run in the lambda environment Taking a look at the logs the critical line is AttributeError module main has no attribute app And this is true if we look at your code at no point do you expose an attribute called app in main py I am not experienced with Tornado but I suspect that if you move the declaration of app out of the main function and into the root scope then the handler should succeed For example"
56023473,56021435,"stackoverflow.com",2,"2019-05-07 16:14:22+03","2024-05-17 04:31:08.611812+03","Zappa is based on WSGI Tornado is not The two are incompatible so you will have to replace one of them with an alternative I am not aware of a simple way to combine Tornado with Lambda so I would suggest using Zappa with Flask In older versions of Tornado you could use WGSIApplication to get partial support for Tornado in a WSGI environment but this is no longer available in Tornado 6 0 "
56074751,56074516,"stackoverflow.com",0,"2019-05-10 12:34:10+03","2024-05-17 04:31:09.642109+03","It is not a DNS problem anymore Could it be because Edge optimized uses CloudFront and it takes some time to deploy the new custom domain to all the edge location How long have you waited after enabling custom domain name "
56074670,56074516,"stackoverflow.com",0,"2019-05-10 12:46:25+03","2024-05-17 04:31:09.64311+03","Base Path Mappings Path Destination [lambda endpoint] [production] It may also be a good idea to host the domain if possible on route53"
56098473,56074516,"stackoverflow.com",0,"2019-05-12 13:49:47+03","2024-05-17 04:31:09.644619+03","Apparently I got it working Moved my Domain nameservers to route53 and in zappa_settings json added route53_enabled true and recertified again using zappa certify production command Had to wait for 40 minutes and it works Although I have no idea why it doesnt work when domain management is with godaddy Lets say route53 is a quick workaround at the moment "
56554141,56552745,"stackoverflow.com",0,"2019-06-12 06:06:39+03","2024-05-17 04:31:11.265507+03","Try adding a line to your boto file If you then get an error telling you When using SigV4 you must specify a host parameter this will help S3 using boto and SigV4 missing host parameter"
47260249,47242351,"stackoverflow.com",0,"2017-11-13 10:53:58+02","2024-05-17 04:31:14.359826+03","I just had to create a hosted zone then update the NS records of the registered domain with the new hosted zones NS records then created new certificate and worked a charm "
60774671,47242351,"stackoverflow.com",0,"2020-03-20 14:58:10+02","2024-05-17 04:31:14.361827+03","Zappa fails with certify sometimes You can enter the apigateway dashboard on AWS and manually delete the entry before trying again "
47518844,47502218,"stackoverflow.com",1,"2017-11-27 21:56:38+02","2024-05-17 04:31:15.022821+03","Just drop the binary in the same directory as your zappa project and zappa will zip it up and ship it to Lambda Since you are compiling your own binary I would strongly recommend using a local docker environment for zappa to more closely match the AWS Lambda environment Also if your binary is big and will make your Lambda package bigger than 50MB then be sure to use the slim_handler option "
47607935,47604237,"stackoverflow.com",2,"2017-12-11 11:29:37+02","2024-05-17 04:31:16.211573+03","Its actually explained in zappas readme Deploying to a Domain With AWS Certificate Manager Amazon provides their own free alternative to Let us Encrypt called AWS Certificate Manager ACM To use this service with Zappa Verify your domain in the AWS Certificate Manager console In the console select the N Virginia useast1 region and request a certificate for your domain or subdomain sub yourdomain tld or request a wildcard domain yourdomain tld Copy the entire ARN of that certificate and place it in the Zappa setting certificate_arn Set your desired domain in the domain setting Call zappa certify to create and associate the API Gateway distribution using that certificate There are also instructions to use your existing certificate etc Dont let it fool you the title of the section makes it sound like its only about certificates but there are detailed instructions regarding using you own domain Edit Im including my own zappa settings for reference Step by step guide for domain bought on namecheap com and served through cloudflare com"
47604828,47604237,"stackoverflow.com",1,"2017-12-02 07:48:21+02","2024-05-17 04:31:16.21365+03","You might not need API Gateway It will depend on site functionality You can host a static website using S3 with a custom domain You can include clientside javascript angularreact etc API Gateway is really for handling http requests and passing it on to defined resources Lambda or others If your site will require backend capability then you have few options 1 Build lambda functions and then use API Gateway REST API to interact with these functions yourhostname com S3 API Gateway aws hostname lambda 2 Use AWS JS SDK to interact with your lambda functions directly yourhostname com S3 AWS SDK lambda 3 Use API Gateway and then forwarding to Lambda or self hosted web server flask node using EC2 instance yourhostname com API Gateway aws hostname lambda yourhostname com self hosted web server Here is a picture that might make it a bit clearer Correct me if I am wrong flask is used for backend development to build microservices To host it you would use either build lambda functions or host it on your own instance httpsandrich blog20170212firststepswithawslambdazappaflaskandpython Let me know if this helps "
56564933,49186886,"stackoverflow.com",1,"2019-06-12 18:04:07+03","2024-05-17 04:31:27.473996+03","I could solve the issue with the following steps Increase memory size of the lambda function in the zappa_settings json dev I had to use a newer version of tqdm Per default it was version 4 19 which had these issues as described here httpsgithub comtqdmtqdmissues466 The described issue is fixed in a newer version Its only to add tqdm to my requirements txt and execute a pip upgrade of the package When I execute zappa deploy dev I get now the following message tqdm 4 32 1 vartaskvelibpython3 6sitepackages Requirement parse tqdm4 19 1 zappa tqdm 4 19 1 was the default version of zappa and tqdm 4 32 1 is the new version containing the fix "
47624563,47624422,"stackoverflow.com",2,"2017-12-04 01:39:01+02","2024-05-17 04:31:17.011698+03","Not exactly What you describe in the aim paragraph between the first and last sentences is something completely unrelated to CDN and should be handled by your application I e to keep track what was already uploaded handle subsequent uploads of the same file differently only generate thumbnail sizes upon first request etc CloudFronts sole responsibilities are to check if a requested resource exists in any optimal edge location serve it if so otherwise check if it exists in origin download and copy to edge if so and serve it otherwise respond with error 900 same user is asking for thumbnail of the image Image is resized and returned Now here you have a problem if I understood correctly and the image is requested from CF and its origin is a static storage like S3 because the requested thumbnail does not exist and S3 obviously will not be able to generate it For that to work the origin must be an application server that is able to process requests from the your CF Not just serve them but execute some logic and possibly generate the missing thumbnail before serving to CF Or you should generate all needed versions on an image when it is uploaded and store them all on S3 "
47662032,47651326,"stackoverflow.com",0,"2017-12-05 22:22:15+02","2024-05-17 04:31:18.175598+03","Zappa does not currently support that model "
47866784,47859420,"stackoverflow.com",1,"2017-12-18 12:46:07+02","2024-05-17 04:31:19.152208+03","Yup There is no way I had the same problem slow home upload and solved it so that I am deploying through a CI system that has a much much faster upload If you are deploying an open source app you can use Circle CIs free tier for open source or for closed source you can take advantage of Gitlabs free CI offering Here is my Circle CI config file for my open source project It took me 23 minutes to just upload zip from my local computer now it takes less than a minute to run all the tests and deploy For me the main advantage is that I do not have to nervously watch shell for the next few minutes and can just continue with my work "
48161283,48156096,"stackoverflow.com",5,"2018-01-09 06:05:45+02","2024-05-17 04:31:20.236647+03","There is an options to nominate the profile name did you try it httpsgithub comMiserlouZappablobb12bc67aac00b1302a7f9b18444a51f21deac46aREADME md"
48167754,48156096,"stackoverflow.com",1,"2018-01-09 14:15:51+02","2024-05-17 04:31:20.238251+03","You can define which profile to use on your own using Zappas setting But in your CI you first have to create your AWS config file and populate it with your profile from environment variables that are set in your CIs web interface In CircleCI same would be done for TravisCI I am doing it like this for my mislavcimpersak profile Complete working CircleCI config file is available in my repo httpsgithub commislavcimpersakxkcdexcusegeneratorblobmaster circleciconfig ymlL58L60 And also complete working TravisCI config file httpsgithub commislavcimpersakxkcdexcusegeneratorblobfeaturetravisci travis ymlL25L29 Also as it says in Zappas docs Removing this setting will use the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables instead So you can remove profile_name default from your zappa_settings json and set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in your CIs web interface Zappa should be able to use those "
49163913,48232283,"stackoverflow.com",7,"2022-03-30 12:54:26+03","2024-05-17 04:31:21.136154+03","You can check the default settings that Zappa applies to all your lambda functions here and you will see that by default timeout_seconds is setup to 30 seconds This will apply over the default Lambda setup in AWS Console because by default this is 3 seconds you can check this limit in AWS Lambda FAQ For your Task you must increasesetup your timeout_seconds in your zappa_settings json yaml file and redeploy this You can put 5 mins 560300 seconds but this increase will be for all your functions defined in your virtualenv deployed with zappa You can check more details exposed in this issue in Zappa repo "
58709530,48232283,"stackoverflow.com",6,"2019-11-07 19:02:18+02","2024-05-17 04:31:21.138433+03","The timeout_seconds parameter in Zappa is misleading That is it does limit the timeout of the Lambda function but the requests are served through CloudFront which has a default timeout of 30 seconds To verify that try lowering the timeout_seconds to 20 it will correctly timeout in 20 seconds However past 30 there is no effect because of CloudFront limitation The default timeout is 30 seconds You can change the value to be from 4 to 60 seconds If you need a timeout value outside that range request a change to the limit In other words there is nothing you can do in either Zappa or Lambda to fix this because the problem lies elsewhere CloudFront I have not tried it myself but you might be able to up the limit by creating the cloudfront distribution in front of Lambda though it seems you are still limited by max 60s unless you request more through AWS support as indicated in the previous link "
48280821,48272415,"stackoverflow.com",3,"2018-01-16 14:02:29+02","2024-05-17 04:31:21.849559+03","Yeah that is not gonna work with Lambda You have to use some sort of 3rd party cache If caching only your GET requests is good enough for you you could use a CDN for that I personally use CloudFlare CDN that caches all GET requests for n minutes And you get a lot of requests for free You just have to define a custom Page Rule to cache everything for a certain URL pattern Of course same thing can be done with CloudFront to stay within AWS ecosystem or probably most other CDNs "
50182586,48276864,"stackoverflow.com",2,"2018-05-04 23:24:16+03","2024-05-17 04:31:22.810058+03","When you are using task decorator with zappa the new lambda instance is created with the same configuration of the father the caller by default the timeout_seconds is 30 in the zappa_settings timeout_seconds 30 Maximum lifespan for the Lambda function default 30 max 300 Take a look at the doc So you probably need to change this value "
48362683,48344595,"stackoverflow.com",3,"2018-01-21 03:02:37+02","2024-05-17 04:31:23.661607+03","Yes this is possible with a bit of configuration in the AWS console In your API Gateway setup you probably have the default resources created with a Zappa deployment and proxy You can now manually create a new resource corresponding to your documentation endpoint e g docs Clicking on Actions Create Resource you can create the new resource After the resource is created click Actions Create Method and choose ANY Point the method and resource to your Lambda function in the dialog By default the new resource should not require the API key Note you might need to redeploy the gateway before the changes take effect Actions Deploy API "
48467366,48464044,"stackoverflow.com",1,"2018-01-26 20:25:25+02","2024-05-17 04:31:24.642475+03","The issue is you seem to be locked out of IAM No amount of permission on LambdaS3API Gateway is going to get you to where you need to go "
49199601,48932980,"stackoverflow.com",2,"2018-03-09 20:01:13+02","2024-05-17 04:31:25.618556+03","I faced a similar problem while trying to install dragnet and spacy in an anaconda environment I believe pip install was using a cached file that was compiled using another gcc version What solved my problem was uninstalling the library and then reinstalling it using the nocachedir flag ex pip install dragnet nocachedir which built the package from scratch by running the setup py for the new environment Also note that you might need to run sudo apt install libxml2dev libxsltdev pythondev for the build to be successful at least for dragnet and spacy but you might need something else "
49084726,49084513,"stackoverflow.com",0,"2018-03-03 15:36:46+02","2024-05-17 04:31:26.630981+03","I can check a variable from httpsdocs aws amazon comlambdalatestdgcurrentsupportedversions htmllambdaenvironmentvariables such as LAMBDA_TASK_ROOT "
46560286,45829958,"stackoverflow.com",1,"2017-10-04 11:26:09+03","2024-05-17 04:31:57.875444+03","You could try troubleshooting with Pythonlambdalocal tool It tries it is best to mimic the real Lambda "
7697433,7697377,"stackoverflow.com",7,"2011-10-08 17:24:57+03","2024-05-17 04:31:28.589699+03","To a certain extent yes Zappa is a very new framework in very active development I wrote an app in it in July and since then the framework has undergone two backwards incompatible updates In an environment like that it is often helpful to be able to understand what the framework is doing behind the scenes Also since coffee script allows you to mix in JavaScript libraries and Zappa itself does that their documentation will provide examples in JavaScript so you have to be able to speak JavaScript fluently On the other hand experience with Node itself is not that essential since Node provides a rather minimal API which is for the most part well abstracted by Express js also a component of Zappa Most of what you need to know about Node can be learned in an afternoon TLDR Not knowing coffee and JS perfectly may be a problem not knowing Node not that much "
7697630,7697377,"stackoverflow.com",3,"2011-10-08 17:59:32+03","2024-05-17 04:31:28.591934+03","If by Ruby developer you mean Rails developer Zappa may be the shortest leap for you to make It provides a lot of magic the way Rails does That is good in some ways it takes less code to execute a conventional web app and bad in others it can be hard to figure out where things are happening e g where is a particular HTTP header being set So is it possible to be a good Rails developer without knowing Rubys core HTTP library or Rack Sure And similarly you can probably be a good Zappa developer without learning much about Nodes HTTP library or ConnectExpress I expect thatll become more true as Zappa matures and its documentation expands "
9328691,8751099,"stackoverflow.com",5,"2020-06-20 12:12:55+03","2024-05-17 04:31:30.580628+03","According to the node js docs server close Stops the server from accepting new connections See net Server close So your server would stop accepting new connections but it will not actually close and emit close event until current connections are closed My guess is that you have clients connected to it perhaps with keepalive requests "
42830856,8751099,"stackoverflow.com",3,"2017-03-16 12:05:54+02","2024-05-17 04:31:30.581628+03","I was searching for the same problem To give you and others searching for this a solution You can close all connections immediately if this is not a problem in your project This solves the closing problem for me completly "
9149680,9124221,"stackoverflow.com",0,"2012-02-05 15:38:08+02","2024-05-17 04:31:31.363865+03","Ok i figured it out thanks to all that had a go the content type had to be set on the response for each css file i e res contentType css"
9496663,9493931,"stackoverflow.com",2,"2012-02-29 11:39:02+02","2024-05-17 04:31:32.203243+03","You need to have the session middleware above your routes Otherwise you will not have session on the request until after your route has executed so to speak Update Also note that like other properties on request in zappa session is copied to this so you can access session directly There is also no sessionId on request but there is a session id you might want the route below instead"
11170168,11166715,"stackoverflow.com",2,"2012-06-24 11:32:41+03","2024-05-17 04:31:33.121859+03","I guess by script function you mean something like At least that is a hackish workaround It could be done with objects as well typically using JSON or similar for serialization But I would not categorize this as a clean coffeescript way so I guess the question still stands Just to elaborate a bit The problem here is that we are executing coffeescript code in two contexts one on the server which generates the html to send to the client and the other on the client itself through in the context of the coffeescript construct The server side knows about locals and similar so for the code that runs on the server it is simple to evaluate the coffeescript based template and replace template variables with values and similar Not so on the client First of all the client knows nothing about locals template variables it only sees whatever those contained when the page got rendered server side And doing simple template expansion inside the coffeescript construct would not work either as coffeescript would not know if you refer to a local variable on the client or a template variable on the server In theory there could be special sequences that signal expand template variable inside coffeescript code as well but then we are just again creating a new template language on top of coffeecup It could be made to look like regular coffeescript of course e g serverLocal SOMEREF where SOMEREF would be replaced with the corresponding value In theory Coffeecup could support a construct that shared all or a selection of locals also as variables to be accessed client side using a hack similar to the script hack I showed above and it would probably be not too different except using JSON or similar for supporting more kinds of data than simple strings "
11829530,11818893,"stackoverflow.com",5,"2012-08-06 16:44:26+03","2024-05-17 04:31:34.085469+03","Came across this myself did not have the time to track down the cause but there is an easy workaround instead of use the more old school connectstyle Hope this helps you get over the hump PS Come to think of it I assume it might have something to do with the way zappa evaluates coffee kc up templates by default In this case it might be trying to apply that logic to jade which breaks "
13934908,12570426,"stackoverflow.com",0,"2012-12-18 16:17:52+02","2024-05-17 04:31:35.15515+03","This is working on my Zappa 0 4 12 server"
15165208,15156827,"stackoverflow.com",1,"2013-03-01 21:18:28+02","2024-05-17 04:31:36.161522+03","Have not tested anything but it looks like you are mixing up a few things here I may be wrong but you should probably either just use require node js style require OR you can use zappa style include but mixing them is probably not a good idea until you really understand what include does The zappa crashcourse you link to shows both defining modules and using them but both places it is done using include Based on what you write I believe you can not mix require and include the way you are trying to do "
15517391,15156827,"stackoverflow.com",1,"2013-03-20 09:09:47+02","2024-05-17 04:31:36.163522+03","try"
15169809,15156827,"stackoverflow.com",0,"2013-03-02 03:56:47+02","2024-05-17 04:31:36.164268+03","Marius is right I was mixing up require and include I can get this to work which is really what I was wanting to do i e call a function in another file with a file called test coffee that looks like this"
19116200,15292889,"stackoverflow.com",0,"2013-10-01 15:29:45+03","2024-05-17 04:31:37.243643+03","You are losing scope when you are calling your function like that foo to keep the scope you have to call foo like that foo apply this or foo call this "
16373223,16372913,"stackoverflow.com",5,"2014-10-17 22:17:32+03","2024-05-17 04:31:38.360973+03","OK I managed to find a way to solve my own question so I thought I would share in case anyone else need this NOTE For CoffeeScript 1 7 require coffeescript needs to be changed to require coffeescriptregister The solution is to instead create a Cakefile as opposed to a Makefile which looks like this And then change the package json to this Finally I had to install Coffeescript into the project using And create a file testtest_helper coffee which contains global declarations for the tests "
19374105,16372913,"stackoverflow.com",4,"2013-10-15 08:50:20+03","2024-05-17 04:31:38.362975+03","I configure the mocha tests directly using npm package json scripts only and then execute the test coffeescript files by running or"
18267710,16372913,"stackoverflow.com",0,"2013-08-16 10:08:14+03","2024-05-17 04:31:38.363975+03","Below is a working Makefile and package json Makefile package json devDependencies only Then do"
21216927,17130292,"stackoverflow.com",0,"2020-06-20 12:12:55+03","2024-05-17 04:31:39.491014+03","In API references Define an inline template Its like you had a file on disk at path inside Express views directory It will have precedence over a template on disk Since Its like you had a file on disk at path inside Express views directory so everything under view is in a separate filemodule which explains why you cannot access all "
20848670,20409449,"stackoverflow.com",0,"2014-01-30 16:08:22+02","2024-05-17 04:31:40.595598+03","IIRC Expresss parameter trick does not match slashes so cannot be used to match a path Just build your own regexp instead o IIRC because I think I had the same problem But use the source Luke right I am guessing the source pun unintended of this limitation is maybe line 300 of node_modulesexpresslibutils js in function exports pathRegexp Or maybe it is line 307 where slashes are explicitly excluded Anyway route parameters assigned here to key are obviously matched by \w I am too lazy to figure what capture and star are used for The API ref doc kind of sucks I guess so maybe you would want to reverse engineer the code HTH"
20791648,20791629,"stackoverflow.com",1,"2013-12-26 23:43:34+02","2024-05-17 04:31:41.81795+03","You will likely want to install CoffeeScript by itself This will give you the coffee command "
28890599,28872290,"stackoverflow.com",0,"2015-03-06 02:51:29+02","2024-05-17 04:31:42.74877+03","zappa middleware serves From httpszappajs github iozappajsdocsreferencerootscope zappa This zappa middleware serves zappasimple js zappafull js zappazappa js zappajquery js and zappasammy js It is automatically added by client and shared if not added before which means that in most cases you dont need to use zappa To minimize page download delay on the client prefer zappafull js which combines Zappa jQuery Sammy js and Socket IO clientside scripts into a single download If your clientside code doesnt require Sammy use zappasimple js which combines Zappa jQuery and Socket IO "
49416824,38548173,"stackoverflow.com",1,"2018-03-21 23:27:50+02","2024-05-17 04:31:43.839382+03","This looks like an old question but I am using Zappa 0 45 1 and I can see my python stack trace when an error occurs through both the CloudWatch AWS terminal and by using zappa tail which provides the CloudWatch logs and some added information "
39274513,39008749,"stackoverflow.com",1,"2016-09-01 17:39:36+03","2024-05-17 04:31:44.893576+03","You definitely need to install your external libraries locally first and then Zappa will take care of uploading them as part of the zip file In other words virtualenv my_venv source my_venvbinactivate pip install zappa flask django etc then run a local webserver to test that your app works e g flask run zappa init zappa deploy dev OR zappa update dev if you have already run deploy "
42825380,39678267,"stackoverflow.com",0,"2017-03-16 06:35:30+02","2024-05-17 04:31:45.952747+03","When I get errors related to werkzeug wrapper it is usually because my packages were not installed in my virtual environment Then run the zappa deploy commands "
41230626,41228986,"stackoverflow.com",2,"2016-12-19 22:38:43+02","2024-05-17 04:31:47.976364+03","I do not know much about zappa but the error message clearly states your problem It seems to me that you have not set the parameter restApiId which should contain a value of type basestring I am not sure what that parameter is about but it seems to have to do something with the AWSCLI "
43317521,41228986,"stackoverflow.com",1,"2017-04-10 10:48:48+03","2024-05-17 04:31:47.978365+03","In your aws cli be sure you are utilizing the IAM that you created for the zappa function then follow the aws configure qa for access key secret key and region I would suggest that you follow this very helpful tutotial that takes it step by step httpsdeveloper amazon comblogspost8e8ad73a99e94c0fa7b360f92287b0bfnewalexatutorialdeployflaskaskskillstoawslambdawithzappa I really hope this is helpful "
41639499,41635482,"stackoverflow.com",4,"2017-01-13 18:44:29+02","2024-05-17 04:31:49.066218+03","You can do it using SNS Subscribe lambda to topic and publish messages there with json payload "
45722169,41635482,"stackoverflow.com",0,"2017-08-16 23:15:04+03","2024-05-17 04:31:49.067218+03","I made a db driven task queue for zappa httpsgithub comandytwoodszappacalllater Early days but we are using it in production Every X minutes a Zappa event pings a function that checks for tasks Tasks can be delayed repeated etc "
41903433,41885825,"stackoverflow.com",0,"2017-01-27 23:39:14+02","2024-05-17 04:31:50.031811+03","The problem was VPC access I had to provide the role the VPC access policy and it worked "
42825306,42384060,"stackoverflow.com",1,"2017-03-16 06:27:58+02","2024-05-17 04:31:51.027676+03","When I get errors related to werkzeug wrapper it is usually because my packages were not installed in my virtual environment Then run the zappa deploy commands "
45982311,42384060,"stackoverflow.com",1,"2017-08-31 16:13:03+03","2024-05-17 04:31:51.028677+03","You are not looking at the real error Do as it says You can investigate this with the zappa tail command I would run zappa tail since 1m in one window and zappa update in another you will see the real exception there "
44209668,44208455,"stackoverflow.com",5,"2017-05-27 00:07:45+03","2024-05-17 04:31:52.026831+03","This is not something Zappa directly supports at this time You will need to perform a hack of some sort around the available scheduling system Schedule an event to run every minute Your code can be along these lines "
45722148,44208455,"stackoverflow.com",2,"2017-08-16 23:20:51+03","2024-05-17 04:31:52.028118+03","I made a db driven task queue for zappa httpsgithub comandytwoodszappacalllater Early days but we are using it in production Every X minutes as suggested in Oluwafemi Sules answer a Zappa event pings a function that checks for tasks Tasks can be delayed Y minutes repeated Z times etc My solution is crude in that it has low time resolution and is quite low level currently "
45196845,44988128,"stackoverflow.com",0,"2017-07-19 20:05:34+03","2024-05-17 04:31:53.274683+03","I was only able to correct this my installing everything from scratch according the instructions at httpsdeveloper amazon comblogspost8e8ad73a99e94c0fa7b360f92287b0bfnewalexatutorialdeployflaskaskskillstoawslambdawithzappa Something in the venv directory just got messed up "
53523848,44999319,"stackoverflow.com",2,"2018-11-28 18:19:16+02","2024-05-17 04:31:54.179153+03","httpsgithub comMiserlouZappaissues795 contains a list of things to try that has worked for different folks in the community One common issue is related to using slim_handler but according to your zappa_config js that does not apply to your case Another common issue has to do with the pyc files getting bundled into the zip package Try deleting pyc files in your local project folder before calling zappa update and see if that addresses the issue for you "
46947684,45106563,"stackoverflow.com",7,"2017-10-26 09:57:06+03","2024-05-17 04:31:55.266289+03","Below is the simple app I built And that gave me traceback The traceback pointed me to httpsgithub comMiserlouZappablobf7a785351450fa9115864c7dfe223660f5a99ae6zappahandler pyL454 Just before that it has These are filled in WSGI app environ for the request This is accessible as flask request environ get So then I changed the code to And the response was as expected PS Detailed the solution so you understand how to find solution to such problems"
46560222,45340414,"stackoverflow.com",1,"2017-10-04 11:22:54+03","2024-05-17 04:31:56.170307+03","You cannot Zappa first uploads your zip file to a bucket and from there does the deployment From the official repo Zappa will automatically package up your application [ ] upload the archive to S3 create and manage the necessary Amazon IAM policies and roles register it as a new Lambda function create a new API Gateway resource create WSGIcompatible routes for it link it to the new Lambda function and finally delete the archive from your S3 bucket Handy So your option is to dig into Zappa and circumvent this on your own or perhaps try Chalice that does the upload directly "
47231144,45340414,"stackoverflow.com",0,"2017-11-10 23:07:57+02","2024-05-17 04:31:56.172308+03","mislav is correct that Zappa does need an S3 bucket But one only gets autocreated if you do not specify one Simply provide a valid bucket name at the prompt and Zappa will use that bucket instead of creating one for you "
45810867,45805036,"stackoverflow.com",3,"2017-08-22 09:59:03+03","2024-05-17 04:31:56.757644+03","There is nothing wrong as long as you can import it in python and utilize it is functionality I am using ROS Kinetic for my research which comes with a built in release of opencv It also has just one cv2 so file and it is working perfectly fine "
49183765,45906285,"stackoverflow.com",6,"2018-03-09 00:55:50+02","2024-05-17 04:31:58.794248+03","After struggling with this myself I realized that the idea is to package up your other code install it in your virtual environment and have app py just be a driver that calls your other functions Heres a concrete minimum example using Flask First let us extend your example with one more file setup py __init__ py is empty The rest of the files are as follows Now you pip install e while in your virtual environment If you now run app py using Flask and go http127 0 0 15000 you will see that you get This is the index content And if you deploy using Zappa you will see that your API endpoint does the same thing "
60464286,46164054,"stackoverflow.com",3,"2020-02-29 13:00:48+02","2024-05-17 04:32:00.641043+03","I faced the same issue Fixed it by setting the VIRTUAL_ENV environment variable and it worked Assuming you have a conda environment created as follows Before zappa deploy ensure you do the following "
50898677,46164054,"stackoverflow.com",2,"2018-06-17 19:53:48+03","2024-05-17 04:32:00.642043+03","You can install it using conda"
53196283,46164054,"stackoverflow.com",0,"2018-11-07 21:15:38+02","2024-05-17 04:32:00.643043+03","Is this in the context of serving machine learning models on AWS Lambda If so I wrote a library called Thampi which uploads your model and your conda environment to AWS Lambda and abstracts away the DevOps part of model serving The caveat is that your conda requirements file has to be manually written "
46590047,46554705,"stackoverflow.com",2,"2017-10-06 08:14:07+03","2024-05-17 04:32:02.551556+03","You can keep your file structure with zappa and it will work perfectly fine Given the file structure You can simplify your Dockerfile to be And it seems from your update that you have gotten similar results because your Django application seems to reach the Django code Presumably you have also edited your ALLOWED_HOSTS to accept incoming connections Now the error ModuleNotFoundError No module named root is because your Django settings file is not expecting to exist within the subdirectory You must update the settings py to update the variable This should get you up and running Side Effects of this docker approach As an aside the way you are setting up your docker container has some implications on coding workflow Your docker image will contain a copy of your code at the time it was built So any edits on the code in a running docker container will be lost when the container ends unless exported somehow e g git While this could be perfectly fine for stable code in a CICD pipeline I would not recommend it for active development If you would like to use it for active development then when invoking docker have your project directly mapped as a volume when the container is invoked For example docker run ti v pwd vartask rm name of your image"
68383492,46554705,"stackoverflow.com",0,"2021-07-14 21:36:03+03","2024-05-17 04:32:02.554557+03","Just move zappa_settings json and the json generated by zappa deploy to your subfolder Then update zappa_settings json django_settings project root settings django_settings root settings cd into project folder and do a zappa update dev Not the best solution but it worked for me"
47122179,47060661,"stackoverflow.com",1,"2017-11-09 16:57:07+02","2024-05-17 04:32:04.445628+03","This API Gateway problem which was confirmed by AWS people has existed for months Luckily they have recently June 2017 published a way to fix it using Gateway Responses In your API Gateway console go to your API and then Gateway Responses Look for Unauthorized 401 and add the following headers you can use your domains of course See image below"
47127965,47060661,"stackoverflow.com",1,"2017-11-10 23:23:30+02","2024-05-17 04:32:04.447629+03","Getting a 401 from your OPTIONS method in API Gateway is very unusual I see 403 s and missing CORS headers on OPTIONS calls quite a bit but generally not 401 If there were a problem with your authorizer I would expect the 401 on the ensuing POST not on the OPTIONS call You also mentioned that you had been changing authorizer code in an effort to handle this Without seeing your API Gateway configuration I cannot say for sure but these two bullet points suggest a possibility It sounds like your custom authorizer may be hooked up to your OPTIONS method in addition to POST or whatever you are trying to expose This should not be the case For example if you attach a custom token type authorizer to the OPTIONS method of an API Gateway resource and then make an OPTIONS call without an Authorization header you will get a 401 Your custom authorizer should only be attached to the methods you are explicitly exposing In many cases this is just POST but could include others like PUT DELETE and so on If this is not helpful you might update the question with your API Gateway configuration and the requestresponse headers from the failing OPTIONS call UPDATE I deployed a HelloWorld flask app using Zappa and I think I was able to reproduce your issue I am using the blueprint you linked to for the custom authorizer Changing policy denyAllMethods to policy allowAllMethods was the only change I made to it When I deploy something like this is created I was able to get 401 from OPTIONS calls that did not contain an Authorization header I added cors true to my zappa_settings which created something much more CORS friendly This configuration looks better No more 401 from OPTIONS whether an Authorization header is present or not My zappa_settings with cors true added looks like this"
47129235,47108354,"stackoverflow.com",0,"2017-11-06 05:17:03+02","2024-05-17 04:32:05.593047+03","You can create your deployment package by using virtual env Refer this AWS documentation for more details httpdocs aws amazon comlambdalatestdglambdapythonhowtocreatedeploymentpackage html"
56599610,47146586,"stackoverflow.com",0,"2019-06-14 16:52:28+03","2024-05-17 04:32:06.571567+03","I solved this issue by upgrading to python 3 7 I would recommend starting a new virtualenv configured to use python 3 7 if you do not have python3 7 on your system you will need to install it This site is the one I used Works on AWS cloud9 too installing python 3 7 virtualenv env p python3 7 source envbinactivate python version output should be Python 3 7 X then continue setting up your app like normal "
47189873,47179907,"stackoverflow.com",1,"2017-11-08 23:19:19+02","2024-05-17 04:32:07.692462+03","You need to run certify to make your zappa exposed via custom domains httpsgithub comMiserlouZappasslcertification and httpsedgarroman github iozappadjangoguidewalk_domain"
49373701,49370016,"stackoverflow.com",4,"2018-03-20 00:53:35+02","2024-05-17 04:32:08.516664+03","Seems like a bug 5079 in pip Please verify with pip version that you have pip version 9 0 2 then downgrade to 9 0 1"
49395069,49370016,"stackoverflow.com",3,"2018-03-21 00:20:05+02","2024-05-17 04:32:08.518665+03","I had the same issue But upgrading to pip 9 0 1 worked for me Now I am facing another issue After deploying with zappa I always get an HTTP 500 error by testing the alexa skill I used zappa tail and figured out following error"
62357855,49563491,"stackoverflow.com",1,"2020-06-13 12:30:35+03","2024-05-17 04:32:09.587147+03","This error can be caused by this SQLite problem httpsgithub comMiserlouZappaissues1880 Please check zappa logs during deployment with zappa tail ENV_NAME"
53510248,49563491,"stackoverflow.com",0,"2018-11-28 02:20:31+02","2024-05-17 04:32:09.58815+03","httpsgithub comMiserlouZappaissues795 contains a list of things to try that has worked for different folks in the community Some common ones are"
50022932,49941407,"stackoverflow.com",1,"2018-04-25 15:54:00+03","2024-05-17 04:32:12.102612+03","Resolved the issue the apigateway was not creating the log groups because it did not have sufficient permission I added the zappa role I created in AWS API Gateway settings CloudWatch log role ARN "
77981655,77980581,"stackoverflow.com",0,"2024-02-12 15:04:38+02","2024-05-17 04:47:38.394992+03","I have created a NestJs Azure function with below commands Local Response Run npm run build before deploying the application to Azure Portal References Deploy NestJS Serverless Apps to Azure Functions Trilon Consulting"
49955820,49941407,"stackoverflow.com",0,"2018-04-21 15:34:45+03","2024-05-17 04:32:12.103613+03","You are managing your own roles Almost certainly you did not give Zappa permission to do something it needs to do Apparently one thing it does not have permission to do is create the log group it wants to which follows a naming convention based IIRC on the directory name and the Zappa profile name production If you remove manage_roles and role_name you will get a user that has all the permissions that Zappa needs and then some You can then start removing them until you have a minimal set of requirements After some trial and error I found that the following was the good minimum set for a basic application Lambda also needs to be able to assume this profile in order to run So you must also edit your trust relationships If you have to talk to a database server be sure to assign a VPC and security group as well "
50317622,49943148,"stackoverflow.com",0,"2018-05-13 18:18:52+03","2024-05-17 04:32:13.200718+03","As correctly suggested by MattHealy AWS Lambda runs in a VPC and by default lambda do not give internet access When you enable VPC your Lambda function will lose default internet access If you require external internet access for your function ensure that your security group allows outbound connections and that your VPC has a NAT gateway To solve this create few public subnets at least 2 which has assigned to a IGW Internet Gateway and create few private subnets at least 2 which has assigned to a NAT gateway and modify the route table accordingly for IGW and NAT to complete the connectivity Now assign the private subnets to the Lambda and the respective security groups and your lambda will have the internet connection "
50948763,50937959,"stackoverflow.com",14,"2018-06-20 15:54:13+03","2024-05-17 04:32:17.103336+03","try installing all the dependencies using pip in the virtual environment where you are using zappa It worked in my case You can also use zappa tail command to see your logs "
52649862,50937959,"stackoverflow.com",2,"2018-10-04 17:59:17+03","2024-05-17 04:32:17.104336+03","This github issue seems to have the same symptoms Downgrading to zappa0 45 1 solved it for me"
55843615,50937959,"stackoverflow.com",2,"2019-04-25 10:05:48+03","2024-05-17 04:32:17.105293+03","If you are using anaconda than create a new virtual environment virtualenv lambda in your project directory and in Scriptsactivate Than deactivate the conda environment with conda deactivate and pip install all the packages pip install numpy pandas sklearn zappa flask PS using slim_handletrue also gives this error so do not use it "
60014463,50937959,"stackoverflow.com",2,"2020-02-06 07:10:36+02","2024-05-17 04:32:17.106546+03","If all of above does not work you can solve it by this way Then You must provide app_function parameter in the zappa_settings json which should point to your entry function The app_function should be provided like this if application is Flask __init__ application so the flask app should be defined as application as follows app py should be __init__ py You have to add __init__ py in order to recognize your project folder as a package So zappa_settings json have parameter like this app_function __init__ application Deploy and enjoy "
53420080,50937959,"stackoverflow.com",1,"2018-11-21 22:33:31+02","2024-05-17 04:32:17.108543+03","I was facing this error when I gave the modular path to my application as main py I fixed it by creating an empty file called main app just next to main py and setting app_function to main app in zappa_settings json Absolutely no clue what happened underneath but it worked for me "
51340448,50937959,"stackoverflow.com",0,"2018-07-14 17:32:10+03","2024-05-17 04:32:17.109543+03","I had same problem After spending couple of hours from cloudwatch logs I noticed the error of sec certificate Solved it by running pip install cryptography2 2"
54204032,50937959,"stackoverflow.com",0,"2019-01-15 19:30:27+02","2024-05-17 04:32:17.110544+03","I faced the same error and same what happened with ScottieB above the reason was that I forgot to do the pip install for one package that my app was using After I did the pip install locally in the project environment then did the zappa update dev the error gone and update been completed "
60081355,50937959,"stackoverflow.com",0,"2020-02-05 19:45:08+02","2024-05-17 04:32:17.111544+03","I had this same error and after many online searches and trying many many suggestions it was actually just a small problem with a code indentation No problem with Zappa config or pip installations at all I notice that in your code sample you have not indented your code at all I do not know if this is how it copy pasted into StackOverflow or if this is how you unintentionally tried to deploy it It should be"
61532078,50937959,"stackoverflow.com",0,"2020-04-30 23:22:31+03","2024-05-17 04:32:17.113545+03","I was getting same error check you have installed zappa in your venv I have installed globally and run in local venv When i installed zappa it works perfect "
61994223,50937959,"stackoverflow.com",0,"2020-05-25 11:15:23+03","2024-05-17 04:32:17.114544+03","I was facing the same issue and I figured out that zappa is installed globally on my system not in the virtual env that I am using try installing zappa in virtual env"
68236387,50937959,"stackoverflow.com",0,"2021-07-03 15:46:31+03","2024-05-17 04:32:17.115573+03","I was running a flask app and forgot to include following statement in my app if __name__ __main__ app run after adding this line it started working fine "
51056293,51020809,"stackoverflow.com",2,"2018-06-27 13:30:57+03","2024-05-17 04:32:17.92948+03","Solved myself Zappa version was the problem I downgraded zappa pip and then it works "
73248826,51064261,"stackoverflow.com",3,"2022-08-05 14:10:01+03","2024-05-17 04:32:18.863078+03","Im not sure if this is exactly what you are looking for but this response httpsstackoverflow coma62119981 was a godsend for me while I was trying to invoke a function that was within zipped API deployed via Zappa on AWS Lambda Relevant fragment in source code httpsgithub comzappaZappablobfff5ed8ad2ee071896a94133909adf220a8e47d9zappahandler pyL380 TLDR use command keyword in payload of what is send to lambda_handler to invoke any function in your API so if you would like to invoke your lambda_handler function which is part of an zipped API deployed on Lambda you can do it via boto3 And in response apart from some Metadata you should receive desired This lambda handler triggers other functions output It is also possible to pass some data into handler BUT im not sure if there is any recommended keyword so you can use any beside those which are reserved Im using data So we will change your lambda_handler function a little bit The invokation has to change too And the response from the invokation should look like this now Hope this helps "
70941480,51064261,"stackoverflow.com",1,"2022-02-05 23:26:36+02","2024-05-17 04:32:18.86608+03","Try"
66404295,51165105,"stackoverflow.com",0,"2021-02-28 01:05:28+02","2024-05-17 04:32:19.562571+03","The issue turned out to be the initial TLS handshake is slow and nothing to do with Zappa or Lambda I was hosting in the US even though I live in Australia because it was a test system and its cheaper to host in the US Upon moving to Australia the speed improved dramatically "
51480233,51473971,"stackoverflow.com",0,"2018-07-23 16:39:39+03","2024-05-17 04:32:20.181638+03","Apparently this is the right format of the application py "
54601680,54600199,"stackoverflow.com",0,"2019-02-09 01:37:29+02","2024-05-17 04:32:36.113096+03","Invoke zappa to keep packaged zip after deploy and check folder structure or try to run the project from that folder Did you add Django to requirements file As i recall Zappa rebuilds lambda package from a fresh virtual environment installs requirements txt and applies your folder Or check carefully cloudwatch logs for import errors There are import errors that show up only when the container is refreshed after a deploy that will not show up for every lambda invocations "
54721499,54721189,"stackoverflow.com",2,"2019-02-16 11:04:10+02","2024-05-17 04:32:37.79296+03","You need to either grant public access to the file or folder in your s3 settings fastest way on Amazon or set up security policies if you need more strict access Maybe this link could help httpswww codingforentrepreneurs comblogs3staticmediafilesfordjango"
51940550,51912630,"stackoverflow.com",2,"2018-09-19 22:25:06+03","2024-05-17 04:32:21.493392+03","First nice job on getting this set up it is quite a challenge I agree with you that you are almost there Since you can connect with PSequel from your local system that validates that your machine is accurately connected to the VPC RDS from a network perspective Next area to look at is the Django setup If the local machine Django settings are incorrect this would cause the error So your database section in your settings file should be different on the local machine As you describe in one of your comments above I believe you have HOST xxxxx useast2 rds amazonaws com When you run python manage py makemigrations django attempts to use that host name and connect to it Unfortunately this bypasses your carefully constructed ssh tunnel To fix this you can either Should be easy enough to try 1 above and see if that works "
52286949,52081623,"stackoverflow.com",4,"2018-09-12 06:10:09+03","2024-05-17 04:32:22.287666+03","Judging from the traceback it seems you are not using virtual envs in your Zappa deployment Zappa needs a virtual environment before you can deploy any AWS Lambda package Read more about how to create virtual environments here httpsvirtualenv pypa ioenstable So basically on your project folder you will have to run"
56645905,52545827,"stackoverflow.com",3,"2019-06-18 12:39:05+03","2024-05-17 04:32:22.877428+03","First you can use slim_handler that allows to upload larger files than 50M Second as bddb already mentioned you can eclude some files such as pyc zip etc with the exclude property Please find more details here httpsgithub comMiserlouZappapackage Here is an example how your zappa_settings json could look like"
53471701,53470739,"stackoverflow.com",2,"2018-11-25 22:36:12+02","2024-05-17 04:32:24.992822+03","If you need permanent storage then yes some flavor of RDS or Dynamodb or any other datastore you have access to would be more appropriate Lambda by itself cannot persist data between executions "
53750646,53736846,"stackoverflow.com",1,"2018-12-12 22:11:24+02","2024-05-17 04:32:26.065993+03","Most session capabilities inherent to Django will work fine with a zappa deployed project Ensure the needed infrastructure is available as per the documentation For example if you choose databasebacked session then your Django deployment must have access to a database "
53739246,53738730,"stackoverflow.com",3,"2018-12-12 10:54:00+02","2024-05-17 04:32:27.09798+03","As you probably know lambdas are stateless You can use some database i e Dynamo DB to store and retrieve session related information i e connection token Otherwise you can integrate with AWS Cognito service which handles authentication and session information for you "
53750579,53738730,"stackoverflow.com",2,"2018-12-12 22:05:32+02","2024-05-17 04:32:27.099981+03","Most session capabilities inherent to Django will work fine with a zappa deployed Django project as long as the appropriate infrastructure is available For example if you choose databasebacked session then your Django deployment must have access to a database But you must avoid Filebased sessions since you cannot guarantee the file will exist for any given user request the lambda container to which requests are routed are not deterministic For more information on Django sessions see the documentation"
53797120,53774428,"stackoverflow.com",0,"2018-12-15 22:45:20+02","2024-05-17 04:32:28.031851+03","Default zappa configuration is a timeout after 30 seconds See documentation for zappa_settings json This is the default because API Gateway timeout is 30 seconds httpsseancoates comblogsapigatewaytimeoutworkaround "
62119981,53856069,"stackoverflow.com",1,"2020-05-31 23:02:23+03","2024-05-17 04:32:29.04597+03","I had a look into the source code of the Zappa CLI to figure out how the zappa invoke command works internally because that is exactly what we want Remotely and programmatically invoke a specific function within your Zappa lambda Keep in mind that your function has to accept the lambda event and context to be a valid entry point For more information on how the zappa invoke command works if command invoke command key function_name "
53979181,53979180,"stackoverflow.com",2,"2018-12-30 18:06:10+02","2024-05-17 04:32:29.574963+03","had to remove manage_roles false from my zappa_settings json"
56633722,54013486,"stackoverflow.com",0,"2019-06-17 17:47:17+03","2024-05-17 04:32:30.677401+03","It cannot call the object Klein You need to add it to the virtualenv in order to import it as only the virtualenv is build and deployed with zappa For example you could create a setup py script which allows to install your custom code with pip to your virtual environment as described here httpsdocs python org3distutilssetupscript html"
54213472,54212375,"stackoverflow.com",2,"2019-01-16 11:01:29+02","2024-05-17 04:32:31.56063+03","Aside from digging deeper as to how AWS Lambda works checkout AWS CodeBuild and AWS CodePipelilne You can trigger build whenever you push changes to your github repository depending on your setup of source in your pipeline step Link your github repository on the step The CodeBuild part serves as your environment that builds the project I am assuming zappa is only one of the tools you are using to deploy you can add that as part of the stagesphases of buildspec required to build the application or maybe just download the requirements of your application andor uploads them to s3 which then be the source code of which your Lambda function will refer to "
54310569,54307715,"stackoverflow.com",1,"2019-01-22 16:35:04+02","2024-05-17 04:32:32.559273+03","I am assuming you have bundled the two required libraries to your Lambda deployment package In the Lambda container that gets extracted inside the vartask directory That directory is already in the LD_LIBRARY_PATH Try setting the other necessary ENVVARS to vartask as well "
54340617,54307715,"stackoverflow.com",1,"2019-01-24 08:33:37+02","2024-05-17 04:32:32.561274+03","Ok I think I almost got it This is what I did Step 1 Step 2 then type zappa init then you will see it automatically creates a file called zappa_settings json Add to your zappa_settings json project_directory tmpcode Copy this as is no if this is the last statement slim_handler true Use this if it gives you a error saying Your file is too big Which I am sure it will as the lib file size is 107 1 MB Also no since this was my last statement in my zappa_settings json no quotes for true Step 3 made a directory called lib in my root directory and copied the files to it Copy these files See images below httpsi stack imgur comyOLsK jpg Step4 In your AWS lambda console Remember do not replace code keep it as is httpsi stack imgur comGxkki jpg Step5 add these to your Django settings py Do not replace code with your path keep it as is Step 6 Finally zappa deploy dev or zappa deploy prod whatever stage you want Step 7 If it gives you errors do zappa tail it will give you all logs and tell you what the error is fix them and do zappa update This was successful Thank you bahoo so much for your help and taking the time to dumb it down for me Also Thank you so much for making geodjango work on zappa It gave me a error saying bad request told me to add a long amazon link to my allowed host Did that Now the next error was to add my data_base I am doing that But I feel I got it For more details refer to httpsgithub comMiserlouZappaissues985"
56648207,54387861,"stackoverflow.com",3,"2019-06-18 14:34:04+03","2024-05-17 04:32:34.626909+03","You only have one stage defined which is called production If you want to name your stage create_pg_db then it would like this At the end you can add more stages and then use the stage for your deployment For example you can have a develop stage that deploys your code to a development environment and a production stage that deploys your code to the production environment "
63292116,54587976,"stackoverflow.com",1,"2020-08-07 06:17:04+03","2024-05-17 04:32:35.530253+03","The README says To capture responses you must configure a async_response_table in zappa_settings This needs to be done "
63745850,63744500,"stackoverflow.com",0,"2020-09-04 20:46:54+03","2024-05-17 04:53:47.830003+03","Have you added applicationjson to the media types of API Gateway Or is that the default I know I had to add like applicationoctetstream "
54743532,54736701,"stackoverflow.com",3,"2019-02-18 10:56:41+02","2024-05-17 04:32:38.897977+03","10 MB limit is not a Zappa limit but an API gateway limit And it being a limit set by AWS there is no way to increase it You could go down a rather complex route of uploading large payloads through API gateway to S3 It is explained in detail in this article httpssookocheff compostapiuploadinglargepayloadsthroughapigateway Or you could do some frontend magic and make several smaller requests one for each photo you are uploading "
56720095,56569076,"stackoverflow.com",0,"2019-06-23 03:17:52+03","2024-05-17 04:32:39.690006+03","The gotcha ended up being that my Atlas Tier did not support Peering Connections Here are two other solutions both of which send traffic across the internet instead of within AWS 1 Whitelist all IPs add 0 0 0 00 in MongoDB Atlas 2 On your VPC Dashboard Attach an Internet Gateway to your VPC Create a new EIP Create a Public Subnet and a Private Subnet By this I mean include public and private in the name We will actually make them public and private when we associate Routing Tables later Create a NAT Gateway and connect it to the EIP and the Public Subnet Create a Private Routing Table which routes all traffic to the NAT Gateway keep the existing route from your VPC CIDR to local Create a Public Routing Table which routes all traffic to the Internet Gateway keep the existing route from your VPC CIDR to local Associate your Public Subnet to the Public Routing Table and your Private Subnet to the Private Routing Table Create a Network ACL and associate with any Subnets if needed On your Lamda Function Dashboard Associate your VPC Associate your Private Subnet Associate any Security Groups if needed On your MongoDB Atlas Dashboard Whitelist your EIP in MongoDB Atlas Note Amazon charges you for the NAT Gateway That is the only nonfree part of this if you are on the free tier "
56573530,56571314,"stackoverflow.com",0,"2019-06-13 07:49:07+03","2024-05-17 04:32:40.637017+03","Go to VPC Dashboard and use the Launch VPC Dashboard Create Allocate an elastic IP address and keep it handy Select the option VPC with Public and Private Subnets Configure the subnet CIDRs and associate the elastic IP created in the previous step in the form This will create a NAT Gateway and set it up automatically with correct route tables Things to note"
56618082,56617343,"stackoverflow.com",2,"2019-06-16 13:44:26+03","2024-05-17 04:32:41.305497+03","If the script runs on local and is not working on lambda the likely cause is that your boto3 library versions are different on local and lambda AWS does not update their boto3 often and I ran into a similar issue with cognito object lock To fix this you can download export the lambda function to your local machine Next get the boto3 version you have running locally by using this where lib is name of the directory where these files will be installed Next copy all the files inside the lib folder do not copy the folder iteself and paste them inside the zip that you downloaded exported DO NOT extract the zip and repackage it just open it in winzip or winrar and paste the filesfolders inside the lib folder to your lambda function zip Next you go to your lambda in console and upload the zip again It will replace your lambda created by zappa and will not change your api path You will also have the correct boto3 version installed "
56640571,56640319,"stackoverflow.com",1,"2019-06-18 04:11:47+03","2024-05-17 04:32:42.159809+03","Zappa excludes boto3 by default because its included in the lambda environment A fudge that worked for me previously was add a pattern that will not match anything to the exclude field in zappa_settings json this then caused zappa to include my bundled boto3 "
56799508,56775625,"stackoverflow.com",0,"2019-06-28 02:39:46+03","2024-05-17 04:32:42.814671+03","This worked for me now using python2 httpswww github comDoergeawslambdapycrypto I just downloaded this project and zipped my lambda_function py file with both the Crypto and pycrypto2 6 1 distinfo folders I see that the compiled so files in CryptoCipher like _AES so lack the python version and OS architecture and distribution that mine had i e AES cpython27mx86_64linuxgnu so I will update my answer if I find a way to build the package properly by myself rather than using a third person compiled library "
56847880,56796521,"stackoverflow.com",0,"2019-07-02 10:59:59+03","2024-05-17 04:32:43.65658+03","For me the following two things fixed that issue To activate this feature you must add a new line to your zappa_settings json"
59536598,56796521,"stackoverflow.com",0,"2019-12-30 22:38:35+02","2024-05-17 04:32:43.65758+03","Heres how I solved the issue there are two ways"
56806781,56804090,"stackoverflow.com",25,"2019-06-28 15:16:30+03","2024-05-17 04:32:44.725369+03","I managed to solve the problem by installing the psycopg2binary package After a redeploy it works fine "
75828367,56804090,"stackoverflow.com",1,"2023-03-24 00:04:01+02","2024-05-17 04:32:44.726371+03","I encountered the same problem when runing django in a raspberrypi model 3 B with the latest os worked for me"
57065782,56985897,"stackoverflow.com",1,"2019-07-17 00:38:55+03","2024-05-17 04:32:45.79543+03","The problem is that awscli and awsebcli need pyyaml3 13 whereas cfnflip through zappa through zappa needs pyyaml3 13b1 pipenv install is also skipping prereleases pyyaml3 131b being one of them You could explicitly require a version of troposphere older than 2 4 7 such that pyyaml3 13 is installed to solve this problem "
61781431,57228938,"stackoverflow.com",6,"2020-05-13 20:48:59+03","2024-05-17 04:32:48.760139+03","For people who are facing similar issue I fixed it by install zappa in the venv itself So follow the steps assuming you have venv in env folder locally "
57990485,57989286,"stackoverflow.com",0,"2019-09-18 13:26:34+03","2024-05-17 04:32:52.383566+03",""
63033766,58013175,"stackoverflow.com",0,"2020-07-23 05:59:02+03","2024-05-17 04:32:53.063572+03","The decorator should be like this "
58340171,58140517,"stackoverflow.com",1,"2019-10-11 14:19:33+03","2024-05-17 04:32:55.217801+03","I had to do install zappa virtualenv using pip3 and then activate it "
58150152,58140517,"stackoverflow.com",0,"2019-09-28 23:43:17+03","2024-05-17 04:32:55.218801+03","This is a short working example that uses python3 virtual env not virtualenv package For it to work you need to set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY as env vars in the CircleCI web UI Note that I am using python 3 6 1 CircleCI image I had some minor trouble with 3 6 9 Full example with dependencies caching running test suite testing if the package can be created deployment to testing and production environment can be found here httpsgithub commislavcimpersakxkcdexcusegeneratorblobmaster circleciconfig yml"
58211508,58182213,"stackoverflow.com",0,"2019-10-03 05:19:23+03","2024-05-17 04:32:55.654793+03","Zappa does the job already I do not need to do anything "
62920384,58219322,"stackoverflow.com",0,"2020-07-15 20:18:31+03","2024-05-17 04:32:56.463538+03","you can try and just execute main kpi_report auto_scraper locally"
58324088,58322715,"stackoverflow.com",2,"2019-10-10 16:27:11+03","2024-05-17 04:32:57.563435+03","One of the solutions is to install the package in your virtual environment pip install in the commons directory Zappa would use all the packages that are installed in it Of course this is a bit cumbersome because you would need to reinstall the package every time you modify it You can either"
58516273,58467642,"stackoverflow.com",2,"2019-10-23 08:53:14+03","2024-05-17 04:32:58.389584+03","After more investigation on the issue and reading an answer from Amazon httpsforums developer amazon comquestions191460asksdkforpython14possibleissuewhendeployed html I figured out that one of my dependencies installs the package typing which can be found as standard python package and by removing it the problem solved So just do and you should be good to go "
58910457,58776141,"stackoverflow.com",1,"2019-11-18 10:24:18+02","2024-05-17 04:33:00.391658+03","Your error is that there is no space left on the lambda function SpaCy requires quite a lot of space Therefore follow the following suggestions AWS Lambda No Space Left on Device error"
63748706,63744500,"stackoverflow.com",0,"2020-09-05 01:32:27+03","2024-05-17 04:53:47.832003+03","Solved Creating the response as follows works fine Instead of assigning another dictionary to the Headers property"
58910570,58790162,"stackoverflow.com",1,"2019-11-18 10:32:28+02","2024-05-17 04:33:01.461339+03","exclude setting is not working as expected in zappa settings as described here httpsgithub comMiserlouZappaissues692 The current behaviour of exclude is destructive because it applies to all files regardless of depth You can try the exclude without wildcard exclude [boto3 botocore tqdm numpy torch] But be aware that this will also delete files with the same name in subdirectories "
59705277,59693924,"stackoverflow.com",0,"2020-01-12 17:25:45+02","2024-05-17 04:33:01.878572+03","Zappa supports remote config that you can create the configuration file and place it in an S3 bucket to which your Zappa application has access zappa_settings json"
60007656,59916021,"stackoverflow.com",0,"2020-01-31 18:20:45+02","2024-05-17 04:33:02.813165+03","Fixed this issue by removing the package from requirements txt reinstall packages"
73369633,59916021,"stackoverflow.com",0,"2022-08-16 09:28:27+03","2024-05-17 04:33:02.814166+03","I also faced the same issue but in my case it was a silly mistake which haunted me for 23 days It was a problem with my flask library which was not installed properly So I resolved the issue by uninstalling flask and reinstalling it using pip uninstall flask and pip install flask"
60006971,60006823,"stackoverflow.com",2,"2020-01-31 17:37:33+02","2024-05-17 04:33:03.339596+03","Choose the list and find the category API Gateway example httpsxxxx executeapi xxxx amazonaws comproductionlogin Create and then you are done It will work"
60037150,60031713,"stackoverflow.com",2,"2020-02-03 12:16:34+02","2024-05-17 04:33:04.165312+03","I am not so surprised Have in mind that under 1GB of RAM the CPU is single core and the CPU grows linearly with memory Try this tool for fine tuning of your Lambda memorypower and cost If you do not want to raise your memory try using Provisioned Concurrency to cut coldstarts PS are you sure that those time are because of cold starts"
60045054,60031713,"stackoverflow.com",2,"2020-02-03 20:25:49+02","2024-05-17 04:33:04.167313+03","Before calling the handler function the underlying CPU is not throttled see this reinvent video Since the billed duration is decreasing as you are increasing the memory something tells me that you might have written function definitions within the handler which is naturally throttled as per the Memory allocated to the function taking longer time Try describing all the functions static variables outside the handler and keep handler code minimal This will ensure Lambda spends more time outside the handler with full CPU capacity before invoking the handler function where it is throttled Best way to get more idea about this is my profiling your function code using XRay segments to see where function is spending more time This will paint a clearer picture if those are indeed cold starts or just function taking longer Note Cold start durations are not counted towards your function duration metrics but rather will show as init duration when you enable XRay tracing "
60212788,60206006,"stackoverflow.com",9,"2020-02-13 18:45:23+02","2024-05-17 04:33:04.738013+03","environment_variables are saved in zappa_settings py when creating a package for deployment run zappa package STAGE and explore the archive and are then dynamically set as environment variables by modifying os environ in handler py To set native AWS variables you need to use aws_environment_variables "
62950537,60218036,"stackoverflow.com",1,"2020-07-17 11:54:37+03","2024-05-17 04:33:05.51839+03","Here are some general guidelines that should assist you with your task Use Pipenv for your project and install all packages via pipenv install command Install Zappa on your pipenv env using pipenv install zappa Run zappa Init to create the zappa_settings json file Edit and configure your zappa_settings json according to your project requirements IMPORTANT you will have to specify the AWS key secret directly to the pipeline For this purpose make sure you delete the profile_name key from the zappa_settings json and provide them via the pipeline Keystore settings Run the first zappa deploy and confirmed everything runs smoothly configure the pipeline to retrieve changes automatically from your repository and deploy it official python docker image Add the following lines to your buildspec yml file Good Luck "
63131312,60218036,"stackoverflow.com",0,"2020-07-28 12:44:25+03","2024-05-17 04:33:05.521391+03","prerequisites Inlocal machine do zappa init which will create zappa_settings json file you can modify zappa_settings json according to your requirements include zappa_settings json and requirements txt in the root directory"
54655520,54620284,"stackoverflow.com",1,"2019-02-12 19:23:05+02","2024-05-17 04:33:10.426566+03","You can quite easily change the LoadBalancer service type to NodePort one by upgrading nginxingresscontroller`s corresponding helm chart You should be able now to hit the openfaas gateway via httplocalhost_ipnode_port"
54621739,54620284,"stackoverflow.com",0,"2019-02-11 00:27:11+02","2024-05-17 04:33:10.428561+03","Assuming HyperV level port forwarding is set up as you mentioned you will want to switch the main Service from a LoadBalancer to a NodePort and either hardwire a particular node port or forward whichever happens to be picked "
56183106,54750562,"stackoverflow.com",0,"2019-05-17 12:06:04+03","2024-05-17 04:33:11.418909+03","Elaborating the link mentioned by viveksyngh tensorflowservingopenfaas Example of packaging TensorFlow Serving with OpenFaaS to be deployed and managed through OpenFaaS with autoscaling scalefromzero and a sane configuration for Kubernetes This example was adapted from httpswww tensorflow orgserving Prereqs OpenFaaS OpenFaaS CLI Docker Instructions Clone the repo Clone the sample model and copy it to the functions build context Edit the Docker Hub username You need to edit the stack yml file and replace alexellis2 with your Docker Hub account Build the function image You should now have a Docker image in your local library which you can deploy to a cluster with faascli up Test the function locally All OpenFaaS images can be run standalone without OpenFaaS installed let us do a quick test but replace alexellis2 with your own name Now in another terminal"
55415317,55414633,"stackoverflow.com",2,"2019-03-29 12:57:29+02","2024-05-17 04:33:12.565539+03","OpenWhisk actually bypasses k8s to schedule the containers that run the user functions known as actions in OpenWhisk The function containers are managed entirely by the Invokers which are OpenWhisks orchestrators custom built for handling functions The OpenWhisk system components are managed by Kubernetes But Kubernetes itself is generally too slow for short lived function containers and can take many seconds to start one That is a non starter for functions that may execute for milliseconds Kube was not really designed to churn through tens of thousands of containers at high rates You may also want to take a look at Knative which is relatively new but aligns itself very closely with Kubernetes Here are 3 articles that may be informative to you What Invokers do in OpenWhisk httpslink medium comMrhGj6RVrV and httpslink medium compifBTkQVrV The Serverless Contract provides some context to understand the performance implications of the underlying platform on the user functions httpslink medium com2EMzs0FVrV Lastly it may be worth noting that OpenWhisk may be deployed on baremetal and VMs and Mesos DCOS or Docker Compose giving you lots of options for custom deployments "
56110049,55744631,"stackoverflow.com",0,"2019-05-13 13:02:26+03","2024-05-17 04:33:13.7157+03","I have created similar example using openfaas minio and Tensorflow Hope this helps httpsgithub comviveksynghopenfaaspipeline Let me know if you any question Happy to help "
56161567,56160430,"stackoverflow.com",0,"2019-05-16 08:47:00+03","2024-05-17 04:33:15.861564+03","No unfortunately quotas in Kubernetes apply to namespaces only "
57259012,57102546,"stackoverflow.com",0,"2019-07-29 20:49:18+03","2024-05-17 04:33:17.119402+03","Figured it out you must run this command first kubectl portforward deploymentprometheus 90909090 n openfaas Source httpsgithub comopenfaasworkshopblobmasterlab9 md"
57263302,57258994,"stackoverflow.com",2,"2022-03-31 22:01:30+03","2024-05-17 04:33:17.926474+03","You can enable a monitoring endpoint in NATS to get a number of general endpoints to query which can go down to a specific channel You would then need to expose a Service for that endpoint in Kubernetes for external access probably via an Ingress if you want more control over which endpoints and how they are exposed Have a look at the templates in a natsstreamingft helm chart Add the monitoring port to your container spec And the chosen monitoring port to your ports list in the Service "
57311319,57298038,"stackoverflow.com",1,"2019-08-01 17:30:13+03","2024-05-17 04:33:18.688846+03","Figured it out when I first configured Minio I set the IP address of the endpointhostname to localhost9000 This worked fine on my local machine but inside an OpenFaaS container localhost refers to the containers IP not my machines IP so OpenFaaS functions could not access Minio "
58200828,58200127,"stackoverflow.com",0,"2019-10-02 14:44:23+03","2024-05-17 04:33:20.151562+03","So I do not know a lot about AWS yet but reading the specification for the S3Client your declaration is possibly incorrect as there does not seem to be an implementation of S3Client getObject that supports two String inputs Instead it is looking for a GetObjectRequest namely S3Client getObject GetObjectRequest getObjectRequest So your implementation may have to be something as follows S3Object s3Object s3client getObject new GetObjectRequest bucket key Heres an example Amazon S3 Model GetObjectRequest"
58482450,58460455,"stackoverflow.com",2,"2019-10-21 11:26:00+03","2024-05-17 04:33:21.163213+03","To fix this delete the openfaas entry from helm using helm del purge and try remaking it Everything worked the second try "
70107377,58460455,"stackoverflow.com",0,"2021-11-25 09:43:11+02","2024-05-17 04:33:21.164213+03","Deploy the Chart with arkade fastest option The arkade install command installs OpenFaaS using its official helm chart but without using tiller a component which is insecure by default arkade can also install other important software for OpenFaaS users such as certmanager and nginxingress It is the easiest and quickest way to get up and running You can use arkade to install OpenFaaS to a regular cloud cluster your laptop a VM a Raspberry Pi or a 64bit ARM machine Install the OpenFaaS app If you are using a managed cloud Kubernetes service which supplies LoadBalancers then run the following Note the loadbalancer flag has a default of false so by passing the flag the installation will request one from your cloud provider If you are using a local Kubernetes cluster or a VM then run After the installation you will receive a command to retrieve your OpenFaaS URL and password Other options for installation are available with arkade install openfaas help For cloud users run kubectl get n openfaas svcgatewayexternal and look for EXTERNALIP This is your gateway address "
60778242,59972193,"stackoverflow.com",0,"2020-03-26 15:03:54+02","2024-05-17 04:33:22.327114+03","You can make the first function invocation with the command line Example echo n google com faascli invoke curl gateway 127 0 0 131112 Here curl is the functions name"
60099536,60098745,"stackoverflow.com",2,"2020-02-06 18:28:21+02","2024-05-17 04:33:23.29001+03","Apologies The threshold was actually being enforced on an external Ingress which can easily be configured using annotations in its yaml file apiVersion extensionsv1beta1 kind Ingress metadata name Chart Name namespace Values namespace labels version Chart Version annotations kubernetes ioingress class nginx nginx ingress kubernetes ioproxybodysize Values size spec "
62484755,62469925,"stackoverflow.com",0,"2020-06-20 13:46:25+03","2024-05-17 04:33:25.155404+03","Just edit the dashboard and add the panels you want The metrics itself must be available through a datasource like Prometheus or InfluxDB of course "
63550917,62676125,"stackoverflow.com",1,"2020-08-23 21:58:50+03","2024-05-17 04:33:26.164573+03","How are you running OpenFaas Assuming you are running in Kubernetes you will need an Ingress for your functions so that they can be accessible outside your cluster There are a bunch of Ingress options If you are not running in Kubernetes let me know and I will see what I can do "
63023873,62732334,"stackoverflow.com",0,"2020-07-22 01:13:07+03","2024-05-17 04:33:27.209614+03","tldr Floriano I know the issue you are facing cuz I just went through it But you need to fix your question and mention Kubernetes there I mean the code you put there is working perfectly fine in Docker But Iam going to assume that you are facing a problem with this because you are doing the lab in Kubernetes And if I am right the answer for the question is as simple as Use KUBERNETES DNS That said Docker World Kubernetes World My case I installed the openfaass almost default helm chart so the namespace was openfaas and the gateway service was just gateway "
62866524,62866325,"stackoverflow.com",1,"2020-07-13 00:14:51+03","2024-05-17 04:33:28.137624+03","The idea is to avoid lockin to a specific vendor With Open Faas you can run wherever you want while AWS Lambda is tied to AWS "
30171236,30171050,"stackoverflow.com",48,"2015-05-11 18:12:16+03","2024-05-17 04:33:28.683992+03","You can probably save the PID of the process in a variable then use the kill command to kill it I have not tested it myself I would like to write it on a comment but not enough reputation yet "
30171578,30171050,"stackoverflow.com",7,"2015-05-11 18:21:23+03","2024-05-17 04:33:28.684992+03","When the script is excecuted a new shell instance is created Which means that the jobs in the new script would not list any jobs running in the parent shell Since the seleniumserver server is the only background process that is created in the new script it can be killed using Or"
49038925,30171050,"stackoverflow.com",3,"2018-02-28 23:31:29+02","2024-05-17 04:33:28.686992+03","As long as you do not launch any other process in the background which you do not you can use directly"
63298883,63297017,"stackoverflow.com",0,"2020-08-07 12:22:10+03","2024-05-17 04:33:29.74013+03","I do not know much about Nuclio but the scenario you described looks possible with Knative Simplest way you can create a Knative Service for your consumer For the Kafka part you can use a KafkaSource to get the events into Knative Eventing system In your KafkaSource you can tell it to call the Knative Service when there is an event coming from Kafka Above is the simplest way If you need more advanced features there is also support for filtering based on event types or having multiple consumers subscribed to events and more features Red Hats Knative Tutorial has a section for serverless eventing with Kafka "
72974561,63297017,"stackoverflow.com",0,"2022-07-14 05:31:10+03","2024-05-17 04:33:29.74186+03","The exact same use case is possible with Fission which is an open source serverless framework for Kubernetes You can create a Message Queue trigger for Kafka and associate it with a serverless function like this fission mqt create name kafkatest function consumer mqtype kafka mqtkind keda topic requesttopic resptopic responsetopic errortopic errortopic This would trigger a function called consumer whenever there is a message in the requesttopic queue of Kafka You can also associate meta data like authentication information as secrets or flags like pollingintervals max retries etc Reference httpsfission iodocsusagetriggersmessagequeuetriggerkindkedakafka"
65311654,64540738,"stackoverflow.com",7,"2021-08-11 22:26:36+03","2024-05-17 04:33:30.406358+03","For faasd you have to make a credential file Your normal dockerconfig json needs to be copied to varlibfaasd dockerconfig json httpsgithub comopenfaasfaasdanoteonprivatereposregistries A note on private repos registries To use private image repos dockerconfig json needs to be copied to varlibfaasd dockerconfig json If you would like to set up your own private registry see this tutorial Beware that running docker login on MacOS and Windows may create an empty file with your credentials stored in the system helper Alternatively use you can use the registrylogin command from the OpenFaaS Cloud bootstrap tool ofcbootstrap The file will be created in credentials"
61462220,61461139,"stackoverflow.com",2,"2020-04-27 18:19:38+03","2024-05-17 04:33:31.511594+03","From the zappa tail output it seems that your function tries to connect localhost but since it is lambda I doubt there is a running postgres instance on localhost You should update your connection settings for postgres correct address of the remote postgres instance If you are using sqlalchemy you can check if SQLALCHEMY_DATABASE_URI is correct "
61477856,61476137,"stackoverflow.com",5,"2020-04-28 13:19:10+03","2024-05-17 04:33:32.624991+03","zappa_settings json is commited to the repo and not created on the fly What is created on the fly is AWS credentials file Values required are being read from Gitlab env vars set in the web UI of the project zappa_settings json gitlabci yml I have not used zappa in a while but I remember that a lot of errors that were caused by bad AWS credentials but zappa reporting something else "
61673260,61542923,"stackoverflow.com",5,"2020-05-10 01:34:14+03","2024-05-17 04:33:33.711385+03","You can access the url by typing zappa status stage where stage is probably something like dev See also httpsgithub comMiserlouZappastatus for details The printout will have your url as well as other status details of your lambda function This works provided you give full permission access to the IAM user for testing purpose only though "
61677737,61542923,"stackoverflow.com",0,"2020-05-08 14:07:14+03","2024-05-17 04:33:33.713386+03","I had to configure my zappa_settigs json by adding these lines"
62263831,62263470,"stackoverflow.com",1,"2020-06-08 17:00:33+03","2024-05-17 04:33:35.499999+03","These invocations probably are related to cold start prevention take a look on this article Regards "
63642707,62541300,"stackoverflow.com",3,"2020-08-29 04:51:16+03","2024-05-17 04:33:36.385262+03","This seems to be related to an open issue open issue on Zappa I had the same issue my Zappa deployment I tried all possible options but nothing was working But after trying different suggestions the following steps worked for me vartask is where AWS Lambda extracts your zipped up code to How to set environment variables in Zappa"
62987140,62541300,"stackoverflow.com",1,"2020-08-31 18:20:54+03","2024-05-17 04:33:36.387263+03","fixed this by adding the cert path to environment python Edit Sorry the issue was not really fixed with the above code but found a hack work around by adding verifyFalse for all ssl requests"
62676460,62659595,"stackoverflow.com",2,"2020-07-02 14:27:47+03","2024-05-17 04:33:37.498317+03","Solution was very trivial thanks to mr mislavcimpersak i tried syncing containers timezone with aws region i was deploying to EDIT this piece of config added to gitlabci deploy stage fixed it"
63359746,63155657,"stackoverflow.com",0,"2020-08-11 16:58:09+03","2024-05-17 04:33:38.261957+03","Ok as it was working locally it turned out to be an APIGateway configuration issue This resolved it"
63175335,63156803,"stackoverflow.com",6,"2020-07-30 17:28:15+03","2024-05-17 04:33:39.197279+03","The problem was due to mysql file missing in MySQLdb directory I tried a work around and importing pymysql and installing goto django_projectdjango_project init py file the same dir where settings py lives add the foll code and deploy again this worked for me"
73109819,63300367,"stackoverflow.com",0,"2022-07-25 16:11:35+03","2024-05-17 04:33:40.160081+03","Here is the docs that have a docker setup that you can use for local testing Docs here Also this post from Ian Whitestone a core Zappa contributor uses the official AWS docker image for the same job And he has given one example for local testing also Blog post "
63564779,63558255,"stackoverflow.com",0,"2020-08-24 19:12:26+03","2024-05-17 04:33:42.33176+03","The builtin basestring abstract type was removed Use str instead The str and bytes types dont have functionality enough in common to warrant a shared base class The 2to3 tool see below replaces every occurrence of basestring with str As you are with python version 3 8 use str instead "
66565234,63634155,"stackoverflow.com",1,"2021-03-10 14:55:41+02","2024-05-17 04:33:42.908837+03","There are several solutions proposed httpsgithub comMiserlouZappaissues1834 1 try to add the following to your zappa settings include []"
63765759,63764298,"stackoverflow.com",0,"2020-09-06 18:22:11+03","2024-05-17 04:33:44.566232+03","[RESOLVED] The reason is folder hiearchy you have to place zappa_settings json with manage py on the same directory it is like"
64029697,63984420,"stackoverflow.com",1,"2020-09-23 17:01:36+03","2024-05-17 04:33:45.707795+03","It was a memory issue Cleaning up the code and the virtualenv solved the problem Removed extra libraries which were not required to reduce the space "
66147024,64064886,"stackoverflow.com",0,"2021-02-11 02:11:00+02","2024-05-17 04:33:47.970922+03","I was stuck with the same error for quite a while when I eventually found the solution described here httpsstackoverflow coma41915180 I had the following role and ARN specified in my zappa_settings json Following the example provided by alexanderbird I changed my ARN as follows Adding in servicerole cleared the ValidationException pointing to the PutRule operation caused by the incorrect ARN "
64279740,64278323,"stackoverflow.com",2,"2020-10-09 15:07:11+03","2024-05-17 04:33:48.746186+03","What I understand is that you use zappa init each time you deploy the code After that you upload your code to lambda manually You must rather use the functionalities of zappa to the fullest See this minimal flask zappa tutorial to know more If there is something I am missing and you really have to pass zappa_settings json file you cannot pass it but you can keep a backup in your repository as zappa_settings json backup and right after zappa init you can call cp zappa_settings json backup zappa_settings json This will save you from creating that file from scratch again and again "
66050525,64624231,"stackoverflow.com",3,"2021-02-05 01:34:08+02","2024-05-17 04:33:51.677174+03","This AWS blog post may sort things out for you Essentially AWS now allows you to connect to SES through a VPC endpoint which means you no longer need to use an internet gateway or NAT device You will have to edit your security groups inbound rules by adding rules for both SMTP and SMTPS Once you have done that you create an endpoint attach the edited security group and it should work from there once the endpoint becomes available "
65097583,64759311,"stackoverflow.com",8,"2020-12-01 21:53:51+02","2024-05-17 04:33:53.841209+03","When the documentation talks about Flasks builtin server it is talking about the server that you get when you run the command flask run or in older applications running a command like python my_application py with a line in the main function like app run When you run flask on Lambda using Zappa or another solution like awswsgi or serverlesswsgi you are not using Flasks builtin server or any server at all the wrapper code in Zappa or anything else is translating the lambda event to a call to your WSGI application Since there is no actual WSGI server it is not possible to use Gunicorn uWGSI etc well it may be possible but it would be very convoluted "
67211185,67199085,"stackoverflow.com",0,"2021-04-22 13:09:29+03","2024-05-17 04:34:09.084111+03","Leaving this answer to help future searches although it is not the route I will take Thanks to Jens in the comments for pointing towards the VPC issue You need to add a NAT Gateway service to the Lambda to add public internet access to a private VPC To grant internet access to your function its associated VPC must have a NAT gateway or NAT instance in a public subnet Source httpsaws amazon compremiumsupportknowledgecenterinternetaccesslambdafunction This is a perhour billed extra so might defeat the point of using serverless if you are using it for small fees and simplicity rather than scale like I was "
67477270,67475390,"stackoverflow.com",3,"2021-05-10 23:26:23+03","2024-05-17 04:34:10.078671+03","You have an exception being thrown in your code I have found NoneType is a generic error when using AWS Lambda Zappa that is masking the true issue Some debugging steps"
72090405,67475390,"stackoverflow.com",3,"2022-05-03 17:24:34+03","2024-05-17 04:34:10.080672+03","I was running into a similar issue tried installing different versions of flask and discovered it was actually in my zappa configuration file I had slim_handler True and removing this solved the issue for me "
65257048,64903806,"stackoverflow.com",6,"2020-12-11 22:54:17+02","2024-05-17 04:33:55.656117+03","API Gateway is a service that allows you to create a RESTful APIs I assume that is exactly what you need if you like to deploy some APIs as you mentioned EventBridge is a serverless event bus where you can publish events and configure your lambdas to consume those That is what you use for decoupled communication E g if a POST to your API Gateway creates a new user to your app then your Lambda could dispatch an event USER_CREATED into EventBridge event bus An other Lambda can then subscribe to event USER_CREATED and send a welcome email CloudWatch Events are just timer based EventBridge events that are published to event bus In that case you could e g create a rule that notifies your Lambdas every morning to run some background task Sending a daily marketing email to your customers could be one such use case I do not think there is a sigle best practise It can be more simple to start with one Lambda serving multiple API endpoints You can also think what kind of permissions and resourcesdatabases your Lambdas need One example is to divide Lambdas to the command and query Lambdas That way POST PATCH PUT and DELETE could call one Lambda that has write permissions to your database Then GET endpoints are handled with another Lambda that has only read permissions to your database This read lambda could use in the future some different kind of database that is optimized for queries These are just some things to consider when your application grows "
64928776,64908458,"stackoverflow.com",0,"2020-11-20 13:37:28+02","2024-05-17 04:33:56.540532+03","My bad I realised that there were a few python libraries not installed properly in my venv and that caused the gateway from launching "
64940550,64940495,"stackoverflow.com",2,"2020-11-21 08:50:41+02","2024-05-17 04:33:57.619422+03","It is best practice not to use AWS Secret Access Keys inside code if you are using Lambda Function You should modify your Lambda Functions Execution Role and add the required permissions for accessing S3 and DynamoDB But if you choose not to use Lambda Functions Execution Role you have many options to store your AWS Secret Access Keys But I recommend you to modify your Lambda Functions Execution Role and add the required permissions for accessing any AWS service in your case S3 and DynamoDB "
44811143,44810237,"stackoverflow.com",24,"2017-06-28 22:51:59+03","2024-05-17 04:33:59.380758+03","Try using instead of source source is not POSIX compliant ss64 combashsource html"
62589427,44810237,"stackoverflow.com",15,"2020-06-26 09:48:20+03","2024-05-17 04:33:59.382759+03","CodeBuild now supports bash as your default shell You just need to specify it in your buildspec yml Reference httpsdocs aws amazon comcodebuildlatestuserguidebuildspecref htmlbuildspecrefsyntax"
44811491,44810237,"stackoverflow.com",4,"2017-06-28 22:56:32+03","2024-05-17 04:33:59.383753+03","The AWS CodeBuild images ship with a POSIX compliant shell You can see what is inside the images here httpsgithub comawsawscodebuilddockerimages If you are using specific shell features such as source it is best to wrap your commands in a script file with a shebang specifying the shell you would like the commands to execute with and then execute this script from buildspec yml buildscript sh buildspec yml snippet build commands pathtoscriptbuildscript sh"
52987736,44810237,"stackoverflow.com",4,"2018-10-25 14:03:20+03","2024-05-17 04:33:59.385273+03","I had a similar issue I solved it by calling the script directly via binbash script sh"
59999409,44810237,"stackoverflow.com",4,"2020-01-31 09:08:45+02","2024-05-17 04:33:59.386304+03","I do not have enough reputation to comment so here it goes an extension of jeffreys answer which is on spot Just in case if your filename starts with a dot the following will fail You will need to qualify the filename with directory name like"
65293029,65292235,"stackoverflow.com",0,"2020-12-14 18:50:59+02","2024-05-17 04:34:00.449881+03","It is doing what you are telling it to do See this shell session assigning x to z caused the function body here the print statement to execute Same is happening with a send_menu which is causing this block to get executed "
66030110,66018600,"stackoverflow.com",1,"2021-02-12 22:33:08+02","2024-05-17 04:34:02.535057+03","For purposes of this answer I am assuming that the bare domain is the secondary and the www is the primary If you choose the opposite simply reverse the terms while reading Redirection means that the browser can load my_domain com but receives the HTTP response of 301 moved permanently This means that the browser is redirected to www my_domain com and navigates to that site The user sees the address bar changed to be www my_domain com although many browsers now hide the www part of the address and new page loads Redirection requires some type of HTTP server running at my_domain com If you already have an HTTP server running then it is pretty straightforward to configure the web server to send the redirect If you do not have a server then you can use a static web site service like S3 Github Netlify to provide the redirect It is kind of annoying to have to set up an entire web server to just send a 301 code But this method is well understood and well supported Alias means that both my_domain com and www my_domain com work and use only one Zappa project It is like two doors that go into the same room The browsers address bar does not change and the user operates on the site normally One caveat of this approach is to be a little more careful if you are using cookies to make sure they are shared between the two domains Aliasing requires some advanced configuration of AWS services You will have to ensure that the certificate covers both my_domain com and www my_domain com Make sure to add both domain names in ALLOWED_HOSTS otherwise Django will reject the request "
66095768,66018600,"stackoverflow.com",0,"2021-02-08 06:03:59+02","2024-05-17 04:34:02.53839+03","I followed this thread Heres the steps that worked though I am not sure why "
66600695,66593641,"stackoverflow.com",0,"2021-03-12 15:20:31+02","2024-05-17 04:34:04.041651+03","AWS lambda currently does not support python 3 9 You can see a list of supported runtimes here httpsdocs aws amazon comlambdalatestdglambdaruntimes html"
66642076,66600865,"stackoverflow.com",0,"2021-03-15 18:41:22+02","2024-05-17 04:34:05.086209+03","I can add an SNS topic as a trigger to the lambda function and point EventBridge at the topic Not perfect as the UI allows EventBridge to trigger Lambda functions directly but it works Just need to add the following into the zappa_settings file and setup the EventBridge outside zappa "
66607208,66607064,"stackoverflow.com",0,"2021-03-12 23:09:57+02","2024-05-17 04:34:05.895069+03","Why do not you just run pip install r requirements txt "
66894938,66889640,"stackoverflow.com",1,"2021-03-31 23:30:47+03","2024-05-17 04:34:07.137481+03","Since you have not yet deployed it would be easier to change your projects name Just edit zappa_settings json and add a key of project_name as documented at httpsgithub comzappaZappa"
67166602,67037706,"stackoverflow.com",1,"2021-04-19 20:24:56+03","2024-05-17 04:34:08.119903+03","I got around this by adding the following permissions to the group my user belongs to My user also has AdministratorAccess permission Looking around I noticed all devops people are complaining about similar issues and recommending giving full admin access to the user "
70057928,67037706,"stackoverflow.com",0,"2021-11-21 21:32:19+02","2024-05-17 04:34:08.121904+03","I also got the similar error message in my case it is i gave s3 bucket name by myself instead of using suggested default name "
67693878,67541167,"stackoverflow.com",0,"2021-05-25 21:43:27+03","2024-05-17 04:34:10.769899+03","The issue was that I initially had two virtual environments one existing and one newly created for Docker The solution was to remove the PYTHONPATH variable from the Dockerfile deleting both virtual environments and creating a new one "
76319010,76188702,"stackoverflow.com",0,"2023-05-24 01:27:39+03","2024-05-17 04:48:21.598678+03","iamRoleStatements in Provider are for Lambda functions see IAM Permissions For Functions It looks like that CloudFormation does not have the permission to set S3 resourcelevel policy "
67580078,67556906,"stackoverflow.com",7,"2021-05-18 07:41:55+03","2024-05-17 04:34:11.943239+03","The likely issue is that you are building your lambda zip package on MacOS When you deploy your lambda function it is running in a Linux environment specifically AWSs Linux2 environment The psycopg2binary is different for the MacOS vs Linux environments so if you build your lambda package including the psycopg2binary on a Mac and then deploy to lambda you will have the issues noted above You will need to build your lambda function inside of an AWS Linux container Heres a Dockerfile you could use to create a container inside of which you install the psycopg2binary and build your lambda zip package Then everything should work Note the amaonzonlinux2 0 operating system then I just install python 3 8 2 into the environment you could use a different version of python if desired From there you can copy in your code and build your lambda zip package and deploy to lambda "
67573844,67556906,"stackoverflow.com",4,"2021-05-17 19:42:57+03","2024-05-17 04:34:11.94524+03","You can use the custom compiled psycopg2 library for AWS Lambda awspsycopg2 "
68353113,67556906,"stackoverflow.com",4,"2021-07-12 22:42:15+03","2024-05-17 04:34:11.946267+03","I built my lambda package on macOS Big Sur with zappa0 53 0 and django3 2 5 I started getting this error after I upgraded to psycopg2binary2 9 1 Rolling back to psycopg2binary2 8 6 fixed the issue for me "
69625713,67556906,"stackoverflow.com",1,"2021-10-19 08:49:47+03","2024-05-17 04:34:11.947468+03","I installed awspsycopg2 1 2 1 to resolve the 500 error that I was getting from AWS every time I executed a post request Note I kept psycopg2 2 9 1 in order so that my local app could still function I have a flask app python 3 8 deployed to AWS lambda via Zappa Local and hosted versions of my app are both hooked up to an AWS RDS PostgreSQL db Trying to avoid using a docker container to keep my stack as simple as possible "
68854749,67556906,"stackoverflow.com",0,"2021-08-20 00:35:17+03","2024-05-17 04:34:11.949471+03","I recently had the same issue popping out despite using psycopg2binary successfully for a long time I found the following workaround using lambda layers which seems to go in the good future proof direction In your zappa config add the following line Referencing one of the layers provided in this repo it depends on your AWS region and Python version and remove psycopg2binary from your environment One of the added benefit is that it should reduce your package size as well "
67728688,67699307,"stackoverflow.com",0,"2021-05-28 13:43:56+03","2024-05-17 04:34:13.538063+03","Add the namespace property in your Gateway yaml file Reference the gateway in your VirtualService yaml file with the following format gatewaynamespacegatewayname httpsistio iolatestdocsreferenceconfignetworkingvirtualserviceVirtualService"
67907337,67897137,"stackoverflow.com",1,"2021-06-09 18:33:22+03","2024-05-17 04:34:14.639728+03","As coderanger pointed out in the comments section the timezone difference is not related to OpenFaaS It depends on the image you are using most of the images use UTC timezone Normally this should not be a problem but in some special cases you may want to change this timezone As described in this article you can use the TZ environment variable to set the timezone of a container there are also other ways to change the timezone If you have your own Dockerfile you can use the ENV instruction to set this variable NOTE The tzdata package has to be installed in the container for setting the TZ variable "
68519745,68143474,"stackoverflow.com",0,"2021-07-25 17:52:58+03","2024-05-17 04:34:15.551176+03","Apparently there is an issue with using helm to install the Nginx ingress controller onto the Docker Desktop Kubernetes cluster This problem has been resolved by manually applying the Nginx ingress controller manifest as per httpskubernetes github ioingressnginxdeploydockerdesktop "
68422220,68415312,"stackoverflow.com",0,"2021-07-17 19:08:54+03","2024-05-17 04:34:16.371171+03","If you use dockercompose for making images for some odd reason there are 2 ways to pass variables Passing them directly into the dockercompose and pointing to the file which contains variables In both cases as you said the variables are passed at the runtime Which means if you use the variable file the one who runs the container has to have that file on their system and if you pass it directly into the docker compose someone has to pass these variables again when running the docker images They need to pass it along with the docker run command "
70604000,68494612,"stackoverflow.com",1,"2022-01-06 09:53:02+02","2024-05-17 04:34:17.03142+03","You can specify the queuename when deploying the function but not when invoking it Reference httpsdocs openfaas comreferenceasyncmultiplequeues"
68572279,68544808,"stackoverflow.com",1,"2021-07-29 11:02:39+03","2024-05-17 04:34:18.006083+03","I created something using the official Java11 template but I am not sure if this is a good solution You can find the code here httpsgithub comPySualkspringcloudfunctionopenfaas"
68934658,68922818,"stackoverflow.com",1,"2021-08-26 10:55:11+03","2024-05-17 04:34:18.937336+03","These errors mean that the HTTP request has failed For the readiness and liveness probe to work properly this type of request must be successful To find out where the problem is you need to get the pod IP address Run You should see an output similar to this Take your IP and run If you get a 200 response code it means the endpoint is properly created and configured Any other answer suggests there is a problem with your image See also"
69112089,69109572,"stackoverflow.com",2,"2021-09-09 07:02:00+03","2024-05-17 04:34:19.820493+03","If you are using the python or Ruby You can create the docker file and use it for creating the docker images and simply deploy it on Kubernetes For OpenFass they have provided good labs with documentation to create the Async function etc Labs httpsgithub comopenfaasworkshop If you are looking for examples you can check out the official repo only httpsgithub comopenfaasfaastreemastersamplefunctions Extra There is also another good option Knative or Kubeless You can find the python Kubeless example and CICD example httpsgithub comharsh4870kubelesskubernetescicd"
69632441,69109572,"stackoverflow.com",1,"2021-10-19 17:11:21+03","2024-05-17 04:34:19.82231+03","Try use a template to build an upstream FastAPI application as an OpenFAAS function This will create a docker image you can run and deploy in your Kubernetes cluster You can see how to do so in the following github repo"
72880222,70174034,"stackoverflow.com",1,"2022-07-06 11:30:40+03","2024-05-17 04:34:21.435377+03","you can use kubectl get service n openfaas get gatewayexternal service IP and then sudo usrlocalbinfaascli deploy f mytemplate yml gatewayhttpIP8080"
70598801,70174034,"stackoverflow.com",0,"2022-01-06 11:00:26+02","2024-05-17 04:34:21.437378+03","You have to install OpenFaas which I did following the Get OpenFaaS part of this guide httpswww openfaas combloggetstartedwithjavaopenjdk11 After installing it it will print out the command which you need in order to forward the gateway and log yourself in "
70406889,70332339,"stackoverflow.com",1,"2021-12-18 22:29:59+02","2024-05-17 04:34:22.366044+03","If you need to use the eventbus across multiple machines then you need to use a clustered eventbus using any one of the many cluster managers that vert x supports When the function comes up it will register it self with the cluster and will be able to communicate with it Using the eventbus in a lambda type application will work but seems like a weird pattern You may want to consider other options such as"
69413153,69413084,"stackoverflow.com",0,"2021-10-02 03:44:01+03","2024-05-17 04:34:55.894025+03","The developer has stated several times that a domain name is needed to overcome the addition of dev and production You can try following what is on this page GitHub issue with some response"
76497564,76222731,"stackoverflow.com",2,"2023-06-17 21:18:36+03","2024-05-17 04:48:22.51486+03","By default serverlessesbuild excludes awssdk version 2 If you are using version 3 the folder name is different It is awssdk so you must add this on your configuration"
70574339,70564456,"stackoverflow.com",0,"2022-01-04 07:15:33+02","2024-05-17 04:34:23.317995+03","Not knowing what do you mean by worker it is very hard to guess why the different number of workers does not have a lot of impact The only mention of worker I was able to find in the OpenFaas documentation is The queueworker acts as a subscriber and deserializes the HTTP request and uses it to invoke the function directly so if this is your worker than increasing the number of subscribers should not increase the processing speed and your results are kind of expected I noticed you are using localhost if you have a k8s local installation and running your tests on a single physical or virtual machine be informed that it is not the best idea to have the load generator hey in your case and the system under test at the same machine due to a possible race condition which will happen for sure Also a good idea is running performance tests against productionlike environment staging because you cannot extrapolate the results and predictcalculate the saturationbreaking points for different hardwaresoftware there are some aspects which could be tested on a scaleddown environment but in general results will not be reliable so consider conducting a test in more realistic conditions and using realistic workloadpayloadconcurrencyetc "
71393268,70564456,"stackoverflow.com",0,"2022-03-08 12:14:56+02","2024-05-17 04:34:23.320996+03","Having a similar Kubernetes cluster to yours and working on something quite relative i wanted to accelarate functions execution in a parallel fashion This can be done with async function invocation and by scaling up the QueueWorker OpenFaas component which serves the async requests Scaling up the functions replicas didnt seem to help at all when a function is shortlived This github issue helped me a lot "
70862782,70847716,"stackoverflow.com",0,"2022-01-26 13:34:38+02","2024-05-17 04:34:25.131154+03","onlychanges is what you are looking for"
70890226,70882063,"stackoverflow.com",0,"2022-01-28 09:32:25+02","2024-05-17 04:34:26.190081+03","I think you need to look at Kubernetes job and specify podAntiAffinity If you are using HPA you will need to keep calibrating resource parameters to achieve what you are doing "
71021516,70961403,"stackoverflow.com",0,"2022-02-08 17:15:19+02","2024-05-17 04:34:26.810227+03","Java also runs on Docker localhost refers to that Java container which is not a Kafka service Make sure your containers run in the same Docker network then you need to reach external services using their container names e g kafka9092 assuming that is what the container hostname is You will also need to make sure KAFKA_LISTENERSPLAINTEXT0 0 0 09092"
71545215,71493306,"stackoverflow.com",2,"2022-03-20 10:31:45+02","2024-05-17 04:34:27.496741+03","When you specify an image to pull from without a url this defaults to DockerHub When you use latest tag it will always pull the latest image regardless of what pull policy is defined So to use local built images do not use the latest tag To make minikube pull images from your local machine you need to do few things Note you have to run eval eval minikube dockerenv on each terminal you want to use since it only sets the environment variables for the current shell session This flow works You can read more at the minikube docs "
71493374,71493306,"stackoverflow.com",2,"2022-03-16 10:10:30+02","2024-05-17 04:34:27.499742+03","If your image has a latest tag the Pods ImagePullPolicy will be automatically set to Always Each time the pod is created Kubernetes tries to pull the newest image Try not tagging the image as latest or manually setting the Pods ImagePullPolicy to Never If you are using static manifest to create a Pod the setting will be like the following"
71546638,71493306,"stackoverflow.com",1,"2022-03-20 14:36:08+02","2024-05-17 04:34:27.501116+03","From comments in initial post I gathered that You can try to build your function image from a shell inside your Minikube instance Or you can Then make sure your openfaas was deployed with faasnetes imagePullPolicyNever or IfNotPresent as I doubt setting the imagePullPolicy directly in your function would do have not read about this in their docs which instead mentions as you pointed it out to override this during openfaas deployment Checking your deployment yaml definition kubectl get o yaml n openpaasfn deployhelloopenfaas should confirm you are not using Always if that is already the case no need to dig further just make sure your image is imported with name and tag matching that referenced by your function Answering your last comment you are not sure how openfaas was deployed One way to make sure the proper option was set would be to look at the gateway deployment in openfaas namespace kubectl get o yaml n openfaas deploygateway In there you should find a container named operator That container should include a few environment variables one of which may be image_pull_policy we can see this looking at the Chart sources You want that environment variable to be set to IfNotPresent add it or edit it if needed Checking your last edit we can see the Deployment object created by your function says So for sure you do need to reconfigure openfaas adding that image_pull_policy environment variable "
71645773,71557667,"stackoverflow.com",3,"2022-03-28 13:13:18+03","2024-05-17 04:34:28.299522+03","I found the way Faasd uses containerd so the way to log into the function container was the following"
75683799,71657530,"stackoverflow.com",0,"2023-03-09 13:10:39+02","2024-05-17 04:34:30.241412+03","You can check logs using faascli logs [functionname] command "
74052966,73483485,"stackoverflow.com",1,"2022-10-13 11:50:30+03","2024-05-17 04:34:31.839475+03","You can try to send request to your other function "
73926125,73742297,"stackoverflow.com",0,"2022-10-02 16:17:13+03","2024-05-17 04:34:32.919904+03","I found out afterwards that Azure already provides a LB so you do not need to create one Not a firewall issue Go to Load Balancing Frontend IP Configuration and choose the appropriate IP "
74843676,74766612,"stackoverflow.com",0,"2022-12-18 21:03:10+02","2024-05-17 04:34:34.125414+03","The problem was misusing the faascli command use docs"
77027293,74839222,"stackoverflow.com",0,"2023-09-02 10:01:22+03","2024-05-17 04:34:35.040577+03","TLDR In most cases function instances are the same as container instances Longer answer On most platforms such as OpenWhisk each function instance operates within its own container However there are exceptions like Nuclio which allows multiple functions to run within a single container to enhance performance Nevertheless it is generally more convenient to have one function instance per container as it simplifies orchestration with tools like Kubernetes Therefore most platforms employ containers to host function instances making it reasonable to assume their equivalence "
75786438,75729270,"stackoverflow.com",1,"2023-03-20 05:46:24+02","2024-05-17 04:34:35.935711+03","You can write the Java Azure Function where you have to modify the Cloud Services Connectivity and Code accordingly Then you can upload this code to the GitHub Repository And then Use the Terraform script with the required cloud provider for deploying the Function as a Service with the code from the Git Repo Source httpsregistry terraform ioprovidershashicorpazurermlatestdocs You can use the Cloud Providers script provided by hasicorp in the terraform for provisioning and deploying the infrastructure for our applications like Web Apps APIs Functions etc and deploying using the code repositories such as GitHub etc "
70474523,70075290,"stackoverflow.com",7,"2021-12-24 21:11:55+02","2024-05-17 04:34:57.953144+03","I would add a more sophisticated solution what mentioned LiriB earlier Use the aws lambda cli which has the functionupdated command documentation Example aws lambda wait functionupdated functionname FN_NAME This command will wait until the function is updated In case it is not upated in 5 minutes it will stop the execution "
70244299,70075290,"stackoverflow.com",4,"2021-12-06 12:49:59+02","2024-05-17 04:34:57.955144+03","You should wait for function code update to complete before proceeding with update of function configuration Inserting the following shell script between the steps can keep the process waiting"
76477912,76409980,"stackoverflow.com",0,"2023-06-15 01:14:50+03","2024-05-17 04:34:36.763531+03","openfaas is running your code in a container containers get their own TCP IP stack namespace so localhost points to the container NOT the host machine it is running on You MUST use the external IP address of the machine running the broker as the broker address If mosquitto is still saying it is running in local only mode then it is NOT running the config file you have provided Most likely you have not restarted the service since editing the file but that is just the best guess since you have not provided any other information The other option is that you are starting mosquitto manually and not providing a path to the config file mosquitto does NOT have a default file it will read you must always provide a path to the config file with the c option "
76817481,76815763,"stackoverflow.com",0,"2023-08-07 00:00:09+03","2024-05-17 04:34:41.438498+03","OpenFaaS functions run in their own container so localhost points to the container not the host system You need to"
76865280,76855656,"stackoverflow.com",0,"2023-08-09 09:54:09+03","2024-05-17 04:34:42.697951+03","Creating a new MQTT client on each message is incredibly expensive do it once and reuse it "
77905097,77323790,"stackoverflow.com",0,"2024-01-30 11:09:53+02","2024-05-17 04:34:45.193948+03","You can use the kubectl portforward n openfaas svcgateway 80808080 to portforward the service in background This will keep the process in background till the terminal is open You can portforward multiple services using this If you want the port to be open permanently change the service type to LoadBalancer and attach it to a public IP or private IP according to your convenience By default the service type is ClusterIP which makes the pod to be accessed only internally "
77680868,77680258,"stackoverflow.com",1,"2023-12-18 19:45:55+02","2024-05-17 04:34:47.662452+03","Reading through the error message it seems like there is an issue with an existing CRD with the same name as profiles openfaas com in the cluster In addition it may also have a mismatch in the ownership metadata in Helm that you might want to double check After checking you may want to attempt to reinstall and see if issues persist For more information and troubleshooting guide I attached a documentation for openfaas [1] [1] httpsdocs openfaas comdeploymenttroubleshooting"
78418215,77854267,"stackoverflow.com",0,"2024-05-02 12:39:59+03","2024-05-17 04:34:48.935584+03","You can use env variables to set the openfaas gateway url if you have one if you do not have any custom gateway url then you must portforward the gateway service to port 8080 For env variable use export OPENFAAS_URLhttpsyourgatewayip8080 "
68591295,68547092,"stackoverflow.com",0,"2021-07-30 15:58:40+03","2024-05-17 04:34:49.968929+03","I have also added some comments to the question to explain that this is probably not due to a cold start As you rightly stated cold starts are explicitly indicated and seem to only take about 500 ms in your case Cold starts this long usually only manifested themselves when lambdas were run in a VPC And AWS has since changed the way lambdas get their network interface which has dramatically sped up that process That being said a quick Google search led me to some interesting discussions on other sites about Django applications and lazy loading I will share some links here even though they are not related to Lambda in the hope they can help you find a solution httpscommunity webfaction comquestions11560djangoappseemsveryslowtostartup10seconds httpsses4j github io20151123optimizingslowdjangorestframeworkperformance As a last note about the keep_warm Sending those requests is quite an old trick in the book However be aware that there are no guarantees as to how long a lambda is kept warm by AWS If an Init duration is indicated in the logs however you can be sure that it was a cold start If you need to ensure that a lambda function is warm and quick to respond to incoming requests you will have to use provisioned concurrency which of course has its own price tag "
68726944,68547092,"stackoverflow.com",0,"2021-08-10 15:36:38+03","2024-05-17 04:34:49.971033+03","I can see some suggestions here on trying to increase the memory for your lambda and I also saw that you tried from 512 to 1024 Have you tried increasing it further say to about 3072 It is a significant increase but this is just to prove that the problem is not due to resource limitations first The keep_warm feature is not guaranteed as far as I have seen and bulk of the cold start time is due to initialisation Since the vcpu allocated to the lambda is proportional to the memory you assign to it your lambda may initialise quicker and somehow mitigate these cold starts "
68662376,68659044,"stackoverflow.com",1,"2021-08-05 12:51:33+03","2024-05-17 04:34:50.694576+03","The URL suffix is an API Gateway convention it automatically appends the mandatory stage name to the URL I do not think there is an API Gateway or Zappa parameters to change it It is recommended to use a custom domain and a custom subdomains for each stage Thus you can choose a userfriendly URL that matches your needs You can also update your STATIC_URL to include the stage One way to do it is to add the stage as an environment variable and write something like this in your settings py Although I recommend to use a custom domain so you have more control "
68703909,68661781,"stackoverflow.com",1,"2021-08-08 22:03:24+03","2024-05-17 04:34:51.578504+03","I suggest you add a stage per app in your zappa_settings json file Then you can deploy each stage independently See the documentation for exact formatting "
68970849,68967389,"stackoverflow.com",2,"2022-01-06 04:27:34+02","2024-05-17 04:34:52.442893+03","You cannot access API gateway from lambda directly if your lambda inside VPC In this case you have to use VPC endpoint You can use Lambda functions to proxy HTTP requests from API Gateway to an HTTP endpoint within a VPC without Internet access This allows you to keep your EC2 instances and applications completely isolated from the internet while still exposing them via API Gateway By using API Gateway to front your existing endpoints you can configure authentication and authorization rules as well as throttling rules to limit the traffic that your backend receives Reference httpsaws amazon comblogscomputeusingapigatewaywithvpcendpointsviaawslambdatextConclusionexposing20them20via20API20Gateway"
69088634,69076040,"stackoverflow.com",2,"2021-09-07 15:57:17+03","2024-05-17 04:34:53.339279+03","As of Sept 2021 there is no support for HTTP API Gateways There is an issue created to add support here httpsgithub comzappaZappaissues851"
69216789,69214257,"stackoverflow.com",1,"2021-09-17 04:28:35+03","2024-05-17 04:34:53.917638+03","It is hard to say as the dependencies vary across applications As far as I know scikitlearn is a huge library and is not used by AWS However if your application or another package in your application uses it as a dependency removing it might break your application Similarly pyarrow is also used by many packages Some of the packages of your application could be using it internally I am not sure but boto3 can also be removed as it is always available by default by AWS But you might want to keep it since you will be needing it for running the application locally For all other packages I suggest you create a dependency tree of your packages using pipdeptree This will serve as starting point to determine which can could be removed "
69337822,69248426,"stackoverflow.com",7,"2021-09-26 21:29:32+03","2024-05-17 04:34:54.850589+03","setuptools 58 broke support for use_2to3 Therefore you should add setuptools58 to your requirements file along with troposphere3 If you still get the same issue downgrade setuptools first and then install troposphere with the other requirements Related question Error while downloading the requirements using pip install setup command use_2to3 is invalid "
69392857,69248426,"stackoverflow.com",0,"2021-09-30 16:11:03+03","2024-05-17 04:34:54.85259+03","We have forked zappa and removed the pinned requirement for Werkzeug Zappa project been restarted but they have been late to update You can read how to do this at this blog"
70172202,70156550,"stackoverflow.com",3,"2021-11-30 17:50:59+02","2024-05-17 04:35:00.028702+03","Your Django Lambda container instances will download the SQLite DB on load and then will cache the file locally until the Lambda container is shut down by AWS This is done automatically by the django_s3_sqlite package Since you are making writes to the database using a separate backend system the Django Lambda container instances are not aware of changes and thus will not redownload the SQLite DB from S3 The functions you list are all Django based caching and will have no effect If you want the Django app to reflect DB changes in a faster time period you will need to migrate to a centralized database such as PostgreSQL on RDS or DynamoDB "
70308576,70307338,"stackoverflow.com",0,"2021-12-10 19:53:15+02","2024-05-17 04:35:00.852219+03","You need to allow the IAM user [email protected] to perform kmsEncrypt on the resource arnawskmsuseast1816087693535key22a02ce62afe4184bfbf18d72d238bdd If you have already tried that but it still does not work my next question would be if you are using AWS Organizations If you do I would visit AWS Organizations on the Management Account and check for any Service Control Policies SCPs which explicitly deny this action If you follow these steps the resolution of your issue should be straightforward Kindly let me know if not "
70410932,70378229,"stackoverflow.com",2,"2021-12-19 13:14:02+02","2024-05-17 04:35:03.071661+03","You need to set trigger on S3 bucket where your CSV files are stored Set event trigger on update object Add your lambdaflask app as a destination for this event This way you can trigger lambda function whenever file is updated in S3 For reference httpsdocs aws amazon comlambdalatestdgwiths3tutorial html"
71032003,71031484,"stackoverflow.com",0,"2022-02-08 12:09:03+02","2024-05-17 04:35:03.707486+03","try to use this hope this works for you although I have not used zappa before but I do not see any issue with your config file "
71048563,71047149,"stackoverflow.com",0,"2022-02-09 13:13:58+02","2024-05-17 04:35:04.652432+03","Do not exactly know what went wrong but I got it fixed by the nocachedir was really important adding "
72409292,72405680,"stackoverflow.com",0,"2022-05-27 20:48:55+03","2024-05-17 04:35:07.650167+03","I am afraid I do not know anything about Zappa We use ElasticContainerService running on EC2 instances we manage We are considering going to ECS on Fargate but have not made the transition yet Looking at your settings file CHANGE YOUR AWS CREDENTIALS AT ONCE After that I am fairly sure you need the MEDIA_ROOT and MEDIA_URL settings Ours are set to MEDIA_ROOT media and MEDIA_URL media And we do have AWS_DEFAULT_ACL publicread so that images can be served straight from S3 using the default for WAGTAILDOCS_SERVE_METHOD of redirect Until you get s3 storage working I would remove the AWS_S3_CUSTOM_DOMAIN setting so you can debug one thing at a time "
73391810,73383470,"stackoverflow.com",1,"2022-08-17 19:18:33+03","2024-05-17 04:35:08.357153+03","My solution was to remove the following properties from zappa_settings json and settings py"
73642079,73638861,"stackoverflow.com",1,"2022-09-08 01:38:07+03","2024-05-17 04:35:09.440734+03","You can define a specific role that you already have created in AWS this can be done by adding the following to your zappa_settings json file where you can define the name of the role or its ARN I would prefer to use the roles ARN You can read more about it in Zappas official documentation Read here you can also find out what are the minimum permissions Zappa needs to work in the following discussion Read here"
73991702,73988381,"stackoverflow.com",0,"2022-10-07 22:09:16+03","2024-05-17 04:35:10.620956+03","The ImportModuleError is related to this python package alpha_vantage and not either of my initial theories I am unsure as to why though It seems like it has something to do with Alpha Vantages setup py file although I could be mistaken I have removed the reference to Alpha Vantage for now but if anyone knows why installing this package would throw a numpy error please elaborate I think the only reference to numpy would be through the pandas packagewhich is only an optional install i e extras_requires in setup py "
74042189,74042045,"stackoverflow.com",1,"2022-10-12 15:55:19+03","2024-05-17 04:35:11.530593+03","seems like you should use src app in your app_function app app"
74896388,74889192,"stackoverflow.com",0,"2022-12-25 10:25:20+02","2024-05-17 04:35:12.41344+03","RESOLVED Root cause was project directory size too big AWS Lambda and Zappa limit project directory including dependencies to 512MB by default and Zappas slim_handler does not get around it automatically So it fails to see the libraries because there is not enough space for them on the Lambda at runtime This can be fixed in AWS Lambda console under functions [select your function] configuration general configuration ephemeral storage edit up to directory size Saw very little info about this error so hope this helps someone "
75131884,75129718,"stackoverflow.com",1,"2023-01-16 10:44:28+02","2024-05-17 04:35:13.409727+03","It seems zappa code in httpsgithub comzappaZappa passes this value directly to boto3 clients create_function method So the issue is with specifying latest layer version in AWS itself It can be manually updated by fetching list of versions and selecting the first result like here httpsstackoverflow coma557521883014044"
75485084,75482907,"stackoverflow.com",0,"2023-02-17 15:46:04+02","2024-05-17 04:35:14.123686+03","Without seeing your exact code and how your file structure is set up it is difficult to give a comprehensive answer especially because there are so many different ways to set up Django React to work together However two of the most common ways are A Deploy a standalone React app on its own server usually using Node Express but deploy a second standalone Django project with a DjangoRestFramework API to model and interact with your database usually using fetch or some other ajax library to communicate between the two This is the method of choice if you are most familiar with React and want to predominantly utilise React and Node features but a downside is you lose a lot of Django functionality like its builtin Auth capabilities B Deploy a Django project but use Webpack and Babel in combination with React to build a dynamic javascript file that takes over the frontend functionality This is the method of choice if you are most familiar with Django and want as many of its builtin features as possible like Auth but you want to use a Javascript framework like React to build your frontend This method usually still uses DRF and fetch for interactions but Django is serving the site rather than a standalone Node server It sounds like your project is closer to B than A so the below fixes which I have used before may help you with your URL issues especially if you are using reactrouter which does not work nicely out of the box with Django urls The key line by far is path pathpath index namefrontend_index_with_path inside our urls py file it should stop your 404 error when you refresh What happens is Django interprets the URL string sent exactly as is and without that line it cannot map the URL to a view because Django cannot natively interact with reactrouter So this pathpath is essentially a dummy variable that always dispatches the index html file regardless of the URL string allowing reactrouter to take control from there SaaSPegasus has much more information on both this trick and other common issues when integrating Django React I also recommend the Tech with Tim tutorial series on this but please note he uses an old version of React Router which may not be compatible with your build Hope this helps"
75538332,75531785,"stackoverflow.com",1,"2023-02-22 23:28:40+02","2024-05-17 04:35:14.869906+03","Set AWS Buckets permissions Set public permission and turn off Block all public access Set bucket policy Bucket Ownership Run in terminal for collect static files Restart your terminal and deploy zappa again for changes to take effect "
76279221,76243462,"stackoverflow.com",1,"2023-05-18 11:57:38+03","2024-05-17 04:35:16.904566+03","See httpsgithub comzappaZappaissues1230 where this issue is logged and there is a workaround "
76348701,76243462,"stackoverflow.com",0,"2023-05-27 22:32:49+03","2024-05-17 04:35:16.905567+03","The last released version 0 57 0 fixes this error If you run pip install zappa0 57 0 you should get the fixed release Full release notes httpsgithub comzappaZappareleasestag0 57 0"
76367493,76367255,"stackoverflow.com",0,"2023-05-30 21:01:47+03","2024-05-17 04:35:17.510784+03","I think you have the incorrect base image it should be amazonawslambdapython3 8 instead of python3 8 "
77153421,76225990,"stackoverflow.com",0,"2023-09-21 23:22:31+03","2024-05-17 04:48:23.025585+03","It is a known limitation of Serverless AppSync plugin it is described here httpsgithub comsid88inserverlessappsyncpluginissues569 As a workaround you should be referring directly the CloudFormation resource that plugin creates for you"
76374216,76373819,"stackoverflow.com",1,"2023-05-31 16:45:57+03","2024-05-17 04:35:18.519992+03","AWS Fargate is a completely different service with a completely different billing model Lambda is billed per number of executions with a freetier that could make some smaller projects completely free Fargate is billed for every second you have the service deployed and running even if it is just waiting for connections to come in and Fargate has no freetier If you have the expertise to do this this is by far the best way to go Zappa seems primarily designed for people that want to run Django on AWS Lambda for the cost savings I think the Flask support was added later and as you have pointed out it really does not give you any benefit unless you want to simply think about coding in Flask and do not want to take the time to learn how to use AWS "
76517061,76507895,"stackoverflow.com",0,"2023-06-21 21:31:12+03","2024-05-17 04:35:20.232299+03","Here is my solutions it perfectly works with zappaS3 and locally You can use this logic in serializer create method also views py tasks py"
76543402,76543114,"stackoverflow.com",1,"2023-06-23 23:51:22+03","2024-05-17 04:35:21.269886+03","Yes is it always good practice to use a custom domain so that your consumers do not have to worry about your DNS name changing Here is the documentation for doing this using Zappa If you follow the Option 1 in that guide which uses Route53 and ACM then you could switch later to an EC2 instance behind a load balancer or one or more services behind CloudFront without the client ever being aware of the change "
78415511,78269495,"stackoverflow.com",0,"2024-05-01 22:06:31+03","2024-05-17 04:47:12.296732+03","I am facing the same issue with sst I downloaded the zip from httpsgithub comSparticuzchromiumtabreadmeovfile and added the zip to the layers attached it to the lambda function But the puppeteer kept got the error ERROR Error generating receipt Error The input directory vartaskappstacksbin does not exist I checked the source code of the sparticuzchromium Then I tried to print all folders in the tmp and the opt only nodemoudles folders but no chromiumrelated layers were found I finally gave up on this solution due to the deadline approaching "
78270945,78270901,"stackoverflow.com",0,"2024-04-04 02:22:25+03","2024-05-17 04:47:13.197349+03","This shows that you are creating 2 Subnets and 2 NAT Gateways in them Both are public subnet and their default gateway points at Internet gateway as expected When it comes to Lambda you will need to create Private Subnets i e 2 more subnets and point their default gateway towards the NAT Gateways Sharing updated file sections below excuse any typos vpc yml Adding 2 private subnets too routing yml Getting Routes setup for 2 private subnets Since you have 2 NAT Gateways we will need 2 Private route tables one for each AZ serverless yml Creating Lambda in Private Subnets With the above flow will be Lambda [Private Subnet] NAT GW [Public Subnet] IGW Internet"
78309160,78308917,"stackoverflow.com",1,"2024-04-11 11:51:21+03","2024-05-17 04:47:15.31435+03","In DynamoDB you only define the attributes that you use as keys Remove the nonkey attributes that you have stored there and it will work "
78316449,78311044,"stackoverflow.com",0,"2024-04-12 15:59:47+03","2024-05-17 04:47:16.124222+03","The problem has nothing to do with Serverless When using Docker Desktop"
78316958,78316692,"stackoverflow.com",0,"2024-04-12 17:32:38+03","2024-05-17 04:47:16.864442+03","Node js 18 or higher version supports Serverless Framework version 2 Serverless Framework version 2 supports Node js 12 and above including Node js 18 and higher versions So you can upgrade your Node js runtime to a version 18 or higher and your Serverless Framework version 2 configuration should still work "
78390702,78351530,"stackoverflow.com",0,"2024-04-26 16:14:33+03","2024-05-17 04:47:19.118286+03","One way to verify this is check the generated cloudformation template file serverless package stage dev package tmp Try changing the Resource value and run the command again Look for the cloudformationtemplateupdatestack json under the tmp folder "
78353794,78351893,"stackoverflow.com",0,"2024-04-19 15:53:42+03","2024-05-17 04:47:20.192056+03","Check that you have the vpc property nested on environment So I guess that is why we see the error message expecting a string rather than a JSON object Apparently you dont have any environment variable so you could just remove that key and move the vpc to be nested directly on the function Here are some examples of a lambda function being set with VPC httpswww serverless comframeworkdocsprovidersawsguidefunctions The link below show info on the environment property httpswww serverless comframeworkdocsprovidersawsguidevariables"
78440946,78411059,"stackoverflow.com",0,"2024-05-07 11:06:09+03","2024-05-17 04:47:22.909878+03","In order to compile your serverless yml file successfully you must provide the handler to your someFunction otherwise you will get an error Error Either handler or image property needs to be set on function someFunction Workaround You can simply create a handler function otherLambdaInvoke js like this and try calling that lambda ARN And that handler function you can use again and again In your serverless yml file you can use Note Do not forget to grant permission to that lambda function to invoke ARN in the serverless yml file too "
77772241,77759920,"stackoverflow.com",1,"2024-01-07 09:24:48+02","2024-05-17 04:47:26.091744+03","Here is what worked for me package json Changes tsconfig json Changes I made some changes from the middy doc here serverless yml and handler ts No changes Run command npx sls offline Because i added local dependency for v3 You might not need npx "
78099838,77791063,"stackoverflow.com",0,"2024-03-04 10:25:57+02","2024-05-17 04:47:27.006938+03","Yes There is no need for an intermediate lambda It looks like your example differs from the structure in the state machine plugin documentation httpswww serverless compluginsserverlessstepfunctions also make sure the iam perms for your busrulesfn are configured correctly"
77841408,77806046,"stackoverflow.com",0,"2024-01-18 19:40:07+02","2024-05-17 04:47:29.573203+03","The answer it seems is no At least nothing vaguely automated However there are manual steps that can be performed to achieve this as documented on the AWS docs httpsdocs aws amazon comAWSCloudFormationlatestUserGuiderefactorstacks html"
77867037,77863279,"stackoverflow.com",0,"2024-01-23 16:28:27+02","2024-05-17 04:47:35.432322+03","Proposed Solution Serverless on a deploy is building and pushing an image to ECR It does not need to understand much to do this it just works If the ECR Repository is not found then it gets created if the repository is there and configured it will use it This is good for those that like the magic To configure it with the serverless yaml there are 2 steps Take your current project assuming you have already deployed it and put the repo in as infrastructure prior to configuring the provider ecr or image options for your lambda Deploy the stack using serverless and you now have your image repository under this Cloud Formation template that is managed by serverless Once you push that any future image configurations will use it Name the ECR Repository as serverless would name it passing an option or just use the name serverlessprojectnamestage Future updates to the ECR infrastructure will now be made here and reflected in your deployments To create ECR Infrastructure given the above helloworld example the resource section should look like this ecrpolicy json this is for stage dev "
77905454,77866176,"stackoverflow.com",0,"2024-01-30 12:04:12+02","2024-05-17 04:47:36.535414+03","I have already solved I have simply commented on the custom configuration part of the python requirements plugin I do not understand what the error is since it only appeared in a library in some functions "
77903388,77901626,"stackoverflow.com",0,"2024-01-30 03:02:01+02","2024-05-17 04:47:38.08422+03","You cannot dynamically create output keys Thus in your case your keys are literal strings UserPool selfcustom serviceName selfprovider stage which leads to errors because keys can only contain az AZ 09 characters not or ref The references are not going to be resolved and you must explicity define them "
77986348,77981105,"stackoverflow.com",0,"2024-02-13 10:03:15+02","2024-05-17 04:47:39.182544+03","I tried creating and deploying Nodejs Function with serverless and yes by default it will take nodejs12 as a Function runtime for nodejs template But nodejs14 nodejs18 nodejs20 are also supported parameters Note Runtime version 3 will also work but its going to be deprecated so you can upgrade it to Version 4 Refer my Github Repository for the serverless project My Default server yml with nodejs18 Commands Refer here for more details In command Prompt Use SET For Bash use Export For Powershell use env I am using command prompt After setting all the environment variables deploy the Function by the command below Output Upgrade the Runtime version to 4 via Configuration settings In order to Run the Function from Portal Code Test Add the CORS settings below If the issue persists Raise a github issue here "
77225674,77181207,"stackoverflow.com",1,"2023-10-04 00:45:08+03","2024-05-17 04:47:42.266738+03","It is possible to do it by writing custom variable resolvers in a plugin as described here httpswww serverless comframeworkdocsguidespluginscustomvariables Great example of a plugin that does just that can be found here httpsgithub comwhardierserverlesspluginpowertools"
77223299,77223170,"stackoverflow.com",0,"2023-10-03 18:02:27+03","2024-05-17 04:47:45.028057+03","The error message mentions preflight This is usually associated to the HTTP OPTIONS method Try to return"
77242489,77229214,"stackoverflow.com",0,"2023-10-06 10:14:45+03","2024-05-17 04:47:46.505336+03","You are right that this happens because your last line is not a key value pair Serverless replaces this with a complete mapping You cannot lexically prepend the key and its corresponding value with it because variable replacement happens after lexically parsing the input One thing you can try is this Theoretically this merges both the default requirements and the items from your custom yml into the target mapping However this has two preconditions which I think are unlikely to both be true I do not know or use Serverless so I do not have a testing setup and cannot test this for you If you try this and it works great If it does not one of the preconditions is not true "
77235146,77234429,"stackoverflow.com",0,"2023-10-05 10:47:51+03","2024-05-17 04:47:47.06301+03","So i have figured it out Issue is not with the way query parameter is handled but how to urls are being handled I have added some conditional checks to check url is valid or not using validURL and URL modules First thing i did was to check if url contains http or https and prefix it if not found and then check the validity of url Here are the modifications done to the recordHit handler "
77249979,77247851,"stackoverflow.com",1,"2023-10-07 16:32:29+03","2024-05-17 04:47:48.009683+03","you environment section seems to be in the wrong place it should look something like below This would fix your error and also include this environment variable in your all your lambda functions automatically "
77796374,77247851,"stackoverflow.com",0,"2024-01-11 02:37:06+02","2024-05-17 04:47:48.010684+03","Five areas I would like to touch Flip your region to useast1 Setting 20201221 for provider lambdaHashingVersion is no longer effective as a new hashing algorithm is now used by default Remove this setting completely see below an example The region and iam field are under the provider Move your environment towards the provider as below Finally update the runtime to accepted runtime nodejs12 x nodejs14 x"
77987527,77257724,"stackoverflow.com",0,"2024-02-13 13:25:25+02","2024-05-17 04:47:48.986998+03","I solved this issue by using lambda layers I was trying to package a platformspecific library mongodbzstd you can follow these steps Now in your serverless file you can reference the lambda layer references httpsdocs aws amazon comlambdalatestdgchapterlayers html httpswww serverless comframeworkdocsprovidersawsguidelayers"
77274966,77268971,"stackoverflow.com",0,"2023-10-11 19:52:02+03","2024-05-17 04:47:49.765105+03","Based on the documentation in creating explicit dependencies it states that you cannot set a dependency on template files or composite types [1] Attached are some documentation to better understand how you can achieve your goals [2][3] [1] httpscloud google comdeploymentmanagerdocsconfigurationcreateexplicitdependencies [2] httpscloud google comdeploymentmanagerdocsfundamentalstypes [3] httpscloud google comdeploymentmanagerdocsconfigurationsupportedresourcetypes"
77306050,77305426,"stackoverflow.com",0,"2023-10-17 06:39:15+03","2024-05-17 04:47:52.077282+03","Bah got it nevermind This at the top of my lambda file"
77411365,77354288,"stackoverflow.com",0,"2023-11-02 18:34:02+02","2024-05-17 04:47:54.61682+03","I believe the issue in your case is coming from use of staticCache in serverlesspythonrequirements plugin I have managed to reproduce this exact issue by first building without dockerizePip true and mismatching architectures and then trying to deploy again with dockerizePip true proper build image When turning on verbose I have noticed that the requirements are not rebuild but rather are injected from previously cached build In order to solve you can either clean cache manually you will see where it is on your machine when running in verbose mode or set useStaticCache false at least temporarily after the cached builds will be correct you can once again depend on cache to speed up packaging "
76660189,76654130,"stackoverflow.com",1,"2023-07-11 11:50:30+03","2024-05-17 04:48:00.488886+03","You may want to add something like console log process env in your lambda handler to learn what environment variables exist in both cases called either from localhost or server then see what you would use to detect how lambda is triggered for example Would return Hope that helps "
76654498,76654130,"stackoverflow.com",1,"2023-07-10 16:58:41+03","2024-05-17 04:48:00.490436+03","You can use LAMBDA_TASK_ROOT as in"
76662128,76654130,"stackoverflow.com",0,"2023-07-11 15:47:53+03","2024-05-17 04:48:00.491434+03","If you want to determine whether the Lambda API is triggered on the localhost or server you can use serverless offline plugins then you can simply use the IS_OFFLINE environment variable If you invoke Lambda using sls offline process env IS_OFFLINE value will be true in package json you can add this dependency in serverless yml you can add this line This is the related documentation that you can read link"
76663372,76654130,"stackoverflow.com",0,"2023-07-11 18:04:34+03","2024-05-17 04:48:00.492434+03","If you are using serverlessoffline plugin the best approach is to use process env IS_OFFLINE variable documented here "
76675478,76668692,"stackoverflow.com",0,"2023-07-13 05:10:49+03","2024-05-17 04:48:01.539652+03","Perhaps experiment with AWS SAM and local invoke You can invoke a single lambda or an entire API "
76714728,76687126,"stackoverflow.com",1,"2023-07-18 19:09:56+03","2024-05-17 04:48:02.642488+03","I would recommend against trying to do that as Serverless Framework internals are operating under the assumption that each stage is deployed as a separate CloudFormation stack Each packagedeployment will generate a stagespecific CloudFormation template that will then be deployed which means that each deploy will remove everything else unless defined under resources explicitly which you already experienced when trying to manually set the stackName Is there any particular reason why you would like to keep all stages in a single CloudFormation stack"
76687947,76687126,"stackoverflow.com",0,"2023-07-14 16:02:27+03","2024-05-17 04:48:02.644488+03","You have have to include the environment name in the stackName property in the provider block "
76714686,76690609,"stackoverflow.com",1,"2023-07-18 19:05:03+03","2024-05-17 04:48:03.400067+03","There is no easy builtin way of doing this One of the workarounds would be to replace the image definition to point directly to an image in ECR that is currently being used by the function s during the infraonly deployments "
77143939,76226777,"stackoverflow.com",1,"2023-09-20 18:28:21+03","2024-05-17 04:48:24.009094+03","Recent releases of Serverless Framework handles this automatically by default No need to use the Split intrinsic function anymore Read more here httpsserverless comframeworkdocsprovidersawsguidevariablesresolvestringlistasarrayofstrings"
76724336,76719522,"stackoverflow.com",2,"2023-07-19 21:37:03+03","2024-05-17 04:48:04.258601+03","The name of the Lambda function in your serverless template is defineAuthChallenge but Serverless autogenerates an AWS resource name from that which is DefineAuthChallengeLambdaFunction So any resource that depends on the function should indicate More generally Serverless capitalizes the first letter of the Lambda function name and adds a LambdaFunction suffix "
76732499,76728382,"stackoverflow.com",0,"2023-07-20 20:49:51+03","2024-05-17 04:48:04.983295+03","You can make use of AWS Lambda Provisioned Concurrency This helps keep the function warm till it is called again Read more about it here Since you are using the Serverless framework you can do it like this"
76773800,76748233,"stackoverflow.com",1,"2023-07-26 20:40:16+03","2024-05-17 04:48:07.014817+03","Are your Subnets and Security groups deployed via CloudFormation If so you could output them from the CloudFormation stack and reference them directly in your serverless config More info here If they are not then you can get them using the AWS CLI and either set them as env vars or pass them in as parameters Then you can reference them directly from within the serverless config Here are some commands you can use to get the IDs Be sure your region is set correctly and this assumes you will run it once for each value you need Security Groups aws ec2 describesecuritygroups filters NametagNameValuesREPLACE_WITH_NAME_OF_SECURITY_GROUP query SecurityGroups[] GroupId output text Subnets Since your VPC already exists you can use that ID aws region useast1 ec2 describesubnets filter NamevpcidValuesREPLACE_WITH_VPC_ID query Subnets[] SubnetId The subnet command will return all of the subnets attached to that VPC If you need specific ones you should tag them appropriately and use that to filter them In my opinion using CloudFormation is the best option here as it makes it the easiest "
76773058,76762400,"stackoverflow.com",0,"2023-07-26 18:52:44+03","2024-05-17 04:48:07.661569+03","Could you just add the headers to the response in the lambda Like this Since you are using a proxy and not an integration request the response from the backendLambda is what the user sees This is how I have added these security headers in the past and you can create a shared library that all your lambda functions can use to reduce the duplication of all these headers "
76767167,76767027,"stackoverflow.com",0,"2023-07-26 03:33:34+03","2024-05-17 04:48:09.183165+03","Might be possible to set up a custom authorizer Try configure authorizers on provider httpApi authorizers in your serverless yml file You can then configure endpoints that are expected to have restricted access by specifying the authorizer property in the events section of your function definition For example the private route is protected by the authAuthorizer authorizer so only authorized users can access it More details here httpswww serverless comframeworkdocsprovidersawseventshttpapi Edit I think setting a default authorizer for the whole gateway is not directly supported in AWS API Gateway with the serverless framework Each endpoint still needs to explicitly specify the authorizer However if you would like to avoid specifying an authorizer for each of the private functions I could suggest a workaround group multiple routes under a single function and specify the authorizer for that function so effectively applying it to multiple routes at once For example using a wildcard in the path property of the endpoint definition"
77153077,76767027,"stackoverflow.com",0,"2023-09-21 22:19:52+03","2024-05-17 04:48:09.185164+03","What we do in AWS SAM we have a root api that attached with authorization mechanism And in Lambda function we mention the root api s ID to the lambdas api event As root api is authorized all other lambdas using root api as its api lambda become authorized by default and when we need a public api we mention the root api s ID to the lambdas api even t and do an explicit mention Authorizer is None to rhe lambda "
76796156,76777918,"stackoverflow.com",0,"2023-07-30 05:41:15+03","2024-05-17 04:48:09.786764+03","There are a few likely causes for this The first would be that a custom Serverless Package configuration is explicitly excluding that dependency The other potential case is that iterall is a dependency of some other module which is listed as a devDependency we would need to see your package json file to confirm Finally these issues can be compounded by typescript and esbuildwebpack so you could check those configuration files if that is something you are using Any of these could cause the iterall package to not be included in the final zip file Sharing the package json file as well as the relevant snippets of the serverless yml file could help clarify this If you remove your node modules entirely and rerun npm install omitdev does the package still appear"
76810217,76808227,"stackoverflow.com",1,"2023-08-01 12:19:54+03","2024-05-17 04:48:10.853971+03","If the code works locally when you run it and when running it in lambda it no longer works that tells us that the problem is what you are sending to the lambda function There are a number of issues with code the first of which is AccessKeyId and secretAccessKey should NEVER EVER be set in a lambda function They are never necessary because Lambda already has its own credentials More details on when to use credentials Future it should be clear that there is a problem on this line Are you actually getting back out the file contents According to your code you later do this Which means that req file has a buffer in it This does not look remotely like it should when sending binary data to a lambda function You should expect the data coming into the lambda to be base64 encoded and require explicit decoding Instead of writing this to S3 it would make more sense to echo back to the caller the file buffer and then you can write that to your local disk After writing it you can validate that it is indeed correct After that mirror echo works you can review the console log req file to see the actual contents Lastly on a metalevel to get better answers I recommend deleting everything from the post which is not absolutely necessary For instance the post only needs It does not need Focus on only the parts that matter I suggest starting with writing the minimal code you can to prove it works locally Can you do it in one line of code two lines of code"
76826090,76825931,"stackoverflow.com",1,"2023-08-03 11:11:12+03","2024-05-17 04:48:12.393216+03","Most likely the console log statements are not being called I sit part of a function which is probably not reached Maybe put the first line as console log hello and see if it is outputted Iam role and internet or endpoint settings are correct since it is writing some messages are to cloudwatch "
76129691,76111099,"stackoverflow.com",0,"2023-04-28 15:40:14+03","2024-05-17 04:48:15.572254+03","I am not sure why but I solved it by replacing with which worked "
76157921,76157652,"stackoverflow.com",0,"2023-05-02 21:39:39+03","2024-05-17 04:48:17.250723+03","It was due to my neglect PG stands for postgress and now I removed that and use the mysql "
76170787,76162375,"stackoverflow.com",0,"2023-05-04 10:53:32+03","2024-05-17 04:48:18.97342+03","Not sure about the root cause of the problem but doing that solved the problem In order to debug I used sls package and compare the zip until the dependencies were there There is still a warning from sls when deploying because it does not expect python3 10 but since AWS CloudFormation is ok with that everything is ok and the lambdas have the python3 10 runtime "
76175127,76164531,"stackoverflow.com",1,"2023-05-04 18:59:23+03","2024-05-17 04:48:19.837882+03","Kinesis Firehose cannot be used as an event source Kinesis Data Streams can be used The link you mentioned makes use of Kinesis data streams which is different from Kinesis firehose or delivery stream Here are the destinations supported by Kinesis firehose httpsdocs aws amazon comfirehoselatestdevcreatedestination html"
76874352,76188702,"stackoverflow.com",0,"2023-08-10 12:24:08+03","2024-05-17 04:48:21.597678+03","Due to some recent changes seems like we have to add few more lines to make it work See the working example shown bellow "
76256842,76240292,"stackoverflow.com",1,"2023-05-15 21:03:53+03","2024-05-17 04:48:25.051464+03","Im not 100 sure what ended up resolving the issue but I think I was added more than one post method in the API gateway and calling the wrong one I ran SLS remove and then SLS deploy and once done I was able to get the api calls to return the 200 status"
76243434,76240292,"stackoverflow.com",0,"2023-05-13 18:18:32+03","2024-05-17 04:48:25.05294+03","Are you using API Gateway I recommend looking at the AWS example photo asset management app that demonstrates how to invoke Lambda functions from a client app using API Gateway In addition there is an AWS CDK script you can use to set up backend resources Once done you can run the app and look at the code See this doc topic in the AWS COde library Create a photo asset management application that lets users manage photos using labels"
75450100,75446911,"stackoverflow.com",1,"2023-02-14 17:44:26+02","2024-05-17 04:48:29.292644+03","The best way is to use an AWS Lambda function That is develop a custom AWS Lambda function that can read and manipulate JSON to meet your business requirments using a JSON library Then hook these Lambda functions into an Amazon States Language document "
75478211,75474825,"stackoverflow.com",1,"2023-02-16 23:40:41+02","2024-05-17 04:48:30.800096+03","Based on documentation here it looks like the s3Sync requires a list of entries whereas you are not providing a list An example would be the following "
76442586,75485025,"stackoverflow.com",0,"2023-06-09 20:42:30+03","2024-05-17 04:48:32.009691+03","You can just add Transform AWSLanguageExtensions under the resources section of the Template file like so"
75487814,75486835,"stackoverflow.com",0,"2023-02-17 20:03:55+02","2024-05-17 04:48:32.830076+03","Along with the typo I was also using the wrong policy type I should have been using AWSSQSQueuePolicy as can be seen below"
75513994,75501372,"stackoverflow.com",0,"2023-02-21 11:15:07+02","2024-05-17 04:48:35.213953+03","I managed to figure out that my login function was only returning the id token Below is the updated login function to return all 3 tokens Note I also had to add scopes to the authorizer to be make the api accept the access token The full signuplogin flow that I am using is based on the following free code camp example httpswww freecodecamp orgnewsawscognitoauthenticationwithserverlessandnodejs"
75502713,75502439,"stackoverflow.com",0,"2023-02-19 21:01:54+02","2024-05-17 04:48:36.215888+03","Have you tried setting the ContentType header of the HTTP response to imagepng "
75646966,75502439,"stackoverflow.com",0,"2023-03-06 06:06:46+02","2024-05-17 04:48:36.217888+03","The issue was using busboy There might be a way to get it working but it was corrupting the image when parsing the blob to buffer The way I got it working with a lambda function and serverless framework when deployed was with awsmultipartparser It was super straightforward and painless using this library with aws lambda No corrupted image issues anymore "
75538730,75505960,"stackoverflow.com",0,"2023-02-23 00:21:12+02","2024-05-17 04:48:38.08917+03","I resolved your problem by adding single quotes example shown below I tested this and it works Partial credit goes to this post Missed comma between flow collection entries when using FnGetAtt in YAML It came up in a google search for the error you mentioned missed comma between flow collection entries in "
76946394,75517643,"stackoverflow.com",0,"2023-08-21 17:44:10+03","2024-05-17 04:48:38.94562+03","In the Serverless Framework documentation there is section called Resolve StringList as array of strings It mentions that you can do the following This seemed to have resolved my error and returned the values I was expecting from the parameter Hope this helps "
75556764,75519340,"stackoverflow.com",0,"2023-02-24 14:52:28+02","2024-05-17 04:48:39.395728+03","I have managed to solve my problem with serverless framework After many tests this is how my files handler py and my serverless yml have stayed serverless yml handler py In this way I have already managed to import functions from other documents By importing it inside each function I manage to contain the errors I have also added try and except to further handle errors I have also added cors to the APIs In addition I have added many more functions which I have not put here with the same structure as the getPrediction function They have all worked perfectly for me "
77063177,75522787,"stackoverflow.com",0,"2023-09-08 01:25:07+03","2024-05-17 04:48:40.371456+03","Not sure what language you using but in python the Credentials class says Service account credentials So instead of loading the file you can just save whatever is in the json file in your secrets manager Then you access the values in your lambda and authenticate using from_service_account_info instead of from_service_account_file service_account_info dictionary has to have the same fields as the original file"
75848357,75570503,"stackoverflow.com",0,"2023-03-27 07:07:40+03","2024-05-17 04:48:41.069921+03","I unfortunately get this same issue frequently when trying to create nested composite types This error gets thrown whenever anything downstream is not correct but is not particularly helpful in identifying the problem I would try setting the date method to a variable and injecting it in case it is an issue with Prisma manipulating the data in the Prisma client It is infuriating the error message and the docs are not very helpful in this case "
76534500,75596213,"stackoverflow.com",0,"2023-06-22 20:48:57+03","2024-05-17 04:48:41.94691+03","I had the same problem and fixed it by just using serverlesswebpack in serverless yml I think the entrypoint is defined first in serverlessbundle and then when it tries to set it again in serverlesswebpack it produces the error"
74738799,74738559,"stackoverflow.com",0,"2022-12-09 06:04:30+02","2024-05-17 04:48:43.710095+03","once you have the env variables set in the provider environment you do not need to assign it to each function and use it as"
74964353,74738559,"stackoverflow.com",0,"2022-12-30 19:43:50+02","2024-05-17 04:48:43.711095+03","There is no way you can direct self refer from provider environment But you can do it this way Another thing also worth to highlight is the provider environment will be available in all functions by default So you need to think again do you really need to redeclare again"
74871625,74794517,"stackoverflow.com",0,"2022-12-21 07:22:41+02","2024-05-17 04:48:44.718943+03","Check node version being used node version I use nvm and was currently switched to an older vesion of node and got same error After installing and switching to node v19 3 0 latest at time of posting the nom install serverless worked "
74803043,74797250,"stackoverflow.com",0,"2022-12-14 21:32:26+02","2024-05-17 04:48:45.491217+03","I was able to fix it by updating the serverlessframework version to serverlessframework circleci [email protected]"
74837838,74837704,"stackoverflow.com",1,"2022-12-18 00:15:42+02","2024-05-17 04:48:47.426952+03","solved by changing the make file according to the instructions on this forum httpsforum serverless comthelloworldresultsininternalservererror13447"
75482037,74869715,"stackoverflow.com",2,"2023-02-17 10:49:59+02","2024-05-17 04:48:48.312484+03","AWS You can make use of intrinsic functions In this case substitution of with _ I would suggest you to use a combination of FnSplit and FnJoin service_name_normalized selfservice replace _ equals service_name_normalized Join [ _ Split [ selfservice ] ] The FnSplit function returns a list of strings which the FnJoin function receives as second argument "
74876081,74869715,"stackoverflow.com",0,"2022-12-21 14:29:10+02","2024-05-17 04:48:48.314484+03","I am personally not aware of such functionality but it is possible to write a plugin that will introduce new utilsfunctions that can be used in your configuration file you can look for inspiration here httpsgithub comwhardierserverlesspluginpowertools I guess a functionality like the one you need could fit nicely into that plugin as well As an alternative you can switch from yml based configuration to JSTS based one You will then have full power of JSTS when writing and processing your configuration "
72883247,72848697,"stackoverflow.com",3,"2022-07-06 15:02:20+03","2024-05-17 04:49:38.591074+03","There is no need to specify the query parameters in the path You should be able to access all passed query params in your event handler "
74999224,74900289,"stackoverflow.com",1,"2023-01-04 00:22:41+02","2024-05-17 04:48:50.395138+03","There are a few things you can do Both of these are good for temporary migrations where you will eventually deprecate the old API If you need both in perpetuity you could also migrate your v0 API into a new stack or similarly create a new stack for the v1 API Lambda is priced perinvocation not perfunction So with that in mind I would suggest creating a totally distinct function that will make it easier to deprecate and delete when the time comes "
74933462,74930304,"stackoverflow.com",1,"2022-12-27 21:34:39+02","2024-05-17 04:48:51.041511+03","There is a Serverless Plugin which does this but you will get 6 different stacks with 6 independent functions dynamo tables ES Clusters etc If that is what you want the plugin should work If you need to synchronize data from Dynamo or ElasticSearch across all regions you will need a different approach Global Tables etc "
75023106,74938315,"stackoverflow.com",1,"2023-01-05 20:50:32+02","2024-05-17 04:48:52.061666+03","I saw this bug in the morning and decided to fork serverlesscloudformationchangesets and update it for V3 It is now published httpswww npmjs compackageserverlesscloudformationchangesetsv3 It is a drop in replacement you simply need to run serverless plugin install n serverlesscloudformationchangesetsv3 and the rest should work "
74964933,74944851,"stackoverflow.com",1,"2022-12-30 20:57:22+02","2024-05-17 04:48:52.976527+03","awsServerlessExpress proxy can create multiple instances of the server You have used listen method on the server Two instances try to setup at the same port which causes the error Remove listen method it is unnecessary aws handles listening for you"
74965201,74959269,"stackoverflow.com",0,"2022-12-30 21:33:51+02","2024-05-17 04:48:53.750515+03","I think you have an error in your syntax Try this Note the difference with quotes I am also not using name here and I also use a custom key to have all the custom variables together "
74998919,74982434,"stackoverflow.com",1,"2023-01-03 23:43:09+02","2024-05-17 04:48:54.753794+03","The Serverless Framework generates two things a zip file containing the code of your Lambda function s which is zipped and uploaded to AWS and the CloudFormation JSON template which is sent to the CloudFormation API If you do not have any functions there are no artifactscode to zip so packaging is not necessary As your stack only contains CloudFormation you can simply run serverless deploy to apply those changes "
75095434,74999939,"stackoverflow.com",0,"2023-01-12 13:11:09+02","2024-05-17 04:48:55.417197+03","Currently the best way to achieve such behavior is to use JSTSbased configuration instead of YAML With TSJS you get full power of a programming language to shape your configuration however you want including use of such conditional checks to exclude certain parts of the configuration It is not documented too well but you can use this as a starting point httpsgithub comserverlessexamplestreev3legacyawsnodejstypescript In general you can do whatever you want as long as you export a valid object or a promise that resolves to a valid object with serverless configuration "
75032715,75024771,"stackoverflow.com",0,"2023-01-06 17:12:27+02","2024-05-17 04:48:56.644773+03","Your cloudformation template is automatically verifying the email attribute Try removing this block"
75039445,75036100,"stackoverflow.com",1,"2023-01-07 12:00:25+02","2024-05-17 04:48:56.985345+03","I have desired this as well and have not found a way to accomplish it in the same fashion as Serverless That being said unlike Serverless you can reference different files with the cli commands with template [template name] so in terms of CICD you could theoretically break templates into separate standalone templates and then have your CICD deploy each separately the downside is you make multiple cloudformation stacks but it could be useful in some use cases especially if you are deploying cloudformation stack sets and you want to break up different infrastructure "
75218954,74058408,"stackoverflow.com",0,"2023-01-24 10:22:05+02","2024-05-17 04:48:58.770894+03","You can create an event source mapping with lambda with SQS as the event source From the docs link to doc httpsdocs aws amazon comlambdalatestdgAPI_CreateEventSourceMapping html so in function name you can pin it down to a specific version"
74102641,74062261,"stackoverflow.com",1,"2023-07-25 13:30:35+03","2024-05-17 04:48:59.852628+03","You are corrrect that it has not been implemented for httpApi It is supported by HTTP API on AWS level though so you can override manually properties of created AWSApiGatewayV2Integration by using the following syntax See CloudFormation docs for that resource for reference httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawsresourceapigatewayv2integration htmlcfnapigatewayv2integrationresponseparameters"
74084482,74065303,"stackoverflow.com",1,"2022-10-16 06:50:11+03","2024-05-17 04:49:01.111438+03","solved use middycore instead of middy itself yarn add npm i middycore instead of npmyarn middy for more info httpsmiddy js orgdocscategoryintrotomiddy"
74182824,74178413,"stackoverflow.com",1,"2022-10-24 17:40:21+03","2024-05-17 04:49:03.010544+03","there is no good way to do it if you need to have two different APIGWs in the same Serverless service The best approach would be to separate it into more services e g serviceapublic serviceaprivate and so on If you really need to use a single service that uses two separate APIGWs then you need to write the integration as raw CloudFormation "
74190351,74187203,"stackoverflow.com",0,"2022-10-25 10:28:03+03","2024-05-17 04:49:04.097183+03","why do you think it is deploying the full stack and not only the function Output says it is only deploys the function A is not it Perhaps you are looking for and include only the files needed for your function"
75483170,74191749,"stackoverflow.com",1,"2023-02-17 12:39:00+02","2024-05-17 04:49:05.301815+03","Gaurav I had the same issue using the Serverless Framework Just updating the DeletionPolicy resulted in a skipped update The workaround for me was to add a dummy resource to the stack to get the update deployed then remove the dummy resource and deploy again Looks like a bug in Serverless Framework "
75487104,74191749,"stackoverflow.com",0,"2023-02-17 18:51:47+02","2024-05-17 04:49:05.30381+03","This the expected behaviour of Retain as explained in AWS docs Apply some subsequent changes to see make sure it is applied Reference httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawsattributedeletionpolicy html"
74206204,74195231,"stackoverflow.com",0,"2022-10-26 13:25:42+03","2024-05-17 04:49:06.184645+03","The code will not be packaged in that case unless you have some functions that do not use images but Serverless Framework still needs to generate and upload CloudFormation file for your deployment so that is probably what is happening How long does the packaging process take in your case"
75967676,74212971,"stackoverflow.com",1,"2023-04-09 00:03:56+03","2024-05-17 04:49:07.238324+03","I am struggling with the configuration of a monorepo with serverless framework Consider that Turborepo is only leveraging npm yarn pnpm and it is more an orchestrator of tasks than a fully monorepo manager I am going in the pnpm workspace way and I am only stuck with the correct dependency bundling of the serverless stack deployed by serverless framework For the remaining packages here what I have done Here you have the dependency resolution setup and then you can use the official guides to setup a common linting package a common tsconfig package to share configuration between all the monorepo packages PNPM package resolution docs httpspnpm iohowpeersareresolved Here is an example of how it will resolve dependencies In my project I have the correct scripts running lint build test and deploy but I noticed that not all the dependencies of the serverless stack I push to AWS are bundled correctly I will keep trying to figure it out and I will reach you if I have some news Hope this little hint can help you solve your problem Good luck "
63796116,63795916,"stackoverflow.com",4,"2020-09-08 17:32:36+03","2024-05-17 04:53:49.326405+03","Move the package outside provider And if you have multiple lambdas in same file then you can add the package as such"
74274599,74252398,"stackoverflow.com",1,"2022-11-01 12:36:17+02","2024-05-17 04:49:07.785249+03","First a quick tip on troubleshooting When I ran into such issues it was helpful to go to the AWS console look at the lambda function and see what the uploaded file structure looks like on that end Is the monitoring folder there Moreover in order to specify how a specific function is packaged you have to explicitly state that you want it to be individually packaged and not follow the general rules of the project as a whole You should try to add More documentation on packaging configuration here You may also have better luck with explicitly stating the patterns you need I know I have had issues with globs in the past So for example you can try"
74264263,74263921,"stackoverflow.com",0,"2022-10-31 15:48:09+02","2024-05-17 04:49:08.575735+03","In DynamoDB you get free lexicographical sorting on the range keys When an item is being inserted first its partition is calculated based on the partition key then the item is inserted into a btree which keeps the partition lexicographically sorted at all times This does not give you all of the features of SQLs Order By which is not supported So if your sort keys look something like this You can do begins_with query with SK StatusActive This will give you all of the items that are in active status ordered by the UserId that has to be zeropadded in order to enforce the lexicographical order "
74264289,74263921,"stackoverflow.com",0,"2022-10-31 15:50:04+02","2024-05-17 04:49:08.577652+03","You cannot do that Sorting can be only done on SK under the same PK You could combine multiple columns into one value and query based on it Something like column1value1column2value2 In that case you will probably have issue in updating that field dynamodb streams could help with it You can trigger event on any modification and asynchronously update that sorting field "
74286911,74283439,"stackoverflow.com",1,"2022-11-02 11:22:28+02","2024-05-17 04:49:09.425957+03","It looks like you are using Serverless Dashboard plugin in your application you have app and org defined in your config file and at the moment ESM modules are not supported by that integration httpsgithub comserverlessserverlessissues11424issuecomment1266584167 You have several options"
74286878,74285785,"stackoverflow.com",0,"2022-11-02 11:19:48+02","2024-05-17 04:49:09.910589+03","Yes it is possible to use existing layers exactly in the way you added them you should be able to use both existing layers via ARN and ones created by the Framework Could you please share the full error and tell us what version of the Framework are you using On the side note module not found might suggest that handler cannot be found I see you have hanlerFile in config instead of probably handlerFile Maybe this typo is causing the problem here"
74291876,74291420,"stackoverflow.com",0,"2022-11-02 17:39:47+02","2024-05-17 04:49:10.593829+03","I spoke with Serverless Framework support and they mentioned that it is by design so that the instrumentation can happen once over the entire SDK instead of potentially multiple times They do not appear to have plans to improve their design at this time A proposed solution from Serverless Framework support is to use their Serverless Cloud offering which is a rework of the Serverless Console found at console serverless com Words from Serverless Framework support about Serverless Cloud It is designed for folks who want a lot more info about their deployed services and is way easier to integrate as it directly attaches to the AWS account and not the individual services deployed with an SDK All instrumentation is activated browserside instead of relying on a clientside package process "
75605678,74296565,"stackoverflow.com",1,"2023-03-06 07:47:10+02","2024-05-17 04:49:12.16608+03","According to httpswww scaleway comendocsserverlessfunctionshowtopackagefunctiondependenciesinzip you cannot use numpy outside of a docker container as it is using C script dependencies If you want to upload any other packages without C dependencies remember to download them into a folder before pushing to scaleway "
74309111,74296675,"stackoverflow.com",3,"2022-11-04 10:08:32+02","2024-05-17 04:49:12.349673+03","There is no way to skip full deployment as Serverless Framework uses CloudFormation under the hood and it has to update the whole stack The only resource that you can update separately are functions as you mentioned but it is only intended for development and it does not recognize all properties during an update "
73309452,73309126,"stackoverflow.com",2,"2022-08-10 19:05:11+03","2024-05-17 04:49:13.857014+03","You would need an architectural change Create a Kinesis data stream and subscribe individual cloudwatch log groups to that single kinesis stream The lambda that processes the error logs needs to have Kinesis as event source This would now enable triggers from the kinesis for logs from any of the lambda functions cloudwatch log groups To enable this subscription automatically use serverless itself with Kinesis CloudWatch subscription and the kinesis stream is going to be same for all functions Serverless has a plugin to do the same serverlesspluginlogsubscription httpswww serverless compluginsserverlesspluginlogsubscription"
73360571,73352359,"stackoverflow.com",1,"2022-08-16 14:27:31+03","2024-05-17 04:49:14.600163+03","I am also facing the same issue I solved it by installing v3 of serverless As the v9 of serverlessoffline supports only v3 of Serverless it worked after I updated my serverless to latest version "
73353873,73353777,"stackoverflow.com",0,"2022-08-15 11:56:36+03","2024-05-17 04:49:15.523664+03","Adding cors true to your configuration only handles preflight requests If you want your endpoint to be compliant with CORSs requirements you need to also return the correct headers in your function Note It is not a good practice to use as your allowed origin in production I recommend you to read a guide about CORS something like this for example Check also this guide about using CORS and serverless framework together "
73362866,73362174,"stackoverflow.com",0,"2022-08-15 18:06:28+03","2024-05-17 04:49:16.565305+03","similar issues being reported here httpsgithub comserverlessserverlesspythonrequirementsissues663 try this httpsgithub comserverlessserverlesspythonrequirementsissues663issuecomment1131211339 You may try running these tutorials on either a cloud9 console or cloudshell you will face a lot less issues in an aws environment "
74107892,73365218,"stackoverflow.com",0,"2022-10-18 11:24:32+03","2024-05-17 04:49:17.517249+03","Follow this guide for nodejs Can you try copy js code For example something like esbuild buildsrcfunctionsaddFriendhandler js instead of this step COPY package json handler ts "
74348819,73375643,"stackoverflow.com",0,"2022-11-07 17:14:31+02","2024-05-17 04:49:18.374142+03","This seems to be a problem with Serverless framework version 3 x I downgraded my serverless framework version to 2 x and the error resolved itself npm install g serverless The following link may help in adhering to the new variable resolution method Serverless Framework Variables resolution error"
73411178,73409743,"stackoverflow.com",2,"2022-08-19 05:07:26+03","2024-05-17 04:49:19.093092+03","What you are seeing is a URL for an AWS API Gateway instance If you delete and recreate your serverless stack a new endpoint will be generated If you do not remove the stack it will stay the same throughout multiple serverless deploy commands If you would like a custom domain instead of one generated by API Gateway you will need to configure a domain name via AWS Route 53 If you are using the Serverless Framework heres a good guide to do that "
73422681,73416601,"stackoverflow.com",1,"2022-08-20 11:30:05+03","2024-05-17 04:49:20.176426+03","The function connectToService may be not same copy between you mocked and called Because you overwrote a new object by module exports This causes you probably get different object for each require Try to do the below approach sharing the same object for all require "
77397038,70755557,"stackoverflow.com",1,"2023-10-31 17:04:49+02","2024-05-17 04:50:23.296902+03","I have run into this error countless times and it is always because docker is not running on my system when I try to deploy Error message is not terribly direct about that one "
73450456,73430558,"stackoverflow.com",3,"2022-08-22 23:03:44+03","2024-05-17 04:49:21.193226+03","It seems to me there might be disconnect in understanding on how the DLQ behavior works for Lambda First and foremost DLQ feature in AWS Lambda is only available for triggers which are invoked asynchronously like S3 SNS IOT etc For the event source AWS SQS Lambda polls the queue and invokes Lambda function synchronously with an event that contains queue messages Hence setting a DLQ on lambda for event source of SQS would not work it would never be triggered by lambda From the context of your problem I think what you are trying to achieve are Assuming my understanding of the premise of this problem is correct you may follow below steps to achieve it SQS queue provide a feature where if the consumers of SQS fails to process the message for a certain number of times These failed message can be moved to DLQ These DLQ can be configured at primary queue level You can follow your existing pattern and create a trigger on this DLQ to process the failed messages from the dispatcher lambda You may use this helpful guide to set these up In this approach you are configuring DLQ at Queue level instead on lambda I hope this would help you in your design "
74087163,73430558,"stackoverflow.com",1,"2022-10-16 15:36:18+03","2024-05-17 04:49:21.196227+03","For anyone who stumbles across this issue in the future My configuration was correct I was missing a SQS queue policy Once I configured a policy everything worked as expected "
73467921,73432238,"stackoverflow.com",0,"2022-08-24 08:39:36+03","2024-05-17 04:49:22.250366+03","Solution If I understood correctly the returned policy is only applicable to Resource arnawsexecuteapi i e it is determining which endpoint URL can be called but does not change permissions of the lambda behind the API gateway I changed the API to e g authUrl projecId itemsget and consider these path parameters in the policy document to allow invokation only for specific projectIds based on the provided authUrl think Linksharing where URLs are associated with a project and admincontributerviewer URLs exist "
73441419,73441175,"stackoverflow.com",2,"2022-08-22 10:16:42+03","2024-05-17 04:49:23.151206+03","But I want check data between Presigned URL and S3 Bucket which data user send Its not possible with your current design You can only perform a check after the user have uploaded the file For example setup an S3 trigger for PutObject event which will trigger a lambda function to verify the file Otherwise you have to change your architecture and put some proxy between users and S3 For example Apigateway or CloudFront or custom application "
73529276,73527986,"stackoverflow.com",2,"2022-08-29 16:06:55+03","2024-05-17 04:49:25.349595+03","You do not have enough permissions to be able to deploy the stack Yes you need to have the permissions around CloudFormation And to deploy resources in it you will need to have those specific permissions too see documentation here Here the error tells you that you need to have cloudformationDescribeStacks permissions to continue "
73531440,73530246,"stackoverflow.com",1,"2022-08-29 19:01:09+03","2024-05-17 04:49:26.443596+03","Not sure why you install the OS image Just install Node js image instead Dockerfile example And then you specify ENV commands ports volumes in your dockercompose file"
73544773,73530246,"stackoverflow.com",0,"2022-08-30 18:20:42+03","2024-05-17 04:49:26.444597+03","So in this case it seems like Node was interpreting node npm bin sls offline start host 0 0 0 0 as run the sls library aka the structured list library httpswww npmjs compackagesls which means it was displaying exactly what was expected If I uninstall sls not sure how it got installed running the container outputs I stumbled across httpsforum serverless comtpossibletorunserverlessinadockercontainer5764 more specifically slinkardbrandons answer Essentially he uses a direct link to the serverless libraries within the Docker environment as parameters to CMD a Gist is linked here Why does not it work otherwise Why have I seen sls offline being used in other docker containers with no issues Who knows I suspect gremlins But at least my application works now I incidentally also did the rewrite to use node as a base image so my Dockerfile is a lot cleaner Thanks to all for your help "
77704808,73549656,"stackoverflow.com",0,"2023-12-22 18:37:23+02","2024-05-17 04:49:27.420272+03","You need to add this to the serverless yaml"
73572766,73571140,"stackoverflow.com",0,"2022-09-01 19:33:39+03","2024-05-17 04:49:28.398119+03","There is unfortunately not enough data to narrow down the root cause I would start by deleting the IAM role related to Lambda service here [] httpsuseast1 console aws amazon comiamv2homeregionuseast1roles If that does not work Hope it helps"
72808996,72721749,"stackoverflow.com",0,"2022-06-30 02:56:31+03","2024-05-17 04:49:31.288559+03","I have come to the conclusion that flattening cannot be done because of the way that Serverless constructs the underlying CloudFormation template Dropping to a parent directory below the base directory for the application services makes it impossible or simply illogical from a purely computer science tree perspective to attach sibling directories to the services It would be helpful if there were documentation related to the way that Serverless path references are resolved and work together e g glob versus file references If this exists somewhere please send me a link "
72733841,72733792,"stackoverflow.com",1,"2022-06-23 19:45:57+03","2024-05-17 04:49:32.93387+03","There is no way to kill a running lambda However you can set concurrency limit to 0 to stop it from starting further executions"
72745675,72733792,"stackoverflow.com",1,"2022-06-24 17:36:03+03","2024-05-17 04:49:32.935065+03","Standard StepFunctions have a max timeout of 1 year yes One year As such any individual task also has a max timeout of 1 year Express StepFunctions have a timeout of 30 seconds mind you Lambdas have a max time out of 15 mins If you need your lambda to complete in a certain amount of time you are best served by setting your lambda timeout to that not your state machine i see in your comments you say you cannot pass a value for this If you cannot change it then you have no choice but to let it run its course Consider StepFunctions and state machines to be orchestrators but they have very little control over the individual components They tell who to act and when but otherwise are stuck waiting on those components to reply before continuing If your lambda times out it will cause your StateMachine to fail that task as as it receives a lambda service error You can then handle that in the StepFunction without failing the entire process see httpsdocs aws amazon comstepfunctionslatestdgconceptserrorhandling html You could specifically use TimeoutSecondsPath in your definition to set a specific result if the task timesout But as stated no once a lambda begins execution it will continue until it finishes or it times out at 15 mins its set timeout "
72739337,72739204,"stackoverflow.com",1,"2022-06-24 08:25:13+03","2024-05-17 04:49:33.665245+03","Sadly you cannot do that with AWSprovided URLs The only way to join several APIs under a single URL domain is to use custom domains Then you can hook up multiple APIs to one domain e g api mydomain org api2 mydomain org "
74637388,72779780,"stackoverflow.com",0,"2022-12-01 07:45:28+02","2024-05-17 04:49:34.802055+03","That is very easy step in the functions section add this In net core lambda below code will work Below is the example in node js using serverless yml"
72883206,72787152,"stackoverflow.com",0,"2022-07-06 14:58:45+03","2024-05-17 04:49:35.889263+03","There is no such option to skip existing resources in Serverless Framework One option would be splitting the resources that you want to share into a separately deployed service "
73015742,72835567,"stackoverflow.com",0,"2022-07-27 17:43:47+03","2024-05-17 04:49:37.480213+03","After chatting with datadogs team they acknowledge that this was in fact a missing feature and create a PR on their github to include it on the 5 3 0 version of the plugin You can find the PR details on httpsgithub comDataDogdatadoglambdapythonpull229 So a simple update of the plugin should be enough to include this behavior on your monitoring "
72866916,72862470,"stackoverflow.com",1,"2022-07-05 12:19:36+03","2024-05-17 04:49:40.469461+03","This issue is likely occurring because the JavaScript for the browser is sending headers that are not in the allowed list in your serverless yml You probably do not need to send those headers either as sending them from the browser does not assist in resolving CORS and may complicate it I would recommend removing all headers from your message ts except Contenttype and trying again "
72867896,72866819,"stackoverflow.com",2,"2022-07-05 13:29:35+03","2024-05-17 04:49:41.171022+03","You can use ReportBatchItemFailures as const to make it type safe"
75742084,72916029,"stackoverflow.com",0,"2023-03-15 10:21:46+02","2024-05-17 04:49:42.129978+03","Maybe you should try to Edit Configurations and regularly Run the app "
73081498,72925522,"stackoverflow.com",2,"2022-07-22 16:54:29+03","2024-05-17 04:49:44.094414+03","The issue comes from the fact that CustomDashresourceDashexistingDashs3LambdaFunction lambda runs as the deployementRole and not under the defined role which is the default role that lambdas run under Given the deployment role does not normally need to assumeRole my deployment role did not have the assumeRole permission The fix for this is to ensure that the stsassumeRole trust relationship has been applied to the deploymentRole like so"
71947266,71933946,"stackoverflow.com",1,"2022-04-21 02:36:29+03","2024-05-17 04:49:45.805685+03","I solved the issue after a bunch of attempts I found this article that is very similarstackoverflow comquestions46345005 The solution that worked for me was to add openid to the apis oauth scopes instead of just email Then I tried access_token as opposed to id_token on both Postman and the browser and hallelujah"
71973054,71937166,"stackoverflow.com",1,"2022-04-22 21:15:04+03","2024-05-17 04:49:47.02671+03","in your case the most likely reason for that is the fact that you have a local installation of Serverless or some of the pluginsyour other dependencies have Serverless v3 in peer dependencies and install it by default in npm and higher To resolve it either remove local installation or pin the version of the locally installed Serverless in devDependencies of package json of your project "
72774924,71937166,"stackoverflow.com",1,"2022-06-27 18:58:19+03","2024-05-17 04:49:47.02871+03"," node_modules binsls deploy does the trick However the proper answer is in the docs There are 2 scenarios httpswww serverless comframeworkdocsguidesupgradingv3usingv2andv3indifferentprojects"
71977086,71975854,"stackoverflow.com",0,"2022-04-23 08:27:11+03","2024-05-17 04:49:47.318915+03","I had the same error in the past Lamentably I did not find a way to set dynamic values with opt command but I found another solution You can define environment vars in your serverless command and then use the env var in your serverless file so First let us define a script in your package json Then in your YML file you should have something like this This means that if you do not set the env variable this will set www stackoverflow com otherwise will be the value that you send from the command Finally just run npm run serverless and that is it"
72740744,71977474,"stackoverflow.com",4,"2022-06-24 10:56:05+03","2024-05-17 04:49:48.146432+03","Looks like you are using esbuild to bundle your lambdas Some node modules do not like being bundled with esbuild and you have to add them as externals Try adding to esbuild config"
75012839,71977474,"stackoverflow.com",1,"2023-01-05 02:53:51+02","2024-05-17 04:49:48.147433+03","In case it might help anyone else I am using AWS SAM and I had to add the configuration suggested by Misha to template yaml I added it to the Metadata section under Resources like this"
77384282,71977474,"stackoverflow.com",0,"2023-10-29 18:50:05+02","2024-05-17 04:49:48.149433+03","This error is particulary from bcrypt library Replace your bcrypt to bcyptjs bcrypt to bcryptjs "
72029581,71998377,"stackoverflow.com",0,"2022-04-27 16:23:41+03","2024-05-17 04:49:49.195505+03","you cant see cors cookies in devtools application tab but you can see them in the network cookie tab"
72007648,72005804,"stackoverflow.com",6,"2022-05-19 13:20:46+03","2024-05-17 04:49:50.316341+03","In AWS the layers are reusable code that can be utilized by multiple Lambdas across AWS accounts as well I believe There are some limits here max of 5 layers per Lambda and layers should be 250MB In Azure it is a little different you can deploy multiple functions within a function app and all functions in a function app can use the same packages shared code Code cannot be shared across different function apps that is the key part There is no limit on the deployment package "
72034799,72034733,"stackoverflow.com",0,"2022-04-27 23:17:35+03","2024-05-17 04:49:52.228885+03","WOO I was barking up the wrong tree The solution is within SemaphoreCI not Serverless httpsdocs semaphoreci comreferencepipelineyamlreferencetheepilogueproperty Options include on_pass and on_fail Whew "
72812475,72041801,"stackoverflow.com",3,"2022-07-06 15:08:16+03","2024-05-17 04:49:53.238856+03","I used resource as and it worked see snippet from httpsdocs aws amazon comredshiftlatestmgmtredshiftiamaccesscontrolidentitybased html"
72046135,72043049,"stackoverflow.com",0,"2022-04-28 18:08:37+03","2024-05-17 04:49:53.954961+03","This can happen if you are changing a route from one lambda function to another It is a consequence of how API Gateway and Lambda interact You will need to first remove the route from the function deploy the stack and then add the route to the new function It works locally because your API Gateway emulator does not actually use CloudFormation which is what Serverless uses to deploy your application "
73029301,72043049,"stackoverflow.com",0,"2022-07-19 01:37:02+03","2024-05-17 04:49:53.956962+03","hi maybe do you need add the parameter in your path note_id on serveless yml some like this example "
72070564,72070528,"stackoverflow.com",9,"2022-04-30 20:32:28+03","2024-05-17 04:49:57.13045+03","FnJoin enables string concatenation which does not inform the Serverless Framework SF about the dependency of the function on the queue We visually can see that but it needs to be done declaratively To make this link obvious to SF use FnGetAtt instead It will inform Serverless Framework about the dependency of the Lambda function on the SQS queue This should work"
72071266,72070528,"stackoverflow.com",1,"2022-04-30 21:46:45+03","2024-05-17 04:49:57.132175+03","The Serveless Framework can automatically create the queue for you No need to define it in resources"
73449527,72096650,"stackoverflow.com",1,"2022-08-22 21:26:05+03","2024-05-17 04:49:58.923543+03","YES assuming your cloud provider supports each language you specify The Serverless Framework supports multiple languages aka multiple runtimes in a single serverless yaml config file Warning I do not have C experience so I will use nodepython as an example of multiple languages Here is an example of multiple languages in a single serverless yaml as you requested Reference httpswww serverless comblogbuildingmutlipleruntimes"
73793181,72133788,"stackoverflow.com",1,"2022-09-21 01:26:25+03","2024-05-17 04:49:59.578505+03","Reply from AWS Support As per the Case ID 10802672001 I understand that you have an SQS event source mapping on Lambda with a batch size of 500 and batch Window of 60 seconds I further understand that you have observed the lambda function invocation has fewer messages than 500 in a batch and is not waiting for batch window time configured while receiving the messages You would like to know why lambda is being invoked prior to meeting any of the above configured conditions and seek our assistance in troubleshooting the same Please correct me if I misunderstood your query by any means Initially I would like to thank you for sharing the detailed correspondence along with the screenshot of the logs it was indeed very helpful in troubleshooting the issue Firstly I used the internal tools to check the configuration of your lambda function sd_dch_archivebatterydata and observed that there is no throttling in the lambda function and there is no reserved concurrency configured As you might already be aware that Lambda is meant to scale while polling from SQS queues and thus it is recommended not to use reserving concurrency as it is going against the design of the event source On checking log screenshot shared by you I observed there were no errors Regarding your query please allow me to answer them as follows Please understand here that Batch size is the maximum number of messages that lambda will read from the queue in one batch for a single invocation It should be considered as the maximum number of messages up to that can be received in a single batch but not as a fixed value that can be received at all times in a single invocation Please see When Lambda invokes the target function the event can contain multiple items up to a configurable maximum batch size in the official documentation here [1] for more information on the same I would also like to add that according to the internal architecture of how the SQS service is designed Lambda pollers will poll the messages from the queue using the ReceiveMessage API calls and invokes the Lambda function Please refer the documentation [2] which states the following If the number of messages in the queue is small fewer than 1000 you most likely get fewer messages than you requested per ReceiveMessage call If the number of messages in the queue is extremely small you might not receive any messages in a particular ReceiveMessage response If this happens repeat the request Thus we can see that the number of messages that can be obtained in a single lambda invocation with a certain batch size depends on the number of messages in an SQS queue and the SQS service internal implementation Also batch window is the maximum amount of time that the poller waits to gather the messages from the queue before invoking the function However this applies when there are no messages in the queue Thus as soon as there is a message in the queue the Lambda function will be invoked without any further due without waiting for the batch window time specified You can refer to the WaitTimeSeconds parameter in the ReceiveMessage API The batch window just ensures that lambda starts polling after certain time so that enough messages are present in the queue However there are other factors like size of messages incoming volume etc that can affect this behavior Additionally I would like to confirm that Polls from SQS in Lambda is of Synchronous invocation type and it has an invocation payload limit size of 6MB Please refer the following AWS Documentation for more information on the same [3] Having said that I can confirm that this Lambda polling behaviour is by design and not a bug Please be rest assured that there are no issues with the lambda and SQS service Our scenario is to archive to S3 and we want fewer larger files Looks like our options are potentially kinesis or running a custom receive application on something like ECS "
74760661,71290401,"stackoverflow.com",0,"2022-12-11 14:06:43+02","2024-05-17 04:50:01.531173+03","Seems like serverless expects you to define HttpAuthorizer instead of Lambda Authorizer for AWS REST API Definitions of the settings are here httpsraw githubusercontent comlalcebojsonschemamasterserverlessreference json Rest API V1 is actually more advanced see httpsdocs aws amazon comAWSCloudFormationlatestUserGuideAWS_ApiGateway html or httpsdocs aws amazon comAWSCloudFormationlatestUserGuideAWS_ApiGatewayV2 html"
74957322,71290401,"stackoverflow.com",1,"2022-12-30 03:10:32+02","2024-05-17 04:50:01.532173+03","Check the following documentation httpsserverless comframeworkdocsprovidersawseventsapigatewayhttpendpointswithcustomauthorizers The authorizer declaration must be completed in v3"
71416660,71292809,"stackoverflow.com",0,"2022-03-10 00:20:41+02","2024-05-17 04:50:02.520907+03","Doing it manually is not that complicated here is the full explanation"
70780726,70770618,"stackoverflow.com",0,"2022-01-20 06:15:04+02","2024-05-17 04:50:24.18371+03","Your YAML file is also showing below part of Deny Do not you think it should be creating an issue"
72181989,71292809,"stackoverflow.com",0,"2022-05-10 10:06:44+03","2024-05-17 04:50:02.521908+03","API Gateway with a custom domain is configured to use TLS 1 2 and it supports some weak cipher These weak ciphers are removed in TLS 1 3 which is only supported in CloudFront distributions at the moment Some security tools like AppScan raised these issues for me during the security testing of these APIs You can check the details here httpssecurity stackexchange comquestions254667areweakciphersuitesfortls12avalidconcern If security is concerned I would suggest using a cloud front distribution in front of the API gateway httpsaws amazon compremiumsupportknowledgecenterapigatewaycloudfrontdistribution"
71300982,71300373,"stackoverflow.com",3,"2022-02-28 23:18:28+02","2024-05-17 04:50:03.485342+03","The issue is that none of the context object values may contain special characters Your context object must be something like You cannot set a JSON object or array as a valid value of any key in the context map The only valid value types are string number and boolean In my case though I needed to send a string array I tried to get around the type restriction by JSONserializing the array which produced [\valueA\\valueB\] and for some reason AWS did not like it What solved my problem was using myArray join instead of JSON stringify myArray "
71311345,71310039,"stackoverflow.com",1,"2022-03-01 18:05:23+02","2024-05-17 04:50:04.496682+03","A colleague pointed out that someone had added manually from AWS Console the OPTIONS method to the endpoint Removing the OPTIONS method fixed the problem"
71335623,71326299,"stackoverflow.com",0,"2022-03-03 12:30:52+02","2024-05-17 04:50:06.382401+03","The authorizer type should be set below the events section like this You have instead specified the type next to handler on the function itself Serverless deploys with a warning if you do this Warning Invalid configuration encountered at functions authorizer unrecognized property type It then defaults to token as type since no valid type was found "
71775756,71333243,"stackoverflow.com",1,"2022-04-07 05:45:07+03","2024-05-17 04:50:07.548047+03","use rate format "
72122779,71333243,"stackoverflow.com",0,"2022-05-05 09:25:09+03","2024-05-17 04:50:07.549053+03","1 install serverlessofflinescheduler npm package plugins"
71374670,71373223,"stackoverflow.com",3,"2022-03-07 00:23:55+02","2024-05-17 04:50:08.183801+03","Yes as your initial research has uncovered this is something you will want to use DynamoDB Streams for You can trigger a lambda function based on an item being written updated or removed from Dynamo DB and you can configure your stream subscription to filter on only attributes and values you care about DynamoDB recently introduced the ability to filter stream events before invoking your function you can read more about how that works and how to configure it here For more information about DynamoDB Stream use cases this post may be helpful "
71395432,71392799,"stackoverflow.com",0,"2022-03-08 14:54:46+02","2024-05-17 04:50:09.318688+03","Found the issue so posting the answer here if anyone else reads it The code I pasted in the question was correct per see I had just placed the entire apiGateway under the custom section in my serverless file Placing it under provider instead solved it and successfully created the resource filter under my API GW and now blocks IP addresses properly "
72132149,71413205,"stackoverflow.com",2,"2022-05-05 21:39:28+03","2024-05-17 04:50:11.813226+03","In the example serverless yml where you have the restApiRootResourceId property set to second you should have it set to the root resource id which is shown in your screen shot as bt6nd8xw4l"
76976653,71470120,"stackoverflow.com",0,"2023-08-25 14:11:53+03","2024-05-17 04:50:13.07446+03","Try adding Default serverless it gets like this which allows only path I know this is late but keeping it for future users "
71488898,71488345,"stackoverflow.com",1,"2022-03-15 22:53:53+02","2024-05-17 04:50:15.119578+03","these deprecations are related to the libraries version that are used directly by serverless You can expect them to sooner or later be addressed on that library level but until that you can use it without worries unless you see the explicit security warnings but that is not the case at the moment "
71508612,71494723,"stackoverflow.com",0,"2022-03-20 22:37:34+02","2024-05-17 04:50:15.805063+03","All I did was to go to API Gateway Custom domain names then I reconfigure API mapping from pointing to the old deleted API to then point to the existing API that I just created in the cli Then now I can redeploy my serverless application httpsi stack imgur comN6Sjh png"
70727656,70727036,"stackoverflow.com",1,"2024-05-13 03:59:41+03","2024-05-17 04:50:17.170187+03"," Update 20220116 1249 AM The problem was indeed with the tags Everything can be seen on the issue TLDR use the full function name e g FnJoin However I am almost certain that the shorthand can work there are just some conditions that need to be met first I will not investigate since a working solution for the original problem has been achieved and anything else would be outside the scope Original After some back and forth between Ryan and myself Ryan used an alternative solution involving Sub and the serverlesscloudformationsubvariables plugin which can be viewed here In short the problem likely laid in dependencies involving tags not correctly being resolved most likely due to misconfigurations Still investigating and will update if the original solution can be resolved with additional plugins The following plugin abstracts the use of Sub which in turn resolved the issue serverlesscloudformationsubvariables"
70736097,70735658,"stackoverflow.com",0,"2022-01-17 05:10:03+02","2024-05-17 04:50:18.279711+03","The issue was identical variable naming causing a overwrite And the following would fix that "
70874291,70742819,"stackoverflow.com",1,"2022-01-28 14:22:39+02","2024-05-17 04:50:19.356467+03","You can check this GitHub issue this may give you some ideas and this is how I end up with"
75463945,70742819,"stackoverflow.com",0,"2023-02-15 20:19:38+02","2024-05-17 04:50:19.357468+03","I think the issue is described here serverlesshttpissues86 What helped in my case please note it is still a kind of workaround I have checked it locally only npm run slsoffline It is a simplified example My custom section lambda ts The result is that httplocalhost3000apiswagger returns the api page httplocalhost3000api is handled by a default"
76397186,70742819,"stackoverflow.com",0,"2023-06-03 20:24:52+03","2024-05-17 04:50:19.359468+03","Try"
70746925,70745945,"stackoverflow.com",2,"2022-01-17 22:05:37+02","2024-05-17 04:50:20.443587+03","Create a new file named as config json put MONGODB_URI value this way Also modify your serverless yml file "
70748745,70748365,"stackoverflow.com",2,"2022-01-18 04:30:02+02","2024-05-17 04:50:21.534718+03","opt is for serverless CLI options These are part of serverless not your own code You can instead use And pass the value as an environment variable in your deploy step "
71896355,70755219,"stackoverflow.com",0,"2022-04-16 21:11:57+03","2024-05-17 04:50:22.344557+03","I am gathering information to setup a stack for a serverless architecture I came across httpsserverlessstack com debug is said to be one of it main features you might wanna try it I will do in next months "
76947439,70755557,"stackoverflow.com",2,"2023-08-21 20:16:31+03","2024-05-17 04:50:23.2929+03","The error message says docker run failed Exited with code 1 were there more error message printed Make sure that docker is working on your system I needed to reboot after installing docker I think there was a permission problem with the SSH session that was running before change to docker group was made and then the next serverless deploy worked without a problem "
70757072,70755557,"stackoverflow.com",1,"2022-01-18 16:18:50+02","2024-05-17 04:50:23.294901+03","If you are using serverless framework to deploy based on the documentation when you want to install the serverlesspythonrequirements you have to use this line on the terminal first serverless plugin install n serverlesspythonrequirements This will automatically add the plugin to your projects package json and the plugins section of it is serverless yml You do not have to do this manually as you can make any mistake in the process "
70780135,70780069,"stackoverflow.com",1,"2022-01-20 04:55:51+02","2024-05-17 04:50:25.015273+03","It seems that you can configure a redirect on S3 itself Here is a link that shares at least 3 steps to do this To configure redirection rules for a static website To add redirection rules for a bucket that already has static website hosting enabled follow these steps Open the Amazon S3 console at httpsconsole aws amazon coms3 In the Buckets list choose the name of a bucket that you have configured as a static website Choose Properties Under Static website hosting choose Edit In Redirection rules box enter your redirection rules in JSON In the S3 console you describe the rules using JSON For JSON examples see Redirection rules examples Amazon S3 has a limitation of 50 routing rules per website configuration Recommending the layering of your automation by separating provisioning and application automation by using Terraform orand Cloud Formation for this "
70803515,70802848,"stackoverflow.com",1,"2022-01-21 17:14:29+02","2024-05-17 04:50:26.589528+03","Ok it was a careless mistake my connectToIndex returns an Algolia index and I have done the search using index search requests which implies client initIndex search requests But for searching you do not call initIndex instead you call the search method of the client directly I had correctly used this in express and somehow messed up inside the lambda Github issue"
70814735,70804311,"stackoverflow.com",3,"2022-01-22 18:16:48+02","2024-05-17 04:50:27.772189+03","It is hard to answer 100 without seeing your configuration but are you sure you have specified apiType http in your customDomain config section If not it defaults to rest Setting it to http should resolve your problem "
74157480,70804311,"stackoverflow.com",2,"2022-10-21 20:50:54+03","2024-05-17 04:50:27.77319+03","Since your complete serverless yml is not here I guess you are using the default one I had the same issue and here is my solution This is probably the customDomain conf in your yml file The only thing you need to do is add these two attributes to convert your domain from edge to regional Remember if you already have run the sls create_domain command with the previous serverless yml configuration you might face this error The reason for this error is that you have created an edge domain with the previous configuration and now the deployment wants to use it as a regional domain To solve this error just delete the domain with sls delete_domain and then rerun the sls create_domain command I will automatically create the new records for you "
76066814,70804311,"stackoverflow.com",0,"2023-04-20 20:32:58+03","2024-05-17 04:50:27.77519+03","Downgrading to 6 4 3 solves my problem "
77325590,70804311,"stackoverflow.com",0,"2023-10-19 19:30:16+03","2024-05-17 04:50:27.776262+03","I am using Python wsgi app and in my case this solved the issue without specifying http source httpswww serverless compluginsserverlesswsgi"
68522297,68513570,"stackoverflow.com",9,"2022-11-11 20:16:27+02","2024-05-17 04:50:28.594784+03","Updated November 2022 Use EventBridge Scheduler as it allows you to schedule events at specific datetime along with timezone Also supports onetime schedules Introducing Amazon EventBridge Scheduler AWS Blog Original July 2021 Either schedule another event to adjust the first event or execute the lambda at both 930 EST and 930 EDT and have the lambda figure out which one should run Run another lambda at 2am and 3am local time that can adjust the schedule of the first lambda for daylight saving time You can use the lambda languages implementation to decide whether the event needs to be adjusted You could also schedule your original lambda to run at both the daylightsavingadjusted and the nonadjusted time so 1430 UTC for EDT and 1330 UTC for EST Then the lambda could decide whether it was the proper time to execute based on a calendar check I prefer the first option because it is a clearer separation of duties "
68514144,68513570,"stackoverflow.com",5,"2021-07-25 01:09:31+03","2024-05-17 04:50:28.597784+03","Sadly you cannot do this as only UTC time zone is used All scheduled events use UTC time zone and the minimum precision for schedules is 1 minute You would need custom solution for what you want to do For example let AWS EventBridge trigger a lambda function and the function evaluates what else should be triggered based on its conversions of UTC to local times "
74848692,68513570,"stackoverflow.com",2,"2022-12-19 11:42:32+02","2024-05-17 04:50:28.598942+03","As can be read in another answer here this is possible using the EventBridge Scheduler In below example I am using CDK to schedule a lambda at 6 am and 8 am from Monday to Friday "
70835122,70807725,"stackoverflow.com",11,"2022-01-24 16:06:54+02","2024-05-17 04:50:29.592379+03","There is a slight change in variables resolution and in your case the best way to resolve it would be to use the following syntax for resolving the stage Alternatively you can use old syntax but provide explicit fallback value for stage I would recommend going with slsstage version "
71741095,70807725,"stackoverflow.com",6,"2022-04-04 20:06:51+03","2024-05-17 04:50:29.593379+03","Changing the way you are writing the stage from To Should do the work You can find the updated documentation in httpswww serverless comframeworkdocsprovidersawsguidevariables or running serverless print for a more detailed response of the problem "
76455158,70846549,"stackoverflow.com",0,"2023-06-12 12:15:07+03","2024-05-17 04:50:30.61853+03","Use serverlesspluginlogsubscription to automatically create subscription filters instead of adding cloudwatch events for each log group serverlesspluginlogsubscription"
70897917,70876546,"stackoverflow.com",2,"2022-01-28 19:37:22+02","2024-05-17 04:50:31.718479+03","it seems like you might be using a version of the Framework that does not support msk event definition It was added in 2 3 0 release httpsgithub comserverlessserverlessblobmasterCHANGELOG md23020200925"
70059614,70059531,"stackoverflow.com",2,"2021-11-22 02:00:34+02","2024-05-17 04:50:33.69845+03","The existingtrue flag only relates to S3 buckets created outside of your serverless project for buckets that already exist which is not the case here The situation you face is that you cannot use the typical serverless framework convenience of defining the bucket in the Lambda event trigger like this The reason that you cannot use that method is that it creates the photos bucket and does not allow you to supply additional bucket configuration e g CORS or bucket policy The solution to this is to create the S3 bucket in the S3 provider configuration with CORS policy and then refer to the bucket from your Lambda function event configuration For example"
70081496,70070354,"stackoverflow.com",1,"2021-11-23 18:16:28+02","2024-05-17 04:50:35.149368+03","Found the solution but it is my own mistake My lambda was actually within a VPC My original question before the edit did not show this Lambda in a VPC cannot talk with S3 buckets unless the VPC has an Endpoint Gateway that enables it to talk with any specifically referenced buckets I had previously created an Endpoint Gateway that let it talk with the initial bucket I created a while back but forgot to update the Endpoint Gateway to let it talk to the new bucket Leaving this answer here unless anyone else spends an entire day trying to fix something silly "
70070677,70070354,"stackoverflow.com",0,"2021-11-22 20:27:18+02","2024-05-17 04:50:35.151368+03","I think you are likely missing some permissions I often use s3Put on my serverless applications which may not be advisable since it is so broad Here is a minimum list of permissions required to upload an object I found here What minimum permissions should I set to give S3 file upload access"
70085154,70073044,"stackoverflow.com",1,"2021-11-23 19:20:31+02","2024-05-17 04:50:35.710435+03","For shared logic Lambda Layers are probably the best way to share code If you would like to dive in further there are several other options each with pros and cons Eric Johnson Sr Developer Advocate AWS gave a talk at a local meetup that covers this quite well It is based in SAM but he and I discuss the parallels available to Serverless Framework users as well You can find the video here"
70110386,70106678,"stackoverflow.com",1,"2021-11-25 13:34:27+02","2024-05-17 04:50:36.832951+03","If the feature is behind a flag you can set the NODE_OPTIONS env var which lambda supports If the feature is not on the AWSprovided node runtimes as of late 2021 v14 x is the highest version you can deploy a custom runtime or a Docker container "
70133914,70132116,"stackoverflow.com",1,"2021-11-27 12:25:24+02","2024-05-17 04:50:37.802429+03","To avoid crossreference errors and similar ones I recommend converting model definitions to functions and registering models and associations in the same one module see this answer and the question"
70176198,70158096,"stackoverflow.com",7,"2021-11-30 23:18:37+02","2024-05-17 04:50:38.879397+03","The issue ended up being that the secret in AWS Secrets Manager was in the wrong region adding it to the correct region fixed the problem "
70190894,70171372,"stackoverflow.com",0,"2021-12-01 22:34:55+02","2024-05-17 04:50:39.984711+03","there is a plugin called safeguardsplugin that might be useful for you httpsgithub comserverlesssafeguardsplugin"
70867460,70192606,"stackoverflow.com",1,"2022-01-26 19:09:07+02","2024-05-17 04:50:40.716453+03","We ran into a similar issue with serverlesspythonrequirements recently For us it was caused by a deployment where serverless did not recognize the requirements correctly for one function and tried to unzip an empty requirements zip file as a result The failing line unzip_requirements pyL22 Therefore I would recommend trying out the following Use pandas as a layer instead e g by using a public pandas layer or creating you own one following the plugin instructions for layers e g or if the requirements are small enough then try the unzipped version This solved it for us with serverless version 2 69 0 I also heard that potentially downgrading to serverless 1 83 addressed a similar issue in the past but could not verify that so far Good luck"
70238699,70225389,"stackoverflow.com",0,"2021-12-05 23:27:49+02","2024-05-17 04:50:41.74105+03","Is it possible to generate the SDK and include it in the project or maybe some random bucket automatically with CDK Based on the documentation you can generate the SDK two ways Through the AWS console or using AWS CLI Reference httpsdocs aws amazon comapigatewaylatestdeveloperguidehowtogeneratesdk html You could write a script that generates the SDK using AWS CLI and upload it to the website folder or bucket Are there some best practices regarding the provisioning of such a Infrastructure in an automated way using a CICD Pipeline This is a very general question This article talks about Best practices using CDK httpsaws amazon comblogsdevopsbestpracticesfordevelopingcloudapplicationswithawscdk awssamples Github repo is also useful and includes samples like this one httpsgithub comawssamplesawsserverlessappsamcdk"
70243090,70225389,"stackoverflow.com",0,"2021-12-06 11:09:12+02","2024-05-17 04:50:41.743051+03","Add a Custom Resource to your CDK stack As the CloudFormation docs say Custom resources enable you to write custom provisioning logic that AWS CloudFormation runs anytime you create update if you changed the custom resource or delete stacks In other words you can natively extract the API client SDKs to a bucket each time the stacks API definition changes You define a lambda that uses the apigateway and s3 SDK clients to generate the API client zip output and saves it to a bucket You then integrate this into the stack lifecycle events with custom resource logic CloudFormation handles calling the lambda on createupdatedelete and the responses Note the term Custom Resource can be confusing Yes they can help fill in CDKCloudFormations gaps to actually create infrastructure resources using API calls But a CustomResource does not have to create any infrastructure at all They are used for lookups running tests and seeding initial data What you can do is up to your lambda awscdksamples has an example of a custom resource implementation Here is a skeleton of the custom resource plumbing for your use case Custom Resources are fiddly to set up But they are wellsupported in the CDK and an elegant way to keep provisioning logic in the same place as your infrastructure code "
70250753,70244709,"stackoverflow.com",1,"2021-12-06 21:15:46+02","2024-05-17 04:50:42.812456+03","If you are not using a monolambda API pattern then I suggest grouping files by function which in this case is the first option you listed along with specifying individual function packaging with in your serverless yml template This ensures that only the code needed by a specific REST resource is included in that function If you want to learn more about the tradeoffs and differences of a monolambda API vs a singlefunction API you can read my post "
70371530,70297377,"stackoverflow.com",1,"2021-12-16 01:26:22+02","2024-05-17 04:50:44.694974+03","The way your events http is configured looks wrong Try replacing it with This might be helpful as well httpsgithub comserverlessexamples I also found this blog useful Build a serverless API with Amazon Lambda and API Gateway"
70372041,70297377,"stackoverflow.com",0,"2021-12-16 02:47:33+02","2024-05-17 04:50:44.695974+03","The way your event is configured looks right Just add a forward slash to your endpoint GET rlambdahello to suppress the warning is not related to the issue at all I tested your serverless yml with the hello world template without the Docker image and it generated the APIs accordingly on the first run but did not output API endpoints on subsequent builds Please make sure you are in the correct AWS Region euwest1 Additionally you can access AWS CloudFormation Stacks rlambdadev Resources to locate AWSApiGatewayRestApi AWSApiGatewayResource AWSApiGatewayMethod and AWSApiGatewayDeployment "
70380369,70297377,"stackoverflow.com",0,"2021-12-16 16:06:12+02","2024-05-17 04:50:44.6983+03"," wanted to post this as a comment but comments do not allow code blocks Your code looks fine I tried a similar setup Which outputs On which version of serverless are you Have you tried using the latest version"
70307988,70298147,"stackoverflow.com",2,"2021-12-10 18:59:40+02","2024-05-17 04:50:45.65819+03","To seed DynamoDB data postcreate with the Serverless Framework you need two things Which triggering option is best depends on your use case There are several options here are two PutItem writes a single record optionally with a condition to prevent overwriting BatchWriteItem puts several items at once but without a conditional check The serverlessdynamodbseed plugin warns about possible overwrites because it calls BatchWriteItem under the hood "
77161378,70302107,"stackoverflow.com",2,"2023-09-23 04:48:49+03","2024-05-17 04:50:46.745299+03","This is a weird issue which I also faced when moving to Lambda layers To fix this as a workaround This steps may look weird but it worked for me"
70305710,70304661,"stackoverflow.com",2,"2021-12-10 16:05:38+02","2024-05-17 04:50:47.722591+03","No you cannot Because the wildcard asterisk character is a valid character that can be used in object key names Amazon S3 literally interprets the asterisk as a prefix or suffix filter You cannot use the wildcard character to represent multiple characters for the prefix or suffix object key name filter You have to register all prefix value strings to the S3 event For you case it will look like this"
69475552,69386274,"stackoverflow.com",3,"2021-10-07 07:38:50+03","2024-05-17 04:50:51.296606+03","Together with the very helpful support from the serverless team I was able to resolve the issue The reason for the issue actually was an outdated serverless version which I was not aware of Usually I use yarn upgrade to upgrade all of my packages regularly But in this case there was a dependency which forced serverless to stay on an outdated version Only after proactively forcing serverless to upgrade to the latest version using yarn upgrade [email protected] and then afterwards calling once more yarn upgrade to also make the dependent packages based upon the new serverless version I was able to upgrade serverless and its dependencies After that I deployed my services again and the error described above disappeared when calling my functions remotely I hope this also will help someone else who comes across this issue "
69395329,69393069,"stackoverflow.com",1,"2021-09-30 18:52:19+03","2024-05-17 04:50:52.397794+03","In your API Gateway Response resource declared as CloudFormation you have misindented the ResponseType attribute It should be inside the Properties block The error is The CloudFormation template is invalid Invalid template resource property ResponseType which tipped me off This should work You can review the rest of the documentation for this here"
70862679,69393069,"stackoverflow.com",0,"2022-01-26 13:24:53+02","2024-05-17 04:50:52.399794+03","Actual error is This means you tried to delete your lambda function or the entire application and it failed at some point maybe you had a nonempty bucket or something like that you can check this in the console under the CloudFormation section search for DELETE_FAILED event for this lambda and it will tell you what is wrong Serverless will not deploy if application is in this state so you have no option but to find what made it fail Once you solve it you will be able to redeploy "
69417144,69407838,"stackoverflow.com",0,"2021-10-02 16:01:17+03","2024-05-17 04:50:52.954547+03","Unfortunately it is simply not supported by both of those Consider creating a PR to add this feature to the plugin or Serverless Framework itself I believe adding it to Serverless Framework would be great as this feature is really useful and also the plugin might be abandoned soon since Serverless Framework provides almost the same features with native useDotEnv true switch If you need this feature quickly you could always clone the plugin repository add the feature and then use the plugin from your own copy You can find out more details in the documentation search for httpswww serverless comframeworkdocsprovidersawsguideplugins"
69420786,69407838,"stackoverflow.com",0,"2021-10-03 01:41:27+03","2024-05-17 04:50:52.956546+03","Could you name each env file a combination of envregion and then use that Ex Then you could deploy with NODE_ENVuseast1 prod which would give you region env specific dotenv files "
72795120,69416876,"stackoverflow.com",0,"2022-06-29 05:26:08+03","2024-05-17 04:50:53.865452+03","I am in the same place as you Here I would like to leave you some repos links to see if they can help you achieve what you need As far as I know the Cognito part is not too much complicated But for the RDS Mysql database you need to play with VPC subnets and security groups Wich I am learning also httpsgithub comtux86nestjsserverlessboilerplate httpsgithub comrevstarconsultingserverlessawsnodejsandrds httpsgithub comxeusteerapatecommlyapi httpsgithub comAnomalyInnovationsserverlesstypescriptstarter httpsgithub comandrenbrandaoserverlesstypescriptboilerplate I hope some of this links or maybe the fussion between them and their resources Help you achive you goal I am playing with them and so far it is been an amazing learning journey besides I could not found yet a proper AWS RDS Mysql configuration Edit for one post I liked and I took as an example also httpsmontecha comblogcreatinganrdsproxyusingserverlessframework"
69424606,69423637,"stackoverflow.com",2,"2021-10-03 14:55:03+03","2024-05-17 04:50:54.822024+03","Somewhere somehow an IAM policy got put into effect explicitly denying you or perhaps anyone the ability to delete this lambda If you have complete control over your account log in with your root user and check what policies are on that lambda Theoretically your root user can also delete it but if it cannot then open up a support ticket to have someone with AWS Root access help you delete it I have done this in the past and bricked an S3 bucket so it could not even access itself "
69530480,69423637,"stackoverflow.com",1,"2021-10-11 20:48:11+03","2024-05-17 04:50:54.82402+03","Same happened to me In my case my account was hacked I got it back talking to AWS support but then it was found suspicious and got blocked automated deployments with IAM roles were failing manually deleting lambda functions with root user account was not allowed so I contacted AWS support again told them that I have got my account back and secured with new login info and asked them to unblock my activities couple hours later everything got back to normal"
70925318,69435456,"stackoverflow.com",1,"2022-01-31 13:37:56+02","2024-05-17 04:50:56.913714+03","I resolved this by creating ensuring that the webpack output has a libraryTarget of commonjs2 Create or edit a webpack config js file in the root of the project with the following content ensuring that we set the libraryTarget to commonjs2 In my serverless yml I have the following defined for my lambda function Please note that main js handler should be changed if your entrypointfunction is different For example if it was entrypoint this string would be distappsexamplemain js entrypoint"
72192067,69435456,"stackoverflow.com",0,"2022-05-10 23:13:49+03","2024-05-17 04:50:56.915339+03","I found the solution for the same problem but with serverlessplugintypescript with nx workspace there is all about tsconfig with module commonjs and then I see in build folder"
76371810,69435456,"stackoverflow.com",0,"2023-05-31 12:02:54+03","2024-05-17 04:50:56.916336+03","Since a lot of people are having the same issue and I could not find a good solution here is a solution that works Simply replace the exports handler with module exports handler in the compiled main js file This is how you simply do it using sed in your build process inside package json Note that there is specific command for MacOS since it has a different sed implementation for some reason "
69446793,69446696,"stackoverflow.com",1,"2021-10-05 11:01:33+03","2024-05-17 04:50:59.722335+03","Typically request validation is only really useful for POST requests that send a body formatted in sme way such as JSON A GET request typically just passes and id within the URL The path property as a part of the serverless yml configuration woudl validate the path and id value on its own with no additional work necessary as there is no body to validate An example of a configuration I mean In this case if there is any path other than getforany with a value of some type to match param at the end as well it will not trigger the Lambda so it is fully validated already"
69503285,69461997,"stackoverflow.com",2,"2021-10-09 05:27:16+03","2024-05-17 04:51:00.672953+03","I solved it using this using awsactionsconfigureawscredentials github actions as it sets temporary access key and id to environment Hence no need of creating aws programmticv keys from here on Note latest update of github OIDC has changed its domain name httpstoken actions githubusercontent com"
69468350,69463434,"stackoverflow.com",0,"2021-10-06 18:15:11+03","2024-05-17 04:51:01.802261+03","In effect you have created a circular dependency Stage is special because it is needed to identify which env file to load envstage is being resolved from stage env but Serverless needs to know what stage is in order to find stage env etc This is why it is evaluated first Stage and region actually are both optional CLI parameters In your serverless yml file what you are setting is a default with the CLI parameter overriding it where different Example Running serverless deploy stage prod region uswest2 will result in prod and uswest2 being used for stage and region respectively for that deployment I would suggest removing any variable interpolation for stage and instead setting a default and overriding via CLI when needed Then dotenv will know which environment file to use and complete the rest of the template "
69587368,69468888,"stackoverflow.com",0,"2021-10-15 18:47:59+03","2024-05-17 04:51:02.719181+03","When you do not make a custom setting for Postgresql by default the sql queries should be like this There are two options or you should specify the table names in all your laravel model files as below or you should connect to rds postgresql via terminal and set the following postgresql schemas"
69498962,69497431,"stackoverflow.com",0,"2021-10-08 19:21:43+03","2024-05-17 04:51:04.464382+03","Related issue"
68932299,68862938,"stackoverflow.com",1,"2021-08-26 06:53:07+03","2024-05-17 04:51:06.508113+03","If you want to run it by entering a command like below you must first install serverless and the serverless plugins you use globally for example and run"
68932266,68891382,"stackoverflow.com",2,"2021-08-26 06:47:35+03","2024-05-17 04:51:07.499406+03","You may be getting this error if you execlude the directory where the layer is running through the plugins you use or in another part of the yml file Maybe editing it this way will solve the problem this link has an explanation about it"
68936294,68935560,"stackoverflow.com",15,"2021-08-26 12:51:32+03","2024-05-17 04:51:08.975253+03","Its not possible to rename a stack You have to delete it and create new one with the name you want "
74686352,68935560,"stackoverflow.com",5,"2022-12-05 12:00:37+02","2024-05-17 04:51:08.976253+03","No longer the case as of 2022 it looks as though you can retain stack resources whilst deleting the stack containing then and import those resources into a new stack tldr I have not done this just reading the docs httpsaws amazon comblogsinfrastructureandautomationkeepyourawsresourceswhenyourenameanawscloudformationstack"
69004299,68935560,"stackoverflow.com",1,"2021-08-31 22:21:45+03","2024-05-17 04:51:08.977327+03","For most resources changing the logical name of a resource is equivalent to deleting that resource and replacing it with a new one Any other resources that depend on the renamed resource also need to be updated and might cause to be replaced Other resources require you to update a property not just the logical name in order to initiate an update So Logically you have to delete the stack and create a new one as you want You can check out this link AWS Official Doc"
76878809,68935560,"stackoverflow.com",0,"2023-08-10 22:33:54+03","2024-05-17 04:51:08.978872+03","Note AWS CloudFormation only supports one level of nesting using resource import This means that you cannot import a stack into a child stack or import a stack that has children httpsdocs aws amazon comAWSCloudFormationlatestUserGuideresourceimport html"
72736261,68941817,"stackoverflow.com",0,"2022-06-23 23:33:35+03","2024-05-17 04:51:10.101241+03","Ended up writing a plugin that updates version on sls deploy httpswww npmjs compackageserverlessssmversiontracker"
68951337,68950864,"stackoverflow.com",1,"2021-08-27 12:46:16+03","2024-05-17 04:51:11.055062+03","I cannot say I understand the question completely but I will give it a shot I assume you mean that a single microservice is a separate API with it is own subdomain e g service1 yourdomain com service2 yourdomain com etc And you try to locally test this on your machine using serverless offline While I do not know how that would work on subdomain level there is a path based option it seems As mentioned here there is also a plugin that internally routes requests based on their path as well It seems to basically put a proxy in front of the other apis and forwards to the correct port httpsgithub comedisslsmultigateways Full medium article here httpsaws plainenglish iorunmultipleserverlessapplicationsd8b38ef04f37 And having said that it is always possible to set up a proxy yourself using docker that forwards the requests to services running on different ports based on hostname or path "
68953592,68950864,"stackoverflow.com",1,"2021-08-30 12:42:20+03","2024-05-17 04:51:11.057481+03","The slsmultigateways package runs multiple API gateways If you have multiple services and want to run than locally at the same time you can add the package But that is not a complete solution as ultimately you probably want the backend to be accessible on a single host That means you are adding a dependency and it gets you halfway When you try to run multiple gateways locally without this package you get error stating that the port 3002 is already being used That is because the serverless offline plugin has 3002 assigned as the default port for lambda functions Since we are trying to run multiple services the first service will hog the 3002 and the rest will fail to start To fix this you have to tell serverless offline which ports it should use for deploying lambda functions for each service by specifying the lambdaPort in serverless yml files for your services This can be done like So for each service the port will be 400n and the lambda port will be 4n00 This pattern is safe if you have less than 100 lambda functions in your service Looks like you just need to assign a port to support manual lambda invocations Now you can run all the services parallelly using concurrently We are now where we would be with slsmultigateways Next what we need is a proxy I used the createHttpProxy middleware with express But you can setup any depending on your project Here is what the code for proxy service looks like"
68971697,68955812,"stackoverflow.com",4,"2021-08-29 12:48:01+03","2024-05-17 04:51:12.109519+03","Found The problem was due to conflict with Serverless framework As described in httpsdocs sentry ioplatformsjavascriptguidesnextjsmanualsetup I just added useServerlessTraceTarget true to serverless yml "
68957517,68955812,"stackoverflow.com",1,"2021-08-27 20:45:21+03","2024-05-17 04:51:12.111185+03","Check the next config js file in the project root folder According to this guide some changes are needed there and maybe the wizard got it wrong httpsdocs sentry ioplatformsjavascriptguidesnextjsmanualsetup Post the file contents here for more help if needed "
69343922,68985606,"stackoverflow.com",3,"2022-08-16 10:32:37+03","2024-05-17 04:51:13.241467+03","Without knowing the details from the CloudWatch logs 20210830[LATEST]4f4f9ec564544ebb979576ed1b6b2879 RequestId 317ecd1cb699479980605168e1947e3c I would say that in your AWS account there is already a S3 event with the prefix selfcustom path_prefix and the suffix json on that bucket It may be that it is there because of a previous or other serverless stack or that somebody created it manually and has not deleted it Can you please check through the AWS console that there are no other events on S3 with the same configuration"
69008001,69007365,"stackoverflow.com",1,"2021-09-01 07:58:00+03","2024-05-17 04:51:14.3237+03","Yes they are abstraction however serverless allows you to granularize more "
69013596,69012620,"stackoverflow.com",0,"2021-09-01 17:49:51+03","2024-05-17 04:51:15.165479+03","When you use serverless framework cli to build your stack a new folder get created serverless with generated Cloudformation tamplates try to run this CLI command or both works The Cloudformation templates will be in JSON "
69106733,69043591,"stackoverflow.com",0,"2021-09-08 19:31:01+03","2024-05-17 04:51:15.729546+03","You can specify custom endpoints when using aws services httpsdocs aws amazon comsdkforjavascriptv2developerguidespecifyingendpoints html"
69152751,69043591,"stackoverflow.com",0,"2021-09-12 18:27:17+03","2024-05-17 04:51:15.73075+03","The understanding I have here is different the endpoints or endpointFile configurations in serverless yml file are to inform Serverless to connect and deploylook for resources such as API Gateway S3 SNS SQS DynamoDB CloudWatch Event CloudWatch Log CloudFront IAM in your localstack List of AWS resources serverless deals with This configuration is not related to the code written inside the Lambda deployed by serverless If your lambda is accessing secrets from secret manager with get_secret Python SDK after making a connection to secret manager service you can define a custom endpoint to your localstack and get secrets from there Similarly if you are trying to connect to any resource such as S3 or SSM in your code written inside the lambda it will connect to default AWS based endpoint until you mention a custom endpoint while making connection AWS SDKs have a default behaviour to connect to AWS endpoints Serverless will consider your endpoint configurations while deployments You can have a localstack S3 bucket based trigger to your local lambda setup with localstack endpoint configurations I created a basic deployment on my local and my configurations are dockercompose yml mostly as yours You can create some secrets on your localstack and try to list them with lambda You can install awslocal for ease handler py serverless yml localstack_endpoints you can mention these under endpoints too "
67260505,67259949,"stackoverflow.com",0,"2021-04-26 06:31:24+03","2024-05-17 04:51:53.469817+03","these days iamRoleStatements are deprecated instead use I am docs for reference example fine grained policies for writing to bucket which needs listing and put permission without join"
69956681,69043735,"stackoverflow.com",0,"2022-03-28 21:23:03+03","2024-05-17 04:51:16.767819+03","Middy does not have a core middleware for routing because this pattern goes against security best practices ie least privilege In your case the GET user endpoint would have permission to add a user if there was such a vulnerability I would recommend having a lambda for each endpoint Each endpoint could import from the same file This ensures each endpoint can have its own IAM role and input validation However if you must use the same lambda you could setup API Gateway to have different path point to the same lambda Then you could use event requestContext to create a simple router to meet your needs Edit As of v3 Middy now contains middyhttprouter which supports routing within a lambda This is to cover the ALB use case "
69073565,69050618,"stackoverflow.com",1,"2021-09-06 14:09:53+03","2024-05-17 04:51:17.626988+03","you can create the acl via the aws console or using cloudformation in both cases to associate it in your project you can use serverlessassociatewaf in the resources section you could also declare the acl cloudformation and associate it to the api gateway generated by serverless using the AWSWAFv2WebACLAssociation resource type if you decide to declare the acl cloudformation in the resources section you can use the stage variable e g devprod to decide when to associate it with your api resource"
69082269,69066223,"stackoverflow.com",2,"2021-09-07 07:02:27+03","2024-05-17 04:51:18.682071+03","This can be achieved using the serverless ifelse plugin httpswww serverless compluginsserverlesspluginifelse You can use the plugin by adding them to your plugin section of the serverless yml and set up conditions to update values in the serverless yml for the functions and exclude them The include option is not available so your condition would be something like "
69073683,69066223,"stackoverflow.com",1,"2021-09-06 14:19:28+03","2024-05-17 04:51:18.684073+03","if you check the serverless yml reference there is no support for conditions key in the lambda Serverless Framework definitions ARE NOT a 11 to CloudFormation you can override the AWS CloudFormation resource generated by Serverless to apply your own options link here which more or less would look like this make sure to double check the name generated to your function the above StartXtractUniversalInstanceFunction could be wrong"
68276272,68275083,"stackoverflow.com",0,"2021-07-07 20:28:06+03","2024-05-17 04:51:22.200903+03","Figured it out First How to use cdk output variables in serverless yml Export them into a file and in serverless yml you can reference it like so Second serverless is setup such that you have to pass userPoolName not userPoolId So I had to generate userpool name and output it Third to avoid AccessDeniedException when calling lambda as a trigger you need to add the following to your resources"
68613032,68276674,"stackoverflow.com",29,"2023-03-30 16:32:49+03","2024-05-17 04:51:22.220907+03","Vercel imposes some limits when using their platform This includes a serverless function execution timeout which is basically the amount of time that a serverless function is allowed to process an HTTP request before it must respond If you need a longer execution timeout you can try using Edge functions They have to return a response in 30 seconds but can stream indefinitely Recently they have silently reduced the serverless function execution timeout from 10s for the hobby plan 60s for the pro plan and 1000s for the enterprise plan to respectively 5s 15s and 30s This might be the reason why you are experiencing this error now I really did not like the fact that they lowered these limits without warning their users I was not warned at least and giving them time to adjust "
73637012,68276674,"stackoverflow.com",8,"2022-09-07 17:14:14+03","2024-05-17 04:51:22.223908+03","As of 7 September 2022 the Hobby plan on Vercel supports Serverless Function Execution Timeout for 10 seconds Pro for 60 seconds Enterprise for 900 seconds Here is a link to their pricing plans httpsvercel compricing"
77370476,68276674,"stackoverflow.com",4,"2023-10-27 00:51:48+03","2024-05-17 04:51:22.224908+03","For anyone landing here from Google you can now configure Vercel Functions to run for up to 5 minutes httpsvercel comchangelogserverlessfunctionscannowrunupto5minutes"
76445894,68276674,"stackoverflow.com",3,"2023-06-10 13:50:40+03","2024-05-17 04:51:22.226216+03","Check if setting region using vercel json will help Deployment regions httpsvercel comdocsconceptsedgenetworkregions"
68332330,68283172,"stackoverflow.com",1,"2021-07-11 02:47:06+03","2024-05-17 04:51:23.056961+03","You can reference ID of you account using awsaccountId This is described in Referencing AWSspecific variables part of Serverless Variables documentation When resolving variables with JavaScript function it receives an object as a parameter with a function called resolveVariable which can be used to resolve provided variable string See here Looking at your code I suspect you are using deprecated resolver not supporting this feature You must upgrade by declaring variablesResolutionMode 20210326 in service section of your YAML Function may also access configuration properties through variable resolution process Full example"
68348883,68287975,"stackoverflow.com",1,"2021-10-07 00:15:34+03","2024-05-17 04:51:23.522207+03","I found the answer to this you can call AWS Dynamodb List streams with the table name link to doc API streams ListStream List Stream method gives ARNs and then we can configure those ARNs in serverless config Since I wanted this to happen every time on serverless deploy I wrote a plugin that does that I am now able to fetch and attach ARN Other properties are attached using existing serverless parameters In case you do not want to implement your own script here is an npm extension I made for my project link to plugin serverlessdynamodbstreamarnplugin"
68328323,68324195,"stackoverflow.com",0,"2021-07-10 16:48:41+03","2024-05-17 04:51:24.272202+03","are you sure the lambda parser unable to parse the image if it really is not able then i would check the logs maybe you need more memory or a bigger timeout you can find how to do it here httpswww serverless comframeworkdocsprovidersawsguidefunctions if you just cannot upload it from the lambda to the s3 bucket i would guess that you need to add IAM policies to give the lambda an authorization to upload an image to the s3 httpswww serverless comframeworkdocsprovidersawsguideiam "
68347567,68344335,"stackoverflow.com",0,"2021-07-12 15:42:32+03","2024-05-17 04:51:25.356926+03","Worked after reducing the imported libraries and adding layers to the the function"
68346984,68346022,"stackoverflow.com",1,"2021-07-12 14:56:45+03","2024-05-17 04:51:26.430251+03","You have a typo in your Schedule value You used corn instead of cron "
68380959,68373955,"stackoverflow.com",0,"2021-07-14 18:28:46+03","2024-05-17 04:51:28.372869+03","The error message indicates that you requested POST devv1authn however the only routes in your app are POST v1authn Remove dev from your request and it should work "
68386217,68384015,"stackoverflow.com",1,"2021-07-15 02:41:39+03","2024-05-17 04:51:29.433103+03","There is no one best directory structure In general what I would recommend is Beyond that you can read more of my thoughts on serverless at team scale and on setting up your project so you can develop against the cloud"
68418242,68418116,"stackoverflow.com",1,"2021-07-17 10:20:19+03","2024-05-17 04:51:30.536885+03","So when I change something in the git repository I want all the frontends to rebuild Yes This is called CICD Can I handle this on one server Yes You might want to use Containerization to deploy similar to multiple virtual server in a single server One of the containerization is Docker are there any other services like serverless services to manage multiple applications After you understand Docker or Containerization you might want to orchestrate multiple containers then you can use a tool such as Kubernetes if you want serverless this article might help you CMIIW "
75538937,65991929,"stackoverflow.com",0,"2023-02-23 00:50:09+02","2024-05-17 04:52:50.268367+03","If you are using Chromium you can load the font with chromium font url as these fonts are available in the header and footer This avoids the complexity of installing the font in the Lambda architecture Example"
68433667,68424460,"stackoverflow.com",1,"2021-07-19 01:58:40+03","2024-05-17 04:51:31.548138+03","I am confused with your explanation of your existing system sorry but the general approach would be one of the following Using Cognito Your backend can use Cognito to authenticate the user and then use AssumeRoleWithWebIdentity to return a set of credentials The users client can then use those credentials to directly access AWS services based on the the assigned permissions For example they might be permitted to access their own subdirectory in an Amazon S3 bucket or read from a specific DynamoDB table This can be done by sending requests directly to AWS rather than going via the backend Using presigned URLs If your goal is purely to grant access to private objects in Amazon S3 then instead of using Cognito your backend can generate Amazon S3 presigned URLs that provide timelimited access to private objects Whenever the backend is generating a page that contains a reference to a private object eg via img src tags it can do the following The benefit of this approach is that the app can determine finegrained access to individual objects rather than simply using buckets and prefixes to define access This can be very useful in situations where data is shared between users eg a photosharing app where users can share photos with other users on a perobject basis Do not mix In looking through your code samples it appears that your Cognito roles are granting access to specific parts of an S3 bucket The clients can then use their Cognitorelated credentials to directly access that part of the bucket There is no need to generate presigned URLs "
68429538,68428955,"stackoverflow.com",2,"2021-07-18 16:25:34+03","2024-05-17 04:51:32.997877+03","You can use copywebpackplugin to copy the views folder to the destination directory In your webpack configuration file webpack config js And also update serverless yml file to include views directory to your lambda function"
68430101,68428955,"stackoverflow.com",0,"2021-07-18 17:34:09+03","2024-05-17 04:51:32.999878+03","The Fix I found a quick solution using the path module "
68438331,68430197,"stackoverflow.com",0,"2021-07-19 12:41:14+03","2024-05-17 04:51:33.539016+03","The Fix Replacing bodyJSON stringify template params with bodytemplate params got rid of the \r\n\r\n\n\r But the css files and images are still not working in the web page How do I configure the CSS and the images directories to load into the web page handler js The ui js looks something like this ui js And the index view engine like this index js"
68434109,68431384,"stackoverflow.com",2,"2021-07-19 03:48:44+03","2024-05-17 04:51:34.580974+03","You can use the following"
67779674,67748123,"stackoverflow.com",0,"2021-05-31 23:05:40+03","2024-05-17 04:51:37.840692+03","Events define what will be the trigger for function a request to an HTTP endpoint a new message in SNS etc Resource policy allows to control what can or cannot invoke HTTP endpoint essentially a policy in API Gateway In serverless js to define a function that triggered by a new log messages in CW Log Group all you need is this"
68009295,67757943,"stackoverflow.com",0,"2021-06-16 22:56:31+03","2024-05-17 04:51:38.930108+03","I finally achieved it using ALB and configuring the route 53 to the ALB and custom domain for adding the host while serving requests The below blog was very helpful httpswww sentiatechblog comconnectingtoaprivateapigatewayovervpnorvpcpeering"
67768091,67766166,"stackoverflow.com",1,"2021-05-31 07:57:18+03","2024-05-17 04:51:39.832016+03","You are right the link in the output directly points to AWS I would suggest taking a look at the localstack logs as those will contain more detailed information about why the deployment failed You can set DEBUG1 in the environment for troubleshooting issues Additionally you can display more logs from the serverless deploy command by setting SLS_DEBUG in your environment but those were often not very helpful for me "
67792581,67777524,"stackoverflow.com",2,"2021-06-01 19:23:42+03","2024-05-17 04:51:40.437501+03","The problema was that i was nesting the 3 levels down the code DB ssmawsreferencesecretsmanagerqaauroratrue As soon as i change it to the same level of the environment attribute it worked"
67883169,67780269,"stackoverflow.com",0,"2021-06-08 10:26:01+03","2024-05-17 04:51:41.337761+03","I think you can try to use the second option Use a custom domain with localstack serverless and docker Use virtual host like Nginx and add a custom domain in it and use it for your API You can add the nginx virtual host file in the docker image and can handover the same And add environment variables for your collection which will include the API endpoint as well "
67823311,67818658,"stackoverflow.com",1,"2021-06-03 17:25:45+03","2024-05-17 04:51:44.723887+03","I suspect the bug is an edge case or encoding issue with your multipart handler or perhaps a bug with the serverlessoffline plugin But that is somewhat irrelevant The reason it is hard to find a solutions to this problem is that your chosen path is not compatible with the serverless development philosophy Lambda does not have any filesystem space within its execution environment well a few MB But once the lambda execution is finished that space is freed You would need to store these images somewhere else someplace durable like S3 It is best practice instead to create an API endpoint which returns an presigned URL for an S3 bucket Then your users could upload the image to that URL Heres an excellent article which lays out this exact use case httpsserverlessfirst comserverlessphotouploadapi"
68087205,67841803,"stackoverflow.com",0,"2021-06-22 19:14:27+03","2024-05-17 04:51:45.9477+03","I fixed that problem using this plugin httpswww serverless compluginsserverlessdeploymentbucket"
69107092,67841803,"stackoverflow.com",0,"2021-09-08 20:01:20+03","2024-05-17 04:51:45.9497+03","You need to make some adjustments in your files Update your dockercompose yml use the reference docker compose from localstack you can check it here Use a template that works correctly AWS docs page have several examples you can check it here Run it with next command aws cloudformation createstack endpointurl httplocalhost4566 stackname samplestack templatebody filelambda yml profile dev You can also run localstack using Python with next commands"
67865133,67851539,"stackoverflow.com",0,"2021-06-07 04:55:41+03","2024-05-17 04:51:48.10456+03","npm install is not ran automatically with the Serverless Framework AWS provides a minimal node environment but the only the awssdk is included You will need to run npm install before deploying "
67864386,67864335,"stackoverflow.com",2,"2021-06-07 02:14:12+03","2024-05-17 04:51:49.455617+03","I have used before the API Gateway Resource Policy httpswww serverless comframeworkdocsprovidersawseventsapigatewayresourcepolicy For the lambda function association directly you can take a look at that thread httpsgithub comserverlessserverlessissues4926"
69992362,67864335,"stackoverflow.com",0,"2021-11-20 22:42:45+02","2024-05-17 04:51:49.456998+03","An example serverless yaml would look like this How to restrict access to a lambda Please note that the resource policy currently only works for the REST API Gateways httpsdocs aws amazon comapigatewaylatestdeveloperguidehttpapivsrest html HTTP APIs do not support resource policies "
67876710,67876387,"stackoverflow.com",1,"2021-06-07 21:09:15+03","2024-05-17 04:51:49.62453+03","This worked for me"
67890724,67880044,"stackoverflow.com",0,"2021-06-08 19:10:43+03","2024-05-17 04:51:50.755406+03","If you have verified that you are using the latest version of the Serverless Framework and the Serverless Azure plugin please open an issue in the github repository"
67905556,67904808,"stackoverflow.com",11,"2021-06-09 16:54:45+03","2024-05-17 04:51:51.375957+03","You should set the deprecation variables in your serverless yml file and verify that serverless deploy is successful and the framework interpolates your variables as you intend The warning messages explain the process Simply add the rules to your serverless yml file The second warning message will be an error provider profile was not able to be resolved You can solve this with a conditional ie Or you can ensure that provider profile is always set It is not possible to help further without seeing the serverless yml file "
68818066,67278136,"stackoverflow.com",5,"2021-08-17 16:34:32+03","2024-05-17 04:51:54.652665+03","This is a fairly simple trick by using a cascading value variable The first value is the one you want the second one being a default or fallback value Also called cascading variables This above with stage set to dev will default to other value of 10 but if you set stage via serverless deploy stage live then it will use the live value of 100 See here for more details httpswww serverless comframeworkdocsprovidersawsguidevariablessyntax You can use an js include and put your conditional logic there It is called asynchronous value support Basically this allows you to put logic in a javascript file which you include and it can return different values depending on various things like what AWS account you are on or if certain variables are set or whatever Basically it allows you to do this Which works if you create a javascript file in this folder called detect_env js and it has the contents similar to For more info see httpswww serverless comframeworkdocsprovidersawsguidevariableswithanewvariablesresolver I felt I had to reply here even though this was asked months ago because none of the answers were even remotely close to the right answer and I really felt sorry for the author or anyone who lands here "
67278337,67278136,"stackoverflow.com",1,"2021-04-28 09:42:08+03","2024-05-17 04:51:54.654483+03","For really sticky problems I find it is useful to go to the Cloudformation script instead and use the Cloudformation Intrinsic Functions For this case if you know all the environments you could use FnFindInMap httpsdocs aws amazon comAWSCloudFormationlatestUserGuideintrinsicfunctionreferencefindinmap html Or if it is JUST production which needs 0 then you could use the conditional FnIf and a boolean Condition test in the Cloudformation template to test if environment equals production use 0 else use the templated value from SLS Potential SLS You can explicitly remove the ProvisionedConcurrency property as well if you want Edit You can still use SLS to deploy it simply compiles into a Cloudformation JSON template which you can explicitly modify with the SLS resources field "
67280625,67278136,"stackoverflow.com",0,"2021-04-27 12:41:21+03","2024-05-17 04:51:54.656629+03","The Serverless Framework provides a really useful dashboard tool with a feature called Parameters Essentially what it lets you do is connect your service to it then you can set different values for different stages and then use those values in your serverless yml with syntax like paramVARAIBLE_NANE_HERE and it gets replaced at deploy time with the right value for whatever stage you are currently deploying Super handy There are also a bunch of other features in the dashboard such as monitoring and troubleshooting You can find out more about Parameters at the official documentation here httpswww serverless comframeworkdocsguidesparameters And how to get started with the dashboard here httpswww serverless comframeworkdocsguidesdashboardenablingthedashboardonexistingserverlessframeworkservices"
67868889,67278136,"stackoverflow.com",0,"2021-06-07 12:10:36+03","2024-05-17 04:51:54.658536+03","Just using a variable with a null value for dev environments during on deploypackage and SLS will skip this property"
67479367,67281837,"stackoverflow.com",0,"2021-05-11 04:12:57+03","2024-05-17 04:51:55.70011+03","Thanks for using serverlsss3local I am the author of serverlesss3local How did you add a file or change the context of the file Did you use the AWS command as following If you do not use the aws command and apply these operations to the files directory these modifications are not detected by S3rver which is the local S3 emurator resize_image example may be useful for you "
67297357,67282392,"stackoverflow.com",1,"2021-04-28 12:20:18+03","2024-05-17 04:51:56.936166+03","It is 2 different products and no you cannot tie them and deploy in the same time So no link between the product and not the same deployment duration The right pattern here is to use versioning You can deploy a service before the others Cloud Functions before API Gateway for minor change does not break the existing behavior For breaking change I recommend you to not update the existing functions but to create a new one The advantage is to have the capacity to continue to have the 2 versions in parallel and a rapid rollback in case of issue Same thing for API Gateway create a new gateway for a new version "
67286101,67285814,"stackoverflow.com",2,"2021-04-27 18:29:05+03","2024-05-17 04:51:57.063092+03","There is no drawback the hash key decides the partition the data will live on and the partitions are designed to handle up to 3k RCU and 1k WCU or up to 10GB of data Performance guarantees are based on them delivering that so it should not matter Having unique partition keys may actually help you with scaling later down the line as chances are that requests can be spread out more evenly since there is no query operation that works on multiple items in an item collection "
67286093,67285814,"stackoverflow.com",2,"2021-04-27 18:28:38+03","2024-05-17 04:51:57.065093+03","would it cause too many partitions There is no such thing in Dynamo DB would it increase the time to seek data in my table using queries You cannot Query a DDB table unless it has a range key With a only a hash key you can only use GetItem Scan is also allowed but you really should not be using that regularly You would have to add a Global Secondary Index GSI with a has range key in order to Query your data "
73215925,67291818,"stackoverflow.com",1,"2022-08-03 06:14:20+03","2024-05-17 04:51:57.813525+03","There is not an email solution outofbox but the setup is relatively straightforward using custom flow lambda triggers and amazon SES You can follow this tutorial on the AWS blog httpsaws amazon comblogsmobileextendingamazoncognitowithemailotpfor2fausingamazonses"
67294252,67291818,"stackoverflow.com",3,"2021-04-28 08:31:08+03","2024-05-17 04:51:57.814526+03","No you cannot be using the Amplify You can select preferred mfa type for example Select TOTP as preferred Select SMS as preferred Select nomfa"
67300083,67296831,"stackoverflow.com",1,"2021-04-28 15:16:26+03","2024-05-17 04:51:58.804325+03","This solved it for me process env VIPS_DISC_THRESHOLD 750m"
67366604,67296931,"stackoverflow.com",5,"2021-05-03 12:35:04+03","2024-05-17 04:51:59.803431+03","I ran into the same problem a while ago and I resorted to using the serverlessaddapikey plugin as it was not comprehensible for me when Serverless was creating or reusing new API keys for API Gateway With this plugin your serverless yml would look something like this You can also use a stagespecific configuration"
74475631,67296931,"stackoverflow.com",0,"2022-11-17 14:39:53+02","2024-05-17 04:51:59.805432+03","This worked well for me"
68734484,67313522,"stackoverflow.com",1,"2021-08-11 02:37:21+03","2024-05-17 04:52:00.836003+03","Jest normally puts describe test and other functions into the global context When you start it programmatically it cannot do it so you have to import these functions explicitly before use "
67355050,67319846,"stackoverflow.com",0,"2021-05-02 13:01:26+03","2024-05-17 04:52:01.794762+03","I achieved this by using STS and assuming a temp role inline within Codebuild Documentation found here In short I have added the following You will need to change ACCOUNT_ID ROLE_TO_ASSUME TEMP_NAME and COMMAND to achieve this "
67328252,67326936,"stackoverflow.com",2,"2021-04-30 07:38:38+03","2024-05-17 04:52:02.776902+03","There is a lot of levels of cold start that all add latency The hottest of the hot paths is the container is still running and additional requests can be routed to it The coldest is a brand new node so it has to pull the image start the container register with SD wait for the serverless planes routing stuffs to update probably some more steps if you dig deep enough Some of those can happen in parallel but most cannot If the pod has been shut down because it was not being used and the next run schedules on the same machine then yes kubelet usually skips pulling image unless imagePullPolicy Always is forced somewhere so you get a bit of a faster launch K8s scheduler does not generally optimize for that though "
67447300,67348789,"stackoverflow.com",2,"2021-05-08 15:00:23+03","2024-05-17 04:52:03.878563+03","a basic yml would look like this"
68542103,67365494,"stackoverflow.com",0,"2021-07-27 12:12:29+03","2024-05-17 04:52:04.96723+03","This is was due to geotz library It was creating the unzip size almost more than 255MB for just geotz on my linux environment on AWS this was the main problem So I just uninstall this package and after that My layer deployed correctly "
67375288,67374932,"stackoverflow.com",2,"2021-05-04 00:35:29+03","2024-05-17 04:52:07.197118+03","There are two possible problems "
73125272,67374932,"stackoverflow.com",2,"2022-07-26 17:32:39+03","2024-05-17 04:52:07.198118+03","Enable simple responses Handler should do a callback instead of returning the value "
71388546,67374932,"stackoverflow.com",0,"2022-03-08 01:55:22+02","2024-05-17 04:52:07.198899+03","SLS documents this here You can set properties for your Authorizer response format like this This will allow isAuthorizedtrue false responses"
66778149,66775493,"stackoverflow.com",1,"2021-03-24 11:29:04+02","2024-05-17 04:52:11.08027+03","Short answer yes this is totally doable with Serverless functions and actually a typical Serverless use case Long answer It is not necessary to use AWS SWF or AWS Step Functions here However you could use Step Functions in case your process gets more complicated e g more external services are involved and you need certain error handling or you want to improve parallel processing powers First of all CloudFront is not comparable to AWS SAM or Serverless Framework Did you mean AWS CloudFormation instead CloudFront is a CDN to serve and cache any kind of content whereas CloudFormation is a tool to describe your infrastructure as code CloudFormation is the basis for AWS SAM and Serverless Framework because they both translate their template code to CloudFormation code in the end However CloudFormation makes developing Serverless Functions a bit complicated in my opinion That is why tools like AWS SAM or Serverless Framework popped up at some point AWS SAM is basically an extension of CloudFormation i e it provides additional resource types like AWSServerlessFunction but everything else is CloudFormation Serverless Framework also lets you add CloudFormation resources but has its own syntax for specifying Serverless Functions In terms of costs CloudFormation AWS SAM and Serverless Framework are all free However you can use some premium features of Serverless Framework but you do not have to However CloudFront is not free to use but I believe it was not the service you were looking for Besides that for SQS SES and Lambda you only pay for what you use I personally prefer AWS SAM because you are closer to CloudFormation code and compared to Serverless Framework you do not need a plugin for some things to circumvent the abstractions that the Serverless Framework does for you You will notice this for bigger projects where you are leaving the standard hello world examples On the other side the Serverless Framework is quite popular and hence there are many resources out there to help you Up to you what you prefer In terms of infrastructure tooling you could have a look at AWS CDK a good starting point is cdkworkshop com which is becoming more and more popular For local development you can have a look at Localstack The free version supports emulating SQS and SES locally so that should be helpful "
66781646,66780749,"stackoverflow.com",5,"2021-03-24 15:02:19+02","2024-05-17 04:52:11.96742+03","If the catalog structure that you are using looks like this and the name of the exported handler is handler and lives in handler js file then the correct config will look like this First part is the path to the module and after you reference specific function exported in that module For handler functionshandler cloudfront to work you would have to export cloudfront function in functionshandler js file "
66919568,66790160,"stackoverflow.com",0,"2021-04-02 15:47:32+03","2024-05-17 04:52:13.068617+03","Our implementation on our every lambda functions with http integrations we name our lambda functions like this By using this naming convention we we are able to integration test every environments that we have "
66811064,66810222,"stackoverflow.com",3,"2021-03-26 06:30:02+02","2024-05-17 04:52:13.937409+03","on the readme file in serverlesss3local we have you can achieve the same with boto which means when you run your serverless offline start you need to set the aws access key id to S3RVER and aws secret access key to S3RVER otherwise the real bucket will be used also in the readme there is instructions to setup a s3local aws profile httpsgithub comar90nserverlesss3localtriggeringawseventsoffline another way to achieve it is to run your command with environment variables in that way the awssdk inside your code will read the correct values for the offline mode"
66832364,66830849,"stackoverflow.com",3,"2021-03-27 16:22:38+02","2024-05-17 04:52:14.747663+03","It appears as though you may have a small typo Replacing events with event in the handler process block seems to work for me When running the command with events I receiving a warning from the serverless framework Serverless Configuration warning at functions process events[0] unsupported function event Here is the documentation which helped me find the correct syntax "
66954510,66845303,"stackoverflow.com",8,"2021-04-05 17:20:12+03","2024-05-17 04:52:15.568405+03","The reason for ModuleNotFoundError No module named _brotli is improper dependencies packaging It is fixed by packaging the app via the use of Docker and the dockerlambda image slim true and strip false minimise the package size while preserving binaries wich is required in some cases in this example it does Having solved the _brotli not found issue I encountered next error message pkg_resources DistributionNotFound The flaskcompress distribution was not found and is required by the application I was able to solve it with a workaround described in Pyinstaller executable cannot find flaskcompress distribution that is included Finally the application is served under apistagename test in this example This requires a modification the Dash app configuration namely providing requests_pathname_prefixtestdash in the Dash constructor Full working example app py serverless yml"
69776310,66861646,"stackoverflow.com",8,"2021-10-30 05:16:22+03","2024-05-17 04:52:16.142834+03","httpsaws amazon comsqsfaqs To process exactly once you need to use FIFO queue with dedeplication ID If your throughput requirement is below the limit mentioned above then you are fine with the FIFO queue If not then using DynamoDB as your original plan is also an alternative option But you have to manage a lot of things yourself here with this approach like deleting the message updating if the message is being read but not yet fully processed and so on "
66985280,66861646,"stackoverflow.com",4,"2021-10-04 09:05:27+03","2024-05-17 04:52:16.144834+03","FIFO SQS queues have different rate limits than a regular SQS queue regardless of the use of message group ids SQS Standard queues support a nearly unlimited number of API calls per second per API action SendMessage ReceiveMessage or DeleteMessage FIFO SQS supports 300 TPS for each API method Look at the quota docs here Also AWS has a new feature for higher throughput FIFO SQS queue which might interest you With batching of maximum 10 messages per API call you can handle 3000 messages per second with FIFO queue Regarding making sure you do not handle the same message twice have you had a look at FIFO deduplication ID I am not sure if that is exactly what you need but it sounds pretty similar to your requirement"
66908116,66861646,"stackoverflow.com",0,"2021-04-01 19:15:41+03","2024-05-17 04:52:16.146835+03","SQS delivery guarantee is at least once Your application must be designed to handle processing duplicate messages I would strongly recommend building your application this way If you must process some type of data exactly once you need a strongly consistent system Consider using dynamodb and conditional updates"
66891876,66862626,"stackoverflow.com",1,"2021-03-31 19:42:08+03","2024-05-17 04:52:16.682691+03","Without seeing the serverless yml file here is the best answer I can provide A quick way to verify if your function has the required permissions to log to cloudwatch If your function does not have permissions to log you will see this message If it does not it would be helpful to see the serverless yml file with secrets and sensitive information redacted and then I can help further "
66893095,66892919,"stackoverflow.com",1,"2021-03-31 21:10:47+03","2024-05-17 04:52:18.538673+03","The deploy target automatically executes the package target in the background i e it creates the JAR file automatically and does not use any locally existing JAR Additionally the deploy target also automatically deploys the artifact to S3 so if you want to skip that part add the parameter noDeploy to your command See also httpswww serverless comframeworkdocsprovidersawsclireferencedeploy"
66907133,66906760,"stackoverflow.com",2,"2021-04-01 18:12:42+03","2024-05-17 04:52:20.257318+03","Currently serverlessjs does not support to set cors for all function at once you have to enable cors for each function event In the normal way you just define the cors setting once and apply it for the functions like a variable "
66921032,66913028,"stackoverflow.com",1,"2021-04-02 17:44:53+03","2024-05-17 04:52:20.648075+03","First please include node modules AWS will not install anything into the lambda node env besides the awssdk Secondly you are seeing this error because your likely developing on a mac OS machine so the bycrypt binary from your machine is ending up getting uploaded to lambda Please double check that when you tried bcryptjs you fully removed bcrypt from your project dependencies "
67426603,66922486,"stackoverflow.com",1,"2021-05-07 01:15:27+03","2024-05-17 04:52:21.638385+03","The issue was solved by following steps"
66925681,66922651,"stackoverflow.com",1,"2021-04-03 00:52:27+03","2024-05-17 04:52:22.71407+03","Maybe there is a nicer way but I managed to get this to work by writing a custom webpack loader This answer is specifically for my case where I needed to export details of multiple cognito pools to a single file for use in a custom lambda authorizer but the pattern should work for any scenario and is not necessarily tied to SSM either you could generate the file using any method as the loader is just plain Javascript It brought my Lambda execution times down from 40ms using SSM down to 2ms First I created an example template json file with the structure matching the data I have stored in SSM This could be anywhere but I put it in generatedcognitoConfig json This is useful for documentation and code assist at pointofuse This can then be imported and used within the lambda code ES6 For example I configured a custom webpack loader that runs against this template file I then wrote a webpack loader that searches for all matching SSM parameters and writes the contents to the JSON file The serverless webpack plugin provides access to the underlying serverless object so the current AWS credentials can be accessed For bonus points I also got it to download the signing keys but I did not include that here as I do not want to clutter the answer"
66975828,66930908,"stackoverflow.com",1,"2021-04-06 23:09:40+03","2024-05-17 04:52:23.614151+03","I resolved the problem by creating preSignUp lambda trigger with autoconfirming user Docs AWS httpsdocs aws amazon comcognitolatestdeveloperguideuserpoollambdapresignup html Serverless httpswww serverless comframeworkdocsprovidersawseventscognitouserpool"
66936677,66930908,"stackoverflow.com",0,"2021-04-04 02:14:36+03","2024-05-17 04:52:23.61615+03","I work with different framework Amplify but as the both frameworks use Cognito User pools I guess the issue would be that you are missing domain name In AWS Console check your User Pool then in left panel find App integration and then open and setup Domain Name Again I am just guessing sorry if my advice will not help but I experienced similar problem "
66414262,66413503,"stackoverflow.com",18,"2021-03-01 00:28:45+02","2024-05-17 04:52:27.825295+03","There seems to be a type Handler in awslambda sdk which is generic and can be used for situations like this You can also define your own type based on these two different function types And then use this new type with your lambda function"
66482976,66435435,"stackoverflow.com",1,"2021-03-04 23:02:37+02","2024-05-17 04:52:28.978796+03","I think what is happening is The straightforward solution is to move res sendFile to the bottom of your handler If you want to serve the next page as soon as possible and you cannot afford to wait until the email is sent you can push an event to a queue for example to SQS and then trigger a lambda that sends emails from SQS This gets you into distributed territory though so not always simple "
66439244,66435435,"stackoverflow.com",0,"2021-03-02 14:18:53+02","2024-05-17 04:52:28.979796+03","Your sendMail does not return a Promise then await keyword will not work as your expectation The function will be finished before the sendMail process finish On the local side I guess you use serverlessoffline plugin to test your function Then the function will not finished it just responds Different behavior with Lambda environment If this transporter is an instance of Nodemaillers Transporter then sendMail function is a callback function You have to convert it to a new function that returns a Promise or just wrap it into a new Promise like this"
73430630,66443490,"stackoverflow.com",0,"2022-08-21 02:00:36+03","2024-05-17 04:52:29.84167+03","You can reference the Output values from your current service using the FnImportValue function The serverless system adds sls[service_name] to the variable but you can find them in the Outputs area of the CloudFormation Stack Navigate to CloudFormation Stacks [select your service] Outputs tab From there you will see a column called Exports name Use that Exports name and use that for the import e g you have a WebSocket service and you need the service endpoint If you look in the tab it will have an export slswss[your_service_name][stage]ServiceEndpointWebsocket Thus you can import that into an environment variable"
66475285,66459622,"stackoverflow.com",1,"2021-03-04 14:44:02+02","2024-05-17 04:52:31.630473+03","You can use a CloudFormation condition like FnIf to conditionally create stack resources The CloudFormation documentation about conditions has all the details but somehting like this should get you started Replace the content of my_condition with your condition It is referenced later in the FnIf the example uses the shorthand for FnIf The AWSNoValue is a pseudo parameter which can be used as a return value to remove the corresponding property It should work here to remove the list item but I am not sure about it you will need to test "
66462968,66461385,"stackoverflow.com",2,"2021-03-03 20:39:48+02","2024-05-17 04:52:32.57467+03","From the comments code given in the question is perfect except the Qualifier parameter Qualifier is used to Specify a version or alias to invoke a published version of the function In this case lambda is not versioned Hence we just need to remove qualifier "
66466635,66461385,"stackoverflow.com",0,"2021-03-04 01:52:02+02","2024-05-17 04:52:32.576671+03","Lambda Asynchronous invocation Amazon Simple Storage Service Amazon S3 invoke functions asynchronously to process events When you invoke a function asynchronously you do not wait for a response from the function code You hand off the event to Lambda and Lambda handles the rest In that case I would simply chain the lambdas using AWS lambda destinations Supported destinations Amazon SQS sqsSendMessage Amazon SNS snsPublish Lambda lambdaInvokeFunction EventBridge eventsPutEvents Configuring destinations for asynchronous invocation Introducing AWS Lambda Destinations"
66465531,66464642,"stackoverflow.com",1,"2021-03-03 23:58:40+02","2024-05-17 04:52:33.459468+03","It looks like you are defining an attribute named userId lowercase u and later referring to UserId uppercase U You want this"
66473081,66471200,"stackoverflow.com",0,"2021-03-04 12:23:38+02","2024-05-17 04:52:34.310524+03","In your synchronous code you are using taskDetails where as in async implementation it is taskdetails I am assuming in your get method you are reading data from taskDetails so your get api is not returning taskDetails in response Go to dynamodb console and check for your id you will be able to see data in taskdetails attribute "
66025095,65983931,"stackoverflow.com",1,"2021-02-03 11:55:45+02","2024-05-17 04:52:49.281488+03","If you are using the Serverless Webpack plugin you should be able to get whatever native modules you need installed by using the packagerOptions config for the plugin and specifying the linux platform for x64 architecture along with the list of npm modules to package See the Custom scripts section of the plugins documentation for more info For example if your Lambda function depends on the sharp npm package you would add something like the following to your serverless yml file"
71681761,65991929,"stackoverflow.com",0,"2022-03-30 20:40:04+03","2024-05-17 04:52:50.266367+03","Headers and footers have some limitations regarding external styling If you want to use a custom google font you will have to download the font and install it as a system font and chrome will pick it up automatically from your fontfamily rule Example"
64867520,42612499,"stackoverflow.com",56,"2021-03-16 17:59:24+02","2024-05-17 05:02:38.068424+03","This is now supported natively in Serverless Framework See Pseudo Parameters Reference for the official docs "
66481447,66473923,"stackoverflow.com",1,"2021-03-04 21:04:48+02","2024-05-17 04:52:35.225542+03","If you think about scaling there are multiple potential bottlenecks here which you could address If you want this to scale to a million students per school I would probably change the architecture to something like this You have a Step Function that you invoke when you want to print the certificates This step function has a single Lambda function The Lambda function queries the table across sharded partition keys and writes each student into an SQS queue for certificateprinting tasks If Lambda notices it is close to the runtime limit it returns the LastEvaluatedKey and the step function recognizes thas and starts the function again with this offset The SQS queue can invoke Lambda functions to actually create the certificates possibly in batches This way you decouple query from processing and also have builtin retry logic for failed tasks in the form of the SQSLambda integration You also include the checkpointing for the query across many items Implementing this requires more effort so I would first figure out if a million students per school per year is a realistic number "
66574679,66483030,"stackoverflow.com",2,"2021-03-11 02:05:45+02","2024-05-17 04:52:37.230686+03","AWS Cognito Identity Pools have an option that may work for you First I will quickly cover the differemt Cognito services since it is something that confused me when I was first trying to understand Cognito At the risk of oversimplifying AWS Cognito exists to answer two questions Cognito addresses these concerns with two distinct offerings User Pools authentication and Identity Pools authorization You can think of Cognito User Pools as your applications user directory At a high level User Pools let you handle user registration authentication account recovery and supports authentication with thirdparty identity providers like Facebook Google etc Cognito Identity Pools provides a way to authorize users to use various AWS services You can think of it as a vending machine for handing out AWS credentials For example if you needed to give your users access to upload a file to an S3 bucket or to invoke an endpoint in API Gateway you could do so with an Identity Pool Heres a handy illustration to describe what I have outlined above Cognito lets you support unauthenticated users in your Identity Pool instructions here So how does this apply to your question You can use Cognito to enable IAM authorization across your entire API which allows you to leverage the power of IAM to control access to your API When you enable unauthenticated user access in Cognito guest visitors to your site will be assigned to the unauthenticated IAM user role Users that log in to your site will be assigned to the authenticated IAM user role You can attach IAM policies to these roles that define what each role has access to For example you might want to restrict unauthenticated users to certain API endpoints On the other hand authenticated users might get access to invoke all API endpoints in your application "
66496802,66484530,"stackoverflow.com",1,"2021-03-05 19:14:08+02","2024-05-17 04:52:37.72979+03","There is a lot here but I will take a shot at addressing these questions at a high level If you have followup questions or want a more indepth answer I would suggest posting a different StackOverflow question that focuses on the specifics You are more likely to get more community participation if your question is focused and concise Using callbacks vs returns in lambda functions Please can you describe the difference between these two I have found some resources on this however they will not clear it for me On Amazon Docs and here Stack overflow The difference boils down to asynchronous vs synchronous handlers I do not think I can explain the difference any better than the resources that you have linked However I will say that it comes down to personal preference You can implement your lambdas with either approach it is up to your personal preference Do you like managing callbacks or do you prefer to use asyncawait Why I do not need to set CORS headers when initializing lambdas with serverless as opposed to lambda functions created in the amazon console like here The serverlessstack demo puts the CORS response neatly into a helper method It looks like you are using it here When you call this method in your lambdas you are setting the CORS headers In serverless frameworks what is the difference between functions events http and functions events httpapi functions events http refers to API Gateways REST API offering APIG v1 functions events httpapi refers to API Gateways HTTP API offering APIG v2 You can read about the differences between REST API and HTTP API offerings here Serverless yml Is there any guide to YAML I understand that Amazon writing user guides and templates like AWSIAMRole or for Dynamo Tables and all others And also that resources make a cloudformation but what is the purpose for example for Lambda functions listing there when I createcode them To reference them Add them privileges The Serverless Framework docs have a guide to the serverless yml options when you set the provider to aws The serverless yml file is an abstraction over Cloudformation The idea is that serverless yml is easier to write than verbose Cloudformation You would define functions in your serverless yml to createconfigure your lambdas in AWS For example heres a snippet from your serverless yml This creates a lambda in AWS with the code you have written in the login js file It also sets the memory size and sets up an API Gateway HTTP API endpoint behind login You can check out the Cloudformation template that is created from this serverless yml file in your serverless directory It is a lot Do I only specify resources that should be created or can I also reference already existing ones via ARN If so how The serverless docs give an example of how to point to existing API endpoints Do the policies are only set globally like documentation showing or the only other option is to use plugin serverlessiamrolesperfunction The same question could be applied to roles and policies in resources I am not sure I understand the question In general I would suggest spending some time reading What is IAM from the serverlessstack website It does a fantastic job of covered IAM UsersRolesPoliciesGroupsetc "
72361116,66495073,"stackoverflow.com",0,"2022-05-24 13:15:12+03","2024-05-17 04:52:39.125208+03","I faced the exact same problem today So OK that is more than a year later and I hope you have found a suitable answer since you wrote this question Give a try to the include option in tsconfig json "
65920283,65916940,"stackoverflow.com",0,"2021-01-27 15:35:27+02","2024-05-17 04:52:43.280497+03","So it happens that when running the sls command to deploy the app it is a different environment that serverless uses In order to pass that environment on serverless build you should pass the desired environment In this case the deploy script should look like this"
65947706,65947548,"stackoverflow.com",0,"2021-01-29 03:46:29+02","2024-05-17 04:52:44.369351+03","The command just only creates a new entry in your awscredentials file Thus to check if it worked inspect awscredentials and see if [serverlessadmin2] profile was created with your aws keys If not you can add the profile yourself there "
65947909,65947908,"stackoverflow.com",1,"2021-01-29 04:17:25+02","2024-05-17 04:52:45.255847+03","I figured this out while I was typing up the question so I will go ahead and write out the answer The problem was that my code was getting minimized when it was deployed as a Lambda function Here is the relevant documentation about minification and sequelizetypescript Once minimized the derived table alias was becoming l and in a subsequent attempt b In order to force the table alias to be a specific name even after minimization you need to define modelName when making your model class Example below The xyz will become the name that the table is aliased to in the raw SQL that is generated "
65956051,65949994,"stackoverflow.com",15,"2021-01-29 16:10:57+02","2024-05-17 04:52:46.274081+03","Serverless will resolve the object for you Assuming that the content of your secret_ID_in_Secrets_Manager looks like this Then if you define your custom variable in serverless yml like this Then this will resolve to You can reference them inside serverless yml by using selfcustom supersecret foo and selfcustom supersecret bar See the Serverless documentation and search for Variables can also be object since AWS Secrets Manager can store secrets not only in plain text but also in JSON "
65967429,65958498,"stackoverflow.com",0,"2021-01-30 13:28:18+02","2024-05-17 04:52:47.129086+03","I would suggest using Middy and validator middleware for handling required parameters Yep the disadvantage is that your lambda is triggered all the time But also you obtain We prefer the middy more than precise configure of Gateway API "
65975873,65975743,"stackoverflow.com",0,"2021-01-31 06:54:57+02","2024-05-17 04:52:48.183297+03","What has happened is that you need to make sure that the name of the table changes on different stages I see you use selfprovider stage to try and do this but all that does is use the value for stage under the provider section and because you have not set one it uses the default of dev always I would suggest adding the following line under providers so that you have something like this What this means is that if you pass the stage on the CLI using stage it will set the provider stage to that value or to the default of dev "
66919349,65992128,"stackoverflow.com",0,"2021-04-02 15:28:53+03","2024-05-17 04:52:51.365041+03","I dont know if this will answer your question but base on the documentation of serverlessbundle Everything will be manage by the serverlessbundle and to change the config of the webpack is on the serverless yml so I think you dont need to commit the webpack on your source code httpsgithub comAnomalyInnovationsserverlessbundleoptions and base on their advance options they dont support the customization of the webpack and babel httpsgithub comAnomalyInnovationsserverlessbundleadvancedoptions"
67506620,66000642,"stackoverflow.com",4,"2021-05-12 18:27:18+03","2024-05-17 04:52:52.114324+03","I have faced a similar problem After 3 days of pulling my hair I have found my problem Everything was ok except In my client there were few wrong URLs spelling mistakes pointing to my server API This is why few API was ok and few of them not working properly After fixing to the right URL everything is ok Here is my learning hope someday it will help others Check you are serverless yml files cors section here is an example Check Lamdba for proper response header as question contains Additional Tools for troubleshooting httpsaws amazon compremiumsupportknowledgecenterapigatewaycorserrors httpsaws amazon compremiumsupportknowledgecentersupportcasebrowserharfile httpstoolbox googleapps comappshar_analyzer Hope it will be helpful Thanks Happy Coding"
75781379,66000642,"stackoverflow.com",0,"2023-03-19 12:34:44+02","2024-05-17 04:52:52.116325+03","I faced the same problem Please check whether you are sending all the correct Headers If any additional header you send and do not fine tune the cors config you will get a CORS error For me I had a typo with Authorization header Took me 3 days to figure it out lol Please check this link for detailed documentation and how to fine tune cors for httpAPi httpswww serverless comframeworkdocsprovidersawseventshttpapi"
78281206,66000642,"stackoverflow.com",0,"2024-04-05 19:14:33+03","2024-05-17 04:52:52.117565+03","I was beating my head against the wall on this issue and only managed to get the AWS deploy and a local environment working with CORS by doing the following CORS worked on the APIGW endpoint after sls deploy for me but not running locally with sls wsgi serve So I ran Flask locally instead via flask run adding flask_cors using the decorator only cross_origin I e I did not run flask_cors CORS app Running this way both the preflight OPTIONS and POST calls are working fine with CORS enabled both locally and in API GatewayLambdaSAM stack deployed by severless "
66114790,66000642,"stackoverflow.com",2,"2021-02-09 09:39:58+02","2024-05-17 04:52:52.119106+03","Have you tried fixing the cors true value in the function event as in Serverless with cors "
66052903,66002119,"stackoverflow.com",5,"2021-02-04 21:54:48+02","2024-05-17 04:52:53.22921+03","What code bundler does As you can see it is a perfect match for AWS Lambda and your use case All of the dependencies from common package will be included in the output file Also code bundlers have other cool features like removing all of the unneeded files that are defined in libraries that you use but you are not using them directly Due to this output package size of your Lambda will be a lot smaller which will decrease cold starts The easiest way is to start with serverlesswebpack plugin which includes Webpack one of the most popular code bundlers and some most common configurations for it After adding this plugin simply configure it in serverless yml Now you need to configure Webpack using webpack config js file There are a lot of possibilities to configure it and the example below is the most basic one Now when you call sls package in projectA or projectB then after unzipping serverlessfunctionName zip you will find just single fat file that will include all of the required dependencies During sls deploy phase this file will be deployed as Lambda handler Make sure that common package is listed as dependency of projectA and projectB Thanks to this you will be able to reference commons in pakcageA imports via Project using this approach can be found on my Github here httpsgithub comPatrykMilewskiserverlessseries"
66052313,66016451,"stackoverflow.com",0,"2021-02-04 21:13:49+02","2024-05-17 04:52:54.238193+03","According to AWS documentation The runtime passes three arguments to the handler method The first argument is the event object which contains information from the invoker The invoker passes this information as a JSONformatted string when it calls Invoke and the runtime converts it to an object When an AWS service invokes your function the event structure varies by service Since handler expects event to be a JSONformatted string then it is normal that this string will be converted into an object Instead of passing raw JSON and expecting it to be parsed as string simply wrap it into one of JSON fields an example For this input and this handler I am receiving following output"
66024672,66024257,"stackoverflow.com",3,"2021-12-14 07:29:19+02","2024-05-17 04:52:55.310633+03","You need to involve Cloudwatch Rules into that There is good tutorial from AWS here httpsdocs aws amazon comAmazonCloudWatchlatesteventsRunLambdaSchedule html Basically there are required actions If you want to run this with the Serverless framework you can use schedule provider httpswww serverless comframeworkdocsprovidersawseventsschedule If you want to run your tasks at specific time you can use this expression More info about Schedule expression is here httpsdocs aws amazon comAmazonCloudWatchlatesteventsScheduledEvents htmlCronExpressions"
66043426,66038142,"stackoverflow.com",3,"2021-02-04 12:08:10+02","2024-05-17 04:52:55.872473+03","It did not work for me either I ended up using Layers using the following option This reduced my deployment package size to less than 10mb All the dependencies then go into a separate zip 100mb which creates a Lambda Layer This helps reduce the Lambda coldstart problem as well "
66143478,66038142,"stackoverflow.com",1,"2021-09-07 16:13:24+03","2024-05-17 04:52:55.873904+03","Really the underlying concept behind this question is flawed The main purpose for this question was how to squeeze large dependencies onto Lambdas size constraints Recently AWS released EFS support for Lambda functions meaning you can mount volumes directly onto your lambda function This means practically infinite storage and no zipping requirements meaning significantly less cold start Also one of these file systems can mount to multiple Lambda functions I will not go into insane detail there is a lot to do but If you want to get large dependency trees on Lambda I recommend the following"
65427139,65413690,"stackoverflow.com",1,"2020-12-23 17:54:08+02","2024-05-17 04:52:57.607048+03","One thing you can do is inspect the headers in the event object to confirm that it is the type you expect and if not return an error httpsdocs aws amazon comlambdalatestdgservicesapigateway html"
65427384,65413690,"stackoverflow.com",1,"2020-12-23 18:10:40+02","2024-05-17 04:52:57.608049+03","We can enforce the ContentType header straight into the API Gateway like below References can be found here httpsdocs aws amazon comapigatewaylatestdeveloperguiderequestresponsedatamappings html"
67682005,65436190,"stackoverflow.com",2,"2021-05-25 07:57:43+03","2024-05-17 04:52:58.429154+03","The comment on the question about pgnative being C misses the key point the pg module works on both browser and node but Webpack gets confused and tries to package the native module The simple solution is to exclude pgnative in Webpack config This works nicely One gotcha is when I upgraded Webpack from 4 to 5 my previously working setup broke Heres what worked for me Webpack 5 Webpack 4"
65449387,65446631,"stackoverflow.com",13,"2020-12-25 17:24:30+02","2024-05-17 04:52:59.337229+03","awsserverlessexpress has rebranded to vendiaserverlessexpress The new path is not yet included in ngtoolkit is serverlessaws yml file and that is what is causing the issue Adding the following in the serverless yml excludes would solve the issue End result would be something like"
65465798,65465640,"stackoverflow.com",13,"2020-12-27 15:07:13+02","2024-05-17 04:53:00.364819+03","I have found the root cause if you wish to deploy serverless framework application you must use the exact same organization org and application name app as one you registered with serverless framework To find out your current apporg name change them or create new apporg login to Serverless Frameworks dashboard account on httpsapp serverless com using same credentials you use for deploying and make sure you are using the exact org and app in your serverless yaml file So you cannot just use any arbitrary orgapp name you must use exact orgapp registered with Serverless framework "
66597871,65465640,"stackoverflow.com",5,"2021-03-12 12:07:49+02","2024-05-17 04:53:00.366056+03","I had to remove org org in order for it to ask me again next time sls is run "
65465796,65465640,"stackoverflow.com",4,"2020-12-27 14:35:22+02","2024-05-17 04:53:00.36718+03","Try using serverless logout or deleting the \ serverlessrc file then run serverless login again and try your command"
65478769,65465640,"stackoverflow.com",1,"2020-12-28 15:58:35+02","2024-05-17 04:53:00.368175+03","you need to specify AWS profile in your serverless yml and set your AWS account credential in awscredentials like below And speciffy this profile in your serverless yml file like this The error you are getting is saying sls framework could not access to your AWS resources It means you did not setup this AWS account credential in your local environment and serverless framework "
66068457,65469201,"stackoverflow.com",1,"2021-02-12 18:44:59+02","2024-05-17 04:53:01.631243+03","I actually was running into this same issue and it took me forever to solve The issue is that you need to initialize the database at the start of every lambda invocation In your case your configureApp method returns a promise which is not being waited for prior to starting the server I am not entirely sure on the solution using Serverless AWS but you should be able to do something like this"
65480784,65473030,"stackoverflow.com",0,"2020-12-28 18:29:27+02","2024-05-17 04:53:02.49792+03","The issue was that I was exporting an async function instead of a regular one What worked for me was"
65476427,65475724,"stackoverflow.com",1,"2020-12-28 12:48:13+02","2024-05-17 04:53:02.915586+03","If you take a look at the SQSQueue CloudFormation resource you can see that the Queue Name is exposed as an attribute As a result of that you can use GetAtt sqsQueue QueueName or FnGetAtt [sqsQueue QueueName] both of which may be a little easier to read than the solution you came up with which still works "
65475852,65475724,"stackoverflow.com",0,"2020-12-28 12:01:58+02","2024-05-17 04:53:02.917587+03","I was able to achieve it by replacing section with which now gives me the output as channels info instead of httpssqs useast1 amazonaws comxxxxchannels fifo"
65575044,65489210,"stackoverflow.com",1,"2021-01-05 10:01:07+02","2024-05-17 04:53:04.868152+03","JSONPath does not support dynamic keys so this is not possible You have to restructure your input data or combine multiple Choice Rules with the And comparison operator to check the corresponding property based on current "
65581272,65580071,"stackoverflow.com",3,"2021-01-05 16:55:58+02","2024-05-17 04:53:06.484873+03","The documentation you are referencing is for Apache Open Whisk If you are using AWS you will need to use input as shown in the aws documentation"
65581478,65580071,"stackoverflow.com",1,"2021-01-05 17:08:35+02","2024-05-17 04:53:06.486874+03","The documentation that you referred to is for OpenWhisk httpswww serverless comframeworkdocsprovidersopenwhiskeventsscheduleschedule Cloudwatch Events now rebranded as EventBridge is at httpswww serverless comframeworkdocsprovidersawseventsscheduleenablingdisabling Sample code for reference Official docs at httpsdocs aws amazon comeventbridgelatestuserguidescheduledevents html"
65580718,65580071,"stackoverflow.com",0,"2021-01-05 16:22:53+02","2024-05-17 04:53:06.488874+03","I could see one of my configuration something like below There we use parameters instead of param "
71351037,65592019,"stackoverflow.com",3,"2022-03-04 13:54:33+02","2024-05-17 04:53:06.867948+03","I might be a little late but for anyone looking for a similar solution Just use different names for the trigger "
65632076,65592019,"stackoverflow.com",3,"2022-10-20 18:51:46+03","2024-05-17 04:53:06.868948+03","I am using a config similar to the following which perfectly works and satisfies my needs "
71936507,65592019,"stackoverflow.com",1,"2022-04-20 11:33:56+03","2024-05-17 04:53:06.870541+03","The serverless docs do provide the following aspect to create multiple cron schedules You can specify multiple schedule events for each function in case youd like to combine the schedules Its possible to combine rate and cron events on the same function too Source Serverless"
72897332,65592019,"stackoverflow.com",0,"2022-07-07 14:51:35+03","2024-05-17 04:53:06.87154+03","Serverless provides options to achieve this Reference httpswww serverless comframeworkdocsprovidersawseventsschedule"
74105034,65600660,"stackoverflow.com",3,"2022-10-18 05:28:27+03","2024-05-17 04:53:08.011958+03","In order to use the split function first install serverlesspluginutils add it to your plugins section in serverless yml The split function is now available If I have a following custom variable section and I want to split it into an array like below then split function can be used from serverless utils plugin"
65675379,65600660,"stackoverflow.com",0,"2021-01-12 00:22:34+02","2024-05-17 04:53:08.013502+03","If Security group is added as Input parameter to template Security Groups can be split with Split as can be split with FnSplit as Parameter to sam deploy can be passed as"
67908836,65600660,"stackoverflow.com",0,"2021-06-09 20:52:24+03","2024-05-17 04:53:08.014681+03","I am trying to achieve the same thing but I am pretty sure you cannot use CloudFormation functions FnSplit in serverless yml Passing YAML listarrays through env vars does not seem possible for serverless because we cannot write code to parse the strings that come from env vars Maybe you could write a serverless plugin but I have not looked into that I assume you need to be able to pass lists of variable length and arbitrary values so the following solution might not work great for you If you have known values you can keep the lists in serverless yml and select a list with an env var then you select a list by If you need arbitrary values and your lists do not get too long then you could get really hacky and make each list item an env var and supply everything as env vars You could also leverage the Serverless feature to reference properties in other files You can create a small YAML file that is ignored from version control and then reference that file in your serverless yml I think your error message is referring to the fact that you are passing a YAML objectdictionary into private when it wants a YAML listarray The key thing is this is all to do with YAML syntax The FnSplit function is something specific to AWS CloudFormation and I guess the execution of this function happens within AWS infrastructure We can test this theory by changing your code to supply a list then you will see an error like This means we fixed the previous error but now we have the same problem but one level down because fnsplit is a YAML objectdictionary You can also add another list item e g and you will see the error message change to reference index [1] To prove the fnsplit value is not special change it to something else e g and you will see the error message does not change This is because Serverless does not care about the CloudFormation functions it is just checking the YAML schema matches "
73127009,65600660,"stackoverflow.com",0,"2022-07-26 19:41:28+03","2024-05-17 04:53:08.01769+03","I had an exact same problem where I was doing split as below but it was not working It was giving split errors like Split object requires two parameters 1 a string delimiter and 2 a string to be split I changed it to the following and it worked"
65456746,64837397,"stackoverflow.com",0,"2020-12-26 15:11:37+02","2024-05-17 04:53:23.950423+03","Maybe you can change your webpack config js to this form This works for me You can also use a function inside your webpack config js file to get all dependencies from package json and insert them into externals property "
65822064,65608537,"stackoverflow.com",2,"2021-01-27 17:24:32+02","2024-05-17 04:53:08.987046+03","The problem is likely to be in ECONNREFUSED 0 0 0 09324 Judging by the port number it is an attempt to reach the sqs service but the IPaddress is bad It should connect to sqs9324 or an IPaddress of that container 0 0 0 0 means any IPaddress and it is usually used to bind a port Check your serverless configuration Also you can easily check if you are in a race condition or not For that simply start your services one by one using several terminals If you can start services one by one then it is likely you are In this case you can add restart onfailure property to a service This way if a container exits with a code other than 0 docker restarts the container "
65931500,65608537,"stackoverflow.com",2,"2021-01-28 07:32:19+02","2024-05-17 04:53:08.989571+03","It turns out my issue was actually in my serverless yml configuration Here I had my serverless yml with a custom configuration as follows The correct endpoint was actually `httpsqs9324 Everything else was correct "
65616088,65608892,"stackoverflow.com",0,"2021-01-07 18:18:18+02","2024-05-17 04:53:10.067063+03","Yes AWS Lambda functions have time limits in their configurations and will be terminated if that is exceeded You can change this value but only up to 15 minutes of run time currently The problem is that when working with large amounts of data that is not enough time to complete some actions In these case you will want to transition to Step Functions state machines that can run Lambdas and Redshift Data API run queries wo have a live connection all the time In this way you can launch a long running query and have the Step Function poll for completion "
65614674,65614528,"stackoverflow.com",1,"2021-01-07 16:53:44+02","2024-05-17 04:53:10.762568+03","Your best bet for information like this is Azure Architecture Center that has articles on best practices and architectural guidance Regarding using Dynamo or Cosmos DB to back Redis I cannot offer any guidance on the efficacy for doing such a thing What I can say is that I do see customers optout of using Redis altogether and use Dynamo or Cosmos as a keyvalue cachelayer because the latency is good enough "
67329025,65628599,"stackoverflow.com",8,"2021-04-30 09:13:44+03","2024-05-17 04:53:12.06382+03","Support for disabling the default executeapi endpoint has recently been added to AWSApiGatewayRestApi cloudformation DisableExecuteApiEndpoint"
65637268,65628599,"stackoverflow.com",1,"2021-01-09 00:30:04+02","2024-05-17 04:53:12.06482+03","You can disable it though a simple custom resource Below is an example of such a fully working template that does that"
66213916,65628599,"stackoverflow.com",1,"2021-02-15 21:03:18+02","2024-05-17 04:53:12.066043+03","In case anyone stumbles across this answer that is using CDK this can be done concisely without defining a Lambda function using the AwsCustomResource construct"
67384561,65628599,"stackoverflow.com",1,"2021-05-04 15:14:16+03","2024-05-17 04:53:12.06704+03","You can disable it in AWS CDK This is done by finding the CloudFormation resource and setting it to true "
73913715,65628599,"stackoverflow.com",1,"2022-09-30 23:35:46+03","2024-05-17 04:53:12.06804+03","Here is a Python variant of the answer provided by snorberhuis Amazons docs on Abstractions and Escape Hatches is very good for understanding what is going on here "
66773052,64732591,"stackoverflow.com",0,"2021-03-24 02:15:34+02","2024-05-17 04:53:13.592953+03","A little later here but I ran into the same error and only found this unanswered question Hoping to save others some time and pain After a process of elimination I have found the following The error came down to this line Though mine was using some other string identifier After looking at Data Sources created through the console I found that AWS is using UUIDs here and other formats are not accepted I decided to use the native uuid package for this This is also true for the other Quicksight resources that require an Id attribute as a parameter such as Data Sets Dashboards etc Additionally you will want to make sure you have an applicable permission set The ones shown in OP would throw a new error Fortunately in those cases the error provides a solution In my case I used the following permission set Cheers "
64755316,64755315,"stackoverflow.com",0,"2020-11-09 18:19:48+02","2024-05-17 04:53:15.220435+03","The solution that I have found to work is to install amazoncognitoidentityjs There is another mentioned solution by adding global crypto require crypto at the beginning of the file line 1 but it did not work for me I am leaving the git issue link for more solutions if this one somehow does not work and for more context httpsgithub comawsamplifyamplifyjsissues7098"
64798364,64755315,"stackoverflow.com",0,"2020-11-12 07:33:14+02","2024-05-17 04:53:15.222436+03","I was facing this issue when I was working with amazon cognito in nodejs After so many hit and trial i found the solution just change the amazoncognitoidentityjs version to amazoncognitoidentityjs ^4 5 4unstable 6 and now it is working for me hope it will save your time and will work for you all who is facing this issue Hit up you found this worthy "
64823228,64765112,"stackoverflow.com",0,"2020-11-13 17:05:26+02","2024-05-17 04:53:16.226531+03","Your problem statement is contradictory do you want to trigger an ECS task using Lambda Serverless or without using Lambda Serverless is a framework to build and deploy Lambda functions on AWS or serverless functions on any number of IAAS providers If you do not want a Lambda solution you need not use Serverless Option 1 Trigger an ECS Task from EventBridge This is the option detailed in the tutorial you have linked EventBridge is an AWS service that connects various AWS thirdparty or custom events to a number of supported targets Option 2 Trigger a Lambda function Looks like you are not interested in this Listing here however because you mentioned Serverless "
68696668,64771296,"stackoverflow.com",0,"2021-08-08 02:01:29+03","2024-05-17 04:53:17.096547+03","Regarding private true Clients connecting to this Rest API will then need to set any of these API keys values in the xapikey header of their request This is only necessary for functions where the private property is set to true httpswww serverless comframeworkdocsprovidersawseventsapigateway Not sure how you are sending your request but consider this part It could be that you are missing header information and that is why it always answers like that i e not your authorizer but your function is protected Therefore this response may come from your function not the authorizer "
64793940,64776802,"stackoverflow.com",0,"2020-11-11 22:51:43+02","2024-05-17 04:53:17.970306+03","Well I think I was a little dizzy about seeing so many special characters and I lost my way At varlibcfninitdatametadata json I was seeing something like this So I thought that was wrong since I was seeing the and \ characters scaped That was my confusion That is OK since that key in that JSON file is a string so the content of the string has to be properly scaped Here I left two ways of declaring a sed command like my example in a serverless template 1 2 I hope this saves time for someone "
73538305,64819729,"stackoverflow.com",0,"2022-08-30 09:55:17+03","2024-05-17 04:53:20.861931+03","There is a nice serverless plugin called serverlessexportenv it exports all environment variables you set in serverless yml so that you can use them in jest or invoke them locally After install the plugin you need to put it at the first item of the plugins key like also specify the export settings in custom in this example the environment variable are exported to a env file in your project root Then you can run serverless exportenv to export the environment variable to env In additional you can automate this process by adding this command to your script in package json so that when you run npm test it also run serverless exportenv for you for more see this doc Hope it helps "
64825698,64821675,"stackoverflow.com",2,"2020-11-13 19:53:17+02","2024-05-17 04:53:21.840881+03","Unfortunately there is no way to proxy a DDB stream event to your local Serverless Offline server AWS controls that integration entirely Occasionally when I face this issue I From there you should be able to debug "
65798796,64837397,"stackoverflow.com",0,"2021-01-19 22:11:51+02","2024-05-17 04:53:23.952424+03","The webpacknodeexternals documentation would have you believe that all you need to do is this It only worked after I explicitly set the node_modules folder this way Then my bundle size went from 1 3MB to 44KB because all node modules were considered external dependencies "
64855553,64854640,"stackoverflow.com",0,"2020-11-16 11:35:57+02","2024-05-17 04:53:24.432637+03","Q1 No with Serverless Framework you can create as many FunctionsEndpoints as you like within one project Q2 You can create 5 different Lambdas for your HTTP Verbs like PUT POST GET etc but you could also combine them into one Lambda See httpsdocs aws amazon comapigatewaylatestdeveloperguidesetuplambdaproxyintegrations html But if you start developing with AWS keep things simple and create 5 different functions Later you will learn more about all those nifty other things "
64856134,64854640,"stackoverflow.com",0,"2020-11-16 12:14:27+02","2024-05-17 04:53:24.434637+03","You may find it useful to look through this free course available on the Serverless Framework website that can help answer a number of questions There are some videos I need to update related to UI changes for the dashboard but other than that its a good introduction httpswww serverless comlearncoursesfullstackapplicationdevelopmentonaws"
64855919,64855191,"stackoverflow.com",0,"2020-11-16 12:00:10+02","2024-05-17 04:53:25.392692+03","Use lambda layer to pack all your requirements Make sure you have all requirements in requirements txt file This works only when serverlesspythonrequirements plugin is listed in plugins section Replace your custom key with this and give the functions that need those requirements a reference to use that layer "
64867678,64865213,"stackoverflow.com",0,"2020-11-17 02:30:13+02","2024-05-17 04:53:26.475479+03","All your risk is coming from the fact that you are wanting to do demos using your personal AWS account with your work computer I for example have a profile on my work computer which relates to a limited read only role of my personal AWS account to some code commit repos and s3 buckets Any more than that would be a risk and depending on the organization you work for completely unacceptable The best solution would be that your work gives you a sandbox account for you to demo and play around with This is what my work does as well I would fully pursue that avenue before going ahead using your own account If you do go ahead with using your own account there are a few choices Profiles have been around for a while and that has been mentioned But personally I have found it easy to make a mistake Limiting permissions of your cli to your work account will mitigate this Doing a Read operation on the wrong account is not as bad as deleting something But I have found that some cli tools will have different conventions with what profile they use so this can be dangerous IMO the best option is to Configure AWS SSO Longlived iam CLI users are considered bad practice anyway and since AWS has come out with this you could even say they are redundant Now with the new version of the CLI you can sign in to an AWS account from the CLI via an SSO page with optional MFA enabled so you do not need CLI users in iam This way you do not need to store any longlived CLI credentials or remember a profile Sign into one AWS account at a time and you will be signed in for an hour This minimizes the chances of you performing actions in the wrong account and there will be no risk of accidentally using the wrong profile If you come back to your computer the next day run some CLI command before remembering to specify the correct account it will not matter because you will not be signed in You need to keep in mind that with this approach you might have 2 different SSO pages one for personal and one for work and you also need your work account to configure SSO "
64309522,64306563,"stackoverflow.com",3,"2020-10-12 01:15:13+03","2024-05-17 04:53:28.8044+03","From docs A rate expression starts when you create the scheduled event rule and then runs on its defined schedule After update the rule stays the same unless your update changes the rule CloudFormation does not redeploy resource which do not change when a resource is not affected by the update process "
64316189,64315636,"stackoverflow.com",2,"2020-10-12 13:42:47+03","2024-05-17 04:53:29.880071+03","In your table definition the key consists of todoId and createdAt Your delete function however uses todoId along with userId as key This doesnt work Im not sure why you need the user ID when you already have the todo ID which I would assume to be unique Does it work if you simply remove the userId from the Key attribute in the delete parameter object like this"
64828364,64321708,"stackoverflow.com",2,"2020-11-13 23:40:02+02","2024-05-17 04:53:30.764256+03","There is possibly an error with your docker container and not the yml file Or a disconnect in the Dockerfile with your yml Dockerfile should have something like this is for Ubuntu If you are trying to setup for the first time you might be missing some environment setup "
64334347,64323888,"stackoverflow.com",0,"2020-10-13 14:27:37+03","2024-05-17 04:53:32.305363+03","I did not try it by myself but I found that there is a serverless plugin for it serverlessofflinedirectlambda Sounds like you may give it a try The complete list of the serverless pluggins"
64325498,64325024,"stackoverflow.com",0,"2020-10-13 01:01:19+03","2024-05-17 04:53:32.740046+03","I am not sure if it is causing your problem but the first thing that stuck out to me is how you are defining the table name in the call to scan According to the docs that should be a keyvalue pair If you are using the Serverless Framework do you get different results if you run the function local vs remote For example running the function locally from the command line vs running the function remotely in AWS from the command line"
64334247,64325024,"stackoverflow.com",0,"2020-10-13 14:20:43+03","2024-05-17 04:53:32.743047+03","Please try to add the callbacks for the promise of await docClient scan promise I mean and check the result maybe then the picture became clear "
64367865,64350072,"stackoverflow.com",2,"2020-10-15 11:41:00+03","2024-05-17 04:53:33.713957+03","Calculating the FnJoin elsewhere then referring to it in the environment configuration as a fallback seems to work "
64366356,64355108,"stackoverflow.com",0,"2020-10-15 09:58:10+03","2024-05-17 04:53:34.368862+03","There are also plugins where you may trigger scripts depends on the lifecycle events from the serverless framework The most complete serverless lifecycle events list "
64356617,64356291,"stackoverflow.com",9,"2020-10-14 18:25:41+03","2024-05-17 04:53:35.271848+03","Composite sort keys refer to how you use your sort keys The only thing you need to do in your serverless yml is to define the sort key name which you have done here Whether or not you use that sort key as a composite sort key or not is up to your application You would either set the value of the sort key to a composite value e g CABEVERLYHILLS90210 or not e g BEVERLYHILLS Neither DynamoDB or Serverless Framework really care it is more about how you use the keys in your application logic Keep in mind that DynamoDB allows you to store multiple entities in a single table These entities may have different primary keys partition keys and sort keys For that reason I would recommend naming your partition key PK and your sort key SK For example lets say you define your Post and User entities to have the following primary keys In the case of the Post your sort key could be a timestamp the time the post was made For Users the sort key could be the ID of the user So what do you name the sort key field when defining your table I like naming the sort key SK as a reminder that you are using that field to organize your data Of course you can name it whatever you would like but if you name it userId as you have done in your serverless yml file you may forget that the field can hold something other than user IDs "
64362324,64362053,"stackoverflow.com",0,"2020-10-15 01:25:16+03","2024-05-17 04:53:36.356512+03","This is not a direct answer but have you looked into lambda layers You may be able to separate all your libs into a layer which would simplify your overall project and keep things lean Is there any way you can avoid using the domain manager plugin"
63794155,63744115,"stackoverflow.com",0,"2020-09-08 15:40:15+03","2024-05-17 04:53:47.15045+03","First thing I would look at is simplifying the mismatch of async handling You are awaiting a promise that is invoking the lambda callback but the lambda function is defined as async so AWS is expecting a promise to be returned You should either define the lambda as async and ignore the callback or define the lambda without async and use the callback Try this"
49305381,45236308,"stackoverflow.com",2,"2018-03-15 19:09:49+02","2024-05-17 05:06:27.627221+03","I solved this problem by adding editing stage prod into serverless yml finally it looks like this "
67003540,64362053,"stackoverflow.com",0,"2021-04-08 14:58:41+03","2024-05-17 04:53:36.358513+03","This is an issue that I am facing too The system that I am working with has 20 services 1 lambda function per API endpoint totalling 200 lambda functions The total size of the lambda deployments is huge and deployment takes a long time I came across a post on serverless architecture pattern and the service pattern which groups multiple related endpoints to a single lambda function seems to be a viable option This can help to reduce the lambda deployment size but in return there will be additional code and complexity in routing the request to the appropriate handler Lambda layer could be useful too although I have not read up much on it There is a sample project that demonstrates AWS lambda layers"
64519712,64365824,"stackoverflow.com",3,"2020-10-25 04:32:20+02","2024-05-17 04:53:37.113438+03","If you just want to secure your api with cognito there is no need to create scopes Scopes do not grant authorizations to a user they grant them to applications API Gateway Cognito Authorizer operates in basically three modes This is a consequence of the following in each case Hope that clears things up Do not get too hung up on scopes unless you know what you are using them for "
64406267,64376481,"stackoverflow.com",1,"2020-10-17 22:04:00+03","2024-05-17 04:53:38.027529+03","I managed to do it but did not use Serverless I set up Lambda functions to POST and GET the data from a url I think the issue previously was to do with the policies This time when making the Lambda functions I set it as the following I clicked on Create a new role from AWS policy templates while creating an execution role for a new function then selected Simple microservice permissions for Policy templates This added Basic execution role policy and below DynamoDB permissions to the role for all the tables in the same region as the function Lambda function for POST request Other issues as well were with the method execution of the API I needed to set a custom model for the Request Body to match my data For each action I also enabled CORS and replaced the existing CORS headers These two videos explains the entire process much better than the documentation and I hope it helps Part 1 Part 2"
64396499,64376481,"stackoverflow.com",1,"2020-10-18 12:07:02+03","2024-05-17 04:53:38.029661+03","By bad request do you mean Status Code 400 It could simply be that you are not correctly calling your API If you are getting a 403 then you need to pass through that you are authorised to access the resource you are trying to get You can see how to do this through the AWS docs This page includes a link to an example List of error codes "
71735114,64395111,"stackoverflow.com",2,"2022-04-04 12:46:03+03","2024-05-17 04:53:39.107864+03",""
64402414,64398596,"stackoverflow.com",0,"2020-10-17 15:25:39+03","2024-05-17 04:53:39.989754+03","Have you explored the options of using a Database Activity Streams feature You can essentially monitor the database activity of Amazon Aurora Database activity streams provide a near realtime data stream of the database activity in your relational database Under the hood it uses below set of services and flow With above details you can make a SAM template and configure the details of each of these services "
64409148,64407961,"stackoverflow.com",0,"2020-10-18 05:28:01+03","2024-05-17 04:53:40.839746+03","The following worked for me eChart exception NgxEchartsDirective InjectionToken NGX_ECHARTS_CONFIG "
64435729,64419049,"stackoverflow.com",1,"2020-10-20 01:06:44+03","2024-05-17 04:53:41.958696+03","For anyone else that runs into this issue I resolved it by specifying the custom IAM role I want the lambda to use in the serverless yml This has drawbacks since the iamRoleStatements does not apply any longer so you have to add log permissions and other permissions your lambda needs to this custom role but for my very narrow use case this was acceptable You can see more here under the section Custom IAM Roles httpswww serverless comframeworkdocsprovidersawsguideiam "
64444300,64421636,"stackoverflow.com",1,"2020-10-20 15:33:42+03","2024-05-17 04:53:42.454624+03","It is your AWS user profile policy issue sls deploy on run try to create all the resources defined under resources of serverless yml config Even if you provide administrator access to profile it will not work As you have mentioned in profile policy it seems like you have already created the queue and topic By defining them under resources of serverless yml simply specifying that you want to create those resources again As you have no admin access right now it shows you have no access to create sqs queue Try to add administrator role in your aws profile you will see error again that queue and topic are already exist Try to delete already existing resources and then sls deploy It will work or simply do not use resources and specify ARN directly Refer to this httpsgithub comserverlessserverlessissues3183"
64516831,64421636,"stackoverflow.com",1,"2020-10-24 21:43:15+03","2024-05-17 04:53:42.456962+03","Thank you folks but the problem ended up being very simple In serverless yml I defined my topic and queue names like this But in my policy I wrote arnawssnsxxxxxx2mytopic and arnawssqsxxxxxx2myqueue The solution was as simple as adding stage suffixes to the resources in my policy or using for the trailing segment of the ARN Thank you for your responses all "
64427170,64421636,"stackoverflow.com",0,"2020-10-19 15:16:47+03","2024-05-17 04:53:42.458493+03","Please verify the user you created has permissions to create SQS queues CloudFormation is telling you that it does not If you have multiple AWS roles configured please ensure the environment variable AWS_PROFILE is set to the correct role If you still are experiencing this error please post the entire IAM policy "
64403553,63710044,"stackoverflow.com",2,"2020-10-17 17:28:48+03","2024-05-17 04:53:44.009686+03","The issue lies within the mangum adapter expecting input similar to the event content specified by AWS API Gateway shown here You will see that there is a requestResponse dictionary there that the Mangum adapter seems to strictly require to function If your AWS Lambda event does not contain that key then you will need to add a placeholder field or construct a new context dictionary for it to look for This can be done by creating a custom handler as shown within this part of the Mangum docs If you do not particularly care about what the adapter reads as the context then you can get a working version going with the following"
71835142,63710044,"stackoverflow.com",1,"2022-04-12 02:32:26+03","2024-05-17 04:53:44.012681+03","You need to provide at least the minimal Event data like in the example below when you invoke a FastAPIbased lambda function for example via AWS console Lambda Test Event Please see details about the Event format in httpsdocs aws amazon comlambdalatestdgservicesapigateway htmlapigatewayexampleevent The need comes from the mangum adapter for FastAPI explained in the answer "
63728746,63728391,"stackoverflow.com",0,"2020-09-03 20:24:13+03","2024-05-17 04:53:45.057344+03","You cannot write to anything but tmp in a Lambda or you need to mount an EFS filesystem You will need to change the deployment location of the ffmpeg executable See this documentation for tmp or this link for information on mounting an EFS volume "
63936950,63728391,"stackoverflow.com",0,"2020-09-17 14:21:23+03","2024-05-17 04:53:45.058344+03","For anyone on Windows like me who finds this in future Lambda file permissions are set outside of Lambda and you will need to boot a Linux VM or a Mac to change CHMOD stuff on files that will end up on Lambda "
63777674,63742723,"stackoverflow.com",2,"2020-09-07 15:29:47+03","2024-05-17 04:53:46.110798+03","Finally I could crack it If we enable epsagon autotracing thru epsagon webapp it actually adds a layer called epsagonnodelayer to that lambda Disabling autotracing helped to not to get this error For more details refer to httpsgithub comepsagonepsagonnodeissues295"
65466882,63744115,"stackoverflow.com",0,"2020-12-27 16:38:54+02","2024-05-17 04:53:47.15245+03","You need to add a VPC endpoint to monitoring service in order to avoid timeout and connect to CloudWatch VPC endpoint selection You can then mention the Private DNS name of the endpoint in the SDK configuration "
66378589,63805110,"stackoverflow.com",1,"2021-02-26 02:46:31+02","2024-05-17 04:53:50.381712+03","I encountered the same issue and resolved it by specifying the stage with s STAGE when executing sls invoke local I am using serverlessdotenvplugin which lets me split my env files by stage like so Adding the stage helps specify which env file to use with your local invocation Without it Serverless was trying to import env with no stage appended which does not exist on my project "
63817268,63805110,"stackoverflow.com",0,"2020-09-09 21:10:13+03","2024-05-17 04:53:50.382942+03","I was experiencing the same issue as well It turns out that the latest version of Serverless seems to have bug Downgrading to a previous version of sls fixed mine I downgraded to verion 1 80 and it works fine now "
63823417,63805110,"stackoverflow.com",0,"2020-09-10 08:36:17+03","2024-05-17 04:53:50.384366+03","I figured out what is wrong There was a warning while the invocation was running Since it was a warning I just ignored it and it previously worked but apparently the framework doesnt like it now Dealing with the warning either by removing the variable or defining it resolved my issue "
64166050,63805110,"stackoverflow.com",0,"2020-10-02 07:10:09+03","2024-05-17 04:53:50.385364+03","For me all the environment variables had to be initialized for resolving error So I did this "
73507752,63805110,"stackoverflow.com",0,"2022-08-27 04:20:15+03","2024-05-17 04:53:50.386466+03","I got stuck with this issue as well The reason is the environment variables defined in serverless yml was not in the local env file Here API_KEY and STAGE should exist in env "
63820231,63820011,"stackoverflow.com",9,"2020-09-10 01:12:27+03","2024-05-17 04:53:51.323415+03","Inside YAML flow collections strings with characters like [ etc have to be quoted because starts a flow mapping and [ starts a flow sequence There is no space required for that So this is how it should look like For information on pretty much all types of quoting in YAML you can have a look at httpswww yaml infolearnquote html disclaimer I am the author "
75625279,63847295,"stackoverflow.com",1,"2023-03-03 11:26:13+02","2024-05-17 04:53:52.385509+03","in data py import the module gather_keys_oauth2 in the following way Please note the dot before the module name "
63951559,63847295,"stackoverflow.com",1,"2022-04-19 15:05:04+03","2024-05-17 04:53:52.387509+03","Your lambda is not packaging the module you wish to import You can use serverlesspythonrequirements plugin for this It can be installed locally or on a pipeline with You can add this to you serverless yml file and try deploy Check out the guide on the plugin here httpswww serverless comblogserverlesspythonpackaging httpswww npmjs compackageserverlesspythonrequirements URL update You can use"
63895227,63881532,"stackoverflow.com",0,"2020-09-15 07:44:25+03","2024-05-17 04:53:53.227177+03","First of all there is an error in yml script Serverless Error Invalid characters in environment variable 0 The error is defining environment variables as an array instead of keyvalue pairs And then after deployment everything works smoothly sls deploy v sls invoke f hello statusCode 200 body The answer is 42 serverless yml variables yml"
63891019,63890216,"stackoverflow.com",4,"2020-09-14 22:53:00+03","2024-05-17 04:53:54.78837+03","You will need to add the secondary key to the KeyConditionExpression as well should look something like this The sort key is more flexible and allows for other operations like BEGINS_WITH BETWEEN etc"
64083140,63896939,"stackoverflow.com",5,"2020-09-27 01:36:46+03","2024-05-17 04:53:55.329575+03","To get this type of interpolation working correctly wrap the inner variable with double quotes This pattern is especially useful for setting up default envvar CLI option heirarchies For example Stage and region each have a default which can be overridden by an environment variable which can in turn be overridden by a CLI argument "
63902801,63896939,"stackoverflow.com",0,"2020-09-15 16:22:18+03","2024-05-17 04:53:55.330576+03","According to your expression you are trying to do a double condition Looks like it does not work optfb optenvironmentdev Try to simplify and split your options"
70522505,63897507,"stackoverflow.com",2,"2021-12-29 18:52:56+02","2024-05-17 04:53:56.383185+03","I am having the same issue using the latest version of serverless 2 70 0 I also have SSM parameters referenced as perstage custom variables Serverless tries to fetch values for ones it does not need to based on the stage And so that fails Because it does not have permission to fetch them For example with the old variables it would throw a warning still wrong but at least it runs And with the new variable resolution you get an error The error is correct the user is not permitted to get the value But it has no need to get the value Since the SSM variable is not needed for the stage "
63903512,63897507,"stackoverflow.com",0,"2020-09-15 16:58:12+03","2024-05-17 04:53:56.385184+03","According to your YAML file the first concern is the spacingpadding of the myEnvVar variable under custom When I tried the sls print I got the same error the problem is that sls try to validateread the variable from SSM Thus if it does not exist the sls built fails no matter for which stage it was executed The following script properly works for me But take into consideration"
63126364,63064610,"stackoverflow.com",3,"2020-07-28 05:56:14+03","2024-05-17 04:53:59.302559+03","The solution was instead of moving everything over to useast1 just maintain two stacks which are the primary stack and the lambda edge stack The primary stack resides in EU and the lambda edge stack resides in useast1 You can reference the lambda edge functions in useast1 by using cf useast1anotherstack lambdaEdgeArn "
65181724,63064610,"stackoverflow.com",1,"2020-12-07 14:26:46+02","2024-05-17 04:53:59.30456+03","It is impossible to reference Lambda deployed to useast1 from other regions by FnImportValue In the CloudFormation template there is a workaround by looking up the versionspecific ARN dynamically and passing it as a template parameter for the CloudFront template in a task file "
63083255,63081991,"stackoverflow.com",0,"2020-07-26 04:42:23+03","2024-05-17 04:54:00.355756+03","Using the wildcard at the end of the query you make your rule subscribes to any topic beginning with If you are publishing a message to a topic like this by any chance with nothing after startnext Then it will not match your rule In such scenario you should query without the wildcard at the end Something like this You can find more details about such wildcards and its coverage right here"
63095563,63081991,"stackoverflow.com",0,"2020-07-26 05:44:30+03","2024-05-17 04:54:00.35697+03","So Reading the MQTT Client Help on the IOT page I have actually found this part which would explain why the shadows topic triggers the Lambda as I mentioned in the question but not my other Lambda based on the jobs topics I guess I will have to find another way to make this work but thanks to who tried to help "
63125815,63107494,"stackoverflow.com",1,"2020-07-28 04:34:44+03","2024-05-17 04:54:01.272035+03","Try this"
63180578,63125147,"stackoverflow.com",4,"2020-07-31 20:18:24+03","2024-05-17 04:54:01.897484+03","The solution is simpler than I thought big surprise 1 Specify bucket name only no arn decoration on the name at all no accesspoint etc 2 Add the region to which the lambda is deployed in the provider section at the top of your serverless yml file "
63181888,63166496,"stackoverflow.com",1,"2020-07-31 01:00:32+03","2024-05-17 04:54:03.265477+03","Here is the serverless guide which define what is possible httpswww serverless comframeworkdocsprovidersawsguideserverless yml You could define resources using cloudformation in Resources section I would say some pieces are only possible by adding them only with cloudformation I see your plugin use templates so I believe you have to follow and create the cloudformation Plugin is just an integration piece between serverless framework and cloudfromation "
63191273,63166496,"stackoverflow.com",1,"2020-07-31 14:47:07+03","2024-05-17 04:54:03.267478+03","Try using slit stacks plugin httpswww npmjs compackageserverlesspluginsplitstacks Worked great for us"
45290199,45273443,"stackoverflow.com",1,"2017-07-25 00:15:49+03","2024-05-17 05:06:28.58874+03","You can just install the necessary dependencies directly Gatsby is just NPM packages like any other node js project "
63169885,63169491,"stackoverflow.com",3,"2020-07-30 12:09:58+03","2024-05-17 04:54:04.398481+03","This happens because post_build executes whether build fails or succeeds Thus it does not meter that build fails post_build will run anyway This is explained in the build phase transitions You can rectify this by manually checking if build failed in post_build by checking CODEBUILD_BUILD_SUCCEEDING env variable Thus in your post_build you can check if CODEBUILD_BUILD_SUCCEEDING 0 and exit 1 if is true "
63178865,63169491,"stackoverflow.com",2,"2020-07-30 21:05:09+03","2024-05-17 04:54:04.400482+03","Your command is not returning a nonzero code on failure which is required to fail the build The command tee is masking the return code from serverless deploy as it itself is responding with a 0 return code I would recommend to rewrite the command as"
69904889,63204150,"stackoverflow.com",1,"2021-11-09 22:47:51+02","2024-05-17 04:54:07.450675+03","The fields to include in the formdata varies based on where the code is run at least that was the case in my problem In Elastic Beanstalk environment there was an additional xamzsecuritytoken field alongside the AWSAccessKeyId key etc which was not present in the fields returned by Boto3 create_presigned_post while run on local env "
63273527,63273102,"stackoverflow.com",6,"2020-08-06 00:08:01+03","2024-05-17 04:54:10.089046+03","I figured it out Since serverless framework extends CloudFormation I found the answer in the Cloud Formation documenation here httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawsresourcessmparameter html It can be done as follows This stores the ARN of the queue in the parameter store I am not sure if getting the url is possible but the ARN is fine for my use case "
63619746,63317067,"stackoverflow.com",0,"2020-08-27 18:55:47+03","2024-05-17 04:54:12.858472+03","The service or URL referenced by spec sink can be written in any language and in your target language you need to start an HTTP server and listen for events In your case I suggest looking at the Python CloudEvents SDK documentation for receiving events that contains a useful example using Flask httpsgithub comcloudeventssdkpythontreev1 0 0stablerequesttocloudevent So the Knative service in the Kafka Source example needs to reference your custom container image Also spec sink can be a Kubernetes service The following is an example configuration to deliver events to a custom path mycustompath for a Kubernetes service myservice "
62769734,62681011,"stackoverflow.com",4,"2020-07-07 10:03:08+03","2024-05-17 04:54:14.947992+03","You can try using this module instead of default one Supports only Python3 x httpspypi orgprojectawspsycopg2 While the above is not actively maintained and has the max version as psycopg22 8 4 it could serve the usecase you might have You can also try zipping and uploading the package from here httpsgithub comjkehlerawslambdapsycopg2 if you need support for older python versions "
64196411,62686712,"stackoverflow.com",7,"2020-10-04 18:19:03+03","2024-05-17 04:54:15.770402+03","A quick workaround is to try to run below command first I have a large serverless project which ran into similar issue when I tried to deploy with sls deply And this workaround works for me Hope it can help "
67643491,62686712,"stackoverflow.com",3,"2021-05-21 23:32:53+03","2024-05-17 04:54:15.772403+03","This was happening to me too I realized I had defined my serverless configuration to package each lambda individually Which looks like this Changing that to worked for me Of course if packaging your lambda functions individually is crucial for you then you will lose that but for me it was not "
62767576,62699333,"stackoverflow.com",2,"2020-07-07 06:11:56+03","2024-05-17 04:54:17.615989+03","You can add another path for your Lambda to respond to Then handle it properly inside your Lambda function "
62716716,62714921,"stackoverflow.com",0,"2020-07-03 16:49:28+03","2024-05-17 04:54:19.162763+03","You can add the created Roles ARN to the Outputs section of the CloudFormation template You can then query the output from the CloudFormation stack"
62797911,62771363,"stackoverflow.com",5,"2020-07-08 18:08:00+03","2024-05-17 04:54:20.741279+03","You can simply use selfprovider environment ACCOUNT_ID "
62774013,62773866,"stackoverflow.com",2,"2020-07-07 14:59:37+03","2024-05-17 04:54:21.690105+03","Dont worry you are using it correctly However your IDE is marking this as an error because it is a CloudFormationspecific extension and not standard YAML If you are using VSCode you can add the following to the settings json in order to make the error message disappear for CloudFormations custom tags"
62796350,62794558,"stackoverflow.com",3,"2020-07-08 16:50:13+03","2024-05-17 04:54:22.776381+03","Maybe the environment variables are not updated yet at the time of the creation of the table definition I am not sure Try selfprovider environment DYNAMODB_ACCOUNTS_TABLE_NAME instead of envDYNAMODB_ACCOUNTS_TABLE_NAME "
62798836,62794558,"stackoverflow.com",0,"2020-07-08 19:02:38+03","2024-05-17 04:54:22.778382+03","I have not seen this behavior yet random characters after deploy it could be a way to force uniqueness when the table has to be replaced You could use another environment variable and have the value populated by the Table resources output That way CloudFormation will inject the actual resource name to the Lambda environment variable I have not tried this but this would be my first go to "
62798902,62795107,"stackoverflow.com",2,"2020-07-08 19:06:17+03","2024-05-17 04:54:23.519247+03","In answering this I am reminded a bit of a Rube Goldberg type setup but I do not think this is too bad From the Google side you would create a Cloud Function that is notified when a new file is created You would use the Object Finalize event This function would get the information about the file and then call an AWS Lambda fronted by AWS API Gateway The GCP Function would pass the bucket and file information to the AWS Lambda On the AWS side you would have your GCP credentials and the GCP API download the file and upload it to S3 Something like All serverless on both GCP and AWS Testing is not bad as you can keep them separate make sure that GCP is sending what you want and make sure that AWS is parsing and doing the correct thing There is likely some authentication that needs to happen from the GCP cloud function to API gateway Additionally the API gateway can be eliminated if you are ok pulling AWS client libraries into the GCP function Since you have got to pull GCP libraries into the AWS Lambda this should not be much of a problem "
62803617,62797958,"stackoverflow.com",2,"2020-07-09 00:04:18+03","2024-05-17 04:54:24.108272+03"," file serverless common yml custom is going to dump the array from common so that wont merge With this approach you need to add each property from the commoncustom section individually or Alternatively if your setup is complex you can write a serverless js file instead which allows JS processing I personally use that to programically merge a dozen yml file for example when I run sls offline I want to addremove a bunch of stuff so that is a pretty powerful approach "
62815699,62798979,"stackoverflow.com",2,"2020-07-09 16:13:33+03","2024-05-17 04:54:25.521054+03","The issue comes from the AssociatedRoles object cloudformation states that the FeatureName field is not needed however if you are wishing for your cluster to access other AWS services it is required In this case as I was wanting to have my cluster access an s3 bucket I had to change my AssociatedRoles object so it looked like this"
62848167,62806727,"stackoverflow.com",0,"2020-07-11 13:47:21+03","2024-05-17 04:54:25.735333+03","You could achieve that with localstack httpsgithub comlocalstacklocalstack This is assuming that you do not want to have this locally The services that you mentioned are in the free version of localstack so you should be good to use it I think there are some people explaining most of steps a quick search I found this httpsdev togoodideahowtofakeawslocallywithlocalstack27me"
72595132,60657261,"stackoverflow.com",1,"2022-06-12 22:09:14+03","2024-05-17 04:55:29.657865+03","Alot has changed since you asked the question but i thought i would share some useful links that helped me sort this out Also I found this simple instructional httpswww youtube comwatchvFINVVmCXms Happy debugging "
62835192,62814461,"stackoverflow.com",1,"2020-07-10 16:19:48+03","2024-05-17 04:54:26.222734+03","I have been using step functions with serverless framework they have an interesting guide that might help you configure it httpswww serverless compluginsserverlessstepfunctions Another thing that you would need to install on you project is the plugin to actually be able to create the step function on your serverless yaml httpsgithub comserverlessoperationsserverlessstepfunctions also according to this PR httpsgithub comserverlessoperationsserverlessstepfunctionspull333 the startExecution sync2 was added in the version 2 19 0 of the package "
68069096,62820073,"stackoverflow.com",0,"2021-06-21 16:47:30+03","2024-05-17 04:54:27.057946+03","I had a similar problem a couple of days The serverlesspythonrequirements was excluding the lib directory because it was outside of the module configuration I set The solution for me was to move the requirements txt into the root directory and set up the module configuration also to the root Could you post your serverless file"
62825640,62825314,"stackoverflow.com",1,"2020-07-10 03:32:25+03","2024-05-17 04:54:28.126244+03","The line that defines the resource for the IAM premissions you have listed has a small typo and is missing the Resource arnawssqsuseast1 optaccID influxdb_perf_mon should be Resource arnawssqsuseast1 optaccID influxdb_perf_mon That may fix your issue"
62826378,62825314,"stackoverflow.com",0,"2020-07-10 05:18:18+03","2024-05-17 04:54:28.127124+03","I redeployed your role in my sandabox account I found that the issue comes from Specifically from optaccID Changing the Resource to real account number fixed the issue in my verification Thus the question is where does this policy come from Its not listed in your Serverless yaml provided in the question Are you sure this is the most recent Serverless yaml you use"
69749799,62221815,"stackoverflow.com",4,"2021-10-28 17:41:00+03","2024-05-17 04:54:29.780218+03","In my specific case i had this error code when i deleted the deployed lambda function in the AWS Management Console but did not delete the Application Stack and tried to deploy the lambda function again using I did not have to delete the s3 bucket it was reused You can find your application stack in the AWS Management Console And delete An error occurred [functionName]LambdaFunction Resource handler returned message Lambda function [application stack name][application stage][lambda function name] could not be found RequestToken 01010011011100111100001100110011 HandlerErrorCode NotFound "
62222163,62221815,"stackoverflow.com",3,"2020-06-05 21:41:50+03","2024-05-17 04:54:29.782212+03","Let me explain here Serverless framework can invoke run lambda in two ways locally and in cloudAWS It seems you are trying to invoke lambda in AWS arnawslambdauseast1155754363046functionsellthelandnowdevhello Basically this arn does not exist in your AWS155754363046 account you need to use to deploy lamdba to aws env If you just want to test locally the command is So I will suggest in case you want to invoke lambda in the cloud You need to first deploy it Or you use invoke local Thanks Ashish"
62373809,62225725,"stackoverflow.com",1,"2020-06-14 17:21:49+03","2024-05-17 04:54:31.682285+03","The export is standard node js this is what maps your function implementation to the function declaration inside your serverless yml The context object is the AWS Lambda context httpsdocs aws amazon comlambdalatestdgnodejscontext html If you look closely the example you linked is for AWS the signature for the handler will be different for other cloud platforms "
62228000,62227576,"stackoverflow.com",2,"2020-06-06 09:04:14+03","2024-05-17 04:54:32.764112+03","How does it will affect the pricing You pay for the lambda function execution costs So if your lambda functions calls another function you will be charged for both executions There are no special discounts for such use case unfortunately Is it good practice to call the lambda from another lambda in terms of cost Its basically same question as the first one You pay for each execution Is it good practice to develop such product in serverless Generally you should not tightly couple functions like this In an ideal case all functions would work independently and would be synchronized using Step Functions andor communicate through distributed message systems such as SQS I can recommend AWS white paper on best practices of using lambda I found there are lots of duplicate code LambdaFunctionOne and LambdaFunctionTwo both have same function performing same operation This seems as a good use case for lambda layers You could put all the common code in the layer shared by all the functions "
62227995,62227576,"stackoverflow.com",2,"2020-06-06 09:03:47+03","2024-05-17 04:54:32.767112+03","The downside of calling a separate function is that you are paying for two simultaneous functions the calling function which is waiting and the called function It would be better to have the code accessible in the one function An exception to this is if you are designing microservices where each microservice owns a process or some particular data In this case it can be better to call another service via a defined API without exposing the inner details of the function "
62229943,62227576,"stackoverflow.com",0,"2020-06-06 12:44:43+03","2024-05-17 04:54:32.769113+03","Same as external invocation Check out the CloudWatch logs it is still in 100ms increments "
62279023,62233176,"stackoverflow.com",0,"2020-06-09 12:14:22+03","2024-05-17 04:54:33.689165+03","Apparently Apigateway with lambda or proxy integration encodes body to base64 so i changed my lambda to and everything worked as expected "
62362152,62233450,"stackoverflow.com",2,"2020-06-13 18:46:39+03","2024-05-17 04:54:34.645482+03","You can remove this error by using below command"
62241896,62240072,"stackoverflow.com",1,"2020-06-07 09:52:34+03","2024-05-17 04:54:36.19319+03","An important point to note when using WebSocket APIs and trying to return a response from your integration back to the client as mentioned in this doc For a route that is configured to use AWS_PROXY or LAMBDA_PROXY integration communication is oneway and API Gateway will not pass the backend response through to the route response automatically For example in the case of LAMBDA_PROXY integration the body that the Lambda function returns will not be returned to the client If you want the client to receive integration responses you must define a route response to make twoway communication possible So if you are using a proxy integration then you cannot return a response back to the client You would have to use the AWS ApiGatewayManagementApi postToConnection method to communicate any data Alternatively if you are using a nonproxy integration then you can set up a route response for your integration "
62294735,62294619,"stackoverflow.com",2,"2020-06-10 05:11:51+03","2024-05-17 04:54:38.457622+03","You are right it may be a bit confusing if you are getting started on it Initially Serverless was used to describe Backend as a Service Functions as a Service more info httpswww martinfowler comarticlesserverless html Now many things evolved to Serverless approach You can pick the latest announcements of SQL Database Serverless Cosmos Db Serverless etc So in summary just consider that serverless is something triggered by event and billed according to compute resources used and which you do not handlemanage the underlying infrastructure IaaS is not Serverless PaaS is not Serverless SaaS is not Serverless but can be implemented using Serverless CaaS can be serverless FaaS is serverless"
62309873,62304631,"stackoverflow.com",2,"2020-06-10 20:46:12+03","2024-05-17 04:54:39.385507+03","According to OPs feedback in the comment sls info v should do the trick"
62442972,62307297,"stackoverflow.com",0,"2020-06-18 08:28:52+03","2024-05-17 04:54:40.422685+03","Finally I got the solution "
60657771,60657498,"stackoverflow.com",3,"2020-03-12 18:01:04+02","2024-05-17 04:55:30.636428+03","Serverless framework does assume that all included files should be under the same directory as serverless yml but it is common in many scenarios that external files are needed For that you can use the serverlesspackageexternal plugin to easily add links to these files you can read more about that in the same blog post you referenced "
62336552,62321856,"stackoverflow.com",0,"2020-06-12 05:29:19+03","2024-05-17 04:54:41.539093+03","The challenge I am facing is passing the AWS credentials to the serverless deployment without making it part of version control You could try to upload the credentials to Azure Devops as Secure file Then you could directly use the Download secure file task to download the credentials In this case the secure files will not be a part of Repo Here are the steps Step1 Upload the credentials to Pipelines Library Secure files Step2 Add the Download secure file task to the Yaml definition For example Note the secure file will be downloaded to Agent TempDirectory When the build is complete this file will be automatically deleted In addition if you want to keep this file or put it elsewhere you can add a Copy file task For example Hope this helps "
62382953,62323975,"stackoverflow.com",1,"2020-06-15 10:01:07+03","2024-05-17 04:54:42.671295+03","This was resolved by using serverlesspseudoparameters serverless plugin Serverless framework also uses placeholder and it conflicts with Cloudformation placeholders serverlesspseudoparameters solves that by allowing us to replace those place holders with which are replaced during sls deploy with cloud formation templates"
62324048,62323975,"stackoverflow.com",0,"2020-06-11 15:29:09+03","2024-05-17 04:54:42.67308+03","Since you have Sub instead of the following should be enough The alternative using Sub in array notation "
62330166,62329048,"stackoverflow.com",5,"2020-06-11 20:31:41+03","2024-05-17 04:54:43.472841+03","I have found the way in the env yml in the serverless yml"
62655257,62335700,"stackoverflow.com",0,"2020-06-30 13:21:18+03","2024-05-17 04:54:44.346487+03","I recently developed a similar application using the serverlesspythonrequirements plugin that encapsulates multiple lambdas as part of one stack and I was receiving ModuleNotFoundError whilst invoking the lambda function locally yet it would work remotely however when I removed the module parameter from my serverless yml file I was able to invoke locally but then it broke for remote executions I have been able to find a workaround by setting a path prefix in my serverless yml When I invoke the function locally I prepend the environment variable to my command I do not include the environment variable when invoking the function in AWS In order for this to work I needed to add an init py file to the root directory where my lambda function resides with the following code taken from this solution so that any modules I am including in the code that exist in the lambdas directory e g some_module see directory tree below My lambdas directory structure As for your question regarding lambdas that use different requirements txt files I use the individually parameter like so Within each requirements txt for each lambda I refer to a separate requirements txt file that resides in the base directory of my project using the r option this file contains libraries that are common to all lambdas so when Serverless is installing packages for each lambda it will also include packages included in my requirements txt file too I have included this solution in an issue regarding the serverlesspythonrequirements plugin in GitHub which would be worth keeping an eye on should this behaviour of the module parameter turns out to be a bug Hope it helps and let me know if you require clarification on anything "
61718238,61717959,"stackoverflow.com",1,"2020-05-11 01:25:09+03","2024-05-17 04:54:48.350872+03","Both of your two approaches will cause two lambda functions i e listPets and addStore to be created since one lambda function cannot have two handlers configured in the function setting However you could put both of the GET and POST logic inside one handler For example Then let the API Gateway call the the same lambda function on GET and POST requests This will slightly reduce the chance of code start but it will result in more complex code structure in the shared lambda function IMO you should always split the code that has separate logics into separate lambda functions if possible to reduce potential bugs "
61724089,61717959,"stackoverflow.com",1,"2020-05-11 10:09:12+03","2024-05-17 04:54:48.352761+03","A separately defined Lambda function with a different event to trigger it even cases where the same handler handles multiple events will invoke seperate instances which means they all suffer some form of cold start However you may want to first determine whether a cold start is an actual problem Looking at a project I have access to that received over 3 000 000 Lambda invocations every 24 hours only something like 0 02 percent of those invocations were actually cold starts So spending an inordinate amount of time reducing cold starts may or may not be actually useful to your optimisation process The easiest and most effective two methods to reduce cold starts is"
61814631,61717991,"stackoverflow.com",5,"2020-05-17 11:55:51+03","2024-05-17 04:54:49.266702+03","Working configuration Python 3 7 Tested on Mojave 10 14 6 18G4032 and Docker v2 1 0 2 37877 Steps requirements txt serverless yml handler py Python 3 8 For some reason Lambda running Python 3 8 ignores the entry for tmpslspyreq on sys path Therefore you need to manually add the library file libgomp so 1 to the root of your application To give it a try update runtime python3 7 to runtime python3 8 on serverless yml and follow these steps FYI How to find the libraries location within the Lambda docker image Run Then"
61786308,61717991,"stackoverflow.com",2,"2020-05-14 13:25:21+03","2024-05-17 04:54:49.268703+03","On Linux the libraries you need are installed into by On newer Ubuntu and CentOS libgomp1 is no longer installed by default I have just checked I have it on xenial do not have it on a fresh bionic This is on a AWS machine unfortunately not very recent I have no other readily available "
75399327,61717991,"stackoverflow.com",0,"2023-02-09 15:30:28+02","2024-05-17 04:54:49.269703+03","For some reason these solutions did not work for me Python 3 6 xgboost1 4 Ubuntu 18 04 aarch64 However I was able to get things working by pointing to the libgomp library packaged with the xgboost Python package For example export LD_PRELOADusrlocallibpython3 6distpackagesxgboostlib xgboost libslibgompd22c30c5 so 1 0 0"
61746543,61737981,"stackoverflow.com",0,"2020-05-12 11:03:28+03","2024-05-17 04:54:50.194764+03","Turns out that I was being a little special and I was packaging source code and not compiled code I added a dotnet publish step to the build process and pointed the cloudformation package at the published output and all was good "
61811765,61747619,"stackoverflow.com",1,"2020-05-15 07:25:43+03","2024-05-17 04:54:51.222721+03","Yes resource blocks can be merged I believe this should do it Not 100 sure whether the dynamic block needs to generate Resources toplevel key or whether you can hardcode it like I have shown in the example "
61769494,61750445,"stackoverflow.com",0,"2020-05-13 11:14:26+03","2024-05-17 04:54:52.677887+03","What Ersoy suggested worked"
61765434,61754003,"stackoverflow.com",1,"2020-05-13 05:39:18+03","2024-05-17 04:54:53.406261+03","httpsgithub comlambcidockerlambda can be used to run Lambdas locally go to httpshub docker comrlambcilambda and pull a container with nodejs tag How well it will work for you depends on what exactly your Lambda does if it is not too tightly coupled to AWS it will probably be okay but if it makes use of other AWS services then it may not work so well as the above project only emulates the Lambda runtime not all of AWS "
61764962,61755763,"stackoverflow.com",2,"2020-05-13 04:43:38+03","2024-05-17 04:54:53.825031+03","You can do this serverles yaml handler py"
60644596,60644595,"stackoverflow.com",13,"2020-03-11 23:20:50+02","2024-05-17 04:55:27.579768+03","There does not appear to be a way stringify JSON directly within a serverless variable However you can reference an external js file and then stringify the json file there serverless yml file mydashboardbody js myDashboardBody is a serverless variable reference It means that we want to use the value from the myDashboardBody module of the mydashboardbody js file mydashboardbody js mydashboardbody json "
61763084,61762127,"stackoverflow.com",22,"2020-05-13 01:29:09+03","2024-05-17 04:54:54.963959+03","You cannot properly limit a sequelize query that including hasMany association until you make sequelize to get included association objects by separate queries In your query you have include with the association Thread hasMany Email so you should indicate separate true in the Email include like this this also goes for Attachment association Also you do not need to indicate distinct true because we already indicated to separate hasMany associations into its own queries Another problem with hasMany includes especially with nested hasMany in queries that they turn into JOINs in a SQL query that means a DB multiplies an amount of main records to an amount of nested records and so on For instance 100 main records each has 100 linked records each has its own linked 100 records Thread Email Attachment All in all you make a DB to query 100100100 1 million records at once It usually leads to out of memory About LIMIT and hasMany a DB selects 100 main records with 100 linked for each 10000 records at once and after that it takes first 10 records from these 10000 records not from 100 main records That is how SQL queries work "
77820739,61762127,"stackoverflow.com",1,"2024-01-15 17:18:45+02","2024-05-17 04:54:54.96696+03","Adding foreign key in the attribute array should do the trick Works fine in my case "
61783536,61781626,"stackoverflow.com",0,"2020-05-13 22:45:00+03","2024-05-17 04:54:56.059539+03","I would suggest taking a look at the serverlesspseudoparameters plug in that can help you simplify the configurative of some of those AWS variable parameters and thereby eliminate the error you are having httpswww serverless compluginsserverlesspseudoparameters"
71739975,61781626,"stackoverflow.com",0,"2022-04-04 18:37:13+03","2024-05-17 04:54:56.061545+03","In this case you just give it the name of the resource When the role is created it will be named what you have under RoleName with the substituted values "
61809300,61788276,"stackoverflow.com",1,"2020-05-15 02:45:13+03","2024-05-17 04:54:56.894754+03","The declared function key i e publicfunction is only a reference in the stack There is no reason to change it at build time as the name is arbitrary If you want to customise the details of the deployed function change its configuration For example change the name of the function to something else using environment variables If you are exporting the resource to another stack you can define the export name like so The stage scoping to both of these examples is recommended but not required you can name your functions resources and outputs anything you want "
61805191,61801520,"stackoverflow.com",1,"2020-05-14 21:50:30+03","2024-05-17 04:54:57.623176+03","You have some issues with indentation"
61805568,61801520,"stackoverflow.com",0,"2020-05-14 22:11:51+03","2024-05-17 04:54:57.624176+03","Indentation is important for serverless yml file In this case AttachmentsBucket is a resource it should be subsection under Resources with one tab space and then Type and Properties should have one tabbed spaces from Resource Name AttachmentsBucket while it actually have two in the sample provided CloudFormation will not be able to process this particular resource since it is not able to identify resource with proper name and properties See the updated sample"
61812226,61801520,"stackoverflow.com",0,"2020-05-15 08:15:57+03","2024-05-17 04:54:57.625176+03","You can validate the cloudformation templates by using the aws cli tool here But your question is regarding how to make lambda and dynamodb load works and in your description you are asking about the deployment part Can you update your question and tags"
61855663,61801520,"stackoverflow.com",0,"2020-05-17 20:07:35+03","2024-05-17 04:54:57.626471+03","I was able to figure out a solution As I am very new and it was my first project I was not very familiar with the terms in the beginning what I did was to name my bucket here Then to upload a file with the deploy I found a plugin called serverlesss3bucketsync And added in the custom attribute and the location of my file under folder And added the IamRole"
61841570,61806437,"stackoverflow.com",2,"2020-05-16 21:19:49+03","2024-05-17 04:54:58.331101+03","I think your problem is related to your serverless yml It is really frustrating because I cannot find anywhere to point you to in the serverless docs but I think your problem is that you are telling Lambda to respond to only one specific route If you load the resources directly in the browser that you are getting a 403 on you will see you are getting the error Googling that message shows it is a lambda specific error Check out this example I found where they are adding another http event to accept requests at all paths"
64579634,61826215,"stackoverflow.com",9,"2020-10-28 21:00:43+02","2024-05-17 04:54:59.432735+03","please find a snippet of a template yaml below that works for me Ofc you need to replace the placeholders "
61926077,61826215,"stackoverflow.com",2,"2020-05-21 05:14:48+03","2024-05-17 04:54:59.434735+03","You need not specify query string parameters in template yml Invoking API like http s URLkeyvaluem then parse them in lambda via event queryStringParameters that is all "
61839113,61838910,"stackoverflow.com",2,"2020-05-16 18:23:56+03","2024-05-17 04:55:00.537263+03","A serverless function is nothing but just a normal server perhaps a container with all the normal dependencies stuff being loaded ondemand So a server is created when you make an API call therefore saving you money when you are not getting those API calls Now once one of these server containers are created you might keep it around for a while to let it handle some more API calls as time goes on When the traffic to your server goes down you can kill it to save cost So you can imagine ending up is a scenario where the first API call you make takes a lot of time coldstart then subsequent requests are fast since everything is already setup Now depending on the amount of resources given to these ondemand servers they can be faster than something more traditional I would also guess that since these are first class offerings by cloud providers their implementation is more optimized than other services which may be doing the same thing like how RDS is faster than a database running on EC2 So on average you may see your serverless function performing faster Of course there is always that coldstart issue which may be problematic for you Of course there are other advantages disadvantages as well"
61838976,61838910,"stackoverflow.com",1,"2020-05-16 18:14:19+03","2024-05-17 04:55:00.539263+03","Its more a misconception that it is quicker a FaaS function as a service is simply a single function being executed when it is invoked If you had unlimited server capacity executing a very simple function it would also be fast However Serverless benefits from the following People love serverless because they can turn their idea to a POC in a quick period of time without having to really think about resource requirements "
61055958,61050997,"stackoverflow.com",0,"2020-04-06 11:51:49+03","2024-05-17 04:55:02.342754+03","after adding it worked in lambda AWS after making deploy but still not working offline using sls offline start it might be a bug in their code or I should add additional thing to let it work offline"
61083607,61081919,"stackoverflow.com",2,"2020-04-07 18:26:28+03","2024-05-17 04:55:04.180011+03","Finally found out what I was doing wrong There is nothing wrong with the above configuration my mistake was on requiring the files from the layers Instead of this const index require optnodejsindex I should be doing this const index require optindex "
68397985,61088533,"stackoverflow.com",1,"2021-07-15 20:00:46+03","2024-05-17 04:55:05.325658+03","I think you are missing the migration block where you specify your migration file put this under your dynamodb key"
60646572,60645671,"stackoverflow.com",3,"2020-03-12 03:45:47+02","2024-05-17 04:55:29.098138+03","Use the EmailConfiguration property in your user pool See the CloudFormation AWSCognitoUserPool documentation for more details "
60664099,60664074,"stackoverflow.com",0,"2020-03-13 04:25:33+02","2024-05-17 04:55:32.46979+03","I was making an error while adding the model into the db object Instead of I needed to do and accordingly everything works fine I can maintain two session on every API call"
70471603,61088533,"stackoverflow.com",0,"2021-12-24 11:50:07+02","2024-05-17 04:55:05.326658+03","The error message is explaining where your issue is the dynamodbhandler yml file is missing the key Resources In the serverlessdynamodblocal docs you can see the redundant resources Resources keys So your dynamodbhandler yml should begin like this Note that all other external resources must begin with the same key i e apigatewayhandler yml in your example Additionally if you are having difficulty creating the table when starting serverless offline or you are using a persistent docker dynamodb instead migrate with the following command"
61122341,61120978,"stackoverflow.com",12,"2020-12-05 01:57:59+02","2024-05-17 04:55:06.41183+03","There are two ways to easily accomplish what you are looking for Firstly a lambda function can be triggered by multiple events You can add another http event to the array of handlers like so Alternatively you could use the proxy argument You can read more about the various proxy methods here"
61144536,61122339,"stackoverflow.com",1,"2020-04-10 19:30:47+03","2024-05-17 04:55:07.57288+03","assuming you are uing the serverless plugin httpsgithub comserverlessoperationsserverlessstepfunctions You can create the activity by adding the activity into the stepFunction"
62045229,61174428,"stackoverflow.com",9,"2020-08-13 11:14:53+03","2024-05-17 04:55:09.276207+03","Include in your project the package Amazon Lambda CloudWatchEvents and wrap your event with CloudWatchEventT object can be substituted with your event class "
72506575,61180556,"stackoverflow.com",0,"2022-06-05 13:27:41+03","2024-05-17 04:55:09.571938+03","I was using TypeScript plugin serverlesstypescript I used it to create Lambda function that will resize images that are uploaded to S3 do some kind of content moderation Here is the content of serverless ts file resizeImageLambda ts I remember there were few issues when I wanted to connect it to existing buckets created outside serverless framework such as IAM policy was not recreated updated properly see forceDeploy end existing parameters in function events[0] s3 properties in resizeLambda ts file "
71519074,61193347,"stackoverflow.com",0,"2022-03-17 23:10:05+02","2024-05-17 04:55:10.257598+03","Taking a look into the plugin code pergroupfunction js The plugin autogroup based on the nestedStackCount option So as an example the following serverless yml Will group into 10 stacks as Based on the condition on the script you need to set nestedStackCount equals or greater than 3 "
61206659,61206124,"stackoverflow.com",0,"2020-04-14 14:29:50+03","2024-05-17 04:55:11.933372+03","If you are just interested in looking at debugging lambda scripts the best place to start is the cloudwatch logs Via the console you can go directly to the service and find the respective lambda or just go onto your lambda click the monitoring tab and choose view logs in cloudwatch Once you can view your logs its up to you to include the appropriate error handling within your script and isolate the error and at what point it timeouts Your script looks fine without testing it so would depend if your packaging your lambda have enough memory and whether the event your passing is valid"
61266146,61265620,"stackoverflow.com",3,"2020-04-17 10:26:51+03","2024-05-17 04:55:14.476661+03","In your serverless yml file you are creating the lambda in region apsoutheast2 but inside the openapi definition uri arnawsapigatewayapnortheast1lambda you are referencing a different region apnortheast1"
61288980,61288412,"stackoverflow.com",2,"2020-04-18 15:20:35+03","2024-05-17 04:55:16.317313+03","It seems that you are installing dependencies required on each build All commands before cd db_update_campaigns Those are not specific to the deployment of the current package and can thus be prepared beforehand so that only the npm install and sls deploy commands need to be run on deployment To achieve this first create a custom docker image based on the dockerstable This image should add all required dependencies Build and push the image to a registry for example dockerhub In your gitlabci yaml reference your custom image instead of the vanilla docker one This should save you some time during deployment as the image does not need to install the dependencies every time If you prefer to use a private dockerhub registry or a gitlab registry please see Define an image from a private Container Registry from the gitlab documentation for how to set that up"
60535411,60535080,"stackoverflow.com",0,"2020-03-05 00:06:21+02","2024-05-17 04:55:19.216337+03",""
60544365,60535080,"stackoverflow.com",0,"2020-03-05 13:28:27+02","2024-05-17 04:55:19.218338+03","OK answering my own question I had multiple problems above My final working solution looked like this if that can help anyone As usual the docs and log messages from AWS is bareminimum "
60567472,60544631,"stackoverflow.com",3,"2020-03-09 13:53:37+02","2024-05-17 04:55:20.270382+03","feliz de ayudar I pasted the snippet for the future set the flag in webpack config js code as explained here If the deprecation error is in a function you can do even any defeats purpose of Typescript is just for locating the problem to be removed after also can check the value Thanks "
60640108,60576587,"stackoverflow.com",2,"2020-03-11 18:04:00+02","2024-05-17 04:55:21.866641+03","You can use Ref ServerlessDeploymentBucket It is created as part of the rest of your application stack so you can reference it within the template More information is available here"
60625389,60578874,"stackoverflow.com",1,"2020-03-10 22:23:59+02","2024-05-17 04:55:23.045475+03","Your last effort is so very close You want envSTAGE instead of env STAGE stage optstage envSTAGE dev More on environment variables in Serverless here "
60594870,60588274,"stackoverflow.com",8,"2020-03-09 06:50:00+02","2024-05-17 04:55:23.759661+03","Under your provider add timeout maximum value of timeout in lambda is 900 seconds place it according to your execution time like 30 seconds and see what happens The error is clearly saying that it is execution exceeded timeout since you have not configured timeout so it was using default timeout of 3 seconds hopefully it will solve the issue"
60609877,60588274,"stackoverflow.com",3,"2020-03-10 02:02:16+02","2024-05-17 04:55:23.760721+03","The issue is likely due to your open database connection While this connection is established any calls to callback will not be returned to the client and your function will timeout You need to set context callbackWaitsForEmptyEventLoop to false Here is the explanation from the docs callbackWaitsForEmptyEventLoop Set to false to send the response right away when the callback executes instead of waiting for the Node js event loop to be empty If this is false any outstanding events continue to run during the next invocation With serverlesshttp you can set this option quite easily within your server js file"
70301070,60606403,"stackoverflow.com",1,"2021-12-10 09:29:26+02","2024-05-17 04:55:24.613137+03","I was able to get this working without creating an empty role roles is not required according to doc but it seems like CFN cant handle null well you just need to set roles cdk code Cloudformation template output from cdk"
60622631,60614295,"stackoverflow.com",5,"2020-03-10 19:10:27+02","2024-05-17 04:55:25.645151+03","With the current version 1 10 6 you cannot link files explicitely You can however add multiple directories This will ignore all the files from folderC or the other files from "
60645879,60640954,"stackoverflow.com",3,"2020-03-12 01:54:27+02","2024-05-17 04:55:26.776616+03","You are going to have to jump through a few hoops to get there The serverless framework allows file imports anywhere in the configuration but only merges resources and functions sections Your example Results in an array of arrays like this You might be able to submit a very small pull request to rectify this It might be possible to define each of your IAM roles as custom resources and use the iamManagedPolicies provider config to point to each of those resources Something like Of course you would need to change the structure of those two files to be AWSIAMRole resources The framework also gives you the option to take complete control which is fully documented I hope this helps "
60667114,60664329,"stackoverflow.com",0,"2020-03-13 10:27:54+02","2024-05-17 04:55:33.397734+03","Well from what I can see the issue is with the return null statement which returns before the callback is being fired and that is why you do not see any results also I do not see any reason to use async as well should do the trick if you could share your lambda handlerconfig it might be even easier to help you perfect this endpointfunction "
60898149,60006437,"stackoverflow.com",1,"2020-03-28 08:51:54+02","2024-05-17 04:55:34.976454+03","Whenever you will deploy your template it will create new version for layer also So i will suggest deploy layer separately and then later use the version in lambda like this layers arnawslambda AWSRegion AWSAccountId layerlayerName1 layer version "
60626041,60017442,"stackoverflow.com",0,"2020-03-10 23:15:28+02","2024-05-17 04:55:35.874702+03","Youll need to configure binary media types in AWS for API Gateway to be able to handle multipartform and that kind of stuff "
60943686,60017442,"stackoverflow.com",0,"2020-03-31 07:54:11+03","2024-05-17 04:55:35.875702+03","Try this Just change awsserverlessexpress to awsserverlessexpressbinary httpswww npmjs compackageawsserverlessexpressbinary"
60024207,60021467,"stackoverflow.com",4,"2020-02-02 09:57:02+02","2024-05-17 04:55:36.817143+03","You can install the Linux compatible package using the following Note that this also specifies a target NodeJS version ensure its the same version of node you are using in your Lambda This is straight out of the docs see here However that did not solve my problems My serverless configuration using serverlessbundle plugin meant that my modules were being installed again in a separate folder wiping out the platformspecific modules I just manually installed Two choices here For my specific edge case I had to go with Docker The build scripts will effect every function you are deploying adding 30mb of Sharp code and Lambda has limitations on source code size "
72317276,60021467,"stackoverflow.com",1,"2022-05-20 13:29:59+03","2024-05-17 04:55:36.820144+03","For AWS lambda deployment with Sharp module the following worked for me when using serverless esbuild and serverlessesbuild Changed the serverless yml file with the below configuration It is basically telling esbuild to download sharp again with the following archx64 platformlinux considering your lambda uses x64 arch Check serverlessesbuild packager and packagerOptions options for more understanding "
60021543,60021467,"stackoverflow.com",0,"2020-02-02 00:17:06+02","2024-05-17 04:55:36.821144+03","The description of httpswww npmjs compackagesharp suggests it is linux compatible I am unfamiliar with how to or if you can force nodes native package resolution to a separate OS Try building your lambda zip inside a docker image httpshub docker com_alpine"
60023412,60021467,"stackoverflow.com",0,"2020-02-02 07:18:55+02","2024-05-17 04:55:36.822345+03","If you have not already I would suggest following the Installing the AWS SAM CLI on macOS guide to insure you have the correct local environment for developing Serverless on macOS This process is designed for the macOS and includes builtin support for Docker so that you can build and deploy packages that are compatible with Lambda directly from your local computer "
62004170,60030568,"stackoverflow.com",1,"2020-12-31 14:48:46+02","2024-05-17 04:55:37.771605+03","I had the same problem with that library So I tried with httpswww npmjs compackageawsmultipartparser there is also a description of what to write in serverless yml file It is working like charm now "
60052456,60043717,"stackoverflow.com",1,"2020-02-04 09:24:38+02","2024-05-17 04:55:38.795036+03","Unfortunately the documentation here is poor and I cannot find a definitive guide but I can point you in the right direction These are not serverless variables they are AWSmade configuration for CloudWatch filtering See httpsdocs aws amazon comAmazonCloudWatchlatestlogsFilterAndPatternSyntax html"
60095260,60079790,"stackoverflow.com",0,"2020-02-06 14:41:24+02","2024-05-17 04:55:40.918514+03","Your Lambda is called synchronously by Kinesis and it will only proceed to the next event if the Lambda returns successfully These circumstances give you the opportunity to write the delay yourself in your Lambda code With a batch size of 1 this Lambda would have to finish first before the next event is run Note though that the additional Lambda run time will incur costs You could go the other direction too execute your actual code and then delay the shutdown of your Lambda You could even use the context object to see how long the Lambda has been running if your processing varies in running time Lastly I would recommend changing your architecture Besides the additional costs incurred you are artificially slowing down your platform "
60585850,60088117,"stackoverflow.com",0,"2020-03-08 10:45:04+02","2024-05-17 04:55:41.867823+03","Like LLL said the lambda function has already been deployed One way you can confirm that is by going to your CloudFormation stack and check the status of the lambda function If it is successfully deployed the status should be CREATE_COMPLETE or UPDATE_COMPLETE If you would like to view the deployed function you can click the Actions dropdown the page you visited in the image and click Export function This will download all the files deployed with your lambda function "
60111580,60096160,"stackoverflow.com",2,"2020-02-10 23:31:05+02","2024-05-17 04:55:42.984264+03","I ended up running a script in custom webpack packagerOptions scripts that will ignore sharp where it is not needed This is the script I used"
60132585,60125824,"stackoverflow.com",1,"2020-02-09 02:45:45+02","2024-05-17 04:55:44.068008+03","Your deployment is trying to create a new DynamoDB table but theres already a table with that name spaceserverlessbetadynamodbtestuser Either do not include a table name in your configuration or ensure the table name you have configured does not conflict with existing tables If you want to deploy this project to the same AWS account but for different stages you can add the stage to the table name ie spaceserverlessbetadynamodb optstage user "
60138484,60138424,"stackoverflow.com",5,"2020-02-09 17:58:27+02","2024-05-17 04:55:45.071497+03","The issue is caused by how Serverless handles tracks and deploys your functions and resources When you deleted the API Gateway method you effectively changed the state of your application manually and it is basically out of sync To resolve this in your serverless yaml file comment out the entire method that you manually deleted and run sls deploy When its finished uncomment the method and sls deploy again This time it should deploy your method again "
60255365,60150356,"stackoverflow.com",3,"2020-02-17 17:07:02+02","2024-05-17 04:55:46.899742+03","I would take a slightly different approach here but similar With this structure you can send a very large number of messages really quickly and probably a lot cheaper Imagine you need to send to 1M users If you send batches of 100 in batches of 25 to SQS then you have 2500 messages per call to SQS That would mean 400 calls to SQS far better than even the 40K you would have to make if you sent single messages in batches of 25 On the receiving side even if you throttled the SQS integration to 1 message per invocation you would have 10000 lambda invocations If you assume even 1s per invocation and 1000 concurrent invocations it would take 10 seconds likely less If you send one message per user you would have to make 1M lambda invocations If you assume each invocation takes 100ms then you can send 10second so with 1000 concurrent executions it would take 100 seconds In reality the numbers are probably even better than that for the batch version especially if you do not limit it to 1 message at a time Edit Based on the comments the question seemed to be a bit more about the first part of the process With that in mind I would suggest the following options "
58007172,58004398,"stackoverflow.com",4,"2019-09-19 11:48:47+03","2024-05-17 04:56:38.052926+03","API Gateway stringify the request body in events body property Currently you are trying to parse event object const requestBody JSON parse event which is wrong You need to parse event body property const requestBody JSON parse event body "
60313316,60150356,"stackoverflow.com",1,"2020-02-20 07:17:00+02","2024-05-17 04:55:46.902743+03","I would suggest an additional piece in your application architecture I personally prefer to avoid using the Primary database for heavy querying assuming you have a large user base I will suggest maintaining your user list in a Search Engine like ElasticSearch or CloudSearch or a simple table with just the user list in AWS DynamoDb or create a Read Replica of your DB To no confuse you use a Search Engine first choice or an AWS DynamoDb This will avoid creating pressure on your database when you query the read specialty datastore and will not affect other modules in operation And it is way fast to query this way Step 2 loop over all of them them Step 3 batch send messages to SQS using its SendMessageBatch method like Jason is suggesting Step 4 Based on your SQS setting you may process multiple messages on your Lambda function"
76175561,60165040,"stackoverflow.com",0,"2023-05-04 19:47:57+03","2024-05-17 04:55:48.060778+03","you may have already found the solution but just in case other wayward developers like myself stumble upon this it is because taskUpload is the function timeout is just a param of that function "
60187492,60176610,"stackoverflow.com",1,"2020-02-12 13:37:53+02","2024-05-17 04:55:48.263225+03","Thanks for all the responses this is the complete solution as i see it including yaml In serverless yml This will also create the topic when running serverless deploy Now all that is left is to create a scheduler GCP Cloud Scheduler Create Job Target PubSub Topic oneminutetopic"
60177492,60176610,"stackoverflow.com",0,"2020-02-11 23:16:53+02","2024-05-17 04:55:48.264225+03","Google Clouds recommended solution for scheduling services such as Cloud Functions is Cloud Scheduler Cloud Scheduler is a fully managed service with enterprisereliability and supports the popular UnixLinux cron format Cloud Scheduler product information Google Cloud Functions Tutorial Using the Cloud Scheduler to trigger your functions Cloud Scheduler Quickstart"
59420057,59410386,"stackoverflow.com",8,"2019-12-20 07:01:44+02","2024-05-17 04:55:50.35733+03","Inside your serverless yml file there is a provider block which includes a runtime argument To move to nodejs12 you just need to declare it and deploy your service Example Here is the full documentation"
59690791,59416413,"stackoverflow.com",6,"2020-01-11 03:02:01+02","2024-05-17 04:55:50.822403+03","The easiest way I found is to declare the aurora DB resource with a YAML like And then create an init sql script that will instantiate all the tables The difference between Aurora and DynamoDB is that you need to declare tables when deploying a DynamoDB but you do not need to with Aurora "
59420031,59416413,"stackoverflow.com",3,"2019-12-20 06:58:44+02","2024-05-17 04:55:50.823403+03","To do this with the Serverless Framework you will need to write a CloudFormation template and include it inside the resources block of your serverless yml file Here are the docs so you can learn more about including CloudFormation in your serverless yml file Heres a set of examples from AWS that can help although they are extremely verbose and include lots of extra things you may not need "
59418887,59418841,"stackoverflow.com",2,"2019-12-20 03:34:24+02","2024-05-17 04:55:51.674546+03","Select the Log Group using the radio button on the left of the Log Group name Then click Actions Remove Subscription Filter "
59418881,59418841,"stackoverflow.com",1,"2019-12-20 03:33:02+02","2024-05-17 04:55:51.675547+03","Via CLI is listed in AWS document This link Via Console UI This capture"
60539376,59418841,"stackoverflow.com",0,"2020-03-05 08:30:03+02","2024-05-17 04:55:51.676547+03","As you created the subscription with cloudformation stack via serverless manually removing the subscription filter as jarmod is not a best practice What you should do is remove the cloudwatchLog event from the lambda functions and deploy it should remove the subscriptions "
59588448,59430509,"stackoverflow.com",2,"2020-01-04 08:24:25+02","2024-05-17 04:55:52.369281+03","I have the same problem and I do not understand why awssdk is still included in the deployment package which makes about 50MB My solution is"
59591448,59430509,"stackoverflow.com",2,"2020-01-04 16:07:45+02","2024-05-17 04:55:52.370281+03","do you have an indentation issue see forceExclude and includeModules suppose to be in the same level should it be Reference httpsgithub comserverlessheavenserverlesswebpack"
67331074,59430509,"stackoverflow.com",2,"2021-04-30 21:53:56+03","2024-05-17 04:55:52.371276+03","There is also one more method to reduce the size of your function when using the aws sdk All you have to do is change how you import stuff For example this import changed to this It is what aws calls it Modular packages The example they showcased there reduced their function size from 817KB to 23KB Check the github repo here"
65456834,59430509,"stackoverflow.com",0,"2020-12-26 15:23:05+02","2024-05-17 04:55:52.372987+03","Maybe you can change your webpack config js to this form This works for me "
60610563,59431650,"stackoverflow.com",2,"2020-03-10 03:52:29+02","2024-05-17 04:55:53.281093+03","This likely only applies to those trying to use Serverless Enterprise with the monitoring dashboards they have set up wintvelts answer would not work for me because if i deleted the org variable it would likely break the connection needed for Enterprise So steps for my CircleCI setup I got this idea from reading how Seed run has users integrate with Serverless For more info read this link httpsseed rundocsintegratingwithserverlessenterprise "
59431773,59431650,"stackoverflow.com",1,"2019-12-21 07:00:49+02","2024-05-17 04:55:53.282094+03","Just checked Circleci stopped supporting AWS Permissions as a configurable option in the settings page You need to set the credentials as environment variables for the projects The credentials should be named exactly AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY that is all you need to do you do not have to do any additional step I tried this on my project and it worked Your deployment step should simply be"
60080112,59431650,"stackoverflow.com",1,"2020-02-05 18:28:40+02","2024-05-17 04:55:53.284094+03","As a followup to the previous answer I had exactly the same error I took the solution from the chat as a solution For me the fixes I applied For me 1 and 2 alone was not enough I also had to remove the line from my yml file to make deployment via CircleCI work For those landing here with the same issue hope this helps "
59466215,59432459,"stackoverflow.com",3,"2019-12-24 10:53:42+02","2024-05-17 04:55:54.350518+03","One of the quota limits of Google Cloud Functions is GHzseconds which means the total amount of CPU seconds consumed by all running functions the error you see is raised when you are reach that quota during a 100s time frame Since your function is carrying a computationally intensive workload it is easier to reach the limit As for how to control CPU usage the only way would be to delay your requests to keep your usage below these limits The alternative would be giving less memory to the function which in turn will be counterproductive because your functions will span longer time and you might run in the same trouble anyway As Doug pointed out Cloud Functions are not intended for your use case and neither Cloud Run GCE instances are better for computationally intensive operations as there are no memory limitations but the amount you have provided they also allow to run containerized code in an Container Optimized OS Hope this helps "
58961682,58919590,"stackoverflow.com",0,"2019-11-20 21:09:12+02","2024-05-17 04:56:08.352663+03","I found the answer While it shows my stupidity I wanted to post it here in case others have the same issue When I created the RDS custom PostGreSQL I specified I database name Little did I know that about 20 fields down in the form there is a second location you specify the dbname If you do not it says it does not create the db and assigns no name the console shows as the name The first name is for the first part of the endpoint The second time you specify the name is for the database itself I am using the PostGreSQL for the securitylogin when I setup the RDS Hope this helps someone "
57585382,57585157,"stackoverflow.com",0,"2019-08-21 09:07:36+03","2024-05-17 04:56:53.574811+03","You can do two things here "
65458343,59444571,"stackoverflow.com",3,"2020-12-26 18:18:49+02","2024-05-17 04:55:55.249376+03","The caveat here is that I do not use AWS SAM as I only use serverless yml ServerlessFramework files to declare my resources to deploy In my ROOT API I declare a series of outputs that my child lambdas depend on so that all my endpoints can have the same Rest API ID This output declaration is nested under my resources declaration in my serverless yml file as such Once you have this implemented you then need to import the reference to this parent API resource in your child lambdas within the provider section of your serverless yml file You must pay attention to the restApiId that you need to import from the CloudFormation stack from your parent API You can review the outputs by going into the CloudFormation stack in your Parent API click on Outputs and find your restApiId values as needed There is also another way that allows you to import the values using FnImportValue which you can review with the example shown here httpsgithub comserverlessexamplesblobmasterawsnodesharedgatewayusersserverless yml I have this method commented out in the code Ive pasted for you above for your reference "
59458028,59457094,"stackoverflow.com",1,"2019-12-23 17:50:48+02","2024-05-17 04:55:56.345137+03","Have you tried using the stage flag To indicate the env"
59463021,59458979,"stackoverflow.com",11,"2019-12-24 03:53:40+02","2024-05-17 04:55:57.896296+03","Ive faced this issue with Serverless The issue is caused by how Serverless handles tracks and deploys your functions and resources When you deleted the step functionstate machine you effectively changed the state of your application manually and it is basically out of sync To resolve this in your serverless yaml file comment out the entire step functionstate machine that you manually deleted and run sls deploy When its finished uncomment the step functionstate machine and sls deploy again This time it should deploy your step functionstate machine again "
76310722,59458979,"stackoverflow.com",0,"2023-05-23 04:09:24+03","2024-05-17 04:55:57.898296+03","Alternative option that has worked for me run sls remove to remove the cloudformations stack you distriupted if its for a specific stage run sls remove stage stage_name Then run an sls deploy and your service should be back up and in a normal state "
59534366,59508877,"stackoverflow.com",1,"2019-12-30 19:05:58+02","2024-05-17 04:55:58.846109+03","I have reviewed and tested your Serverless Framework code and after slight modification I was able to get the setup working by making the following changes handler py Serverless yml Curl Test Request Response The takeaway point is that when working with Serverless Framework and AWS Lambda Lambda proxy you have to add to the following line in your serverless yml under events Hope this helps "
59534018,59508877,"stackoverflow.com",0,"2019-12-30 18:44:07+02","2024-05-17 04:55:58.847702+03","You response body is the problem here It should be something be as you are setting header as json and passing normal text "
59514481,59511237,"stackoverflow.com",3,"2020-09-27 04:33:22+03","2024-05-17 04:55:59.696794+03","The purpose of the attribute endpointTypeprivate is to make the entire api internal to the VPC Therefore the property is not applicable to an endpoint If you would simply like to protect an api endpoint you can configure the api endpoint to be accessible only when an api key is provided you can do it by setting the attribute private to true If you do so The api is still public accessible but the api endpoint is protected by an apikey The api key is expected to be sent in the xapikey header remember this is different to an internal api where the api is internal to VPC and not publicly accessible "
59514341,59511237,"stackoverflow.com",2,"2019-12-28 21:46:56+02","2024-05-17 04:55:59.698482+03","Yes you can have private function So what you can do here is make ScoringServices post function private So now clients connecting to this post Rest API will then need to set any of these API keys values in the xapikey header of their request This is only necessary for functions where the private property is set to true For more information about setting api keys for read this "
59570191,59529304,"stackoverflow.com",1,"2020-01-02 23:24:13+02","2024-05-17 04:56:00.6759+03","You Need to do following API Gateways context identity sourceIp demonstration in a Lambda function API Mapping template Lambda Function"
59537886,59536442,"stackoverflow.com",4,"2019-12-31 18:52:06+02","2024-05-17 04:56:01.248376+03","The data section that you get back is base64 encoded and compressed To get the information out from the event you just need to decode the base64 information and unzip the data Heres a code snippet that shows basically what needs to be done in order to read the log data "
63215219,59537033,"stackoverflow.com",1,"2020-08-02 14:01:48+03","2024-05-17 04:56:02.094984+03","Although you say you are not reaching the maximum connections you still might want to try creating an Amazon RDS Proxy for your Lambda function to access hit it with a high load and see if you are able to reproduce the error You do not really have enough logs to diagnose the issue if the above does not work you will need to dive deeper potentially enable more RDS logging to see if that tells you the problem Other ways you can troubleshoot the issue is if you right the same queries you are executing in another languageframework simulate and see if problems persists You may also want to check Cloudwatch metrics for any other tells which could give you a clue as to what the problem is Graph your Lambda resource metrics and RDS instance metrics on the same chart to see if there are any patterns with when the Lambda function errors and what your DB is doing such as if the error occurs if your write or read latency increases If the issue persists and you are not able to solve it the best you can probably do is implementing retries which will simply mask the issue but if the boss is at you for a solution this might be your best bet Hope my suggestions help I have had similar issues with DBLambda DBECS and found those to be effective troubleshooting strategies "
59537193,59537156,"stackoverflow.com",1,"2019-12-30 23:48:18+02","2024-05-17 04:56:03.157039+03","No you cannot I had the very same issue with Axios All prefight requests must be anonymous See also this ticket in Tomcat So does the spec require it and browsers will not send any auth header regardless what you do "
58959795,58903023,"stackoverflow.com",1,"2019-11-27 18:44:06+02","2024-05-17 04:56:05.700885+03","Is it possible to achieve with serverless yml Yes Yes it is possible and it is already built in Your config will be simpler if you use the plugin serverlessapicloudfront rather than declaring a AWSCloudFrontDistribution resource yourself You will get one distributionperstage without having to jump through any hoops EDIT Expanding on my original post It does seem that the serverlessapicloudfront plugins README is lacking in usage examples at the time of this writing I do suggest that anyone reading this post should go to the plugins github and help the author by PRing a better README Heres a link to a blog article I found that shows an example of how to write up a AWSCloudFrontDistribution directly in your serverless yml It is only 1 resource so you may not even want to depend on a plugin anyway httpsmedium comyldblogcachinginwithcloudfrontusingserverless5a174651ab14"
60609096,58904172,"stackoverflow.com",3,"2020-03-10 00:19:28+02","2024-05-17 04:56:06.787215+03","Realised that with the recent update in serverless framework function timeout is respected across the integration All you have to do is pass timeout value in your function Here is an example functions myfunction handler publicindex php timeout 15 This will be same for integration timeout in API Gateway tags project myproject"
58948066,58929018,"stackoverflow.com",6,"2019-11-20 08:30:17+02","2024-05-17 04:56:09.341464+03","Use json dumps to convert JSON to string Use lambda integration and avoid json dumps But this will transform your output as"
58929279,58929018,"stackoverflow.com",4,"2019-11-19 10:08:21+02","2024-05-17 04:56:09.343464+03","The body needs to be stringified when working with API Gateway The pythonish way to do it is to pass the JSON body into the json dumps function "
58935565,58933941,"stackoverflow.com",6,"2019-11-19 16:22:38+02","2024-05-17 04:56:10.903638+03","You cannot reference AWSAccountId in your serverless yml because it does not quite translate when creating the CloudFormation template The workaround is to use the serverless plugin Pseudo Parameters You can install the plugin using npm You will also need to add the plugin to the serverless yml file Then you can reference your AccountId with AWSAccountId Note that the reference begins with a hash instead of a dollar sign "
58969429,58949826,"stackoverflow.com",1,"2019-11-21 11:20:28+02","2024-05-17 04:56:12.229163+03","Your relationship between supplier and category is defined with a join table SupplierCategory Forest Admin is plugged to you SQL database and uses ExpressSequelize to provide you admin API if you installed using Lumber So in your case you will have to use a belongsToMany association See here for the link Your models should look like modelssupplier js modelscategory js modelssupplier_category js"
58978435,58951709,"stackoverflow.com",0,"2019-11-21 17:31:29+02","2024-05-17 04:56:13.037543+03","The whole point of OAuth2 is that at the point of authorization you do not care who the bearer presenting you with a token is or which client they are using web app Postman etc only that the token is valid for whatever the token bearer is trying to do You trust that the authentication has happened because the token issuer has done the authentication When you talk about the aud audience from comments in JWTs which may or may not be included dependent on the JWT issuer this is intended to reflect the service or services that the JWT is valid for This could for example have a value of myAPIName or myAPINametest or even [myAPINametest1 myAPINametest2] If you need to validate an individual aud claim you have two choices you can either have different Lambdas authorizing different api routes and methods with hardcoded aud variables or you can get the name of the api being called and map it back to something that would match an aud claim For example the method arn for the incoming request can be found in event methodArn This looks like arnawsexecuteapi regionId accountId apiId stage httpVerb potentially with [ resource [ childresources ]] dependent on your implementation With a bit of string manipulation you could map this back to the format of an audience claim whatever that looks like for you If you would rather work an api name instead of an api name you can use the apigateway getrestapi method for whatever sdk you are using Documentation on this method for the JavaScript sdk can be found here The JWT may have a sub subject claim again dependent on implementation of the JWT issuer this could relate to any of your users or at least the users the JWT issuer knows about Validating this beyond checking the signature of the JWT is pointless The best you could do is check if that user exists and only if you have access to the same user database that the JWT issuer does Although this can be used to ensure that UserA only has access to UserAs data Again you are trusting that the fact a bearer has a token is proof that they have authenticated proved who they are I hope that answers part one of your question In regards to part 2 the advantage of using a Lambda Authorizer over doing the authorization in the target Lambda is caching So envisage I have a token valid for one hour and I call your API once every second for 30 minutes 1800 calls You have a Lambda authorizer with a response cache time of 10 minutes That means you check the JWT 3 times for 1800 api calls But if you do the validation of that token in your target Lambda you are doing that processing 1800 times "
58974135,58951709,"stackoverflow.com",0,"2019-11-21 13:40:23+02","2024-05-17 04:56:13.042545+03","There is authentication with AWS Cognito which will allow you to use authentication outside your application and still being able to let your application divide the users into subgroups based on custom roles or similar If for some reason you cannot use AWS Cognito Why not there is both the possibility of custom authentication the same for each function with SLS or even authenticating with custom code inside each lambda function I would recommend to use Cognito and only custom if you know you need it "
59005249,58958175,"stackoverflow.com",1,"2019-11-23 08:45:59+02","2024-05-17 04:56:13.906819+03","I faced the same problem and finally got boto3 SQS documentation click here that must meet your expectation I believe A sample example i can show you here "
62049130,58968492,"stackoverflow.com",1,"2020-05-27 20:22:36+03","2024-05-17 04:56:14.82715+03","One of the comments mentioned CopyWebpackPlugin which I think is what you are looking for as the most canonical webpack way to Cop[y] individual files or entire directories which already exist to the build directory After install npm install copywebpackplugin savedev Require it in your webpack config file const CopyPlugin require copywebpackplugin And update the plugins property in the config"
62028309,58968492,"stackoverflow.com",0,"2020-05-26 20:50:32+03","2024-05-17 04:56:14.82915+03","Webpack just build referenced files with require if your file ejs in your case is not required in any js file it will not be include Also webpack is almost always to transpile or build several js files into one minified or compresed javascript file A ejs file is required in a static way so no minified or compresed is required As comments said you add the ejs files after webpack result as a folder"
58968847,58968720,"stackoverflow.com",3,"2019-11-21 08:48:17+02","2024-05-17 04:56:15.610554+03","The approach I like using here is a singleton class like this So you import MyMongoConnection anywhere and just ask for the instance using getInstance if it exists it reuses it Hope this helps "
59063355,58976146,"stackoverflow.com",3,"2019-11-27 07:08:24+02","2024-05-17 04:56:16.919859+03","There is a typo in your IAM policy The word Resources should be changed to Resource "
58992688,58983901,"stackoverflow.com",2,"2019-11-22 12:51:38+02","2024-05-17 04:56:17.496057+03","Running serverless deploy is not just one call it is many AWS example oversimplification And those calls can change dependent on what you are doing and what you have done before The point I am trying to make is is that these calls which contain your credentials are not all located in one place and if you want to do a full code review of Serverless Framework and all it is dependencies have fun with that But under the hood we know that it is actually using the JavaScript awssdk go check out the package json and we know what endpoints that uses service region amazonaws com So to prove to your employers that nothing with your credentials is going anywhere except AWS you can just run a serverless deploy with wireshark running other network packet analyzers are available That way you can see anything that is not going to amazonaws com But wait why are calls being made to serverless com and serverlessteam com when I run a deploy Well that is just tracking some stats and you can see what they track here But if you are uber paranoid this can be turned off with serverless slstats disable "
58990235,58989363,"stackoverflow.com",0,"2019-11-22 10:27:46+02","2024-05-17 04:56:18.489089+03","You should probably consider using a module designed to parse multipart form data such as httpswww npmjs compackageparseformdata "
58424764,58406594,"stackoverflow.com",0,"2019-10-17 07:07:11+03","2024-05-17 04:56:20.267048+03","The solution was to wrap the response body with JSON stringify The response body must be a string I had nested objects in my axios result that I was including in my function response as mockResponse "
58406957,58406594,"stackoverflow.com",0,"2019-10-18 12:19:10+03","2024-05-17 04:56:20.268494+03","With the await syntax available now for Node in Lambda you can make some changes that better fit that style For example you no longer need a callback and you can directly return the response object This may solve your problem"
58527102,58450176,"stackoverflow.com",0,"2019-10-23 19:12:39+03","2024-05-17 04:56:21.415398+03","If you are using npm I recommend creating a package json file in shared Then you can simply require it in the package json files in function1 with a relative path Then you can remove the package logic from the serverless yml file and just let the package manager handle things The Serverless Framework will resolve those packages when packaging your application for deployment "
58802897,58455418,"stackoverflow.com",1,"2019-11-11 16:03:50+02","2024-05-17 04:56:22.301135+03","I think the best way is to use the serverlesspseudoparameters plugin and then do something like arnawslambda AWSRegion AWSAccountId functionAsyncServicedevAsyncWorker"
58472969,58471308,"stackoverflow.com",1,"2019-10-20 15:25:47+03","2024-05-17 04:56:23.386986+03","Okay in case someone else faced the same issue this SO answer helped me "
58479296,58479173,"stackoverflow.com",0,"2019-10-21 06:05:44+03","2024-05-17 04:56:24.224482+03","I suppose you are using an ORM for postgress You can check the documents of the ORM provider you are using For using queries I suggest nodepostgress nodepostgres is a collection of node js modules for interfacing with your PostgreSQL database then an example using nodepostgress Note you can write any query "
58499277,58482806,"stackoverflow.com",0,"2019-10-22 10:39:40+03","2024-05-17 04:56:25.056108+03","Since you are using the Serverless Framework in combination with the serverlesspythonrequirements plugin you should make sure that slim packaging is off as this interferes with some libraries Essentially some libraries assume they are installed via a real package manager and in that case the directory layout and files look slightly different I was having this problem with the jsonschema package a few months ago and wrote about it in a blog post if you are interested Full disclosure this is the blog of my employer Essentially your serverless yml as a custom section which you can use among other things to configure the plugins The slim true parameter effectively removes some information that should not be necessary for most libraries but some rely on it For information on this parameter see the packages docs Try again with this configuration"
59111860,58482806,"stackoverflow.com",0,"2019-11-30 01:11:22+02","2024-05-17 04:56:25.058639+03","The solution to the error is to remove the version of Python 3 7 and implement 3 6 since Graphene does not support versions greater than 3 6 "
58526977,58487344,"stackoverflow.com",0,"2019-10-23 19:05:32+03","2024-05-17 04:56:25.8458+03","If you are using npm I recommend creating a package json file in common_code_1 and common_code_2 Then you can simply require them in the package json files in api1 with a relative path Then Serverless Framework will resolve those packages when packaging your application for deployment "
58514186,58499081,"stackoverflow.com",1,"2019-10-23 04:20:16+03","2024-05-17 04:56:26.546182+03","Changing the service name is not a very scalable solution You should retain the same service name and deploy using a different stage to get a different endpoint depending on the stage it is deployed to You can use the stage to manage environment variables so that it can be set to one value on one stage and another value on another stage See an example in this SO Question Also nodejs8 10 is EOL on AWS Use nodejs10 x instead "
58503210,58499081,"stackoverflow.com",0,"2019-10-22 14:25:44+03","2024-05-17 04:56:26.547797+03","Well after a while I figure out the solution The first parameter of the serverless yml file is service myService Changing that parameter actually generates a deployment which creates different endpoint "
58506242,58503480,"stackoverflow.com",0,"2019-10-22 17:13:02+03","2024-05-17 04:56:27.336004+03","If you do not need API gateway specific features such as usage plan You can put two lambda behind ALB per path routing "
58512539,58512322,"stackoverflow.com",1,"2019-10-23 00:27:05+03","2024-05-17 04:56:28.428031+03","The major difference between versions 12 and version 3 is JS syntax Version 12 use more modern asyncawait syntax Version 3 is older and uses callbacks You can read about more of the differences here httpsmedium comThatGuyTinuscallbacksvspromisesvsasyncawaitf65ed7c2b9b4 As you noted the only difference between Versions 1 and 2 is the Context object This is provided by AWS and includes helpful lambdaspecific attributes and methods like getRemainingTimeInMillis which tells you how much longer your function can execute You can read more about that here httpsdocs aws amazon comlambdalatestdgnodejsprogmodelcontext html I would say that most people are using Version 1 these days "
65698363,58524563,"stackoverflow.com",7,"2022-02-28 20:10:30+02","2024-05-17 04:56:29.265533+03","I do not know what you exactly want to do with Django but I implemented Azure Functions to run Django commands i e httpsdocs djangoproject comen3 1howtocustommanagementcommands First I followed the steps to create a function app in a custom docker Linux container httpslearn microsoft comdedeazureazurefunctionsfunctionscreatefunctionlinuxcustomimagetabsbash2Cportalpivotsprogramminglanguagepython This includes setting up the function app via the Azure functions CLI at the root of my Django project and a Docker container which is based on the docker image mcr microsoft comazurefunctionspython2 0python3 7slim Then I created an Azure function via the CLI func new name myfunction template Timer trigger and to spin up Django and call the custom command my function code looks like The important part is to setup Django inside the function code The Azure Function folder e g myfunction is on the top level of the project I e the same level as the Django projects which contain the command e g project_1managementcommandsmy_command py I e you have Maybe it helps also for your purpose If you have any questions I can also give you more details "
58582529,58528061,"stackoverflow.com",2,"2019-10-27 21:14:27+02","2024-05-17 04:56:30.782791+03","I think I only had that issue when was using the custom authorizer with the type token The query string information will only be present on the authorizer with the type request httpsserverless comframeworkdocsprovidersawseventsapigateway Note that changing the type from type token to type request it will change the way you cache key of the policies Also more information here httpsaws amazon comblogscomputeusingenhancedrequestauthorizersinamazonapigateway"
58528536,58528061,"stackoverflow.com",2,"2019-10-23 20:56:27+03","2024-05-17 04:56:30.784373+03","I think you need to use curly braces id instead of colons id"
58533706,58531837,"stackoverflow.com",3,"2019-10-24 06:25:14+03","2024-05-17 04:56:31.104808+03","SAM is a superset of CloudFormation so the CloudFormation commands should work see httpsdocs aws amazon comAWSCloudFormationlatestUserGuidedynamicreferences html"
58938749,58548512,"stackoverflow.com",2,"2019-11-19 18:29:46+02","2024-05-17 04:56:31.803197+03","The newer versions of node fetch is having issues with webpack that you can find about in this thread httpsgithub combitinnnodefetchissues450 Downgrade to [email protected] to get your cognito working "
63398810,58548512,"stackoverflow.com",0,"2020-08-13 18:53:22+03","2024-05-17 04:56:31.804932+03","if webpack is used in the project global fetch will not work Add following library in app root level recommended for serverlesswebpack any other web pack for lambda codestart "
58578933,58574842,"stackoverflow.com",1,"2019-10-27 13:34:39+02","2024-05-17 04:56:32.912334+03","You can always pass an input to your schedule event and use that to determine that is a schedule event There is more examples here httpsserverless comframeworkdocsprovidersawseventsschedule EDIT Forgot about this you can also check detailtype field httpsdocs aws amazon comAmazonCloudWatchlatesteventsRunLambdaSchedule html"
58604510,58589368,"stackoverflow.com",0,"2019-10-29 11:25:04+02","2024-05-17 04:56:33.797936+03","I think the error message was a bit misleading As per my comment there was an offending line in requirements txt The line i httpspypi python orgsimple needed to be removed "
57978009,57972248,"stackoverflow.com",12,"2019-09-17 18:58:29+03","2024-05-17 04:56:34.97785+03","Solved this issue by removing the app and org option son the serverless yml file "
57974572,57974473,"stackoverflow.com",2,"2019-09-17 15:49:02+03","2024-05-17 04:56:36.000674+03","You have understood it correct but Lambda in VPC works slightly different To get Internet Access from Lambda in VPC you need to have a NAT gateway NAT instance etc you cannot work with IGW with Lambda because ENI used for Lambda does not get assigned a public IP address hence IGW route does not work Here is the document httpsaws amazon compremiumsupportknowledgecenterinternetaccesslambdafunction You can attached a NAT gateway to the Subnet Routing table and it would work NAT gateway should be launched in Subnet with routing table with IGW "
57974632,57974473,"stackoverflow.com",1,"2019-09-17 15:51:40+03","2024-05-17 04:56:36.002674+03","Lambda will not get a public ip so you need a NAT gateway in a public subnet The routing table to internet gateway is not a valid option When you turn on a VPC in your Lambda function the info message will appear When you enable a VPC your Lambda function loses default internet access If you require external internet access for your function make sure that your security group allows outbound connections and that your VPC has a NAT gateway "
67784248,62836558,"stackoverflow.com",1,"2021-06-01 10:12:17+03","2024-05-17 05:21:34.695393+03","Add type property in your defined schema and set it to strict type In your case Worked for me "
58007446,58007371,"stackoverflow.com",12,"2019-09-19 16:26:44+03","2024-05-17 04:56:38.954209+03","Updating only the infrastructure with the Serverless Framework is not something achievable as of right now You will need to perform a full deployment even if there were no code changes However executing a regular sls deploy will not do the trick if no code has changed as the framework will not detect infrastructure changes only If you want to force a redeployment i e you have hooked up a new trigger for your Lambda function in your serverless yml file you must force the deployment by using the force flag sls deploy force"
58030387,58029898,"stackoverflow.com",2,"2019-09-20 20:37:20+03","2024-05-17 04:56:39.715546+03","You have not said much about how you create and deploy the function The link you posted mentions it is possible to edit the VPC Connector field after a function is deployed So that should be one way Deploy the function and then edit its settings to specify the VPC Connector to use Update A more configurable solution might be to use a deployment manager template You can use the one available here as a starting point But it does not have VPC connector configuration builtin so you will need to update that template to include VPC connector configuration "
60365562,58029898,"stackoverflow.com",1,"2020-02-23 20:41:50+02","2024-05-17 04:56:39.717546+03","You can add a VPC connector by adding vpc property to a function in serverless yml No idea why it is not in the documentation "
58077090,58036272,"stackoverflow.com",1,"2020-06-20 12:12:55+03","2024-05-17 04:56:40.52271+03","That was because No Method Response"
58039345,58036272,"stackoverflow.com",0,"2019-09-21 13:22:55+03","2024-05-17 04:56:40.52371+03","You should not manually go on the console to enable CORS Instead follow this guide from the serverless framework In short"
77178342,58036272,"stackoverflow.com",0,"2023-09-26 11:52:49+03","2024-05-17 04:56:40.52471+03","You can unselect the methods except options and then change the AccessControlAllowOrigin and then click on the enable cors button there wont be any changes even after you enabled it just redeploy your apis it might work this worked for me httpsmedium comgeekculturesimplestepstoenablecorsinapigatewaythroughconsolecloudformationc09d9df31c07"
58056585,58054877,"stackoverflow.com",0,"2019-10-09 05:13:06+03","2024-05-17 04:56:41.29363+03","The typical configuration when connecting from an AWS Lambda function to an Amazon RDS database is That is RDSSG specifically references LambdaSG in the inbound rule If the Lambda function also needs to connect to the Internet then there will need to be a NAT Gateway in the public subnet of the VPC "
58064381,58061250,"stackoverflow.com",3,"2019-09-23 17:11:56+03","2024-05-17 04:56:41.879853+03","Add empty __init__ py files in the serverless src and projectRepo folders Then you can use the from projectRepo src utils import whatever_functions in handler py provided you are running your code with projectRepo as working directory unless you actually add a package setup to install your code as a package e g in a local virtual environment "
58095673,58073360,"stackoverflow.com",0,"2019-09-26 11:48:45+03","2024-05-17 04:56:42.575662+03","On the scenario that you are using the imagemagick you can simply download the images into the tmp folder and process it or by running process with node child processes If you want to use other cli that the lambda image does not have there is also ways to include it in the zip of the lamdba httpnmajor compostsserverlessframeworkexecutablebinariesawslambda Notes another references that might help AWS lambdas limits httpsdocs aws amazon comlambdalatestdglimits html"
64618596,58083730,"stackoverflow.com",1,"2020-10-31 07:03:57+02","2024-05-17 04:56:43.290786+03","My code was inspired read mostly copied from Faraday Gem at Faraday Gem Github You will need to use gem multipartparser 0 1 1 and create a util rb file Then you can use it like so Not sure how this will handle images but it works fine for CSV You can modify parse_multipart to just return the hash but I left it because it seem to provide a more generic structure to the data You can use the first method like this"
73551605,58083730,"stackoverflow.com",1,"2022-08-31 11:35:38+03","2024-05-17 04:56:43.29258+03","Racks multipart parser can parse multipart form data In case it helps I have published the snippet above as a ruby gem httpsrubygems orggemsmultipart_form_data_parser httpsgithub comnisanth074multipart_form_data_parser"
58098117,58091043,"stackoverflow.com",2,"2019-09-25 15:13:47+03","2024-05-17 04:56:44.695262+03","You have the batch size set to 100 which tell Lambda to read 100 records before invoking your function There are 2 settings related to batch Before invoking your function Lambda continues to read records from the stream until it has gathered a full batch or until the batch window expires I have not done performance testing with these 2 setting but I would start by setting my size to 1 and my window to 0 However there could be side effects from launching a large amount of Lambdas but it should give you the minimum delay possible "
58093124,58092402,"stackoverflow.com",0,"2019-09-25 10:21:07+03","2024-05-17 04:56:45.175895+03","You have to give AWS SES access to your Lambda existing role Just go to"
63684696,58110563,"stackoverflow.com",2,"2020-09-01 12:25:56+03","2024-05-17 04:56:45.936313+03","You can follow the aws documentation to configure the life cycle rule for s3 httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawspropertiess3bucketlifecycleconfigrule html ex in YAML format "
58211186,58113632,"stackoverflow.com",1,"2019-10-03 04:15:53+03","2024-05-17 04:56:47.224876+03","Since YAML is just from the docs a human friendly data serialization standard for all programming languages there is no proper way to have one file that fits both architectures by looking at both examples the just change a few linesso you wont be able to use the same file but it will be very similar Yes both services need specific configurations for them to work correctly To access GCP resources you will use service accounts this is all managed by Cloud IAM and it is made to represent a nonhuman user in this case a app an API service etc EXTRA Some useful links"
58210792,58116840,"stackoverflow.com",1,"2019-10-03 03:06:44+03","2024-05-17 04:56:47.767831+03","Problem turned out to be an issue with the environment variables When using FFmpeg as a layer assuming it is in a directory called ffmpeg use these environment variables"
57588870,57576427,"stackoverflow.com",1,"2019-08-21 12:48:01+03","2024-05-17 04:56:49.790126+03","I found a solution to this by adding a Gateway Response to override the default Bad Request Body response Unfortunately as of Serverless 1 50 0 this is not directly supported see github issue But was able to add the Gateway Response via a Serverless resource i e Cloudformation see above link Note I was not able to find a way to indicate which property failed validation but at least the validation message is a little more detailed"
57579135,57578938,"stackoverflow.com",2,"2019-08-20 20:52:33+03","2024-05-17 04:56:50.514607+03","The serverless folder will get regenerated every time you deploy and is just a build artifact you can safely deleteignore it "
57580168,57579797,"stackoverflow.com",1,"2019-08-20 22:10:41+03","2024-05-17 04:56:51.120496+03","You can do it in provider section"
57586868,57580631,"stackoverflow.com",7,"2019-08-21 10:51:17+03","2024-05-17 04:56:51.845991+03","Since serverlesswebpack does the packing for you and not the serverless framework you will need to use a Webpack plugin As mentioned by hephalump it is better to use AWS Secrets Manager or Parameter StoreEnvironment variables "
57584468,57580631,"stackoverflow.com",1,"2019-08-21 07:31:26+03","2024-05-17 04:56:51.846992+03","Although you can definitely include your certificate files as part of your deployment package and without more info Im not certain why theyre not being included a more secure method would be to store your certificatekey in AWS Secrets Manager and then access that secret in your Lambda You can learn more about AWS Secrets Manager here and there is a tutorial to store and retrieve a secret here "
57586564,57580849,"stackoverflow.com",1,"2019-08-21 10:29:21+03","2024-05-17 04:56:52.56928+03","I have done something similar here I have trimmed the serverless yml for the relevant parts"
57669162,57585157,"stackoverflow.com",0,"2019-08-27 09:56:25+03","2024-05-17 04:56:53.576811+03","You are getting quotes in start and end of the html because of you are returning it as string What you can do is rather returning the replay in the string you can return the Html One more thing is not to forget the response header ContentType as texthtml once you set the response header the client browser will understand the response and start parsing it "
57669197,57585157,"stackoverflow.com",0,"2019-08-27 09:58:30+03","2024-05-17 04:56:53.577811+03","I think you just forgot to set ContentType to texthtml "
57595726,57594671,"stackoverflow.com",2,"2019-08-21 19:19:16+03","2024-05-17 04:56:54.285266+03","This is the way to do it No need to use await for synchronous functions and by wrapping subprocess with a promise you can await on it "
57609758,57608422,"stackoverflow.com",9,"2019-08-22 15:49:48+03","2024-05-17 04:56:55.400146+03","The Lambda function should only be connected to a private subnet This enables Internetbound traffic to be routed to the NAT Gateway in the public subnet If the Lambda function connects to the public subnet it will be unable to reach the Internet "
57611491,57608422,"stackoverflow.com",2,"2019-08-22 17:21:17+03","2024-05-17 04:56:55.401803+03","A mixture of two things I think 1 as was pointed out the Lambda should only be connected to private subnets I think I would also mixed up the availability zones between the public and private subnets Anyway the Lambda is now successfully connecting to RDS hitting external APIs and I can also connect through DBeaver Thanks a lot "
57626520,57624100,"stackoverflow.com",3,"2019-08-24 10:31:43+03","2024-05-17 04:56:56.498224+03","According to the manpage it should be just like the following Or in more context from the serverless yml More info httpsserverless comframeworkdocsprovidersawseventsschedule"
64372357,57624100,"stackoverflow.com",0,"2020-10-15 16:13:13+03","2024-05-17 04:56:56.500206+03","functions myjob handler handler myjob events schedule rate 1 minute Source httpswww serverless comblogcronjobsonaws"
57640010,57639922,"stackoverflow.com",2,"2019-08-24 20:10:04+03","2024-05-17 04:56:56.93118+03","Just only using rate or cron not both This mean your syntax is wrong A suggestion for your case Official document Serverless Schedule"
59938001,57648157,"stackoverflow.com",1,"2020-01-27 21:54:40+02","2024-05-17 04:56:58.06462+03","The error message Unable to marshal response repr is not JSON serializable occurs when the handler returns a value that is not JSON serializable to Lambda At some point something is trapping an exception and returning the exception object Let us then look at OSError 30 Readonly file system This is also a common issue with Lambda writing is only permitted under tmp Your code is mounted in a readonly volume I have wrapped my handler in a trycatch but it does not even get to my code Strictly it does not get to your handler Importing a module runs it from the top those def and class blocks are just glorified assignments into the module namespace And when you import other modules they can run arbitrary code and they can absolutely try to write to the filesystem One simple way to diagnose this further would be to Since you are pulling in PythonRequirementsLambdaLayer directly you could also unpack that into your test rig to figure out where it is breaking "
57661374,57653077,"stackoverflow.com",1,"2019-08-26 19:06:41+03","2024-05-17 04:56:58.610923+03","In the resources section of your serverless yml you will need to add the CloudFormation definition of the IoT rule you created Something like See httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawsresourceiottopicrule html for information on the topic rule definition for CloudFormation"
57717271,57663547,"stackoverflow.com",1,"2019-08-31 00:06:52+03","2024-05-17 04:56:59.57148+03","I experienced this issue because I was using the serverless framework without docker There is currently as of Aug 29 2019 a bug in serverlesspythonrequirements that prevents dockerizing pip with private repositories My solution was to remove serverless and convert to the gcloud CLI When you upload requirements txt to GCloud it automatically installs the public ones but cannot install private repos because it does not have git credentials To get around this you must install those requirements locally before uploading the package to gcloud Here is a link to my solution "
57666642,57665970,"stackoverflow.com",7,"2019-08-29 02:16:23+03","2024-05-17 04:57:00.303314+03","I figured it out I had a NodeJS project and was using the serverless command line sls to deploy using serverless yml It turns out it creates a serverless subdirectroy with some files in it One of them is a compiled template for AWS Cloud Formation called cloudformationtemplateupdatestack json It appears that the utility likes to mangle the names by making the first character uppercase and adding LambdaFunction to all the function names for whatever reason In this case bearerTokenAuthentication was renamed to BearerTokenAuthenticationLambdaFunction the actual resource name After looking into the compiled template it all became clear The utility also seems to figure out the dependencies as well which was good to know This was the final result Other Gotchas DO NOT define the AWSApiGatewayRestApi resource like I did in my question if you are also using event mappings with the functions otherwise you will get 2 APIs created event entries automatically cause an API to be created called ApiGatewayRestApi which is the resource name generated by the sls utility The last line of the last file was changed to this And my ApiGateway section was removed Credit goes to this post which helped make it more clear to me what was really going on httpsforum serverless comtfixedhowdoigetreferenceapigatewayrestapiidinserverlessyml33975 Previous Answer I found another way as well This is what I resorted to doing until I found the proper shorter way I was able to pull the lambda name and manually stitch together the required URI I hope that helps save someone some time trying to understand the complicated yml file I also cannot understand why it is so hard to make it simple to understand All someone had to do is say for me was sls takes a serverless yml file and optional include files such as declarations specific to the cloud system itself like AWS Cloud Formation and generates a template JSON file that is used by the target cloud services system to deploy your solution Also the names you give may get mangled so check the template I am also surprised that no one has created an editor to make all this easier by now perhaps something I will look into myself one day "
61290992,57665970,"stackoverflow.com",1,"2020-04-18 17:42:53+03","2024-05-17 04:57:00.307315+03","You can always go to the deployed lambda and look for the awscloudformationlogicalid tag That way you get the logical ID you should be using in your serverless yaml Do not like this behindthescenes names tricks either "
58126990,57677713,"stackoverflow.com",0,"2019-09-27 05:32:05+03","2024-05-17 04:57:01.337532+03","I have just gone through the steps on the documentation on serverless that you had referenced and everything is working for me You would definitely want to make sure that you run sls offline prior to npm start so that function json file for each of the hello and goodbye functions get generated "
57746439,57691028,"stackoverflow.com",3,"2019-09-01 16:35:29+03","2024-05-17 04:57:02.358642+03","We can send json message to AWS SNS to send push notification to application endpoint Which allows us to send platform APNS FCM etc specific fields and customizations Example json message for APNS is This is how you can send request If you need to support multiple platforms then as per AWS docs To send a message to an app installed on devices for multiple platforms such as FCM and APNS you must first subscribe the mobile endpoints to a topic in Amazon SNS and then publish the message to the topic APNS specific payload keys can be found APNS Payload Key Reference AWS SNS documentation can be found here Send Custom PlatformSpecific Payloads to Mobile Devices"
74375728,57123404,"stackoverflow.com",1,"2022-11-09 15:35:32+02","2024-05-17 04:57:04.528204+03","Super late to the game but maybe someone will stumble upon this question in the future In order to query dynamoDB you have to use the correct format the one that you are using when running the query from the terminal notice the JSON structure S yourid Keeping this in mind your query should look like this Marshalling your object will create the needed format for DynamoDB If you still have issues print out the marshalled version of the ID and make sure it is defined as S meaning that it is a string If it is a number you might get issues A simple toString would suffice to resolve that tho Cheers"
59329383,57129011,"stackoverflow.com",0,"2019-12-13 22:25:23+02","2024-05-17 04:57:05.317559+03","Just run this command sls logs f hello t"
63419531,57167856,"stackoverflow.com",10,"2020-08-14 23:19:15+03","2024-05-17 04:57:05.914436+03","Your function defined in serverless yml is converted into a cloudformation resource under the hood The resource is called XLambdaFunction where X The name of your function with the first letter capitalized So if you have You can reference I can only assume that if your function is already named CognitoCustomMessageLambdaFunction then you would have to reference"
57168701,57167856,"stackoverflow.com",8,"2022-01-07 12:41:24+02","2024-05-17 04:57:05.915435+03","The DependsOn attribute should have the logical name of the Lambda Function in the Cloud formation template not the ARN of Lambda Function For example if your Lambda functions logical name in the Cloud Formation template is MyLambda then DependsOn should be like this NOTE open cloud formation template serverlesscloudformationtemplateupdatestack json and look for logical lambda function name Example MonitorLambdaFunction is the name you are looking for "
64992488,57167856,"stackoverflow.com",0,"2020-11-24 20:14:53+02","2024-05-17 04:57:05.917473+03","I faced similar issue adding dependsOn key worked for me "
59709017,57183102,"stackoverflow.com",2,"2020-08-10 07:11:53+03","2024-05-17 04:57:09.779565+03","It is not possible to return unencoded binary data from a direct invoked AWS Lambda function Per the docs If the handler returns objects that cannot be serialized by json dumps the runtime returns an error The reason you can do this with API Gateway is because API Gateway is performing the conversion of the base64 JSON content your function returns into binary for you See documentation here I would need to know more about how you are invoking Lambda to be sure but I suspect you could implement this same base64 decode logic into your direct invoke client Alternatively if you wanted to keep the client as simple as possible use S3 with a lifecycle hook to keep the bucket from filling up with temporary files "
57296868,57202252,"stackoverflow.com",2,"2019-07-31 21:25:22+03","2024-05-17 04:57:11.686717+03","Resolved the issue by updating the serverless framework to the most current version 1 49 0 in my case using"
59937949,57206342,"stackoverflow.com",2,"2020-01-27 21:51:07+02","2024-05-17 04:57:13.076657+03","After much digging I found out that serverless does not support the pathing options offered by typescript For anyone else it appears the options are 1 Copy the code 2 Utilize Webpack plugin to override servlesss and allow setting the rootDir in typescript 3 Symlink your shared module code to the each of your microservices directories I wrote a bash script that will symlink a directory called _shared to all directories one level under a parent directory called microservices find microservices path microservices type d maxdepth 1 d exec ln s _shared \ You will then want to add each symlink to your gitignore microservices_shared So the workflow for the project is First time you clone the repo run the bash script to create the symlinks When you are in a microservices sub one level directory reference imports from _shared from the symlink on that same level Make and commit all changes to the top level _shared directory"
57209971,57209484,"stackoverflow.com",3,"2019-07-25 23:52:24+03","2024-05-17 04:57:13.557997+03","The reason it happens is that you are trying to set the Node js version running in your Lambda function to Node js 6 and this is no longer supported Change your serverless yml file to The error message is pretty clear though AWS has discontinued the support for Node js 6 one or two months ago Only functions that were originally created in this version will still work New functions can no longer be created in Node js 6 Honestly there is not a single reason somebody would want to do it anyways httpsaws amazon comblogsdevelopernodejs6isapproachingendoflifeupgradeyourawslambdafunctionstothenodejs10lts"
57214037,57213929,"stackoverflow.com",1,"2019-07-26 09:07:26+03","2024-05-17 04:57:15.516113+03","Your IAM policies seem ok I think the problem might be with the response Why are you trying to stringify the url returned This should be directly used Try logging the returned url and using that If that fails you might want to look at IAM policiesroles further "
57320709,57228078,"stackoverflow.com",0,"2019-08-02 08:59:21+03","2024-05-17 04:57:16.536803+03","PDF generation in Lambda functions can be quite finicky as it typically requires and assumes a number of things from the runtime under which it runs fonts OS versions etc After some trial and error in the end my approach to PDF generation in Lambda was to use wkhtmltopdf and upload the binary for it along with my function code though my use case vastly preferred HTML formatting for the rendered document either way Note that in order to return binary documents through API Gateway you have to explicitly map certain content types as binary else you are going to end up with a base64 encoded string response An easy way to do this is to use a Serverless plugin such as serverlessapigwbinary and make sure you set the response content type correctly in your Lambda code "
57243348,57239101,"stackoverflow.com",0,"2019-07-28 20:27:42+03","2024-05-17 04:57:17.524033+03","You cannot delete from a GSI These indexes are pretty much read only you cant mutate the data in the table through a global secondary index so no inserting deleting or updating You can only read from the GSI and then implement the necessary logic to delete the items in the main table by key Also a batch operation doesnt make it a whole lot more efficient to delete those items yes it saves on network calls up to 251 but not on used write capacity "
57322659,57257457,"stackoverflow.com",0,"2019-08-02 11:31:03+03","2024-05-17 04:57:18.42+03","The problem is in these lines where I promisifying the startExecution Function aswell as there is the callback the right code should be like this below"
56703600,56589113,"stackoverflow.com",0,"2019-06-21 15:35:42+03","2024-05-17 04:57:22.35422+03","This was due a plugin not handling variables properly and has been fixed "
56624060,56623501,"stackoverflow.com",1,"2019-06-17 05:25:05+03","2024-05-17 04:57:24.234553+03","For reference in the future I sort of found my answer I do not have time at the moment to dig much deeper It was to do with the permissions of my custom auth function not allowing the specific Arn to go through even though it was setting it So all I did was put an for now rather than the specific arns for the functions and called it like this instead of passing the methodArn Not the safest but I will fix it up latter "
56631826,56627160,"stackoverflow.com",0,"2019-06-17 15:57:10+03","2024-05-17 04:57:24.949935+03","It looks like it is not implemented"
56687901,56687900,"stackoverflow.com",3,"2019-06-20 17:12:09+03","2024-05-17 04:57:25.959146+03","Well through prolonged research and some guessing I found the answer and decided to post it here for future me and others to find But first let us take a look at the actual problem The body will not be in the root but under input body and then Jackson does not know where to find your person So first we need to change from lambdaproxyintegration to lambdaintegration And then we need to tell the integration to hand over the body as payload to the function This gives us the following serverless yml e voila now your POJO will be populated Hope this helps and let me know if anybody found a simpler or better solution to this Sources httpsserverless comframeworkdocsprovidersawseventsapigatewayrequesttemplates Could not parse request body into json Unexpected character \\ code 45 AWS Lambda API Postman for formatting the yml "
56876878,56690298,"stackoverflow.com",6,"2019-07-03 22:34:21+03","2024-05-17 04:57:27.025462+03","Found the issue it was actually not related to the user pool I had a resource that created the default user which had not set the DesiredDeliveryMedium property said property defaults to SMS setting it to EMAIL solved it "
78172399,56690298,"stackoverflow.com",0,"2024-03-16 17:53:10+02","2024-05-17 04:57:27.026968+03","As of 20240316 the argument is DesiredDeliveryMediums[EMAIL]"
50153363,50105577,"stackoverflow.com",0,"2018-05-03 13:56:28+03","2024-05-17 05:00:20.112785+03","It looks like you hit a hard limit here As a workaround you could try to split those subscriptions in 2 lambdas with the same handler"
56768192,56698808,"stackoverflow.com",0,"2019-06-26 11:13:21+03","2024-05-17 04:57:27.832059+03","I do not think it is a best practice to call another lambda function like this I would suggest to extract the code you need in a third sharedutil module and have both modules import the functions they need from the third module"
56768629,56698808,"stackoverflow.com",0,"2019-06-26 11:38:48+03","2024-05-17 04:57:27.83306+03","I have figured this one out"
56700800,56700731,"stackoverflow.com",4,"2019-06-21 12:54:59+03","2024-05-17 04:57:28.907544+03","In order to get response you have you implement callback function on caller function like below "
56908948,56709408,"stackoverflow.com",1,"2019-07-14 23:21:11+03","2024-05-17 04:57:29.9988+03","Digging into this it is because you are trying to upload an object with a public ACL into a bucket that does not allow public objects Optionally remove the public ACL statement or Ensure the bucket set to either Basically you cannot upload objects with a public ACL into a bucket where there is some restriction preventing that you will get the 403 error you describe HTH "
56768056,56712973,"stackoverflow.com",1,"2019-06-26 11:04:37+03","2024-05-17 04:57:30.99817+03","If this is a simple POST request using a JSON as the body that is how I usually get the body Then you can use different ways to get its keys and vaues Object keys Simply accessing the properties of body or object destructuring EDIT adding info for lambda invoked using an event file In that case I usually do instead of Rest remains the same Hope it helps "
61002745,56739402,"stackoverflow.com",0,"2020-04-03 02:20:18+03","2024-05-17 04:57:33.172153+03","You can allow for assumed roles using an external ID by creating an AWSIAMRole resource The example below gives access to an S3 bucket but the approach is the same "
56765893,56741638,"stackoverflow.com",0,"2019-06-26 08:38:11+03","2024-05-17 04:57:33.964061+03","There are a few things to check How long does your Lambda function take to execute Your function will only handle one request at a time If you make a second request before your first request completes a new instance of your function will spin up Ensure that mongoose is not closing the connection after your function completes "
55986019,55978956,"stackoverflow.com",0,"2019-05-04 22:18:57+03","2024-05-17 04:57:35.903104+03","Just figure it out The solution basically stands on top of the AWS Lambda Layers I created a sklearn layer that contains only the relevant compiled libraries Then I run sls package in order to pack a bundle which contains those files together with my own handler py code The last step was to run sls deploy package serverless Hope it will be helpful to others "
68537508,55978956,"stackoverflow.com",0,"2021-07-27 03:01:53+03","2024-05-17 04:57:35.905105+03","If you simply want to serve your sklearn model you could skip the hassle of setting up a lambda function and tinkering with the API Gateway just upload your model as a pkl file to FlashAI io which will serve your model automatically for free It handles high traffic environments and unlimited inference requests For sklearn models specifically just check out the user guide and within 5 minutes you will have your model available as an API Disclaimer I am the author of this service"
56000036,55999325,"stackoverflow.com",1,"2019-05-06 09:44:53+03","2024-05-17 04:57:38.734293+03","So I did not find the reason for the error but it fixed with removing the function and redeploying it "
55999376,55999325,"stackoverflow.com",0,"2019-05-06 09:04:43+03","2024-05-17 04:57:38.735073+03","Is it possible that the Identity Aware Proxy has been enabled for the Cloud Function URLs If you navigate to Cloud Console and then to Security and IdentityAware Proxy you should be able to see the IAP settings and whether the Cloud Function is being protected by IAP If that is not the cause I would advise putting some logging in your function that would make it clear whether the function is getting called and then returning a 403 somewhere within the execution of the function indicating a problem with the function itself rather than the identity infrastructure or if the function is never getting called the 403 is being produced outside of the Cloud Function in which case you may need to reach out to Cloud Support for help with this if IAP is not the cause "
62302362,55999325,"stackoverflow.com",0,"2020-06-10 14:23:59+03","2024-05-17 04:57:38.73707+03","Google Cloud Functions added some new IAM functionality not sure how recently and now new functions dont have public access by default Incase someone else comes here I thought I would share this information here To allow your function to be invoked you first have to add permissions to the function you can do this by selecting the function in the functions list and adding allUsers to the Cloud Invokes role you can see the step by step at httpslukestoolkit blogspot com202006googlecloudfunctionserrorforbidden html"
56013088,56012953,"stackoverflow.com",2,"2019-05-07 01:19:18+03","2024-05-17 04:57:39.596807+03","I think the AWS docs are actually incorrect the JSON and YAML examples differ in output The Protocol property is indented once too many which means Endpoint would evaluate as an object Here is what your config evaluates to in JSON Here is how I think it should be"
56029121,56027074,"stackoverflow.com",1,"2019-05-07 22:13:31+03","2024-05-17 04:57:40.233734+03","When you define a service in the serverless framework you specify its behaviour in the serverless yml file for example from their Get Note chapter The line authorizer aws_iam is what is configuring your lambda function to use an authoriser in that case an IAM role If you remove this line you will deploy a function without an authoriser Functions without authorisers can be invoked by anyone This configuration is specific to each function and so you can remove the authorizer from one specification and leave it in for another In your case then and without code I am just guessing all you need do is remove the authorizer line from the specification for getPublished "
56038567,56036806,"stackoverflow.com",0,"2019-05-08 13:26:03+03","2024-05-17 04:57:41.136276+03","You cannot write to custom loggroup from AWS lambda The default loggroup associated with lambda will be at awslambdafunctionname This is how the AWS lambda is designed AWS Lambda service is kind of convention over configuration So the default log stream pattern is already defined You can do similar behavior if you happen to use EC2 machine by installing cloudwatch agent on machine cloudwatch agent configurationEC2"
56053950,56045868,"stackoverflow.com",1,"2019-05-09 10:07:48+03","2024-05-17 04:57:42.456552+03","You can implement this using cognito user groups Create 3 groups in your user pool for your 3 different package users Next go to your identity pool and click on edit Now under Authentication providers select your user pool and ensure Choose role from token is selected like this You can now create 3 different roles for your 3 different groups and attach whatever policy you want After this just put users in groups according to what plans they select When they log in they will only have the rights that were provided to them by the role attached to their group You can read more about groups here "
56065496,56064035,"stackoverflow.com",1,"2019-05-09 21:26:00+03","2024-05-17 04:57:43.38621+03","the batchSize parameter indicates the maximum number of messages to process per function invocation and does not affect when the lambda function is triggered When you define SQS as your event source AWS created 5 parallel longpolling connections and when it receives something in the queue it triggers your function How long do you wait after sending the first message to see if it triggers the function or not there might be a small delay between the message receiving in the queue and the function being triggered "
73713990,55542422,"stackoverflow.com",10,"2022-09-21 10:30:50+03","2024-05-17 04:57:55.761574+03","Here is a simpler and more modern alternative Note the code above applies to REST API API Gateway v1 not HTTP API If you are using the new HTTP API API Gateway v2 the solution is simpler"
55574456,55574283,"stackoverflow.com",1,"2019-04-08 16:20:22+03","2024-05-17 04:57:59.318027+03","You should have package json and node_modules inside each service directory it is needed because otherwise the node_modules directory will not be deployed "
56065356,56064839,"stackoverflow.com",0,"2019-05-09 21:15:21+03","2024-05-17 04:57:44.302608+03","Serverless yml file compiles down to valid CloudFormation It will only create a new VPC if it is being created for the first time If some part of its configuration are changed in the file it will update the existing one If nothing is changed it will not touch VPC at all When you deploy your CloudFormation template the service looks for differences between current deployed stack and the stack that you described in your file That is how it figures out what actions to perform So the answer is no It will not duplicate your AWS resources on subsequent deployments "
56073723,56064839,"stackoverflow.com",0,"2019-05-10 11:36:16+03","2024-05-17 04:57:44.304484+03","I do not know anything about the serverless framework but if it is properly based on CloudFormation as the other answer suggests then this is what is true about CloudFormation It depends on whether you change a property of the underlying resource that requires replacement For VPC see list of properties here httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawsresourceec2vpc html You can see that updates to CidrBlock require replacement this means a new VPC will be created and then the old one will be deleted For Subnet see httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawsresourceec2subnet html there is CidrBlock AvailabilityZone and VpcId which require replacement Obviously if the VPC is recreated so will be the subnet "
73314543,56064839,"stackoverflow.com",0,"2022-08-11 05:10:50+03","2024-05-17 04:57:44.306481+03","If you deploy the VPC using Serverless then only changes will be deployed and configured on every subsequent sls deploy You are able to change settings and your changes will be reflected on the same VPC resources You can make Serverless VPC creation a lot easier with the serverlessvpcplugin httpswww serverless compluginsserverlessvpcplugin You can see a full working serverless template here httpscarova iosnippetsserverlessawsvpccreatecustomvpcwithtemplate"
56098535,56088468,"stackoverflow.com",2,"2019-05-12 13:57:35+03","2024-05-17 04:57:45.359265+03","So in my case what I was missing For anyone with a similar problem Read this below Learning from the lesson I spent literally one day and a half stuck on this and aware that Stackoverflow is a repository of experiences and knowledge I would point out This is what my StaticSiteBucket resource declaration looks like now"
56092517,56088468,"stackoverflow.com",1,"2019-05-11 20:03:35+03","2024-05-17 04:57:45.360453+03","Some ideas to consider"
59814839,56096916,"stackoverflow.com",0,"2020-01-20 00:08:34+02","2024-05-17 04:57:46.369+03","Perhaps you can try to pass WebSocketApiId parameter value trough WEB_SOCKET_API_ID environment variable available in the handler like this"
56102464,56098792,"stackoverflow.com",2,"2019-05-12 21:52:52+03","2024-05-17 04:57:47.573989+03","Yes you can have multiple functions in the same serverless framework service definition For example you can see further information in the documentation "
56107345,56107144,"stackoverflow.com",1,"2019-05-13 10:26:59+03","2024-05-17 04:57:48.621731+03","It is because your code is async but the function passed to your forEach loop is also async so you have an async function invoking another chunk of async code therefore you lose control of the flow Whatever is inside forEach will run although anything after forEach will run before whatever is inside forEach but it will execute asynchronously and you are unable to keep track of its execution But if the code as I said will run why do not you see the results Well that is because Lambda will terminate before that code has the chance to execute If you run the same piece of code locally you will see it will run just fine but since the original code runs on top of Lambda you do not have control when it terminates You have two options here The easiest is to grab the first item in the Records array because s3 events send one and only one event per invocation The reason it is an array is because the way AWS works a common interface for all events Anyways your forEach is not using anything of the Record object but still if you wanted to use any properties of it simply reference the 0th position like so If you still want to use a for loop to iterate through the records although again unnecessary for s3 events use a for of loop instead Since for of is just a regular loop it will use the async from the function it is being executed on so await is perfectly valid inside it More on asyncawait and for of"
56117191,56107229,"stackoverflow.com",2,"2019-05-22 19:10:23+03","2024-05-17 04:57:49.591091+03","The deployment of lambda function comes down to packagemodule organization and automation deployment tools The first looks like being address in your question where the shared code is placed to util and each lambda has an individual package In the question it is not clear what the deployment method is being used There are various way to get lambda deployed While I have been e2e and automation advocate with endly runner multi lambda deployment workflow for various events may look like the following Finally you can check serverless e2e with lambda e2e practical testing examples "
55517925,55515815,"stackoverflow.com",2,"2019-04-04 17:02:22+03","2024-05-17 04:57:52.679526+03","It looks like your yaml is not correctly indented at it should be"
55518387,55517630,"stackoverflow.com",1,"2019-04-04 19:52:40+03","2024-05-17 04:57:54.322102+03","Turns out the following lines Needed to be"
55741019,55517630,"stackoverflow.com",0,"2019-04-18 10:22:26+03","2024-05-17 04:57:54.324102+03","AWSSQSQueue resource has Arn in return values so it can be accessed by FnGetAtt intrinsic function The following syntax should work or"
58072177,55539223,"stackoverflow.com",5,"2019-09-24 05:12:29+03","2024-05-17 04:57:54.818039+03","I was having a similar error to you while using the explicit layers keys that you are using to define a lambda layer My error for the sake of web searches was this Runtime ImportModuleError Error Cannot find module package name I feel this is a temporary solution bc I wanted to explicitly define my layers like you were doing but it was not working so it seemed like a bug I created a bug report in Serverless for this issue If anyone else is having this same issue they can track it there SOLUTION I followed this this post in the Serverless forums based on these docs from AWS I zipped up my node_modules under the folder nodejs so it looks like this when it is unzipped nodejsnode_modulesvarious packages Then instead of using the explicit definition of layers I used the package and artifact keys like so In the function layer it is referred to like this The TestLambdaLayer is a convention of your name of layerLambdaLayer as documented here"
59904913,55539223,"stackoverflow.com",2,"2020-01-25 01:24:21+02","2024-05-17 04:57:54.821039+03","Make sure you run npm install inside your layers before deploying ie Otherwise your layers will get deployed without a node_modules folder You can download the zip of your layer from the Lambda UI to confirm the contents of that layer "
61709006,55539223,"stackoverflow.com",2,"2020-05-10 11:13:33+03","2024-05-17 04:57:54.82304+03","If anyone face a similar issue Runtime ImportModuleError is fair to say that another cause of this issue could be a package exclude statement in the serverless yml file Be aware that if you have this statement It will cause exactly the same error on runtime once you have deployed your lambda function with serverless framework Just ensure to remove the ones that could create a conflict across your dependencies"
62795085,55539223,"stackoverflow.com",0,"2020-07-08 15:43:06+03","2024-05-17 04:57:54.82404+03","I am using typescript with the serverlessplugintypescript and I was having a same error too When I switched from to the error disappeared It seems like the files were not included into the zip file by serverless when I was using require PS Removing the serverlessplugintypescript and switching back to javascript also solved the problem "
55542762,55542422,"stackoverflow.com",17,"2019-04-05 23:31:24+03","2024-05-17 04:57:55.759579+03","I was able to pass the URL and unique ID for the API Gateway endpoint to a Lambda function as environment variables as follows Thanks to goingserverless "
55606906,55590156,"stackoverflow.com",9,"2019-04-10 10:24:06+03","2024-05-17 04:58:00.034019+03","Ok after many trials I figure out a working solution I added a npm run command which export a temporary node_module path to the list of paths So node can lookup for the node modules inside the sub folders"
62748400,55590156,"stackoverflow.com",1,"2020-07-06 05:23:30+03","2024-05-17 04:58:00.035255+03","I got around this by running serverlessoffline in a container and copying my layers into the opt directory with gulp I set a gulp watch to monitor any layer changes and to copy them to the opt directory "
65150491,55590156,"stackoverflow.com",0,"2020-12-04 22:46:16+02","2024-05-17 04:58:00.036253+03","I use layers in serverless offline via installing a layer from local file system as a dev dependency "
57353152,55590156,"stackoverflow.com",4,"2019-08-05 09:23:11+03","2024-05-17 04:58:00.037583+03","BTW this issue was fixed in sls 1 49 0 Just run Then you should specify package include in serverless ymls layer section Tested on nodejs10 x runtime"
55602228,55600767,"stackoverflow.com",1,"2019-04-10 01:31:31+03","2024-05-17 04:58:01.04146+03","I was finally able to overcome my issues LambdaEntryPoint cs example Startup cs example Note some of this can be generated from templates Outside of what is readily available on the Internet these steps highlight some of the hurdles I had to overcome to get mine working "
55626556,55606567,"stackoverflow.com",0,"2019-04-11 10:05:59+03","2024-05-17 04:58:02.071059+03","To get the Lambda working check a couple of things"
55695307,55612331,"stackoverflow.com",2,"2019-04-15 21:32:18+03","2024-05-17 04:58:02.787889+03","You can include the node_modules dir while excluding the node_modules bin dir like this By default only these directories are excluded So you do not need to specify that node_modules and server are to be included they will be be default Just specify which subdirectories inside them you want to exclude Source httpsserverless comframeworkdocsprovidersawsguidepackaging"
55617635,55614236,"stackoverflow.com",0,"2019-04-10 19:47:11+03","2024-05-17 04:58:03.550547+03","To use XRay in a Lambda function you need to enable XRay for that Lambda function In the console this is done under the Debuggin and error handling configuration section the configuration is called Enable active tracing See the documentation for further details "
55633945,55633805,"stackoverflow.com",1,"2019-04-11 16:41:57+03","2024-05-17 04:58:04.334586+03","You can externalise sensitive information in AWS Parameter Store The main difference from Environment Variables is that it is only available over API calls On the other hand it does give you a lot of flexibility to change the values as you wish on Parameter Store and leave your Lambda functions untouched You can also control the access to the Parameter Store with IAM Roles which gives you an extra layer of security You can check this tutorial to see how to storeretrieve data from AWS Parameter Store "
70093527,55633805,"stackoverflow.com",0,"2021-11-24 11:23:25+02","2024-05-17 04:58:04.336587+03","There is a good solution I found here using terraform but the approach with for example would be the same httpstranscend iobloglambdaedgefunctionsinterraform The author uses a special terraform module but you can also do this manually or even use a fixed file per environment I used a different terraform module to deploy my Lambda function "
55646310,55640097,"stackoverflow.com",1,"2019-04-12 10:11:25+03","2024-05-17 04:58:05.251873+03","Cognito Authentication does sound like a good option for this usecase You can have a flow as follows Cognito User Pool Authentication Token passed as header to the API in API Gateway API returns JSON data after successful authentication [a] I would like to emphasise that a Cognito User Pool is enough to satisfy this usecase Cognito User Pools are used for Authentication and Cognito Identity Pools are used for Authorization Cognito Identity Pools essentially generates temporary AWS credentials which are vended by AWS STS Hence I do not see where you would require Cognito Identity Pools here And to generate a JWT Token you would need to have the user perform a successful authentication operation To perform a successful authentication operation that returns tokens you could have a look at the InitiateAuth API call[b] [a] httpsdocs aws amazon comapigatewaylatestdeveloperguideapigatewayintegratewithcognito html [b] httpsdocs aws amazon comcognitouseridentitypoolslatestAPIReferenceAPI_InitiateAuth html"
54978477,54977598,"stackoverflow.com",4,"2019-03-04 09:57:47+02","2024-05-17 04:58:07.177929+03","1 you can use a modern front end framework like Angular React Vue etc and host your website statically on S3 Check Hosting a Static Website on Amazon S3 for more info Your static website will then interact via HTTP with API Gateway You then map your lambda functions to events from API Gateway Check this tutorial 2 depends on how the framework chosen on step 1 deals with it 3 it can be NodeJS Go Python and other supported languages if you wish Each microservice lambda function can be written in a separate language For more info see the supported languages at Lambda FAQ 4 again depends on the framework of your choice 5 this youll have to lookup for yourself but anything like Blog Tutorial in Framework of Choice should do it Heres an example using React Let us now say you have deployed your application you can then make use of other Events supported by AWS Since you are creating a Blog example you may want to upload pictures to your Post itself so it looks fancy but you do not want users on mobile phones to load these high resolution pictures when they are only browsing through your Blog so you could make use of an S3 Event to generate a thumbnail for your picture so they can have a preview before actually clicking to see the content The possibilities are endless Using a Serverless model to create applications also enables building eventdriven applications out of the box These applications are highly available and autoscalable by default "
54980493,54980249,"stackoverflow.com",15,"2019-03-04 13:25:56+02","2024-05-17 04:58:07.773281+03","It is most likely you are trying to exclude devdependencies Install your dev dependencies using npm i somedependency savedev and by default Serverless will not pack them into the final artifact If that is not sufficient you can exclude everything and include only what you need You can see how Serverless behaves when packaging an artifact as well as seeing how to excludeinclude dependencies on the official docs Heres an example extracted from their docs to exclude node_modules but to keep node_modulesnodefetch which is pretty much what you are looking for And this is how you can include only what you need EDIT after the OPs comment heres how to handle the desired behaviour If the module names are very close to each other like the above you can use the wildcard to exclude them all in a oneliner EDIT 2 Heres a working example see that I am excluding awssdk Since the package is too large and I am not able to see it through AWS Lambdas console I am attaching two screenshots one with exclude and one without exclude see how the package size changes With exclude Without exclude This way you do not even need to add the node_moduleswhateveriwanttokeep argument EDIT 3 Using the wildcard in a sample project with manually created node_modules and respective my_module_ directories"
75580354,54980249,"stackoverflow.com",5,"2023-02-27 14:12:43+02","2024-05-17 04:58:07.775613+03","I think serverless latest version 3 x does not support the exclude keyword anymore I was able to get this working using"
54995078,54984191,"stackoverflow.com",3,"2019-03-05 05:41:04+02","2024-05-17 04:58:08.693283+03","Tags for API Gateway arrived after serverless core functionality was developed There is an open issue on Github discussing the inclusion of this functionality Right now you will have to use the serverlesstagapigateway plugin to manually add tags to these resources "
53509465,53398606,"stackoverflow.com",2,"2018-11-28 00:51:55+02","2024-05-17 04:59:02.818265+03","I ended up customizing the serverlesss3local library to be able to call py files I may end up creating a new github repo and node package for this customization but for now I will refer users to said customizations that I described here "
55013393,54996261,"stackoverflow.com",1,"2019-03-06 01:48:55+02","2024-05-17 04:58:09.638232+03","The serverlessoffline plugin understands the serverless landscape but does not understand custom AWS resources Its also unlikely to in the future as serverless is an abstraction layer Perhaps awssamcli would be a better fit for your application It seems to support swagger docs and a local environment "
55013459,55007081,"stackoverflow.com",1,"2019-03-06 04:33:43+02","2024-05-17 04:58:10.553476+03","You have configured serverless to create a bucket with the name snapnextimages and when it tries to do it it cannot because it already exists Provided the bucket name is available bucket names are shared with everyone either This has nothing to do with custom variables "
55014319,55007081,"stackoverflow.com",0,"2019-03-06 03:52:40+02","2024-05-17 04:58:10.555198+03","You are trying to create the s3 resource when you deploy your stack You will get this error only of the bucket name already exists The bucket names should be unique across regions and accounts You need to be carefulmindful with creating s3 bucket resources in cloud formation "
55034689,55026399,"stackoverflow.com",1,"2019-03-07 03:28:48+02","2024-05-17 04:58:12.616+03","Yes You can use wild cards to make it generic "
55082649,55068409,"stackoverflow.com",1,"2019-03-10 00:37:11+02","2024-05-17 04:58:13.207918+03","You can use DependsOn functionality of CloudFormation in the resources section I have assumed your lambda function key is login which gets translated to LoginLambdaFunction If not check the serverless documentation on how the resources get named In short serverless translates your configuration to a CloudFormation template and the resources section allows you to customise what gets generated which is why you can use DependsOn to solve your issue "
55093053,55069013,"stackoverflow.com",0,"2019-03-11 00:30:07+02","2024-05-17 04:58:14.222834+03","Looks like Pino Pretty is the way to go according to pino docs httpsgetpino iodocspretty"
55083965,55073549,"stackoverflow.com",1,"2019-03-10 13:59:14+02","2024-05-17 04:58:15.122117+03","While the question was specifically about management of env yml files the bigger underlying question is how to manage sensitive environment variables The link in the comment from Alex is all I needed Our solution is so AWSoriented that the AWS Parameter Store is worth exploring Alex DeBries article Yan Cuis article on referencing parameter store values at runtime"
55082816,55074559,"stackoverflow.com",0,"2019-03-10 01:00:17+02","2024-05-17 04:58:16.170059+03","All serverless does is transform some abstract configuration into CloudFormation templates and other provider templates too Adding a plugin to help you reconfigure the same stack will not reduce your number of resources that get generated The serverless blog has a great article on this httpsserverless comblogserverlessworkaroundcloudformation200resourcelimit The TLDR is that there is a limit to resources in stacks so you have to break your stacks up You can"
55509611,55079725,"stackoverflow.com",2,"2019-04-04 10:08:50+03","2024-05-17 04:58:17.284249+03","It is because Webtask has been official deprecated and support for it has been removed by webtask team from serverless the pull request here You could try other providers such as google cloud or aws provider list here "
64911999,55091595,"stackoverflow.com",0,"2020-11-19 14:48:44+02","2024-05-17 04:58:18.073644+03","There are multiple reasons for this problem Let me list down some of them If these do not work please send the error logs here"
55114207,55094540,"stackoverflow.com",0,"2019-03-12 06:32:53+02","2024-05-17 04:58:19.526613+03","Lambda function will not send emails for you but it will start the process Here is a solution I developed last week AWS Email Sending Solution You post email messages on the queue then a Lambda function is triggered the lambda uses awssdk to send the email with AWS SES "
54429870,54428908,"stackoverflow.com",10,"2019-01-29 23:29:14+02","2024-05-17 04:58:23.278487+03","These values will be passed to your lambda function as part of event object nodejs lambda code"
54663757,54429564,"stackoverflow.com",1,"2019-02-13 08:18:26+02","2024-05-17 04:58:24.040106+03","The error message is horrible but right The bucket is trying to be created with config to send notifications to your lambda At this point in the deployment the lambda has not given the bucket permissions to invoke and so the bucket creation fails If you did not specify a custom bucket resource to change the bucket name serverless would have added the dependency automatically That all said you are not the first and the docs have been updated to reflect this issue Add this additional resource and apparently see below it should work I say apparently because I resolved this differently see here using DependsOn to control the order in CloudFormation "
54445025,54430743,"stackoverflow.com",11,"2020-09-04 00:16:51+03","2024-05-17 04:58:24.690773+03","You can do something like this"
54446903,54440671,"stackoverflow.com",1,"2019-01-30 20:07:10+02","2024-05-17 04:58:25.528596+03","Yes the best practice as of now would be to create a new API If you must introduce breaking changes to the API and keep the old version of the API functioning you can create a new API and point it to the same underlying data sources If you want to automate the deployment process of multiple environments you can take a look at this small sample that I have used to start projects in the past AppSync CICD Starter There is a backlog item that addresses schemaapi versioning and I would be interested to hear more about your use case Is the outcome that you would be able to have 2 live versions of the same API For example you might hit the old version at xxx appsyncapi uswest2 amazonaws comv1graphql while at the same time there is a new version at xxx appsyncapi uswest2 amazonaws comlatestgraphql Do you have other requirements"
54461120,54454395,"stackoverflow.com",0,"2019-01-31 14:55:10+02","2024-05-17 04:58:26.630111+03","Serverless has special syntax on how to access stack output variables cfstackName outputKey Note that using the FnImportValue would work inside the resources section "
54490838,54490606,"stackoverflow.com",3,"2020-03-20 05:21:51+02","2024-05-17 04:58:27.497356+03","additional Therere three Expressions we could use to query conditions the first two of them are used for Dynamodb query and the last one is used for Dynamodb updateItem or Dynamodb putItem "
54509208,54496641,"stackoverflow.com",0,"2019-02-04 03:24:34+02","2024-05-17 04:58:28.534308+03","I think you have use include option combined with your exclude option You can use serverless package s dev to see what has been zip before you decided deploy services to real world "
54514358,54507574,"stackoverflow.com",1,"2019-02-04 12:36:33+02","2024-05-17 04:58:29.614615+03","I do not think you can do it with serverless deploy You can try serverless package command that will store the package in serverless folder or you can specify the path using package Package will create a CloudFormation template file e g cloudformationtemplateupdatestack json You can then call Create Stack API action to create the stack It will return the stack ID without waiting for all the resources to be created "
54517118,54509173,"stackoverflow.com",1,"2019-02-04 15:25:17+02","2024-05-17 04:58:30.697575+03","I am not sure what exactly is going on in your case but if you want to use numpy in Lambda have a look at Lambda Layers and the official AWS layer with numpy and scipy To use it add a layers section to the function in your serverless yml You might need to change the ARN depending on the AWS region you are targeting "
78033517,54526895,"stackoverflow.com",0,"2024-02-21 20:20:03+02","2024-05-17 04:58:32.420181+03","You can run your code locally using the serverlessoffline plugin Supply your JDWP agent string via the environment variable JAVA_TOOL_OPTIONS Be sure to set suspendy to ensure you can connect your debugger before the function finishes executing There are lots of ways of doing this I suggest using the serverlessdotenv plugin For example Using JAVA_TOOL_OPTIONS should work in AWS according to the docs But your mileage may vary with tunneling etc "
76085202,75954437,"stackoverflow.com",1,"2023-04-23 16:42:06+03","2024-05-17 05:33:10.86385+03","I got the same issue recently However using either nodehttps or fetch the request is completed successfully "
54545905,54528932,"stackoverflow.com",0,"2019-02-06 07:51:04+02","2024-05-17 04:58:33.397208+03","if number of characters in your beginswith query is always going to be random i do not see an option solving it with dynamodb but let us say there are going to be at least 3 characters then you can do the following Update your dynamodb schema to And instead of storing store as Where id is always the first 3 character of original some_string Now let us say you have to query all items that start with abcx you can do select where idabc and some_string startswith abcx but you should always try to have more number of characters in id so that load is randomly distributed for example if there are only 2 character only 3636 ids are possible if there are 3 character 363636 ids are possible "
54531901,54530537,"stackoverflow.com",2,"2019-02-05 12:07:33+02","2024-05-17 04:58:34.514689+03","Pre Token Generation is currently not available in the UserPool LambdaConfig and hence not supported by CloudFormation which serverless framework use At the moment it can only be configured via console or AWS CLI "
58073836,54530537,"stackoverflow.com",2,"2019-09-24 09:01:22+03","2024-05-17 04:58:34.516689+03","According to Serverless documentation you should inform the attribute existing true and this is very critical if you do not want to create a new Cognito User Pool usingexistingpools Also according to this forum this feature is now covered by AWS CloudFormation AWS Forum This is a recent feature implemented by Serverless so make sure you have the latest version installed Here is my Serverless configuration code I literally duplicated an existing function and changed its trigger cognitolambdafunction"
54562329,54557325,"stackoverflow.com",3,"2019-02-06 22:48:43+02","2024-05-17 04:58:35.540215+03","DynamoDB does not have any concept of schema so the whole thing about editing is kind of unrelated to DynamoDB If by schema you mean objects with certain properties then it depends on the use case If you are OK with having object in a table that do not share the same schema then it is extremely simple as that is allowed by default On the other hand if you need all the objects to share the same set of attributes and you are going to change them frequently then this is indeed not as easy and straight forward compared to RDS Next if you have a relational schema tables and are planning to do some JOINs on them then DynamoDB is really not a good solution DynamoDB is good for a specific type of use cases like storing sessions or something with similar low complexity Writing more complex queries in DynamoDB can get very tedious and painful Considering the price Well I would not really say that DynamoDB is that cheap It seems like that at a first glance but if you dig deeper into it then you find that it is actually pretty expensive mainly from the perspective of writes You need to provision read and write capacity and more throughput you require the more costly it gets you can go with autoscaling for burst traffic but in case of consistent traffic this will not help you that much At larger scales RDS not Aurora will cost only a fraction of what the DynamoDB will cost you assuming that we are talking about use case which can be handled by RDS If you are worried about RDS integration with Lambda then the complexity is not that bigger compared to DynamoDB There are some considerations that need to be taken such as the lambda execution time hard limit which is currently 15 minutes and RDS may be slower to respond compared to DynamoDB but if your query is taking that long then you are either doing something wrong or misusing those tools All in all if you are comfortable with using RDS and do not need that millisecond latency provided by DynamoDB or even microsecond latency if using DAX as well then I would definitely go with RDS over DynamoDB in your case Again DynamoDB is not a general purpose solution to every data related problem and more often then not I see it being heavily misused for stuff that can be easily handled by RDS "
54603691,54589188,"stackoverflow.com",0,"2019-02-09 08:02:45+02","2024-05-17 04:58:37.654125+03","This should work I do this every day I use Python I set the CORS to true in serverless yml and also must explicitly manually set the AccessControlAllowOrigin to in every response Maybe that is not ideal but it works for us "
53921877,53921419,"stackoverflow.com",0,"2018-12-25 13:20:09+02","2024-05-17 04:58:39.93222+03","I am using typescriptes7 for serverless functions please try this in serverless yml"
53931399,53921419,"stackoverflow.com",0,"2018-12-26 13:22:39+02","2024-05-17 04:58:39.93322+03","The solution was to use the Exclude Include commands in the serverless yml So i just added And now the NodeJS function only includes the nodes modules dependencies and source code of the function "
54028911,53936747,"stackoverflow.com",4,"2019-01-04 22:18:16+02","2024-05-17 04:58:40.894708+03","I was unable to make the plugin work but I found a better solution anyhow Lambda Layers This is a bonus because it reduces the size of the lambda and allows codefile reuse There is a prebuilt lambda layer for numpy and scipy that you can use but I built my own to show myself how it all works Heres how I made it work Create a layer package Make a dependencies package zip Must use the directory structure pythonlibpython3 6sitepackages for python to find during runtime Push layer zip to AWS requires latest awscli To use in any function that requires numpy just use the arn that is shown in the console or during the upload above As an added bonus you can push common files like constants to a layer as well Heres how I did it for testing use in windows and on the lambda"
55037915,53936747,"stackoverflow.com",0,"2019-03-07 09:07:05+02","2024-05-17 04:58:40.896708+03","when I got the same issue I opened docker went to settingsshared drive opted to reset credentials and after applied my changes and this cleared the error"
59655184,53936747,"stackoverflow.com",0,"2020-01-09 01:23:31+02","2024-05-17 04:58:40.898709+03","I fixed this issue by temporarily disabling Windows Firewall "
53951955,53938737,"stackoverflow.com",1,"2018-12-28 02:03:48+02","2024-05-17 04:58:42.021407+03","serverlesswebpack is not designed for nonJS runtimes It hijacks serverless packaging and deploys ONLY the webpack output Here are your options P S The package individually property is a rootlevel property in your serverless yml It should not be in provider or in your function definitions "
54113377,53938737,"stackoverflow.com",0,"2019-01-09 17:26:30+02","2024-05-17 04:58:42.022408+03","For those who may be looking for options for multipleruntimes other than serverlesswebpack I ended up switching to this plugin httpswww npmjs compackageserverlesspluginincludedependencies It works with my runtimes Ruby and Node and lets you use package individually with package includeexclude at the root and function level if the plugin misses something "
54065636,53962442,"stackoverflow.com",1,"2019-01-06 22:34:55+02","2024-05-17 04:58:43.859408+03","You cannot use Cloudformation intrinsic functions within the functions block inside the serverless yml file Instead try using nested variables"
53967187,53967127,"stackoverflow.com",3,"2018-12-29 08:15:01+02","2024-05-17 04:58:44.808675+03","The cron expression you are using is invalid If you want your cron to run every minute then the correct expression will be as shown below For more examples refer this "
53999028,53998940,"stackoverflow.com",1,"2019-01-01 23:21:19+02","2024-05-17 04:58:46.593041+03","When uploading an image file you should also specify the content type For jpeg it is imagejpeg For png it is imagepng If you leave out content type the image might not display at all just black space ContentType There are node js libraries to figure out the image type for you This is just one of many such libraries imagetype"
62702121,53998940,"stackoverflow.com",0,"2020-07-02 20:38:17+03","2024-05-17 04:58:46.595041+03","For those still dealing with a similar problem Consider also if you read the file after the upload then send the same file without resetting the cursor to 0 you will damage the file since S3 will start reading from where the cursor was left In my case I was reading it to find out the content of the image So if you reset the cursor before calling upload_fileobj it will fix it"
54001509,54000463,"stackoverflow.com",0,"2019-01-02 08:11:19+02","2024-05-17 04:58:47.570218+03","Not sure if it answers your question but since majority of the business logic for serverless code is just regular javascript you can use mocha directly for unit testing In package json Create a test folder and corresponding test files under it Ex for srcinsiderscreate js create testinsiderscreate test js For every handler test the outputs for different inputs For callback style For async await style We use this style and it works very well for us Edit Fix the mocha glob pattern to search recursively for all test files "
76392155,76388904,"stackoverflow.com",0,"2023-06-02 20:01:16+03","2024-05-17 05:33:30.3486+03","This is not a JSON object You need to wrap it in brackets to make it a JSON object"
54036432,54015460,"stackoverflow.com",0,"2019-01-04 11:45:30+02","2024-05-17 04:58:48.696701+03","The command serverless deploy package pathtopackage seems to be what you are looking for as specified in the Serverless Framework documentation This deployment option takes a deployment directory that has already been created with serverless package and deploys it to the cloud provider This allows you to easier integrate CI CD workflows with the Serverless Framework You were probably missing the package option "
54041679,54041479,"stackoverflow.com",2,"2019-01-04 17:27:54+02","2024-05-17 04:58:49.475718+03","It is not an error in the ffmpeg binary you downloaded The ELF which is the unexpected token means that one of your require statement loads a binary instead of a JavaScript file or module what is ELF something line feed The origin of the error as the stack trace says at download js1718 which is var ffmpeg require ffmpeg so the problematic require statement is your require ffmpeg The reason why require loads the ffmpeg instead of the ffmpeg modules is that the binary on the one hand lies in one of the places where require looks for the modules and is found before the ffmpeg modules "
54083401,54083057,"stackoverflow.com",3,"2019-01-08 01:41:51+02","2024-05-17 04:58:52.705202+03","State Task field TimeoutSeconds must be a positive nonzero integer as defined in the documentation You cannot use state path to define the TimeoutSeconds If set generally it is likely to be identical to the resource function timeout "
53333846,53333602,"stackoverflow.com",4,"2018-11-16 10:14:12+02","2024-05-17 04:58:54.65148+03","The Serverless framework tooling uses AWS CloudFormation for provisioning resources in the AWS cloud Have you checked the AWS CloudFormation web console "
53700045,53333602,"stackoverflow.com",0,"2018-12-10 07:37:37+02","2024-05-17 04:58:54.65248+03","I faced this same issue you cannot find the app bucket in S3 or any stack in cloudformation console because the possibility is that the location is different Check the region of your running stack from serverlessstate json file and then select that region in your cloudFormation console You will find your stack running here "
58494451,53344944,"stackoverflow.com",17,"2019-10-22 00:36:33+03","2024-05-17 04:58:56.105613+03","I had to specify sls deploy awsprofile in my serverless deploy commands like this sls deploy awsprofile common"
53345151,53344944,"stackoverflow.com",3,"2018-11-16 22:46:36+02","2024-05-17 04:58:56.106653+03","Can you provide more information Make sure that you have got the correct credentials in awsconfig and awscredentials You can set these up by running aws configure More info here httpsdocs aws amazon comclilatestuserguideclichapgettingstarted htmlcliquickconfiguration Also make sure that the IAM user in question has as an attached security policy that allows access to everything you need such as CloudFormation "
65986064,53344944,"stackoverflow.com",3,"2021-02-01 03:34:11+02","2024-05-17 04:58:56.108614+03","Create a new user in AWS do not use the root key In the SSH keys for AWS CodeCommit generate a new Access Key Copy the values and run this"
69114172,53344944,"stackoverflow.com",1,"2021-09-09 10:42:55+03","2024-05-17 04:58:56.109201+03","In my case I added region to the provider I suppose it is not read from the credentials file "
68597503,53344944,"stackoverflow.com",0,"2021-07-31 01:02:27+03","2024-05-17 04:58:56.11011+03","In my case it was missing the localstack entry in the serverless file I had everything that should be inside it but it was all inside custom instead of custom localstack "
70149318,53344944,"stackoverflow.com",0,"2021-11-29 05:03:56+02","2024-05-17 04:58:56.111226+03","In my case multiple credentials are stored in the awscredentials file And serverless is picking the default credentials So I kept the new credentials under [default] and removed the previous credentials And that worked for me "
73429113,53344944,"stackoverflow.com",0,"2022-08-20 21:24:10+03","2024-05-17 04:58:56.112412+03","to run the function from AWS you need to configure AWS with access_key_id and secret_access_key but to might get this error if you want to run the function locally so for that use this command it will run the function locally not on aws"
74341860,53344944,"stackoverflow.com",0,"2022-11-07 06:04:53+02","2024-05-17 04:58:56.11341+03","If none of these answers work it is maybe because you need to add a provider in your serverless account and add your AWS keys "
77976533,53344944,"stackoverflow.com",0,"2024-02-11 13:06:05+02","2024-05-17 04:58:56.114539+03","If your credentials on awscredentials file are fine Delete line on your yml config file region useast1 and try again"
55642476,53345168,"stackoverflow.com",22,"2019-04-12 02:21:04+03","2024-05-17 04:58:57.512567+03","AWS error callbacks for Node js do not work as advertised According to the docs all one needs to do is ensure custom errors extend the Error prototype However after over 10 hours of testing I have found this is completely untrue The only way to return an error callback that will return anything other than message Internal server error i e if you have your Lambda function triggered from the API gateway is to callback the error as though it were a success TLDR callback errorResponse null does not work but callback null errorResponse does "
53352419,53345168,"stackoverflow.com",0,"2018-11-17 17:01:04+02","2024-05-17 04:58:57.514568+03","Your lambda function needs to return success in order for APIgateway to detect your response Try this"
53351067,53350777,"stackoverflow.com",0,"2018-11-17 14:03:52+02","2024-05-17 04:58:58.56134+03","It is possible to lock yourself out of a bucket with a bad policy not sure if that applies to your case If so you can try the below CLI command using the root users API key to remove any existing bucket policy aws s3api deletebucketpolicy bucket mydomain com If this is your issue you should be able to insert a new bucket policy as normal now "
53407782,53350777,"stackoverflow.com",0,"2018-11-21 10:19:03+02","2024-05-17 04:58:58.563341+03","If could help someone I finally get the solution changing the setting of the S3 bucket in Permissions Public access setting Manage public bucket policies Block new public bucket policies false Doing this I am able to save the policy "
53353236,53351843,"stackoverflow.com",1,"2018-11-17 18:36:13+02","2024-05-17 04:58:59.9751+03","The intent of infrastructure as code tools such as CloudFormation is to make infrastructure setup and updates repeatable so what you are describing is fine You can retrieve the template for your stack via the cli using aws cloudformation gettemplate stackname yourstacknamehere More info here As long as there are not hardcoded values that are specific to an environment in the template for things such as shared resources it should work "
53368071,53361254,"stackoverflow.com",3,"2018-11-19 05:49:24+02","2024-05-17 04:59:00.143203+03","You tried to run saveRatingsToDB as an asynchronous function by doing Unfortunately with the way this function is written currently it is not an asynchronous function The reason while you do not see the log inside dynamoDb put is because the saveRatingsToDB function does not wait for the asynchronous callback to finish but return earlier To make saveRatingsToDB properly an asynchronous function and force it to wait for the callback you can return a dynamoDb put as a promise from that function"
53562224,53384450,"stackoverflow.com",0,"2018-11-30 19:28:14+02","2024-05-17 04:59:00.927667+03","It is trying to make a change but the cloudformation stack already exists You need to go into the aws console cloudFormation delete the stack associated with your serverless and then re run your deploy "
53395349,53395187,"stackoverflow.com",1,"2018-11-20 16:42:00+02","2024-05-17 04:59:02.01127+03","After you receive the token from Cognito Token endpoint you can store it in the Browser storage e g LocalStorage SessionStorage ClientSide Cookies using JavaScript and send it in Authorization header for API requests Ajax requests You can use AWS Amplify JS library to simplify the authentication and refreshing the token Note One can argue that it is not safe to store the tokens in Browser Storage but if you look at AWS Amplify JS library it uses LocalStorage to store both id token and refresh token "
64039613,53399929,"stackoverflow.com",1,"2020-09-24 16:33:24+03","2024-05-17 04:59:03.824529+03","I found a solution if you are using serverless framework 1 Create a Resource to create the ConfigurationSet in serverless yml 2 Install serverlesssessns 3 Add to serverless yml 4 Finally add the configuration in serverless yml Reference for the plugin"
58951981,53399929,"stackoverflow.com",0,"2019-11-20 12:21:59+02","2024-05-17 04:59:03.82653+03","I also stumbled upon the same issue so as of now SNS cannot be passed as an event destination using cloudformation For more info refer to the link Do checkout the NOTE its explicitly mentioned there "
53822665,53409369,"stackoverflow.com",0,"2018-12-17 22:40:00+02","2024-05-17 04:59:04.733807+03","Use the Serverless Webpack plugin and in the includeModules serverless yml configuration include the package json custom webpack webpackConfig webpack config js Name of webpack configuration file includeModules packagePath package json Node modules configuration for packaging packager npm P"
61317755,53440948,"stackoverflow.com",1,"2020-04-20 11:07:05+03","2024-05-17 04:59:05.80726+03","The issue is existing one and looked by developers Follow the below link httpsgithub comserverlessserverlessissues4415"
53506586,53471498,"stackoverflow.com",0,"2018-11-27 21:11:51+02","2024-05-17 04:59:06.472548+03","It turns out serverless have any option to pass for method and then in your Handler you can check the method type and throw an error as you want "
55024588,53474231,"stackoverflow.com",0,"2019-03-06 16:03:28+02","2024-05-17 04:59:07.500904+03","add at the beginning Remove those two lines and run on production mode"
52551887,52548570,"stackoverflow.com",0,"2018-09-28 17:27:08+03","2024-05-17 04:59:08.940029+03","The first thing you need to focus on is DocumentClient is different from AWS DynamoDB documentation says The document client simplifies working with items in Amazon DynamoDB by abstracting away the notion of attribute values dynamoDb[action] is not a function means it does not have that function try to add batchGet instead since the case sensitiveness I went through the documentation and found that documentation documentclient supports this functions from httpsdocs aws amazon comAWSJavaScriptSDKlatestAWSDynamoDBDocumentClient html in order to take a list of items and take desired properties only you can use scan hope this will help you feel free to ask if you have any further questions "
52573106,52564831,"stackoverflow.com",1,"2018-09-30 01:51:33+03","2024-05-17 04:59:09.898328+03","You can have the second function write to a file on S3 the state of the switch ON or OFF Schedule the first function to run every min But make sure it checks the content of the switch file from S3 before it starts executing it is logic Cost It will not cost you a lot because 60 times an hour 24 hours a day 31 days a month 44640 calls month If it would take an extra 100ms to read the flag and you have set the memory to 1GB then this will translate to 44640 0 00001667 GBSECOND 10 100ms per second 0 07441488 month In addition to 44640 S3 GET request 0 001 per 1000 requests 44640 0 001 1000 0 04464 month In function 2 using the AWS CloudWatchEvents API you can createupdate the rules ScheduleExpression e g cron that that triggers function 1 Read more here"
54551200,52570011,"stackoverflow.com",7,"2019-02-06 12:19:36+02","2024-05-17 04:59:10.878201+03","The issue seems to be within serverlesspythonrequirements As it looks like with version 4 1 1 it works flawless while with 4 2 5 it does not I raised an issue on github"
54515463,52570011,"stackoverflow.com",1,"2019-02-04 13:49:15+02","2024-05-17 04:59:10.879202+03","dockerizePip true pip will run in the container which does not share the filesystem with your host OS by default but it can download python packages from the internet It looks like the plugin serverlesspythonrequirements does not mount your local files to the container properly so pip is not able to see your local package pkgg0 1 0 tar gz IMHO the best solution is not to use local files in the requirements txt Cross reference httpsgithub comUnitedIncomeserverlesspythonrequirementsissues258"
52657572,52570011,"stackoverflow.com",0,"2018-10-05 05:28:49+03","2024-05-17 04:59:10.880807+03","Project lambcilambda describes itself as Images that very closely mimic the live AWS Lambda environment if you want to make sure the environments is 100 compatible spin up an small t2 micro instance on EC2 connect via SSH and setup your project there run pip install r requirements txt on your shell download all libraries and dispose the instance That is exactly what Lambda will do next time it provision the environment for running your code "
52705830,52570011,"stackoverflow.com",0,"2018-10-08 18:43:42+03","2024-05-17 04:59:10.882249+03","Consider building in a docker container as described in this serverless blog post"
65041534,52583162,"stackoverflow.com",3,"2020-11-27 19:34:47+02","2024-05-17 04:59:11.621531+03","Last answer by Nick is actually the correct one If and when you set up your resources S3 Bucket SQS Queue Policy it will work I did it like Finding this out might take you some time Ask me how I know "
63501700,52583162,"stackoverflow.com",1,"2020-08-20 12:00:13+03","2024-05-17 04:59:11.623532+03","You need to add a SQS policy to your queue before you can add the S3 SQS event Cloudformation SQS Policy for S3 events"
52591335,52583162,"stackoverflow.com",1,"2018-10-01 15:38:02+03","2024-05-17 04:59:11.624532+03","You need to use s3CreatedObject See httpsdocs aws amazon comAmazonS3latestdevNotificationHowTo htmlnotificationhowtoeventtypesanddestinations"
60937530,52583162,"stackoverflow.com",1,"2020-03-30 21:54:19+03","2024-05-17 04:59:11.625532+03","I have followed the instructions on the AWS docs to create the SNS topic first in a different deployment You can find my working application config here httpsgithub comdrissamriserverlessarchitectureblobmasterinfrastructureserverless yml httpsgithub comdrissamriserverlessarchitectureblobmasterapplicationserverless yml If you are using Serverless Framework you can also use plugins that hide all the necessary configuration with a simplified config like httpswww npmjs compackageagiledigitalserverlesssnssqslambda"
52587934,52585572,"stackoverflow.com",0,"2018-10-01 12:17:36+03","2024-05-17 04:59:13.585267+03","This is by design Currently Serverless Framework produce a whole new stack for each stages you deploy to This design does not allow it to take advantage of API Gateways stages feature There is a pull request referencing your issue here According to this you might find an alternative with the serverlessawsalias plugin "
53273765,52601859,"stackoverflow.com",24,"2018-11-13 06:22:43+02","2024-05-17 04:59:14.532087+03","Set it in your system environment variable For MacOS Open Terminal and type the following For Windows Set it in the System Environment variable "
60920609,52601859,"stackoverflow.com",9,"2020-03-29 23:55:38+03","2024-05-17 04:59:14.534088+03","for powershell"
58361147,52601859,"stackoverflow.com",7,"2019-10-13 09:35:22+03","2024-05-17 04:59:14.535088+03","For dummies like me who has been using Windows since the very start but still does not know what For Windows Set it in the System Environment variable means a picture tells a thousand words "
61317736,52601859,"stackoverflow.com",1,"2020-04-20 11:05:59+03","2024-05-17 04:59:14.536088+03","SET export SLS_DEBUGtrue in command prompt or set in System environment variable"
62037163,52601859,"stackoverflow.com",0,"2020-05-27 09:58:27+03","2024-05-17 04:59:14.537088+03","if you are using command prompt you can use SET SLS_DEBUGtrue then your sls command eg SET SLS_DEBUGtrue sls deploy stagedev"
52607300,52602809,"stackoverflow.com",1,"2018-10-03 11:42:50+03","2024-05-17 04:59:15.632224+03","Edit I now understand that your goal is to create a lambda warming system You can use the very good serverlesspluginwarmup for this It does exactly what you are trying to achieve in the way you are trying to do it The events property in your functions declaration in serverless yml is a list You can add several events that will trigger your lambda Each item on the events property will create an event in this example only schedules but you can mix different event sources like schedules http streams "
57566515,52632229,"stackoverflow.com",1,"2019-08-20 06:56:38+03","2024-05-17 04:59:16.706641+03","Comment out the function in serverless yml deploy uncomment the function and deploy again As far as I can tell serverless tries to remove the log group when you remove the function and then create it when you add the function Also afaict you have to actually deploy You cannot just save you have to do the full stack update for serverless to notice "
52634248,52632229,"stackoverflow.com",0,"2018-10-03 22:18:55+03","2024-05-17 04:59:16.707641+03","Or you can manually just create the log groups for now"
68577052,52640684,"stackoverflow.com",0,"2021-07-29 16:41:12+03","2024-05-17 04:59:17.680814+03","In 2021 true false variables in serverless yml are still translated to true false strings in Python You can use strtobool from distutils util to parse them safely This is a bit cleaner than string comparison and will work for many truefalse synonyms like yesno 10 onoff See httpsdocs python org3distutilsapiref htmldistutils util strtobool An undefined variable will evaluate to False "
53659264,52716949,"stackoverflow.com",22,"2018-12-06 22:34:21+02","2024-05-17 04:59:19.320894+03","Since axios will use nodes httphttps modules under the covers if you globally capture http and https before you importrequire axios things should work as expected "
59506029,52716949,"stackoverflow.com",1,"2019-12-27 23:06:50+02","2024-05-17 04:59:19.322564+03","Simple example that should just work is Make sure Lambda has correct acces rights "
57916961,52717634,"stackoverflow.com",20,"2019-09-13 06:31:03+03","2024-05-17 04:59:20.106936+03","I am defining functions in individual serverless yml files and include file reference under functions in the main serverless yml file and it works for me I am also naming individual yml files as postssls yml userssls yml etc Referenced here httpsgithub comserverlessserverlessissues4218 httpsserverless comframeworkdocsprovidersawsguidefunctions"
52718812,52717634,"stackoverflow.com",0,"2018-10-09 13:27:45+03","2024-05-17 04:59:20.107936+03","The easiest way to do this is to use a plugin like this one httpsgithub comeconomysizegeekserverlessdirconfigplugin If you want more control you can also doityourself For example you could put the function specific configurations in each directory and then use a tool like cat to join them with a common one for the project e g cat serverless common yml usersserverless yml postsserverless yml commentsserverless yml serverless yml Although you may have to write something more complex if you want to merge together keys c "
52740265,52736029,"stackoverflow.com",4,"2018-10-10 15:26:33+03","2024-05-17 04:59:21.159713+03","Try this using the Ref method"
52754482,52736029,"stackoverflow.com",1,"2018-10-11 10:22:24+03","2024-05-17 04:59:21.160961+03","After checking and retrying i found that srings should be maked using After replacing below settings works smoothly Version 20121017 Action dynamodbGetItem dynamodbPutItem dynamodbDeleteItem dynamodbUpdateItem dynamodbQuery dynamodbScan dynamodbBatchGetItem dynamodbBatchWriteItem"
52741238,52736029,"stackoverflow.com",1,"2018-10-10 16:20:22+03","2024-05-17 04:59:21.16211+03","You can read more about Return Values here "
52766480,52764757,"stackoverflow.com",12,"2018-10-12 17:27:20+03","2024-05-17 04:59:22.124506+03","As you said when your skill is published you will have a live version and development version Both the live version and the development version of your skill are shown on the developer console You can now make changes to the development version of the skill like change in interaction model endpoint change etc Any change in the developer portal requires your skill to go through certification process When you submit your new version for certification both versions remain in your list until the new version is certified Once the new version is certified it becomes live and replaces the previous live version All the existing users will have the livelatest version of your skill A new development version is then created so that you can continue making updates When you create a Lambda function for the first time the default version is Latest The Latest is your development version and you can make changes to this During development you will use the arn of the Latest version development version of Lambda function in Alexa portal Once you are done with the changes and updates of your Lambda function and you feel that this has to be released you version it When you publish a version you can provide the version name of the Lambda function you will have two versions Latest and releasedversion You will no longer be able to make changes to releasedversion of your Lambda And you can continue making updates to the Latest version and release it as you wish It is always a good practice to publish a version of Lambda and use it in Alexa developer portal before you submit the skill for certification This way you can link the versioned uneditable Lambda to your live skill And for the development version of the skill you can use the Latest version of the same Lambda function to continue making updates More on versioning Lambda functions here "
55926309,52764757,"stackoverflow.com",0,"2019-04-30 20:50:38+03","2024-05-17 04:59:22.128507+03","Alexa creates development and a live version of the skill You can change the development version to include the updates For me when specifying the AWS Lambda arn I could not specify the version field Whenever I tried saving ARN with version set to Latest or a version number or an alias Alexa developer console was giving an error So I created 2 lambda functions One pointing to the old version of skill and the other one pointing to the new version of the skill "
52791272,52780026,"stackoverflow.com",1,"2018-12-07 11:57:46+02","2024-05-17 04:59:22.818416+03","Try the below Though I am not Node user I have seen those example codes using Error object in Node For Python I test it if working with your serverless config on the response block Edit I do not think my serverless yml has a difference against yours since I just copied the part of yours However I attach my test code hoping that it would help you serverless yml handler js curl"
54158928,51939709,"stackoverflow.com",1,"2019-01-12 12:59:52+02","2024-05-17 04:59:24.816728+03","I have uploaded the image to s3 Bucket In Lambda Test Event I have created one json test event which contains BASE64 of Image to be uploaded to s3 Bucket and Image Name Lambda Test JSON Event as fallows Following is the code to upload an image or any file to s3 "
51957504,51944673,"stackoverflow.com",4,"2018-08-22 01:24:14+03","2024-05-17 04:59:25.607515+03","I suspect you just need to get rid of the port 9229 bit I have never specified a port for debugging Serverless functions locally but adding it to any of my working configurations produces the symptoms you observe Incidentally you might be able to take some of the other stuff out as well For reference my Serverless debug configurations typically look like this for invoking locally"
52095622,51951810,"stackoverflow.com",1,"2018-08-30 14:12:45+03","2024-05-17 04:59:26.696231+03","Ok the solution for the problem that the validation works with the internal test but not with an external request was very obvious I forgot to deploy the new apidefinition Also I have changed my APIdefinition I now inegrate to my Post request an normal lambda This is the only way I can make sure that only json content gets validated and afterwards get passed through to the lamda function Because I use no lambdaproxy the request event gets transformed from the apigateway so I have to define an request template that put the whole request body in an new request With this way I also transformed the lambda response in an predefined api response with Cors headers I the end my solution is 1 Write a swagger api definition 2 Overwrite the existing serverless api 3 Do not forget to deploy the new api"
51480738,51464084,"stackoverflow.com",1,"2018-07-23 17:10:25+03","2024-05-17 04:59:53.139837+03","I figure out how to create an API Gateway if i do not have any lambda function in serverless I just need to add this to resources and change Ref ApiGatewayRestApi to Ref ProxyApi To fulfill my requirement to use AppSync without any ApiKey it is possible with these lines For this you need an working and configured serverlessappsyncplugin in your serverless config"
51958516,51956895,"stackoverflow.com",1,"2018-08-24 20:27:15+03","2024-05-17 04:59:27.404806+03","When I try to call the function as below kubeless function call smk I get FATA[0000] Unable to find the service for smk Running kubeless function ls namespacesmktest Then surely you would need to include the namespacesmktest in your invocation command too How do I specify Environment variables needed by this function Thank you As best I can tell there seems to be two approaches in use"
51976746,51958749,"stackoverflow.com",1,"2018-08-23 03:11:40+03","2024-05-17 04:59:28.600478+03","Have you created OPTIONS method with the mock integration type to return the response headers like these AccessControlAllowMethods AccessControlAllowHeaders AccessControlAllowOrigin Just reference httpsdocs aws amazon comapigatewaylatestdeveloperguidehowtocors html "
53309584,51964517,"stackoverflow.com",2,"2024-02-01 04:41:00+02","2024-05-17 04:59:29.360346+03","I did get a version working The only issue I have which I have not worked through yet is all the lambdas in that particular serverless yml include the wkhtmlpdf so they are all around 16 mb Heres what I did Created a package json and added dependencies wkhtmltopdf ^0 3 4 memorystream ^0 3 1 Ran ndm install Added WKhtmltopdf in the directory Added this in serverless yml package include Added this in the lambda var wkhtmltopdf require wkhtmltopdf var MemoryStream require memorystream "
53405031,51974560,"stackoverflow.com",1,"2018-11-21 05:53:41+02","2024-05-17 04:59:30.436983+03","The problem is with the indentation in your YAML file just add two spaces in front of arn and batchSize"
54117615,51974560,"stackoverflow.com",1,"2019-01-09 22:12:46+02","2024-05-17 04:59:30.443607+03","Add 2 spaces on the sqs Added a picture for good measure "
51975375,51974560,"stackoverflow.com",1,"2018-08-23 00:26:15+03","2024-05-17 04:59:30.453723+03","You have to create queue by yourself as mentioned in documentation Because serverless will not create SQS for you it can only put listener on already existing queue You can find how to do it here Serverless do not create SQS for you because there is two types of queues available to choose according to your needs It is up to you to choose the queue type then create it and only after this serverless will find your queue and attach worker "
51996126,51975528,"stackoverflow.com",5,"2018-08-24 04:00:42+03","2024-05-17 04:59:31.334569+03","How many Cloudwatch rules are there already in your account By default you can only have 100 httpsdocs aws amazon comAmazonCloudWatchlatesteventscloudwatch_limits_cwe html If I had to guess the error is occurring because you have reached the limit on your account"
52041396,51975717,"stackoverflow.com",26,"2018-08-27 17:20:52+03","2024-05-17 04:59:32.468598+03","The problem was that there was already a lambda which had the CloudWatchLog event subscription AWS limits each CloudWatch Log Group to a maximum of one subscription as specified by the last row of this documentation "
60539728,51975717,"stackoverflow.com",1,"2020-03-05 08:55:46+02","2024-05-17 04:59:32.470789+03","To solve this you have to remove the already attached subscription from the log group Just removing the cloudwatchLog event from lambda and deploying will remove the subscription from the log group You can then add the cloudwatchLog event for the required log group and deploy it again "
52414036,52009801,"stackoverflow.com",3,"2018-09-20 02:23:48+03","2024-05-17 04:59:32.675609+03","Figured it out The error object coming back a generic 500 and NetworkError contained my constructed error under the response key So if I log error response I get my fully constructed error object with its own message and statusCode even though the error object itself has a code of 500 and message of Error Network Error and the error catching process seems to throw out inaccurate error logs CORB stuff in the console Weird Not sure if that is expected Lambda behavior or something with Amplify or something I am doing wrong but if anyone else runs into this issue I hope this helps EDIT Apparently that is expected Amplify behavior which I missed in the docs But I figured out another issue that seems to be a bug This response key only shows up when I do not specify headers in the request options In other words this request returns an error with a response key But this one returns an error without the key Very weird but true for me Even other options are fine but the headers option seems to mess things up and only for the error response "
52118449,52009801,"stackoverflow.com",1,"2018-08-31 18:16:56+03","2024-05-17 04:59:32.685649+03","This does end up depending on how you configure your API Gateway e g Lambda proxy etc If you are NOT using a Lambda proxy setup you will need to map these error responsescodes within API Gateway see extensive post on that here httpsaws amazon comblogscomputeerrorhandlingpatternsinamazonapigatewayandawslambda If you are using a Lambda Proxy you will just return as a specific type of proxy response from Lambda httpsdocs aws amazon comapigatewaylatestdeveloperguideapigatewaycreateapiassimpleproxyforlambda html"
66083852,52009801,"stackoverflow.com",1,"2021-02-07 04:23:20+02","2024-05-17 04:59:32.687647+03","If you are using API Gateway and are running into this issue make sure you update the API Gateways Gateway Responses Default 5XX and Default 4XX headers to contain correct response headers andor messages or else AccessControlAllowOrigin will not be sent on error responses and your browser will throw Network Error For example set response headers for Default 5XX and 4XX to Updating these default header values resolved my issue now proper error messages are being resolved "
52935918,52019039,"stackoverflow.com",79,"2024-04-13 06:55:48+03","2024-05-17 04:59:34.691647+03","This is what I did index js package json event json Shell Output"
52019193,52019039,"stackoverflow.com",50,"2018-12-01 23:17:48+02","2024-05-17 04:59:34.693648+03","You need to call your handler function from another file lets say testHandler js in order to run via NodeJs This will be done like this Now you can use node testHandler js to test your handler function EDIT Sample Event and content data as requested Event Content"
52019098,52019039,"stackoverflow.com",2,"2018-08-25 19:14:26+03","2024-05-17 04:59:34.694643+03","In your index js just defined and exported a handler function but no one calls it In the Lambda environment some AWS code will call this handler with message In your local environment you have to call your handler by yourself You could also have a look of this doc it is a way to simulate Lambda in local environment "
52613066,52019039,"stackoverflow.com",2,"2018-10-02 20:02:36+03","2024-05-17 04:59:34.695642+03","You can check out lambdalocal It is a little fancier than the accepted answer above For example it supports passing environment variables and using JSON files for your payloads "
68630396,52019039,"stackoverflow.com",2,"2021-08-03 07:31:21+03","2024-05-17 04:59:34.696643+03","Perhaps the simplest way to get started after some testing with Node 14 17 3 "
73399411,52019039,"stackoverflow.com",2,"2022-08-18 12:29:53+03","2024-05-17 04:59:34.697643+03","Here I am giving a solution for the general case of an synchronous call Function signature in your entrypoint index js exports handler function event context callback Tip you may need to clean up the cache with node clean cache if you notice that it is the previous code that is called Other Tip in the AWS environment you do not need to zip up the node_modulesawssdk folder as it will be part of the standard execution context But on your test machine you will need to So in development mode simple run npm install not npm install omitdev So before zipping the folder that contains your function just remove node_modules folder recursively and rebuild the dependencies with npm install ommitdev Example of zipping the folder zip r \resizeImage zip This is what your package json file could contain"
60570632,52019039,"stackoverflow.com",1,"2020-03-06 21:45:05+02","2024-05-17 04:59:34.699644+03","If you want just to execute it locally you can use the official sam cli tool httpsdocs aws amazon comserverlessapplicationmodellatestdeveloperguidesamclicommandreferencesamlocalinvoke html If you use VSCode you can also check out this extension httpsmarketplace visualstudio comitemsitemNamebogdanonu invoke"
73407396,52019039,"stackoverflow.com",1,"2022-08-18 20:46:05+03","2024-05-17 04:59:34.701644+03","IMO The best way to test a lambda is to actually just test it What does that mean Will simply use some testing library like jest for example and simply create a test on your handler function Mock anything you need and provide some data you expect that will come into the lambda for the event and context if needed And that is it You have some tests written and a quick way to test your lambda at the same time "
52022740,52022189,"stackoverflow.com",1,"2018-08-26 14:42:03+03","2024-05-17 04:59:35.535046+03","AWS Lambda can deploy another lambda with a s3 trigger The simple flow is like this Bundle project and upload to S3 S3 Trigger Lambda Create or Update function Code Here is the complete documentation"
52078741,52071539,"stackoverflow.com",1,"2018-08-29 16:34:04+03","2024-05-17 04:59:36.467728+03","This is a bit of guessing since I am new to serverless framework but you can set the default value that is used when value is not provided with command line option The following will set the default value to dev Now if you set the default value to empty or something that does not exist i e foobar maybe then you will get the wanted effect and have the execution abort Also the documentation on overwriting variables might give other helpful tips in this case "
52740380,52071539,"stackoverflow.com",0,"2018-10-10 15:33:15+03","2024-05-17 04:59:36.469039+03","Maybe you can write a serveless plugin for that Another option is to use this plugin from Jeremy Daly httpsgithub comjeremydalyserverlessstagemanager and remove dev from custom stages"
52091356,52073161,"stackoverflow.com",1,"2018-08-30 10:24:16+03","2024-05-17 04:59:37.496964+03","I found the issue I had to deploy kubeless to the Kubernetes cluster I had to do this for that as given here link"
53011944,52122905,"stackoverflow.com",1,"2018-10-26 18:28:33+03","2024-05-17 04:59:38.647966+03","It is enabled by default You can check it from shell And It is not a solution but found that fact I realized that I do not have an issue "
51315486,51313677,"stackoverflow.com",1,"2018-07-13 04:49:48+03","2024-05-17 04:59:40.67426+03","Best is a relative term but one way is with the systems manager parameter store You do not specify your programming language but some code I have in Java looks something like with things like null checks left out This does two things First it creates a single place to store common for things like credentials or table names so that you do not need to store them elsewhere Secondly is that it allows you to create a hierarchy of parameters so that for example you could have parameters for your development staging and production environments in one place Plus you can permission the parameters per environment so that for example the development IAM role cannot see the production parameters I am showing it with encrypted strings that is not needed if you do not want Other ways could be a DynamoDB or other persistent store like a RDS "
59153005,51343271,"stackoverflow.com",9,"2019-12-03 10:36:33+02","2024-05-17 04:59:42.236101+03","Running serverless v give details of all version example below"
51349249,51343271,"stackoverflow.com",2,"2018-07-15 17:15:44+03","2024-05-17 04:59:42.237101+03","I do not think there is any difference between your Serverless version and your Serverless CLI version You are running 1 28 while 1 9 or newer is required for the tutorial Since 28 is higher than 9 you can be confident that you are in the or newer camp 1 9 was released in March of last year 2017 In other words your setup is fine and there are no other Serverless versions to verify Hack away "
51355836,51355700,"stackoverflow.com",1,"2018-07-16 10:18:03+03","2024-05-17 04:59:43.913844+03","If you log into API gateway then select an API by clicking on its title click Resources on the left expand the tree out under Resources to view the methods and then select the method here you can view the endpoint If you are using stages ie prod test etc and are using environment variables you will need to select the API then click Stages on the left and look under the relevent stage and method to see the endpoint EditAdd OK now I understand what you need Under resources look at the methods Look at the Integration Request which is presumably of type Lambda Click on integration request It has a parameter under there called Lambda Function it will either be defined here or otherwise refer you to your stage variables If it refers to your stage variables look at Stages on the left click on the stage environment NOT the methods and look at the tab Stage Variables Hope that makes sense "
51372369,51361719,"stackoverflow.com",0,"2018-07-17 05:21:23+03","2024-05-17 04:59:45.512476+03","You cannot JSON parse that event body cause it is not JSON It looks like whatever POSTed that data is using a multipart form POST style request rather than sending JSON How are you invoking the HTTP POST"
52188967,51361719,"stackoverflow.com",0,"2018-09-05 18:39:51+03","2024-05-17 04:59:45.513476+03","I had the same issue and after a lot of debugging noticed 2 important things 1 When the content type is applicationxwwwformurlencoded you might need to parse the data in a different way 2 When the ContentType of the request is multipartformdata the parsing will be even more complicated I will suggest extra dependency to parse it like multiparty or any other of your choice"
62827662,51361719,"stackoverflow.com",0,"2020-07-10 08:07:15+03","2024-05-17 04:59:45.515476+03","Thank you Brian Winant I am putting the answer here as a screenshot so it is clearer In Postman do the following AWS Lambda would return event body as encoded query strings if the contenttype is xwwwurlencoded To have it return a JSON string you can then parse send JSON data and set contenttype as applicationjson "
51364202,51363983,"stackoverflow.com",1,"2018-07-16 17:32:24+03","2024-05-17 04:59:45.926536+03","You can use forceInclude Reference document"
51407005,51406480,"stackoverflow.com",6,"2018-07-18 19:34:33+03","2024-05-17 04:59:48.07557+03","There is usually no need for explicit singleton implementation JS modules CommonJS and ES modules in particular are evaluated only once under normal circumstances exported class instance is efficiently a singleton There is no need for IIFE as well because modules have their own scopes Since init function is not reused there is possibly no need for it either It can be simplified to This abstraction is not practical There is already sequelize instance creating wrapper methods for its own methods does not serve a good purpose Since a connection is usable after it is established it makes sense to just export a promise of a connection similarly to the one that is shown in this answer If the configuration database name is available in database manager module it is preferable to just use it inplace Which is used like If there may be several connections or the configuration is not available on import factory function is exported instead Singleton instances are naturally handled with modules And used like"
51441061,51433042,"stackoverflow.com",0,"2018-07-20 14:01:08+03","2024-05-17 04:59:49.177929+03","It turns out I had not translated the Node js code correctly To access the CognitoIdentityId I had to get the requestContext from the request object then get the identity object like so"
51614862,51436924,"stackoverflow.com",0,"2018-07-31 17:26:50+03","2024-05-17 04:59:49.985206+03","Try referencing the serverlesscors yml file relative from the directory that the serverless yml file is in For example"
51463893,51458578,"stackoverflow.com",0,"2018-07-31 17:51:08+03","2024-05-17 04:59:52.02344+03","If it is static bit of JSON i would simply importreturn it from within the function rather than enable caching but hey it is your API To answer your question you can use caching within API Gateway to do so documentation can be found here Update I would actually misread the question so apologies for that whilst that caching works what the Op is asking is where to store data retrieved if your retrieving it from an external API you can just write it to s3 like so Then you just need to read the object back in your serving endpoint "
62255084,51470960,"stackoverflow.com",5,"2020-06-08 07:36:56+03","2024-05-17 04:59:53.652904+03","This error is known NodeJs API Webpack 2 Production Builds Error Received packet in the wrong sequence The explanation is shown in this post Error Received packet in the wrong sequence when connect to serverless aurora You can fix it by uninstalling mysql and installing mysql2 mysql2 works with promises"
50699109,50693046,"stackoverflow.com",4,"2018-06-05 14:35:40+03","2024-05-17 04:59:56.326882+03"," TableName messagestabledev IndexName roomIndex KeyConditionExpression room room ExpressionAttributeValues room everyone There is no need of putting data type while querying to dynamoDb"
50709284,50693046,"stackoverflow.com",4,"2018-06-06 00:36:17+03","2024-05-17 04:59:56.328185+03","I have to admit to a bit of embarrassment I said several times in the question that my code was not using the DocumentClient but was using the DynamoDB class With DocumentClient this query does indeed work That is because DocumentClient does not require those data types and instead it deduces the data types Turns out that my code actually was using DocumentClient and that as soon as I removed the data type from the query the HTTP request worked as expected And when going to the DynamoDB client and adding back the data types that also worked If I had looked more carefully at the application code this question would never have been posted "
50924627,50704566,"stackoverflow.com",0,"2018-06-19 12:13:02+03","2024-05-17 04:59:57.254646+03","Personally I would not worry about it There is some level of verbosity needed because of the sheer number of elements that can be configured but the redundancy in no way negatively affects the performance of your application And while it may seem redundant what you really have is the granularity to affect change very uniquely across your functions as well as the clarity to see what the differences are between them If you want to clean up the serverless yml overall look at breaking out into separate files perhaps for all the function definitions or perhaps per event httpsserverless comframeworkdocsprovidersawsguidevariablesreferencevariablesinotherfiles That will probably server you better by keeping related entities together but segregated so there is less cognitive load when looking at the base serverless yml file "
50724449,50708598,"stackoverflow.com",1,"2018-06-06 18:42:34+03","2024-05-17 04:59:58.215238+03","The issue was that AWS was not automatically calling JSON stringify on the response but the offline serverless plugin was I am still not sure why that would lead to a logless error but it definitely does fix it "
50709811,50708602,"stackoverflow.com",1,"2018-06-06 02:22:55+03","2024-05-17 04:59:58.996382+03","Javascript by default is synchronous but when you write asynchronous callback promise async code you need to handle it properly In your code there is 2 issues This should work "
50753932,50740129,"stackoverflow.com",1,"2018-06-08 08:31:14+03","2024-05-17 05:00:00.842029+03","I got the solution of my problem First I need to check ARN of table A into AWS dynamoDB table section which i created into my first application Now add that ARN into second serverless applications serverless yml files Resource section Suppose my ARN of table A is like arnawsdynamodbuseast1XXXXXXtabletodotask Now I need to add this ARN into my second apps serverless yml files Resource section it is like"
50741847,50740129,"stackoverflow.com",0,"2018-06-07 15:57:58+03","2024-05-17 05:00:00.844029+03","If both applications and dynamoDB tables are located in same region it is possible You should set a corresponding role for the second application to have access to Dynamo DB table Please check permissions of role serverlessrestapidevuseast1lambdaRoleserverlessrestapidevtodoList"
50777465,50746902,"stackoverflow.com",1,"2018-06-09 21:44:33+03","2024-05-17 05:00:01.988789+03","Syntax Error Your FnGetAtt syntax is just a little off You need two colons between Fn and GetAtt and then you need a colon at the end of that line Like this This will fix the strange error message that contains FnGetAtt where a real role name should be FnJoin You can get rid of the FnJoin call by just using Ref like so DependsOn The DependsOn line is fine but not needed CloudFormation is smart enough to figure out this dependency for you YAML note Finally while this boils down to a readability preference I usually put short lists like the ones you pass to FnGetAtt in square brackets So you can replace this with this Rewritten The result is shorter and arguably easier to read Combining these suggestions results in this role attachment resource Tested using Serverless 1 27 2"
55691035,50761591,"stackoverflow.com",15,"2019-04-15 17:08:17+03","2024-05-17 05:00:02.546437+03","CORS setup in Serverless is explained in detail here httpsserverless comblogcorsapigatewaysurvivalguide In addition to the config in serverless yml which is for the preflight requests you need to return the headers AccessControlAllowOrigin and AccessControlAllowCredentials from your code In your example and a Node js implementation Make sure to include the https part in the first header I have stumbled over this previously "
77031296,50761591,"stackoverflow.com",1,"2023-09-03 10:17:19+03","2024-05-17 05:00:02.548438+03","I had issues having it work with a private endpoint I was setting cors true which by default has an allowCredentials false Instead I had to expand the cors field like this"
50764244,50763163,"stackoverflow.com",0,"2018-06-08 18:48:49+03","2024-05-17 05:00:03.412999+03","A promise was present with the serverless init method and we would have to wait for it to complete "
50836841,50832013,"stackoverflow.com",4,"2018-06-13 15:04:05+03","2024-05-17 05:00:05.862864+03","The problem is that the cf syntax requires the output of an existing CloudFormation stack and when you have not yet deployed the project the stack and its outputs do not exist yet If you need to access that output from inside the current stack you should look at how the output is defined by Serverless this example is from one of my projects You can use the same syntax to generate that value in your own stack in places where you need it replacing the dynamic parts with Serverless variables like selfprovider region and selfprovider stage or whatever your project has chosen to use instead of them For example to add it to the Lambda environment"
52524086,50832013,"stackoverflow.com",4,"2018-09-26 21:30:30+03","2024-05-17 05:00:05.864864+03","In my case I deleted the cloudformation stack manually and ran sls deploy s stage force and it worked "
50832014,50832013,"stackoverflow.com",3,"2018-06-13 11:01:10+03","2024-05-17 05:00:05.865864+03","It turns out the offending bit was cf selfservice selfprovider stage ServiceEndpoint in my serverless yml Apparently cf stuff or at least that particular case in serverless yml fails if main stack does not exist i e you have not deployed yet I cannot decide if this is sls bug or should I have known better "
75957207,50832013,"stackoverflow.com",0,"2023-04-13 09:54:58+03","2024-05-17 05:00:05.867865+03","CloudFormation is region specific I encountered the same error I created a stack using the command But when I tried to describe the stack I got the same error because I have set default region as useast2 in my awsconfig file You can check this file using cat awsconfig I got this error then resolved it using region tag in the command Reference serverless com answer"
50842284,50842194,"stackoverflow.com",0,"2018-06-13 19:39:59+03","2024-05-17 05:00:06.867867+03","The answer to this is easy If you have already deployed your serverless package you must first remove the stack and redeploy then the event gets created "
51383234,50069352,"stackoverflow.com",10,"2018-07-17 16:54:35+03","2024-05-17 05:00:18.395078+03","I finally could figure out a working implementation for this specific setup which I want to share with all of you Check out my serverlessgraphqlappsyncrds repository on GitHub and leave me some feedback Note that this repository contains just the source code without any explanations I will create a better documentation in the near future "
50105626,50105577,"stackoverflow.com",0,"2018-04-30 20:44:12+03","2024-05-17 05:00:20.111783+03","You cannot subscribe to multiple topics using wildcards like you are doing as far as I know You have to list each topic out like so"
50867648,50855281,"stackoverflow.com",2,"2018-06-15 03:24:34+03","2024-05-17 05:00:07.875331+03","The AccessControlAllowOrigin header relates to many types of crossorigin AJAX requests It is not a general access control mechanism By AJAX kind of an outdated term I essentially mean a request originating from a browser via javascript This is kind of long but it is worth reading the entire thing at least twice So this header can prevent crossorigin AJAX requests because all browsers respect it It does nothing for regular requests i e pasting the URL into your browser or Postman To allow requests of any type from only one IP you could check the origin or referrer header in your lambda code but headers can be spoofed Using a WAF Web Application Firewall with a proper ACL Access Control List is probably a more robust solution "
53978930,50855281,"stackoverflow.com",1,"2018-12-30 17:33:27+02","2024-05-17 05:00:07.877332+03","In Serverless offical document APIGateway config only accept origin value I think you can retry again with correct setting and combined response header in login function the same as your function "
51106501,50855281,"stackoverflow.com",1,"2018-06-29 20:24:13+03","2024-05-17 05:00:07.878596+03","CORS will not prevent your function being accessible to the world it just means that a reputable browser will refuse to honour an async call the service if the domains do not match What you probably want to do is create a custom authorizer this can be set up to authorise your logged in admin You may be able to do this with the referrer headers too though not as secure "
51970306,50865741,"stackoverflow.com",0,"2018-08-22 18:32:15+03","2024-05-17 05:00:08.999048+03","Finally had some time to circle back to this problem and came across this git issue Essentially the dialogflow instance needed to be encapsulated by the lambda "
50938474,50881012,"stackoverflow.com",1,"2018-06-20 03:07:25+03","2024-05-17 05:00:09.4155+03","Unfortunately there is no native way to do it You would need to write a bash that will loop through the changed files and call sls deploy s production f for each one of them "
55783165,50881012,"stackoverflow.com",1,"2019-04-21 16:27:51+03","2024-05-17 05:00:09.4165+03","I was also faced this issue and eventually it drove me to create an alternative Rocketsam takes advantage of sam local to allow deploying only changed functions instead of the entire microservice It also supports other cool features such as Hope it solves your issue "
49991394,49928742,"stackoverflow.com",14,"2018-04-24 02:40:10+03","2024-05-17 05:00:11.548738+03","The name of the argument is expected to be the same as the name of the field in the mutation response that triggered the event If your mutation returns a value of type Post that contains a field title then passing an argument named title to the subscription that is subscribed to that mutation will only get pushed values where the title passed to the subscription equals the value of field named title returned by the mutation "
49929323,49928742,"stackoverflow.com",1,"2018-04-19 23:02:04+03","2024-05-17 05:00:11.550738+03","The arguments control what data clients will get subscription notifications from For example if you put in an argument via the schema which is required by using the bang symbol then clients can only subscribe to data on a specific mutation for that parameter GraphQL arguments including those passed in a subscription should be available via ctx args in your resolver this is shorthand for context arguments For example if you have a query of getThing nameXYZ then you can access in your resolver with ctx args name For your use case I would suggest using arguments along with a resolver on the subscription so that users can only subscribe to a channel by that argument if they match some authorization criteria such as the password or looking at the logged in user You can find an example of this here httpsdocs aws amazon comappsynclatestdevguidesecurityauthorizationusecases htmlrealtimedata"
49935095,49933952,"stackoverflow.com",9,"2018-04-20 09:10:53+03","2024-05-17 05:00:12.66103+03","As far as I can tell you cannot set multiple cookies in the same SetCookie header There is literature on the internet saying that you can but my attempts to replicate this in API Gateway were fruitless Additionally at the time of writing API Gateway does not allow you to set multiple SetCookie headers This has been a longrequested and still not implemented feature If you can I would suggest packing all your information into a single object and sending the JSON in one cookie If you really need to though here is a disgusting workaround API Gateways response headers are casesensitive so you can define multiple instances of the setcookie header by varying the case like this"
73615532,49933952,"stackoverflow.com",2,"2022-09-06 02:56:39+03","2024-05-17 05:00:12.663303+03","With the Lambda function response for format 2 0 as stated in the official documentation to customize the response your Lambda function should return a response with the following format "
70208435,49933952,"stackoverflow.com",0,"2021-12-03 03:24:39+02","2024-05-17 05:00:12.664472+03","Setup mapping with Response header SetCookie and mapping value integration response multivalueheader SetCookie source httpsmedium comnirmantechblogawsgatewayapiendpointsmultivaluecookiemappinga36586cebd5e"
49962803,49955595,"stackoverflow.com",1,"2018-04-22 08:28:23+03","2024-05-17 05:00:13.574261+03","I think there were two issues here From httpsserverless comblogserverlesspythonpackaging"
49964034,49963141,"stackoverflow.com",3,"2018-04-22 11:34:10+03","2024-05-17 05:00:14.473047+03","What is about to have for each version its own resource and separate it by folders Like this That gives you the flexibility to test everything and its easier for newcomers to your project because the versioning is explicit through folder separation "
53311686,49963141,"stackoverflow.com",2,"2018-11-15 04:45:03+02","2024-05-17 05:00:14.475047+03","After reading many sources and trying to understand how AWS API Gateway works from the versioning perspective I decided to simply duplicate the API on my major versions and keep track on them through diff branches The benefits I am using serverlesscustomdomain which creates a recordset during my deployment "
49992024,49991349,"stackoverflow.com",3,"2018-04-24 04:12:04+03","2024-05-17 05:00:15.651896+03","Cloudformation resources support the DependsOn attribute resources Resources MyFirehoseStream Type AWSKinesisFirehoseDeliveryStream DependsOn MyElasticSearch httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawsattributedependson html"
50041254,50037552,"stackoverflow.com",0,"2018-04-26 13:58:36+03","2024-05-17 05:00:16.594976+03","The only way i can see to achieve this is by using the custom authorizer it is not for that was designed but you can have it to send to your request context with your client and the policies to allow your api Then you can cache the authorizer response which means it will cache your client example of authorizer response Then you can get your client This only makes sense if you are going to cache the authorizer response otherwise you would be better to have your payments handler to fetch the client instead "
50080646,50048368,"stackoverflow.com",3,"2018-08-15 19:37:00+03","2024-05-17 05:00:17.71246+03","We do the same as you suggest in B basically we have our backend on AWS lambda and access data from dynamodb All our translation happens in the frontend Only difference we use i18next more specific reacti18next but makes no difference if this or reactintl just offers a little more backends caching language detection httpswww i18next com If you like to learn more or see it in action checkout httpslocize com or directly try it at httpswww locize io 14d free trial while the app currently only is available in english all the texts comes in via xhr loading and get applied on runtime i18n If interested in how we use serverless at locize com see following slides from a speech we gave last year httpsblog locize com20170622howlocizeleveragesserverless Last but not least if you like to get most out of your ICU messages and validation syntax highlighting and proper plural conversion and machine translation by not destroying the icu dsl during MT Just give our service a try it comes with 14d free trial "
50141349,50140885,"stackoverflow.com",72,"2023-07-06 12:01:23+03","2024-05-17 05:00:20.348276+03","From CloudFormations perspective SAM is a transform Meaning SAM templates are syntactically equivalent but they allow you to define your serverless app with more brevity The SAM template eventually gets expanded into full CFN behind the scenes If you already know CFN but want to write less YAML code SAM may be beneficial to you The idea is to reduce your effort Furthermore besides the template definition itself SAM allows you to test your serverless app locally i e before uploading it to AWS And can be used to simplify certain operational tasks for example by automatically uploading artifacts to an S3 bucket or ECR repository or by automatically creating a basic CICD pipeline for you One last thing to take in mind is that SAM CLI tends to be a bit more highlevel For example the single sam deploy command is a replacement for both aws cloudformation createstack and aws cloudformation updatestack meaning that it will create or update the CloudFormation stack depending on its existence Similarly sam logs will let you see the logs of all the resources of your serverless app "
68155484,50140885,"stackoverflow.com",35,"2021-07-03 22:16:13+03","2024-05-17 05:00:20.350881+03","SAM templates are a superset of Cloudformation Any Cloudformation template can be run through SAM unchanged and it will work SAM supports all the types available in Cloudformation templates so you can think of SAM as CloudFormation However SAM also gives you additional transforms that allow you to define certain concepts succinctly and SAM will figure out what you mean and fill in the the missing pieces to create a full expanded legal Cloudformation template Example For SAM and Serverless Framework users who deal mostly in Lambda functions one of the more most useful transforms is the Events property on the Lambda function SAM will add all the objects needs to access that function through an API path in API Gateway The SAM template snippet shown above gets transformedexpanded into several API Gateway objects a RestApi a deployment and a stage The AWSServerlessFunction type used in this snippet is not a real Cloudformation type you will not find it in the docs SAM expands it into a Cloudformation template containing a AWSLambdaFunction object and several different AWSApiGateway objects that Cloudformation understands To give you an idea of how much manual coding this saves you heres what the expanded version of the above SAM template looks like as a full Cloudformation template Previously if you were authoring pure Cloudformation you would have had to code all this by hand over and over for each API Gateway endpoint that you wanted to create Now with a SAM template you define the API as an Event property of the Lambda function and SAM or Serverless Framework takes care of the drudgery In the old days when we had to do all this by hand it totally sucked But now everything is glorious again "
61199693,50140885,"stackoverflow.com",11,"2020-04-14 05:14:18+03","2024-05-17 05:00:20.353726+03","Like Luis Colon said SAM is a transform What that means is that at the top of a SAM Template there is a Transform statement that lets CloudFormation know to run an intrinsic function Transform on this SAM template to turn it into a CloudFormation template So all SAM Templates will eventually be converted into CF templates but for the enduser in most cases it is easier to just use the SAM template For instance for a simple application with Lambdas triggered by a new API you are creating the SAM template will let you accomplish this in fewer lines than CloudFormation To extend this the Serverless Framework behaves similarly Serverless is designed to work across platforms AWS Azure etc It is syntax looks very similar to SAM and it too converts the template into the target platforms ie AWS fuller version of the template ie CloudFormation template "
71907977,50140885,"stackoverflow.com",2,"2022-04-18 08:42:19+03","2024-05-17 05:00:20.355729+03","You can imagine SAM as an extended form of CloudFormation SAM makes ServerlessLambda deployments easier Even CloudFormation can deploy lambda scripts using inline scripts but it has a limitation of 4096 characters and you cannot pack custom dependencies python libraries So to make LambdaServerless deployments easy SAM is used SAM is a CLI tool You cannot find SAM in AWS Console In case of python deployment sam will read the requirements txt file build a package and will deploy the package when you wish to sam deploy So at the end of the day you can write as much lengthy Lambda Code use as many libraries you want and even import your custom libraries i e complete flexibility "
50779701,50145879,"stackoverflow.com",3,"2018-06-10 03:45:22+03","2024-05-17 05:00:21.400154+03","As noted in another answer hard coding the ARN works So intuitively you might think something like this would work Sadly it does not It looks like Serverless bumps your arn up against a couple of regular expressions to determine whether you are pointing at a lambda or a user pool This approach does not seem to play nicely with approaches using things like Ref FnJoin or FnGetAtt As of Serverless 1 27 3 which was released since this question was asked there is a workaround of sorts available Essentially you declare your Authorizer in your resources section instead of letting Serverless automagically create it for you Then you use the new authorizerId key in your functions section to point at this authorizer A minimal example It is not great but it is better than having to hard code the user pool ARN into your template "
50147377,50145879,"stackoverflow.com",1,"2018-05-03 08:13:41+03","2024-05-17 05:00:21.403155+03","Is this what you are looking for httpsserverless comframeworkdocsprovidersawseventsapigatewayhttpendpointswithcustomauthorizers Example"
50159079,50158593,"stackoverflow.com",2,"2018-05-08 16:02:25+03","2024-05-17 05:00:23.426206+03","One way is to give the S3 buckets explicit names so that later instead of relying on Ref bucketname you can simply use the bucket name That is obviously problematic if you want autogenerated bucket names and in those cases it is prudent to generate the bucket name from some prefix plus the unique stack name for example Another option is to use a single CloudFormation template but in 2 stages the 1st stage creates the base resources and whatever refs are not circular and then you add the remaining refs to the template and do a stack update Not ideal obviously so I would prefer the first approach You can also use the first technique in cases when you need a reference to an ARN for example When using this technique you may want to also consider using DependsOn because you have removed an implicit dependency which can sometimes cause problems "
50304667,50158593,"stackoverflow.com",2,"2018-05-30 23:31:04+03","2024-05-17 05:00:23.428207+03","This post helped me out in the end httpsaws amazon compremiumsupportknowledgecenterunablevalidatedestinations3 I ended up configuring an SNS topic in CloudFormation The bucket would push events on this topic and the Lambda function listens to this topic This way the dependency graph is as follows Something along the lines of this some policies omitted "
54075540,50188932,"stackoverflow.com",2,"2019-01-07 15:41:58+02","2024-05-17 05:00:24.201183+03","I was able to simulate this offline recently using the following codeconfig serverless yml Heres the code that triggers my offline lambda trigger js Run the trigger normally Note In your example the way you declared the sns subscription is not yet supported AFAIK You can check this github issue for more info and updates "
50193095,50190553,"stackoverflow.com",3,"2018-05-05 22:08:46+03","2024-05-17 05:00:25.225047+03","Adding the MIXPANEL_TOKEN environment variable to the serverless yml fixed this"
49149306,49148771,"stackoverflow.com",9,"2018-03-07 12:14:26+02","2024-05-17 05:00:27.098745+03","For actions like s3GetObject the resource should be arnawss3myBucket You are missing the trailing "
64132273,54310730,"stackoverflow.com",4,"2020-09-30 09:49:48+03","2024-05-17 05:13:10.990273+03","You can use any arbitrary name for your environment variable and pass it to serverless yml with You reference this inside serverless yml with"
51508049,49179199,"stackoverflow.com",2,"2018-07-25 00:53:30+03","2024-05-17 05:00:28.867453+03","Getting the API Gateway URL The only way I have found to get my hands on the API Gateway URL in my Serverless templates is to piece it together myself I am certainly not the first to do it this way The API created by serverless is called ApiGatewayRestApi Since your template is aware of the region and stage you can piece together the URL like so Unfortunately this will not work Although I believe API Gateway uses CloudFront under the hood at least for edge optimized endpoints plugging the above code into your script will result in an error complaining that the hosted zone is wrong for this URL This is unsurprising since executeapi amazonaws com is clearly not a subdomain of cloudfront net I think you have to use API Gateways custom domain feature to pull this off When you create one you get a real CloudFront URL back that you can use with the CloudFront hosted zone Z2FDTNDATAQYW2 The downside of this is that you get a CloudFront distribution created for your API This implies that deploying a brand new API is going to take 2030 minutes rather than just a minute or two Without a plugin In your serverless template this means creating an API Gateway DomainName and probably a BasePathMapping in addition to the Route 53 RecordSet In this example I hit my API at api example comr53STAGEAPI_RESOURCE My certificate is for example com You can also specify stage in the BasePathMapping if you do not want it in the URL With a plugin This is a fair amount of boilerplate and I hate hard coding ARNs into my templates the certificate in this case The serverlessdomainmanager plugin recommended here can make this less cumbersome Not only does it reduce the amount of boilerplate you write it allows you to specify certificate name rather than ARN I had no trouble making it work the first time I tried it Using this plugin the CloudFormation above can be replaced by a short stanza in the custom section of your template"
49251616,49221791,"stackoverflow.com",0,"2018-03-13 10:40:21+02","2024-05-17 05:00:30.153899+03","create should be without hyphens "
53049094,49223133,"stackoverflow.com",1,"2018-10-29 17:43:35+02","2024-05-17 05:00:31.009055+03","By removing the following section Your deployment should work without an issue To get the User Pool properties the User Pool ID can be found in the event object other properties are just a matter of querying "
49266455,49248164,"stackoverflow.com",1,"2018-03-13 23:54:11+02","2024-05-17 05:00:31.485615+03","I was checking with some microsoft guysRight now there is No specific integration with serverless yml and Cosmos DB You would need to use a custom ARM template to make that happen "
49274722,49274262,"stackoverflow.com",5,"2019-07-08 22:27:59+03","2024-05-17 05:00:32.508508+03","You can use Lambda Proxy Integration based on the documentation you need to create a function which will run when someone accesses your API endpoint As an example And in your serverless yml"
49290401,49274262,"stackoverflow.com",3,"2018-03-16 08:53:19+02","2024-05-17 05:00:32.510046+03","Since you use Lambda Integration you have to put it in your serverless yml Reference httpsserverless comframeworkdocsprovidersawseventsapigatewaycustomresponseheaders"
49313564,49274262,"stackoverflow.com",0,"2018-03-16 07:07:17+02","2024-05-17 05:00:32.511146+03",""
49335418,49289302,"stackoverflow.com",9,"2018-03-17 13:23:59+02","2024-05-17 05:00:33.02252+03","It looks like your Serverless endpoint is receiving data with ContentType applicationxwwwformurlencoded You could update the request to use JSON data instead to access the post variables the same way you would other JavaScript objects Assuming this is not an option you could still access your post body data using the node querystring module to parse the body of the request here is an example Just remember if some of the parameters in the post body use names that are invalid JavaScript object identifiers to use the square bracket notation e g "
49289312,49289302,"stackoverflow.com",0,"2018-03-15 01:39:51+02","2024-05-17 05:00:33.024439+03","You can use a the Node querystring module to parse the POST body "
49305243,49289302,"stackoverflow.com",0,"2018-03-15 19:01:43+02","2024-05-17 05:00:33.025436+03","The handler documentation for the various programming models implies that the event type from the API Gateway is a low level stream That means you have to use other methods to extract the body content from the POST Input Format Dotnet Only the System IO Stream type is supported as an input parameter by default Python event This parameter is usually of the Python dict type It can also be list str int float or NoneType type "
49326821,49320549,"stackoverflow.com",7,"2018-03-16 19:53:33+02","2024-05-17 05:00:34.135795+03","I also found this to be counterintuitive and confusing at first However this is actually the expected and documented behavior When you attach a Cognito event to a function as a trigger Serverless will create a user pool for you without even being asked Source This will create a Cognito User Pool with the specified name So in your case one user pool is being created by the cognitoUserPool event and the other is being created by your Resources section The one created by Resources is correct has the custom password policy and the one created by the lambda trigger has default configuration The fix is described under the Overriding a generated User Pool heading You prefix the User Pool key in the Resources section with CognitoUserPool which will cause both your trigger and your resource to refer to the same User Pool in the generated CloudFormation template In your case this means simply changing this to this Tested with Serverless 1 26 0"
60019640,49320549,"stackoverflow.com",6,"2020-02-01 19:57:04+02","2024-05-17 05:00:34.137914+03","The correct way to do this is by setting existing true on your functions cognitoUserPool properties like so Serverless added support for this in July 2019 "
54986275,49330149,"stackoverflow.com",6,"2019-08-01 08:36:05+03","2024-05-17 05:00:34.722168+03","To achieve this without using a Serverless plugin add the follow to your package json file This will create the file slsinfo containing your serverless outputs amongst other things Run by calling npm run slsinfo You can then update package json Now you can call npm run slsdeploy and it will deploy your service and add your outputs to slsinfo file To use the info in slsinfo the easiest way I have found is to use regex Example below Using the above method you can get your output as follow"
49331654,49330149,"stackoverflow.com",4,"2019-07-10 19:03:01+03","2024-05-17 05:00:34.724169+03","To get outputs from serverless you can use the serverlessstackoutput plugin or you can deduce the stack name and use the aws command Replace SERVICE with your service name and STAGE with your stage You should get a JSON object with the outputs from this command If you want to get just specific outputs try Replace SERVICE STAGE and OUTPUT_KEY with the values you want On Windows use the quotes work differently For more details on query see httpsdocs aws amazon comclilatestuserguidecontrollingoutput html"
76897484,49330149,"stackoverflow.com",0,"2023-08-14 11:43:34+03","2024-05-17 05:00:34.72617+03","Follow up on kichik answer The query option can be extended to output TOMLlike keyvaluepairs in the format of OutputKeyOutputValue Output This output can directly be piped to an env file And this file can automatically be loaded by a package like dotenv or by the Serverless Framework "
49352588,49352041,"stackoverflow.com",3,"2018-03-18 22:37:52+02","2024-05-17 05:00:35.930359+03","The HTTP response code in the CORS error message says 502 This means the server you are requesting is unavailable In an event of a 502 the browser cannot make successful OPTIONS requests so even if your CORS setup in the server is correct you will still get a CORS error since it was not resolved properly Look at your server and make sure it is running as it should Once the 502 error is fixed try again and you should be good to go Try making a manual OPTIONS request similar to that of what the browser would make and see if you get the same error "
49383189,49358739,"stackoverflow.com",8,"2018-03-20 13:35:51+02","2024-05-17 05:00:36.561209+03","You are confusing the boundary between AWS API Gateway and AWS Lambda It is not your fault Serverless Framework is that good that it almost blurs those two things Strictly speaking AWS Lambda Functions DO NOT need custom authorizers Authorizers are used for securing API Gateway endpoints NOT for AWS Lambda functions Therefore you need to define the authorizer for each endpoint you need to require authorization for If you are after making your serverless yml more concise by not repeating the authorizer definition multiple times you can define it once and just reference it in your endpoints "
49566345,49369688,"stackoverflow.com",2,"2018-03-30 02:13:21+03","2024-05-17 05:00:37.1269+03","Have you verified your security group for your RDS service It needs to allow access from the security groups given to your Lambda function It is not enough that they are in the same VPCsubnets The security group still needs to allow traffic on the ports for postgres 5432 Note that for security groups you do not have to select an origin IP which can be tricky for Lambda But i notice you are giving your Lambda function the group sg29aac25d You can use that ID to give access to the RDS IAM policies should be irrelevant as you are authenticating against postgres Unless your IAM does not allow your Lambda to execute the problem is not IAM "
57645148,49409515,"stackoverflow.com",0,"2019-08-25 13:14:42+03","2024-05-17 05:00:37.949987+03","It is been a while but recently i noticed that there is a plugin now that helps setup this exact configuration httpsgithub comhorike37serverlessapigatewayserviceproxy Hope this helps others that are still having this problem"
48466766,48462965,"stackoverflow.com",2,"2018-01-29 18:36:23+02","2024-05-17 05:00:40.970262+03","This might be a bit bias because I have used custom authorizers in the past But they are probably the most flexible solution from those you mentioned I have actually used the custom authorizer with cognito user pools before there was a cognito authorizer At the time this httpsaws amazon comblogsmobileintegratingamazoncognitouserpoolswithapigateway helped me a lot to get started Later i found that i should take advantage of the caching of the policies sent by the authorizer once I was doing a lot of validation and since user permissions did not change that often For that i had to send as a response a policy that would cover all the endpoints that were allowed to be invoked by the authenticated user EDIT On the custom authorizer you are able to check the user information by calling cognito After that you can give the policies for the user if it is a default user And for admins you just send another Policies"
48497319,48497318,"stackoverflow.com",6,"2018-01-29 10:42:59+02","2024-05-17 05:00:42.910187+03","After some time researching i found solution So here it is So as you can see in package section i have added includeexclude part at first i am excluding all files then i am include 2 needed folders common and node_modules After this for each function i also use include command for to add only needed file "
48517377,48510018,"stackoverflow.com",0,"2018-01-30 10:55:30+02","2024-05-17 05:00:43.996629+03","I think you need to pass back the response object in the callback in the handler e g "
48517320,48516592,"stackoverflow.com",2,"2018-01-30 10:51:36+02","2024-05-17 05:00:44.898813+03","You must set environment variables in powershell like so"
48529282,48522653,"stackoverflow.com",14,"2018-01-30 21:39:09+02","2024-05-17 05:00:45.918266+03","1 You can resolve the topic dynamically This can be done through CloudFormation Intrinsic Functions which are available within the serverless template see the added environment section In this case the actual topic reference name generated by the serverless framework is SNSTopicConvertTextToSpeach The generation of those names is explained in the serverless docs 2 Now that the ARN of the topic is mapped into an environment variable you can access it within the GraphQL lambda through the process variable process env topicARN "
50628842,48532841,"stackoverflow.com",6,"2018-05-31 19:27:32+03","2024-05-17 05:00:46.833254+03","This is not possible at the moment As you mentioned the only exposed parameter is DistributionDomainName and this works only for edgeoptimized endpoints As a workaround until it will be implemented in CloudFormation you could use a CustomResource backed up by your own Lambda function to return the regionalDomainName attribute Heres a sample CloudFormation YAML code that does this"
48658537,48532841,"stackoverflow.com",1,"2018-02-07 12:08:45+02","2024-05-17 05:00:46.835254+03","I am having the same Issue scripting it in the Cloud formation When I read more about it There is no supported Return Value for the Regional Endpoint as of now from what I have researched You can use this to read the Regional Endpoint using Python Script create a Custom Resource using AWS Lambda so that your automation wont break due to the unavailability of the return value client boto3 client apigateway response client get_domain_name domainNameapi example com print response[regionalDomainName] "
48546806,48534740,"stackoverflow.com",13,"2018-01-31 18:22:11+02","2024-05-17 05:00:47.748043+03","Obviously Lambda needs to be setup to run inside the same VPC but I am assuming you already got that You need to Does it work"
50191133,48534740,"stackoverflow.com",2,"2018-05-05 18:32:46+03","2024-05-17 05:00:47.74902+03","Make sure your Lambda function is in the VPC and the security group allows connections from IP addresses within the subnet of the VPC The amount of available IP addresses is going to affect how many lambda functions can be run concurrently Also make sure that the Lambda functions role has the ability to describe the VPC the AWSLambdaVPCAccessExecutionRole policy should do the job for you "
48557933,48554917,"stackoverflow.com",12,"2018-02-01 10:02:42+02","2024-05-17 05:00:48.753142+03","Thanks to dashmugs comment after some investigation on this page httpsgithub comserverlessheavenserverlesswebpack there is a section on Forced Inclusion I will paraphrase it here Forced inclusion Sometimes it might happen that you use dynamic requires in your code i e you require modules that are only known at runtime Webpack is not able to detect such externals and the compiled package will miss the needed dependencies In such cases you can force the plugin to include certain modules by setting them in the forceInclude array property However the module must appear in your services production dependencies in package json So I simply did this Now it works Hope this helps someone else with the same issue "
56844797,48554917,"stackoverflow.com",2,"2019-07-02 06:08:33+03","2024-05-17 05:00:48.755142+03","None of the previous helped me I used this solution httpsgithub comsequelizesequelizeissues9489issuecomment493304014 The trick is to use dialectModule property and override sequelize The previous so far works on MySql but is not working with Postgres"
72831747,48562937,"stackoverflow.com",5,"2022-07-01 18:48:47+03","2024-05-17 05:00:49.452582+03","For those struggling to find a solution in 2022 use package patterns parameter Example in front of the file path excludes specified pattern Documentation httpswww serverless comframeworkdocsprovidersawsguidepackaging"
48628750,48562937,"stackoverflow.com",3,"2018-02-05 20:05:36+02","2024-05-17 05:00:49.453582+03","It depends on how you are actually trying to load the file Are you loading it with fs readFile or fs readFileSync In that case Serverless does not know that you are going to need it If you add a hint for Serverless to include that file in the bundle then make sure that you know where it is relative to your current working directory or your __dirname Are you loading it with require Do you know that you can use require to load JSON files In that case Serverless should know that you need it but you still need to make sure that you got the path right If all else fails then you can always use a dirty hack For example if you have a file x json that contains then you can change it to an x js file that contains that you can load just like any other js file but it is a hack "
64946034,48562937,"stackoverflow.com",3,"2020-11-21 19:53:16+02","2024-05-17 05:00:49.455277+03","From what I have found you can do this in many ways As it is stated in another answer if you are using webpack you need to use a webpack plugin to include files in the lambda zip file If you are not using webpack you can use serverless package commnad includeexclude You can create a layer and reference it from the lambda the file will be in optlayer_name Take in consideration that as today Nov20 I have not found a way of doing this if you are using serverless ts without publishing the layer first lambdas layer property is an ARN string and requires the layer version If your worried about security you can use AWS Secrets as it is stated in this answer You can do what rsp says and include it in your code "
48745284,48573826,"stackoverflow.com",0,"2018-02-12 13:21:26+02","2024-05-17 05:00:50.221066+03","you could use the copywebpackplugin to include the models instead of referencing make sure you import the models from the right folder since you are importing from relative path This might change if you are bundle is in a different folder "
48575313,48574178,"stackoverflow.com",0,"2018-02-02 05:31:36+02","2024-05-17 05:00:50.634366+03","Figured out the problem that was due to improper indentation of iamRoleStatements statement Due to that Serverless has not updated policy under lambda execution role "
48627731,48591940,"stackoverflow.com",2,"2018-02-05 19:21:05+02","2024-05-17 05:00:51.361044+03","I guess that what you are doing is using someObject processPayload and passing it as a callback to some function or saving it somewhere as an event handler This function in turn calls this preHandlePayload but it cannot because not having been called properly it has its this undefined There are few possible things that can cause this to be undefined inside of a method and it is not important if it is private or public because as you can see all of them end up being public after the transpilation One possible way is for example inside method1 if you call method2 instead of this method2 then you will have this undefined in method2 Another common mistake is saving a method as an event handler or using it as a callback for example or For those cases to work you either need to pass an anonymous function like or binding the method to that object Of course it is hard to tell what is really the problem in your case because you did not how us how you actually call your methods and this is the calling not the definition of methods that usually puts undefined in this "
48605153,48594646,"stackoverflow.com",2,"2018-02-04 07:48:26+02","2024-05-17 05:00:52.427008+03","After 2 weeks of being stuck on this I learned that I needed to specify my environment variables in my serverless yml for it to work My functions were never connecting to the database because my mongo string was never set in lambda Hope this helps someone "
48627557,48604282,"stackoverflow.com",0,"2018-02-05 18:53:05+02","2024-05-17 05:00:52.905174+03","You can see the usage plans associated with an API key and act on that information see So instead of requiring a mere presence of an API key you can inspect some properties associated with it and decide to either allow or disallow the usage of your function based on that but you will have to implement that logic inside of a function for the biggest flexibility For more info see especially links pointing to info on plans and marketplace "
48609353,48606485,"stackoverflow.com",1,"2018-02-04 16:51:36+02","2024-05-17 05:00:53.964993+03","I like the simplicity and readibility of the sub function over the join Sub arnawskinesis AWSRegion AWSAccountId streamxxxx Rereading that it relates to the serverless framework I would leverage this plugin httpswww npmjs compackageserverlesspseudoparameters"
48606555,48606485,"stackoverflow.com",0,"2020-06-20 12:12:55+03","2024-05-17 05:00:53.966994+03","use FnGetAtt from aws documentation FnGetAtt returns a value for the Arn attribute Arn The Amazon resource name ARN of the Kinesis stream such as arnawskinesisuseast2123456789012streammystream For more information about using FnGetAtt see httpsdocs aws amazon comAWSCloudFormationlatestUserGuideintrinsicfunctionreferencegetatt html"
47581126,47567549,"stackoverflow.com",15,"2017-11-30 21:23:03+02","2024-05-17 05:00:55.771529+03","The AWS recommended solution is to rename httpsaws amazon compremiumsupportknowledgecentercloudformationcustomname"
51777281,47567549,"stackoverflow.com",3,"2018-08-12 18:07:54+03","2024-05-17 05:00:55.77253+03","I found I needed to insert some variables to make it work Environment variable USERS_TABLE users optstage selfprovider stage selfprovider environment BUILD_NUMBER Table name TableName selfprovider environment USERS_TABLE In my code const existingUser await dynamoDb get TableName process env USERS_TABLE Key email promise "
72557959,47567549,"stackoverflow.com",3,"2022-06-09 12:34:33+03","2024-05-17 05:00:55.77453+03","Rename your resource to something else deploy it rename it back if you need and deploy again "
74980445,47567549,"stackoverflow.com",3,"2023-01-02 10:50:22+02","2024-05-17 05:00:55.77553+03","This problem usually occurs when you modify the partition keys of your DynamoDB table DynamoDB cannot make the change for you You must delete the database making sure to back it up and recreate it Once the new DB has been created you can perform your migration "
74679966,47567549,"stackoverflow.com",1,"2022-12-04 21:03:44+02","2024-05-17 05:00:55.77653+03","According to httpsaws amazon compremiumsupportknowledgecentercloudformationcustomname This error typically occurs when a stack update tries to replace resources that have properties with custom names AWS CloudFormation does not replace a resource that has a custom name unless that custom name is changed to a different name To prevent a stack failure and avoid the error message change any resources with custom names to use different names before you update a stack To resolve this you need change TableName to some other string What that will do Serverless will delete your table because that is no longer a part of a stack and will create a new table with a new name and a keys "
47570369,47569793,"stackoverflow.com",19,"2017-12-01 08:26:26+02","2024-05-17 05:00:56.819653+03","If you are doing query then you have to pass the primary key which in your case is userId If you do not have primaryKey and if you want all the logged in true fields then you can do scan with filterExpression like this Update Since the scan operation is less efficient the other way to solve this problem is to create a GSI with primary key loggedIn But the problem here is that you cannot make any field primary key which have boolean data type It has to be number string binary So to create a gsi you need to store accepted data type in loggedIn field instead of boolean Though I am not sure how much performance impact it will have for a table of thousand records but the good thing about gsi is that you can create them later even on the existing table if in future you find out some performance impact Also the number of gsi you can create on table is limited to 5 So utilise gsi wisely "
47573696,47569793,"stackoverflow.com",4,"2017-11-30 14:46:55+02","2024-05-17 05:00:56.822206+03","A Scan operation always scans the entire table or secondary index then filters out values to provide the desired result essentially adding the extra step of removing data from the result set Avoid using a Scan operation on a large table or index with a filter that removes many results if possible Read more You should use global secondary index AWS Console DynamoDb tab Indexes of your table Create index We should add secondary key to have unique pair Do not use index name loggedIn so as loggedIn should be unique Than you can use a Query method with primary key loggedIn "
47570356,47569793,"stackoverflow.com",4,"2021-07-22 00:39:17+03","2024-05-17 05:00:56.823922+03","In order to query a DynamoDB table you can only query attributes that are part of the Primary Key or part of an index The Primary Key can be either In addition to the Primary Key you can also create two types of index In order to query for loggedIn records you need to either include this attribute in your Primary Key or add a Local or Global Secondary Index including the loggedIn attribute "
47698093,47577377,"stackoverflow.com",0,"2017-12-07 17:10:53+02","2024-05-17 05:00:57.770529+03","If your serverless is creating the IAM role by it self you should had that to the iamRoleStatements like shown here but it could be easier to create an IAM role in aws console and manage that yourself and use it like here"
47603995,47585581,"stackoverflow.com",2,"2017-12-02 04:35:21+02","2024-05-17 05:00:58.861888+03","Link to my other answer that uses the main table as designed This approach requires modifying group modeling in UserStatus from a single record with a string set to multiple records with a string This is because DynamoDB does not support yet that makes for a good feature request though keying on sets The main table is used for updatesinsertsdeletes and looks like this Setup a GSI on group priority This will be used for queries Yes the combination chosed for this index will have duplicates DynamoDB does not bother with this and works nicely Tasks Should I follow this approach I think it is a good design when there are many groups and a single group contains up to 20 of total users and users belong to 2 or 2 groups "
47592450,47585581,"stackoverflow.com",1,"2017-12-02 03:58:09+02","2024-05-17 05:00:58.863342+03","Indexes are not about sorting Sorting is but one method used to efficiently retrieve rows because search in sorted array can be done in logarithmic time O log n instead of linear time O n It is only a consequence that rows are returned in sorted order But let us focus on the capability of finding the exact rows to be returned with less effort IO e g disk reads The filtering needs for this type of query by group status and a couple more columns are really hard for DynamoDB fo process efficiently By efficiency I mean how many rows DynamoDB needs to retrieve from disk to determine which rows to return to the client If it returns 10 of total rows read from the table that is not efficient This is why a regular Scan together with filters is not as good as an indexed query Filters are a lie as they still read from items from database and count towards provisioned capacity An indexed query will retrieve from storage rows close to the number it actually returns This is achieved with DynamoDB but limiting to a single partition items with the same partitionhash key and a range starting with for a sort key Why rows are not returned sorted by sort key when doing a scan Because DynamoDB uses the sort key inside Item Collections each collection being determined by the hash key When the result set contains for instance 2 unique hash keys the result set will contain 2 separate sections sorted by the sort key in other words rows will not be sorted in a single direction they will restart in the middle of the result set Sorting in memory will be required to have a single sorted collection Why not create then an index on a column that can have a single value for all rows If we run a scan then rows will be returned sorted by priority sort key But having all items containing the same value for a field this is a data anomaly So when should I create an index Given the group attribute should likely be the most selective it would be faster to hash on this attribute on a global index but this would change the model requiring to store each group in a separate item instead of using a string set That is not very convenient in the NoSQL world requiring more of a relational model So one way to do it considering it is okay to use a scan but without a separate index is running a scan and sorting in memory Use the Arraysort method to do it in node js The performance characteristics are closer to the approach with secondary index only a index would be just a waste of resources in this case Because if a queryscan on an index returns the same amout of information a scan on a table would go with the table approach Remember indexes are about being selective when retrieving rows How would I know if this is a good approach for my use case Well this is not a definitive rule but I would say if you want to retrieve more than 50 of the table rows this would be okay In matter of costs it would not pay off keeping a separate index Even looking into another design perhaps because this is not very selective Now if you want like 20 or less data then a different approach would be nice to look into "
47588137,47585581,"stackoverflow.com",1,"2018-02-11 13:17:22+02","2024-05-17 05:00:58.868555+03","So I found an interesting solution to this problem Here is my new code Note the use of IndexName typePriorityIndex the trick here is to find something or make something in your table that the records will all have the same and make that the hash key then the sort key should be what you will want to sort by which in my case is priority The index looks like this to give an idea My serverless file looks like this for defining it"
47608543,47594168,"stackoverflow.com",79,"2021-08-31 19:23:45+03","2024-05-17 05:00:59.760811+03","The default timeout for AWS Lambda functions when using the Serverless framework is 6 seconds Simply change that to a higher value as noted in the documentation "
47605576,47594168,"stackoverflow.com",12,"2017-12-04 09:51:46+02","2024-05-17 05:00:59.762361+03","Since you mentioned that your DynamoDB table is provisioned with only 5 WCU this means that only 5 writes are allowed per second DynamoDB does offer burst capacity allowing you to use 300 seconds worth of accumulated capacity which at 5 WCU it is equivalent to 1500 total write requests but as soon as those are exhausted it will start to throttle The DynamoDB client has automatic retries built in with exponential backoff and it is smart enough to recognize throttling so it will slow down the retries to the point that a single write can easily take several seconds to complete successfully if it is being repeatedly throttled Your Lambda function is very likely timing out at 6 seconds because the function is waiting on retries to Dynamo So when doing load testing make sure that your dependencies are all scaled appropriately At 1000 requests per second you should make sure to scale the ReadWrite capacity allocation for your DynamoDB table s andor Index s accordingly "
47618381,47617935,"stackoverflow.com",1,"2017-12-03 14:33:53+02","2024-05-17 05:01:00.521462+03","At the moment Serverless Framework does not have the support for it However it is also not difficult to implement the support so hopefully it will come quite soon "
47626297,47617935,"stackoverflow.com",1,"2017-12-04 05:58:51+02","2024-05-17 05:01:00.522462+03","Right now No but AWS recently announced support for it If you want to see this feature in sls create an issue and track its progress "
47622192,47621920,"stackoverflow.com",15,"2017-12-03 21:13:25+02","2024-05-17 05:01:01.353674+03","AWS SAM The focus on the AWS Serverless Application Model At least for the moment is to simplify defining AWS Resources related with Serverless Technology Stack Lambda API Gateway etc in CloudFormation There are several limitations that comes with SAM Its currently not providing full support for Custom Authorizers in a flexible manner Also the DevOps tooling and plugins support is minimal compared to Serverless Framework Since SAM is native to AWS support for AWS features will be available in SAM earlier than the Serverless Framework Serverless Framework A DevOps Framework which allows to simplify defining and deploying AWS Azure Google Cloud and IBM Open Wisk in an unified manner It uses CloudFormation underneath for AWS Serverless Stack Provisioning and comes with Plugins Eco System as well as simple commands to carryout DevOps tasks Serverless Framework has several limitations"
47646243,47627036,"stackoverflow.com",2,"2017-12-05 06:44:37+02","2024-05-17 05:01:02.347783+03","Fixed my problem by adding the following role in iamRoleStatements in serverless yml"
47643656,47632899,"stackoverflow.com",2,"2017-12-07 20:42:17+02","2024-05-17 05:01:03.158746+03","You will have to specify this particular folder inside serverless yml if it is not specified or referenced from your code files See the docs on serverless packaging here Thanks dege for pointing out I missed the to note the contents of the actual folder"
47750727,47655872,"stackoverflow.com",2,"2020-03-04 11:40:43+02","2024-05-17 05:01:03.99595+03","It was a bug in Sam local fixed with new update If you still have a problem in windows then try this this should help if your Path is wrong \"
50478069,47693281,"stackoverflow.com",1,"2018-05-23 03:05:25+03","2024-05-17 05:01:04.818892+03","According to the AWS documentation Native modules are similarly installed and deployed but youll need to build them against the Amazon Linux libraries So spin up an EC2 instance build your project there zip it up and upload it to Lambda It appears to make a difference which Amazon Linux AMI you use though Probably because they are different architectures or have different library versions or something In my case which was building a NodeJS project using the x509 module the Amazon Linux 2 AMI ami922914f7 did not work it resulted in an error very similar to yours If you try this one and it does not work try rebuilding it on the original Amazon Linux AMI ami2a0f324f After I did that it ran perfectly in Lambda Thanks to Tommaso for the inspiration behind this solution "
46709809,46698545,"stackoverflow.com",0,"2017-10-12 15:37:48+03","2024-05-17 05:07:34.616374+03","The Update API call will not work as the data email and data password are undefined I would suggest altering the UpdateExpression when you do not need to update those attributes "
47925941,47693281,"stackoverflow.com",0,"2017-12-21 15:22:22+02","2024-05-17 05:01:04.820808+03","This happens when you package node dependencies on a system with a different architecture due to some dependencies that have native code that gets compiled in your system which is different than AWS Lambda Run docker run v PWDvartask lambcilambdabuildnodejs6 10 instead of npm install to setup the dependencies in an AWS Lambdalike environment before packaging your module before uploading it to AWS "
55814803,47715448,"stackoverflow.com",0,"2019-04-23 18:38:11+03","2024-05-17 05:01:06.88067+03","My solution was to have the following in my babelrc You can also play around with these options"
47724130,47718004,"stackoverflow.com",8,"2017-12-09 02:42:41+02","2024-05-17 05:01:07.838316+03","You can put those definitions on a different property and use variables in order to choose which definitions to use You may need to change this depending on how you specify your stage on deployment optstage or envstage "
54588243,47718004,"stackoverflow.com",1,"2020-11-08 17:58:09+02","2024-05-17 05:01:07.839782+03","I am using SLS 1 32 0 I was not able to get functions selfenvironmentfunctions optstage to work Not sure why It returns the following However using the same logic in dashmugs answer file worked for me serverless yml serverlessdev yml serverlessprod yml "
65680996,47718004,"stackoverflow.com",1,"2021-01-12 11:17:30+02","2024-05-17 05:01:07.840779+03","If you are using Serverless framework you could use serverless plugin serverlesspluginifelse Then If you want to exclude say func1 for useast1 Then use below code snippet"
66369111,47718004,"stackoverflow.com",0,"2021-02-25 15:08:44+02","2024-05-17 05:01:07.842342+03","Create a file for example envfunctions yml and add content as below After this in serverless yml set"
47802442,47775546,"stackoverflow.com",1,"2017-12-13 23:33:08+02","2024-05-17 05:01:08.865378+03","My issue was incorrect encoding of the ResourcePath The above is correct with ResourcePath 1events1 eventId 1geo1 ipAddress However my script I wrote to automate this did not have the first 1 after the initial slash I did not notice this until I printed it out and stared at it for far too long "
47883517,47775785,"stackoverflow.com",1,"2017-12-19 11:15:22+02","2024-05-17 05:01:09.732679+03","I resolved this issue by changing the logging dependency to I have found a very useful article here httpsblog symphonia ioalovelettertolambdalogging974b0eb49273"
46770550,46770045,"stackoverflow.com",0,"2017-10-16 15:38:59+03","2024-05-17 05:01:11.263053+03","If does not work then you will have to stop defining the VPC in the provider and define them elsewhere in the function or as references or whatever Or raise an issue in GitHub and see whether anyone is willing to fix it "
50518374,46770045,"stackoverflow.com",1,"2018-08-14 09:04:45+03","2024-05-17 05:01:11.265054+03","Try this it works for me"
46788995,46786688,"stackoverflow.com",4,"2017-10-17 14:57:48+03","2024-05-17 05:01:12.083374+03","Here is the event json structure sent by S3 upon object creation httpdocs aws amazon comAmazonS3latestdevnotificationcontentstructure html You can get the file names sizes and source ip like this"
46790513,46786688,"stackoverflow.com",3,"2020-07-09 18:44:10+03","2024-05-17 05:01:12.085375+03",""
46826168,46813324,"stackoverflow.com",0,"2017-10-20 12:45:54+03","2024-05-17 05:01:12.996024+03","According to your description I checked this issue and reproduced this issue as follows Please tell how can i get the entire documents binded with my DocumentClient with the new updated package Based on your scenario I would recommend you construct the DocumentClient by yourself instead of using the binding to DocumentClient for a workaround to achieve your purpose And you could configure the serviceEndpoint and accountKey under your local settings json file just as the app setting AzureWebJobsStorage Then you could use the following code for retrieving your setting value Moreover here is a issue about constructing the DocumentClient from the connection string you could refer to it UPDATE For 1 18 the following code could work as expected"
48193196,46833984,"stackoverflow.com",1,"2018-01-10 19:42:38+02","2024-05-17 05:01:15.024111+03","Cognito User Pools is used to authenticate users and provides you JWT tokens When you want to access any AWS Services you need AWS Credentials access key and secret key This is where you should use Federated Identities The tokens you get from Cognito User Pools should be exchanged with Federated Identities to get AWS credentials to access other AWS services The serverlessstack also covers this in detail Now since you have not added the user pool as an authentication provider in your identity pool my observation is that you are getting an unauthenticated identity from Federated Identities you can confirm this from the Amazon Cognito console which is why it is different for each of your team members You should add the user pool as an authentication provider in the identity pool and follow the documentation to provide the information required in logins map "
46865970,46857335,"stackoverflow.com",50,"2017-10-21 20:23:22+03","2024-05-17 05:01:16.045599+03","One thing you can do is use a custom domain that you own e g mycompany com and map that to your API Gateway This way rather than making a request to httpsab1cd2ef3g executeapi euwest1 amazonaws comdev you would make a request to httpsapi mycompany com There is a plugin called serverlessdomainmanager that makes it much easier to set up this custom domains Check out this blog post for a full walkthrough on how to use it "
64423199,46857335,"stackoverflow.com",37,"2020-10-19 10:51:50+03","2024-05-17 05:01:16.0476+03","In the local environment we may use the flag noPrependStageInUrl when running the dev server sls offline start noPrependStageInUrl when using serverless offline Online we may set up a CloudFront or custom domain "
46860528,46857335,"stackoverflow.com",26,"2017-10-22 02:45:58+03","2024-05-17 05:01:16.048471+03","This is an API Gateway featureconvention NOT from Serverless Framework so serverless cannot do anything about it API Gateway requires you with a stage and it is appended at the end of your endpoint API Gateway endpoints are meant for developers though so it is not meant to be userfriendly If you want it to be userfriendly you can add a custom domain for it Different stages can have different custom subdomains "
69417221,46857335,"stackoverflow.com",14,"2022-11-28 14:35:50+02","2024-05-17 05:01:16.056802+03","This can be solved by using httpApi which does not prepend the stage to the URL path instead of http which does Instead of use this The http key creates an API Gateway REST aka v1 endpoint whereas httpApi creates an HTTP aka v2 endpoint v1 has more features but v2 is faster and cheaper From the Serverless docs Despite their confusing name both versions allow deploying any HTTP API like REST GraphQL etc Full comparison httpsdocs aws amazon comen_usapigatewaylatestdeveloperguidehttpapivsrest html"
56042401,46857335,"stackoverflow.com",7,"2019-05-08 16:52:41+03","2024-05-17 05:01:16.058799+03","Triggered by dashnugs answer API Gateway requires you with a stage and it is appended at the end of your endpoint and another reply that I read elsewhere I solved the problem by making the stage specification a bit less telling about which stage environment was referred to by using v1 as a stage That also suggests some sort of API versioning which is acceptable in my case as well So my serverless yml section now contains"
46863653,46861678,"stackoverflow.com",10,"2017-10-21 16:19:35+03","2024-05-17 05:01:16.800881+03","I agree that documentation on this would make an excellent pull request here You are correct that serverless is using CloudFormation under the hood The framework does expose the underlying CloudFormation machinery to you by way of the resources key of your serverless yml I think the intent of the framework is that you would put the rest of these resources Cognito stuff S3 etc in the resources section of your serverless yml file using regular old CloudFormation syntax For example this file will create a DynamoDB table and S3 bucket in addition to the serverless function If you are new to CloudFormation I would also recommend taking a peek at CloudFormer "
46878795,46861678,"stackoverflow.com",0,"2017-10-22 23:27:45+03","2024-05-17 05:01:16.802732+03","Base on Mike Patricks options adding my understanding for serverless framework and other similar serverlessfocus tools As you have mentioned for serverless projects there are a lot of resources involved Combine them together is not simple job So choice a right tool is hard Compare Serverless framework to Cloudformation and Terraform serverless framework is serverless specialist Cloudformation and Terraform are GP Cloudformation and terraform are fully Infrastructure as Code which covered most resources Serverless framework is a middle layer only to generate Cloudformation template which mostly only for serverless related resources You can write all in Cloudformation template directly but the template file will be large it is hard to maintain by its JSONYaml template With a few dozen lines in serverless yml serverless framework can generate a thousand or several thousand lines of cloudformation template It saves a lot of time to deal with the cloudformation codes It does not make sense to let serverless framework deal with all AWS resources that other tools do most well already Serverless framework is still in developing because of its popularity many developers are involved to add features into it daily Maybe one day you can get what you need but now you have to mix serverless framework with Cloudformation or Terraform or other tools together in some case "
54156018,46861678,"stackoverflow.com",0,"2019-01-12 03:27:28+02","2024-05-17 05:01:16.805302+03","You can surely already deploy almost everything as IaC in fact we do this daily at work using various deployment tools If you happen to work primarily with Serverless then you can pick something like Serverless Framework SF to abstract some of the complexityinflexibility inherent in working with CloudFormation CF Whatever CF can do SF can do but SF have a plugin system which allows to run codes to cal out APIs which for example can allow you to create resources that have not been supported by CF "
46870185,46870035,"stackoverflow.com",2,"2017-10-22 06:30:59+03","2024-05-17 05:01:18.391755+03","First of all you cannot create a composite key as in relational databases in DynamoDB The keys you get in a table are Hash and Range Also called Sort and optional keys Since this limits the querying capability DynamoDB supports creating indexes called Global Secondary Index GSI and Local Secondary Index LSI to extend the query capabilities Based on your schema since the combination of id server and room are unique you can use a concatenation for the Hash key such as id_server_room so that the items in the table are enforced for uniqueness You can then create id server and room as attributes To enable querying from those attributes efficiently create GSIs as required "
46875182,46875006,"stackoverflow.com",2,"2017-10-22 17:24:21+03","2024-05-17 05:01:18.51697+03","There is no way to do what you propose AWS Lambda requires you to upload the entire package including all dependencies each time Everything has to be inside the zip file that is deployed to Lambda "
50839554,46875006,"stackoverflow.com",1,"2018-06-13 19:21:52+03","2024-05-17 05:01:18.51797+03","This was my solution This allow you to deploy only the small files "
74168950,46875006,"stackoverflow.com",0,"2022-10-23 08:51:55+03","2024-05-17 05:01:18.51897+03","You can try using lambda layers All you need to do is create separate serverless project for dependencies management for ex node_modoles and rest of the services will refer to it follow docs This should reduce the deployment or package size of individual lambda significantly "
74621486,46875006,"stackoverflow.com",0,"2022-11-30 01:50:58+02","2024-05-17 05:01:18.519971+03","Use lambda containers and your problems will be solved Lambda containers have a 10 GB image size limmit You can add anything you want in there I have made many express apps with Serverless http and lambda containers You can also add an efs to your lambda and acess your files from there Check this tutorial"
55634289,46887794,"stackoverflow.com",2,"2019-04-11 22:38:00+03","2024-05-17 05:01:19.82673+03","The virtualenv directory should not be included in the zip file If the directory is located in the same directory as serverless yml then it should be added to exlude in the serverless yml file else it gets packaged along with other files"
46888079,46887794,"stackoverflow.com",6,"2017-10-23 14:18:37+03","2024-05-17 05:01:19.827729+03"," Are you sure you need pandas and numpy in a microservice There is nothing micro in those libraries There is a way Deploy you Lambda with Zappa httpsgithub comMiserlouZappa It is a convenient way to write deploy and manage your Python Lambdas anyway But with Zappa you can specify an option called slim_handler If set to true most of your code will reside at S3 and will be pulled once a Lambda is executed AWS currently limits Lambda zip sizes to 50 megabytes If your project is larger than that set slim_handler true in your zappa_settings json In this case your fat application package will be replaced with a small handleronly package The handler file then pulls the rest of the large project down from S3 at run time The initial load of the large project may add to startup overhead but the difference should be minimal on a warm lambda function Note that this will also eat into the memory space of your application function "
56129424,46912563,"stackoverflow.com",13,"2019-05-14 14:31:37+03","2024-05-17 05:01:21.350677+03","See the serverless documentation httpsserverless comframeworkdocsprovidersawsguidefunctionsawsxraytracing See also tracing for api gateway httpsserverless comframeworkdocsprovidersawseventsapigatewayawsxraytracing"
46921101,46912563,"stackoverflow.com",3,"2017-10-25 01:40:38+03","2024-05-17 05:01:21.351678+03","It is in development If you are unwilling to wait until the official functionality is released you can install the plugin until it is ready "
76407442,46912563,"stackoverflow.com",0,"2023-06-05 17:27:05+03","2024-05-17 05:01:21.352678+03","It is also worth noting that currently XRay support is dependent on which type of Aws API is selected If you chose REST which by your syntax you did XRay is supported BUT if someone chooses HTTP it is NOT supported For reference see httpsdocs aws amazon comapigatewaylatestdeveloperguidehttpapivsrest htmlhttpapivsrest differences monitoring"
46931998,46928105,"stackoverflow.com",32,"2018-04-06 12:33:23+03","2024-05-17 05:01:22.276135+03","I suggest to add to your IAM policy also the access to Cloudwatch Actually your lambda function is not returning anything but you can see your log output in Cloudwatch I really recommend to use logger info message instead of print when you are setting up logger I hope that this helps to debug your function Except the part of sending this is how I will rewrite it just tested in the AWS console "
46943426,46935384,"stackoverflow.com",1,"2017-10-26 02:14:04+03","2024-05-17 05:01:23.281554+03","Perhaps use doublequotes and stringify the json e g If you need more examples the AWS documentation has sample events that you can modify and when you use AWS console to test your lambda you can select many different event templates from a dropdown "
46956661,46956660,"stackoverflow.com",20,"2018-07-10 01:23:19+03","2024-05-17 05:01:23.792327+03","Even though serverless yml does not support conditional logic it is possible to simulate conditions In serverless yml add Depending on the sls deploy stage parameter either the dev or prod domain will be configured More info on conditional logic httpsforum serverless comtconditionalserverlessymlbasedonstage1763"
46964331,46960611,"stackoverflow.com",7,"2017-10-27 00:44:58+03","2024-05-17 05:01:24.875412+03","The shortcut syntax of Ref is not yet supported within the serverless framework As the bug ticket below suggests you have to use the object based form for now Failure to Create Resource Using Ref The feature is currently tracked through that issue FnSub and Sub"
57355241,45612940,"stackoverflow.com",0,"2019-08-05 12:05:30+03","2024-05-17 05:01:49.04298+03","This problem solved in a better way in new release Add support for existing Cognito User Pools Your function event should look like this No need to add additional CognitoUserPool in front of the resource definition "
46311637,46161538,"stackoverflow.com",7,"2017-09-20 03:45:10+03","2024-05-17 05:01:27.070174+03","Returning a 403 instead of 404 is a deliberate design decision This is a pattern that is used in many other AWS APIs most notably S3 In S3 if the user would have had permissions to the see presence of the key via the ListBucket permission a 404 will be returned otherwise a 403 will be returned Because API Gateway enables permissions at the method level we cannot know whether or not the user should be permitted to have knowledge of the existence of the API resource level and default to the 403 as a result You can elect to catch all missing API methods using a proxy pattern "
58450379,46161538,"stackoverflow.com",0,"2019-10-18 14:55:22+03","2024-05-17 05:01:27.071789+03","I did something a bit different its not relavent to API calls but it is to the final goal of using serverless to host a website in the CloudFrontDistribution section I added this "
46170870,46154758,"stackoverflow.com",2,"2017-09-12 10:52:53+03","2024-05-17 05:01:27.077156+03","Giting hints from httpsmedium comamanwithnoserverdeployingaserverlessapplicationusingwebpackandbabeltosupportes2015toaws2f61cff8bafb I modified a serverless azure functions startup test project with serverlesswebpack which seems to be satified with your requirement I built a src folder in the root directory of serverless azure functions project as the develop source code folder With 2 test files handler js tool js webpack config js in root directory With which configuration file the out bundled file will be located in servicehandler js in root directory So I modified serverless yml as well now it partly looks like After these modified use serverless deploy will bundle the files in src folder then package and deploy to azure function Hope it helps "
46174453,46168826,"stackoverflow.com",2,"2017-09-12 13:48:40+03","2024-05-17 05:01:28.084398+03","If you are seeing errors like execution timed out than you are probably cutting the execution of your Lambdas with a too low timeout There might be several reasons for this To mitigate the problem you should temporarily add some logging to your Lambda and increase the timeout so that you can figure out what actually takes so long Unless you are already a heavy Lambda user you are unlikely to use up your 400 000 free GBseconds a month If you run your Lambdas with 128 MB this equates to 3 200 000 seconds per month 103 225 seconds per day 28 5 hours per day Try to test with higher memory settings as well depending on case this can even reduce the total GBs consumed As others pointed out already you only pay for the time actually used so if your Lambda finishes faster than the timeout you only pay for the actual time consumed in 100 ms increments "
46177431,46173773,"stackoverflow.com",11,"2017-09-12 16:14:09+03","2024-05-17 05:01:29.167975+03","Apparently you need to run admin app delete otherwise the connection to Firebase remains active preventing the process from terminating So in my example code considering that I will not need to use Firebase anymore after running this function"
46198644,46189597,"stackoverflow.com",9,"2018-07-19 19:52:36+03","2024-05-17 05:01:30.254784+03","Question 1 Update Serverless framework now supports referencing variables from the Parameter Store That means you can skip defining them in CodeBuild as serverless will retrieve them from Parameter Store upon deploy Example serverless yaml Original Answer If you want to stick with your config yml then the only way to make it work is through hacks similar to what you are already doing since it is not versioncontrolled What I would suggest is to have your environment variables stored in EC2 Parameter Store which you can reference in your CodeBuild buildspec yml These variables are accessible in your serverless yml using envVARIABLE_NAME For local development you should also use real environment variables versus storing them in config yml Tools such as direnv are great at this Question 2 You can do manual rollbacks by rerunning the previous CodeBuild job I cannot think of an easy way to do it automatically like what CodeDeploy does Maybe a Lambda function can do a postdeploy test and if it fails it can trigger rerunning the previous CodeBuild job Question 3 CodePipelines are tied to a single branch so to make it work on PR branches you have to do hacks like the article you mentioned I have resorted to using API Gateway Lambda CodeBuild No CodePipeline to do this "
51425207,46189597,"stackoverflow.com",0,"2018-07-20 10:58:49+03","2024-05-17 05:01:30.258417+03","Just to add to the accepted answer given for Question 1 Thanks dashmug and Lakshman Diwaakar This does sort out how to get the Parameter Store values into your Lambda However the values are shown as plain text in the Lambda console I need to work out next is how to add encryption Parameter Store Add your env variables as parameters in AWS Systems Manager Parameter store httpseuwest1 console aws amazon comsystemsmanagerparameters buildspec yml serverless yml Reference the created Params in serverless yml Append true for secure strings handler js Use the env vars in your handler"
46223153,46212719,"stackoverflow.com",2,"2017-09-14 18:34:14+03","2024-05-17 05:01:31.165402+03","Just adding some thoughts on the previous answers First of all its a bit timeconsuming to test what I am building in the local setup Even if we can test the code through the unit testing we cannot remove the possiblity of faliure of mocked request and response objects The fact that I cannot invoke the lambda in my local setup makes me very bored when I am testing my Lambdawritten API during the development process For sure you can build test and simulate a Lambda Invoke in your local environment it is just a new paradigm and there is some tools to help you out Secondly as far as I know so far there is no promised SLA for AWS Lambda for its availability and reliability This just make me kind of hesitate adopting Lambda to build RESTful API AWS Lambda operates on the AWS compute layer infrastructure so I believe if they face an issue on the compute layer for sure your EC2 instances will also face an outage Once we have set up Node API on EC2 instances I think it will be more productive to think about setting up autoscaling or how to utilize all the resources running on the current EC2s I do not think so The serverless stack scales with zero effort you do not need to manage infrastructure I tried it before but it caused a lot of confusion to me I feel like it will be better to choose either Node API or AWS Lambda API when building an API instead of mixing them together Welcome to micro and decoupled services Where developing the service is easy but managing the whole infrastructure is hard Another thing to keep in mind when talking to managers about architecture Cost It is hard to argue and makes every managers eyes shine when they see the possibility of running their businesses with a low cost And having your service running with a serverless stack is really cheap Bottom line No it is not a bad idea to mix resources as your manager wants Yes it is easier to just get a framework to write the api setup an EC2 instance and a autoscaling group Yes there is a heavy lift when decoupling services but it pays its price when running in production "
46219778,46212719,"stackoverflow.com",2,"2017-09-14 15:54:43+03","2024-05-17 05:01:31.169403+03","OK let us go one by one First thing first Your first problem is to test Lambda in your local and it completely possible to do with SAM Please have a look on httpdocs aws amazon comlambdalatestdgtestsamlocal html The most important design decision If your application is monolithic and you do not want to redesign it to microservices then stick with EC2 Next regarding your design for hybrid API Lambda and EC2 I do not think that is an antipattern or bad idea to do It completely based on your requirment Say you have an existing set of APIs in EC2 May be monolithic and you want to slowly migrate it to serverless and micro services You do not need to migrate it all together to serverless You can start one by one Remember the communication is happening through Http and it does not matter if your services are ditributed between EC2 and Lambda In micro service world it does not matter services are in same server or ditributed across many servers The communication speed has drastically changed over last few years and that is one of the main reasons of emergence of micro service So in a nut shell it is not a bad idea to have hybrid APIs but completely based on your design architecture If monolithic then do not go for Lambda "
54325684,54310778,"stackoverflow.com",1,"2019-01-23 12:59:28+02","2024-05-17 05:13:11.887898+03","Added Property sourceRaw true to warmup config which generates a clean source in the Function JS Config"
46213179,46212719,"stackoverflow.com",1,"2017-09-14 10:37:01+03","2024-05-17 05:01:31.171403+03","There are instances where you do need to run Lambda and EC2 instances together E g Monolith to Microservices migration projects NodeJS with Express as a WebServer which can make sense There are multiple approaches to achieve this Two common approaches For requestresponse are to use Note For asynchronous workflows you can use Lambda along with other AWS services such as AWS Step Functions SQS SNS based on the nature of workflow "
46216736,46215800,"stackoverflow.com",11,"2017-09-14 13:38:29+03","2024-05-17 05:01:32.043514+03","From httpsserverless comframeworkdocsprovidersawsguidefunctionsvpcconfiguration This object should contain the securityGroupIds and subnetIds array properties needed to construct VPC for this function You would also need to add VPC IAM Permissions The Lambda function execution role must have permissions to create describe and delete Elastic Network Interfaces ENI When VPC configuration is provided the default AWS AWSLambdaVPCAccessExecutionRole will be associated with your Lambda execution role To do this add the following in your serverless yml "
46287661,46221827,"stackoverflow.com",1,"2017-09-18 23:25:35+03","2024-05-17 05:01:33.013674+03","Create a json file with an APIGatewayEvent object or one of the supported aws events Then run serverless with option p or d see httpsserverless comframeworkdocsprovidersawsclireferenceinvokelocaloptions Take a look at httpsgithub comvandiumiolambdaeventidentifierblobmasterlibidentify js for more information"
46267206,46267065,"stackoverflow.com",1,"2017-09-21 23:15:27+03","2024-05-17 05:01:33.913665+03","[Edited] pip conf is actually a standard INI file so it allows only for absolute paths absolute yet it takes as root the lambda functions root moreover it cannot use variables internally Therefore you should include the full path starting from root in the target option In order to do so you can try to retrieve the current directory in AWS Lambda with or something similar I e try to get a shell and invoke pwd print working directory Use this information to fill in the right path to the target var However a INI file can be createdchanged programmatically Thus you can create your project as a distributable one like one for pypi or other DVCS and set up a setup py A tutorial to ready your project for distribution can be found here The specific discussion on the setup py can be found here A function that can be added to your setup py can be as follows adapt freely check the abovementioned tutorial for the usage of setuptools in your project "
46280889,46276828,"stackoverflow.com",3,"2017-09-18 16:45:04+03","2024-05-17 05:01:34.980577+03","Your process role will need IAM permission either at the role or at the user level depending on your implementation Assuming you want the process to create and have all permisions to the bucket you would need something like this"
46310899,46294293,"stackoverflow.com",2,"2017-09-20 02:02:13+03","2024-05-17 05:01:35.741602+03","There is a few ways to do it more to the point you do not need an identity pool at all "
46300886,46300614,"stackoverflow.com",3,"2017-09-19 15:48:03+03","2024-05-17 05:01:36.750451+03","The string eucentral1a does not represent a region it represents an availability zone within a region You should be using the string eucentral1 to refer to the region "
47699502,46302742,"stackoverflow.com",8,"2019-03-09 11:40:07+02","2024-05-17 05:01:37.611555+03","Serverless offline is a plugin to run only on your development machine not in production To enable it add the following to serverless yml and remove the following lines also check your package json and make sure it is a devDependencies "
58648771,46302742,"stackoverflow.com",6,"2019-10-31 19:46:03+02","2024-05-17 05:01:37.613555+03","Please ensure that serverlessoffline package is included in dev dependencies if not then add it serverlessoffline 3 20 2 and run This solved my issue "
58452534,46302742,"stackoverflow.com",3,"2019-10-18 17:06:27+03","2024-05-17 05:01:37.614852+03","To resolve this error while running an automated CI pipeline or locally try the following Also check your package json and ensure the serverlessoffline package is included in devDependencies This fixed the issue for me Happy Serverless "
70985955,46302742,"stackoverflow.com",1,"2022-02-04 13:52:53+02","2024-05-17 05:01:37.615856+03","I encountered a similar issue on bitbucket pipelines I fixed the issue after many trials and errors by updating script section of your bitbucketpipelines yml file with the following Also check your package json and ensure the serverlessoffline package and any other plugins in the serverless yml file are included in devDependencies Big thanks to K Manoj Kumars answer for giving me a clue If you are not using bitbucket pipeline I feel replacing the pipe section on the bitbucketpipelines yml file with something like npm run deploy and adding deploy serverless deploy to the scripts section on your package json will work "
72989283,46302742,"stackoverflow.com",1,"2022-07-15 08:08:58+03","2024-05-17 05:01:37.61885+03","The error is resolved by installing the package If the serverless package is not added do add that The terminal commands and the console logs shown below Error is shown in the picture below The error being addressed in the picture below"
75318239,46302742,"stackoverflow.com",0,"2023-02-02 04:59:53+02","2024-05-17 05:01:37.619835+03","I kept getting this error as I had the NODE_ENV set to production in my local dev environment serverlessoffline does not work with this I believe So set NODE_ENV to development or something that is not production"
46323508,46322899,"stackoverflow.com",3,"2017-09-20 16:32:06+03","2024-05-17 05:01:39.065894+03","You can use CloudWatch for this You can create a cloudwatch rule Then use an SNS target to deliver email "
46326483,46322899,"stackoverflow.com",2,"2017-09-20 18:45:33+03","2024-05-17 05:01:39.066894+03","Using serverless you can define the event trigger for your function like this Then you can expect your lambda to be called every time that event happens "
46323484,46322899,"stackoverflow.com",2,"2017-09-20 16:31:05+03","2024-05-17 05:01:39.067937+03","What you want is a CloudWatch Event In short a CloudWatch event is capable of triggering a Lambda function and passing it something like this From there you can parse this information in your Python code running on Lambda To get Instance ID of shuttingdown instance you will use something like this Then you can use Amazon SES Simple Email Service API with help from official boto3 library and send an email See httpboto3 readthedocs ioenlatestreferenceservicesses htmlSES Client send_email Of course you will also need a proper IAM role with necessary privileges to use SES attached to your Lambda function You can make a new one easily on AWS IAM Roles page It might seem overwhelming at first for starters"
46348429,46342792,"stackoverflow.com",1,"2017-09-22 07:25:07+03","2024-05-17 05:01:39.557776+03","Lambda runs on either Node v4 3 or v6 10 Both versions do not support ES6 modules You have to configure your transpiler to transpile to ES5 For local development I would suggest you use the same Node version that you use in Lambda so you do not have these but it works on my local moments "
46360433,46342792,"stackoverflow.com",1,"2017-09-22 11:42:20+03","2024-05-17 05:01:39.559776+03","Something else it could be I was getting this issue a few versions of Serverless back when deploying functions individually The rub was if you deployed the function individually web pack did not refactor the es6 down So if you find you deploy and its ok then you deploy a function by itself and it gets this error then update serverless and all your plugins and this will go away from memory I got past this at about v1 19 0"
63617138,46351920,"stackoverflow.com",28,"2020-08-27 16:31:55+03","2024-05-17 05:01:40.121952+03","I am facing the same error I solved this by removing the body from my postman request "
43801165,43778562,"stackoverflow.com",0,"2017-05-05 12:20:56+03","2024-05-17 05:02:20.557589+03","This error comes when the DB host value is not resolvable from the controller container or when the DB which the controller trying to connect to is not created in the couch DB Mine was the second case once __subjects db was there it was able to run"
46357358,46351920,"stackoverflow.com",5,"2017-09-22 08:22:33+03","2024-05-17 05:01:40.123265+03","require http That is an HTTP client not an HTTPS client Specifying port 443 does not result in an HTTPS request even though port 443 is the assigned port for HTTPS It just makes an ordinary HTTP request against destination port 443 This is not a valid thing to do so CloudFront is returning a Bad Request error You almost certainly want to require https "
46352336,46351920,"stackoverflow.com",4,"2017-09-21 23:03:05+03","2024-05-17 05:01:40.125262+03","I have seen this problem before It happens due to the following reasons"
71989955,46351920,"stackoverflow.com",3,"2022-04-24 18:41:25+03","2024-05-17 05:01:40.126263+03","In my case I had the same problem as Kireeti K where I solved this by removing the body from my postman request it seems that Cloudfront throws an error if you send a GET request with a body if you want to use the body you will need to change your method to something else than GET for me POST worked perfectly the error was gone and I was able to read the body "
60780807,46351920,"stackoverflow.com",0,"2020-03-20 21:52:07+02","2024-05-17 05:01:40.128263+03","I encountered the same problem this thread worked for me This error message The request could not be satisfied Bad Request is from the client and the error can occur due to one of the following reasons In my case the reason was 2 "
63986890,46351920,"stackoverflow.com",0,"2020-09-21 09:04:13+03","2024-05-17 05:01:40.129264+03","For me the problem was that I restarted the EC2 which changed the Instance ID but my cloudfront origin was still pointing to the previous ID So once I changed it it worked fine "
52961274,46351920,"stackoverflow.com",1,"2018-10-24 07:48:42+03","2024-05-17 05:01:40.131264+03","In my case I have a clientside load balancer when calling CloudFront As a result I am calling CF by IP address instead of hostName I checked with Amazon AWS Support team in this case CF rejects the request and returns 403 Error The request could be satisfied "
58724099,45482657,"stackoverflow.com",1,"2019-11-06 08:14:55+02","2024-05-17 05:01:41.929944+03","Try to add the serverlessawsdocumentation plugin in the serverless yml Add the infor and models documentation in the custom section Add the function documentation If you want to add the custom models like RequestStore and StoreAudioSuccess check the serverlessawsdocumentation documentation and the jsonschema docs To download the swagger documentation you need to run this command First you need to deploy you project"
45605871,45482657,"stackoverflow.com",0,"2017-08-10 09:20:01+03","2024-05-17 05:01:41.931945+03","Which version are you using According to their latest documentation httpsgithub com9cookiesserverlessawsdocumentation you need to provide tags as follows i e within double quotes "
47640437,45537170,"stackoverflow.com",1,"2017-12-04 21:18:19+02","2024-05-17 05:01:43.453344+03","The yml file needs to be indented just after stream See the Serveless Framework examples at httpsserverless comframeworkdocsprovidersawseventsstreams"
45577217,45551806,"stackoverflow.com",8,"2017-08-08 23:07:44+03","2024-05-17 05:01:43.703925+03","If you want to integrate more cleanly with the Serverless Framework you could install your NPM packages inside a Docker container that is mounted to your working directory For Node v6 10 docker run v PWDvartask lambcilambdabuildnodejs6 10 npm install For Node v4 3 docker run v PWDvartask lambcilambdabuildnodejs4 3 npm install This will install all the packages in your package json and mount the node_modules in your directory This is using a Docker container from Lambci which is very close to the actual AWS Lambda environment "
45557726,45551806,"stackoverflow.com",1,"2017-08-08 03:59:55+03","2024-05-17 05:01:43.705925+03","I had similar issue when developing NodeJS image manipulation application for Lambda in my Windows machine I managed to resolve the issue by using Docker Since AWS Lambda underlying execution environment is based on Amazon Linux image in which the image is made public by AWS for Docker then you can actually pull the image and run the Amazon Linux container in your Windows machine So in the container I had my code cloned in there run the npm install zip and upload them into S3 bucket and finally createupdate the Lambdas code from the S3 "
45575363,45569065,"stackoverflow.com",8,"2017-08-08 21:15:42+03","2024-05-17 05:01:44.833266+03","Yep the command line option is awsprofile E g serverless deploy awsprofile prod Docs here "
48913640,45569086,"stackoverflow.com",8,"2018-02-27 00:07:15+02","2024-05-17 05:01:45.319734+03","In my case it was API Gateway fault it turns out you have to enable binary support otherwise the lambda function will never receive the binary content Also if your Lambda function is behind a VPC and a security group first check if it works without VPC first "
45592962,45578021,"stackoverflow.com",3,"2017-08-09 17:03:43+03","2024-05-17 05:01:46.394724+03","So it turns it was an issue registered and solved here httpsgithub com99xtserverlessdynamodblocalissues120 You can download the version 0 2 24 by typing npm i [email protected] savedev if you are using it as a development dependency "
53500572,45578021,"stackoverflow.com",1,"2018-11-27 15:14:10+02","2024-05-17 05:01:46.396725+03","First add Serverless Offline to your project Then inside your projects serverless ym l file add following entry to the plugins section serverlessoffline If there is no plugin section you will need to add it to the file It should look something like this You can check wether you have successfully installed the plugin by running the serverless command line the console should display Offline as one of the plugins now available in your Serverless project then In your project root run Reference link"
67616151,45578021,"stackoverflow.com",0,"2021-05-20 10:54:58+03","2024-05-17 05:01:46.397843+03","For me it was a very subtle detail I had stage set as local So I had to put the following line in the custom section of serverless yml file"
55113975,45578021,"stackoverflow.com",1,"2019-03-12 06:03:31+02","2024-05-17 05:01:46.399031+03","I tried the below and it resolved the issue for me Reference httpsgithub com99xtserverlessdynamodblocalissues210"
67941714,45578021,"stackoverflow.com",1,"2021-06-11 21:09:04+03","2024-05-17 05:01:46.402028+03","serverlessdynamodblocal 0 2 35 worked for me"
63127901,45578021,"stackoverflow.com",2,"2020-07-28 09:02:56+03","2024-05-17 05:01:46.403029+03","run command"
45597337,45589504,"stackoverflow.com",0,"2017-08-09 20:41:06+03","2024-05-17 05:01:47.334379+03","For JSON format validations and to prevent injections you can use AWS API Gateway Models and Mapping Templates for Request and Response Mappings which uses JSON Schema to define the models "
74627176,45589504,"stackoverflow.com",0,"2022-11-30 13:37:19+02","2024-05-17 05:01:47.335379+03","I used httpsajv js org to validate the event object Once validated I use a mapper to create an entity EventDto AjvValidation EventMapper EventEntity Once you get your EventEntity you are 100 sure the validation sanitization and types you work on are the right ones "
45612592,45611268,"stackoverflow.com",5,"2022-04-13 01:17:15+03","2024-05-17 05:01:47.985051+03","Instead of using an env file which is a simple properties file if you are following dotenv package you can do the following Then in serverless yml How to test locally In your tests you can load the secrets file this way Reference httpwww goingserverless comblogusingenvironmentvariableswiththeserverlessframework"
45667442,45612940,"stackoverflow.com",4,"2017-08-14 07:22:58+03","2024-05-17 05:01:49.039979+03","As per the Serverless documentation in httpsserverless comframeworkdocsprovidersawsguideresourcesawsresources If you have created a Cognito User Pool resource which follows this format CognitoUserPool normalizedPoolId you can give the normalizedPoolId for each of your lambda functions In your case you have defined the Cognito User Pool as CognitoUserPool TestPool which allows you to use TestPool in your lambdas "
45613401,45612940,"stackoverflow.com",0,"2017-08-10 15:11:58+03","2024-05-17 05:01:49.04198+03","It works now but can someone explain to me how How the code below knows that the Cognito resource and the pool created by the event are the same"
45658178,45631460,"stackoverflow.com",3,"2017-08-13 10:45:20+03","2024-05-17 05:01:50.540736+03","I have been through the same trouble in understanding the way how AWS Cognito works and what options are available to implement authentication authorization Unfortunately there is no outofthebox method available to do it for your requirement Nevertheless let us hope that Amazon comes up with a feature very soon Basically there are 3 options available to implement authentication In addition to authentication this method can be used to implement authorization using IAM Roles or IAM Users easily The only downside of it is that you need to send a request signed with an awssignature4 which is not the standard way that we have seen in IDP services like Auth0 This method meets the expectation of sending a JWT token with API requests You can create users in Cognito User Pool and then use it to authenticate and generate an IdToken However this method will only allow you to authenticate users authorization needs to be handled in method level This method can be used to write your own way of authentication and authorization Also it helps to eliminate writing authorization logic in API methods The ideal solution would be to use AWS Cognito User Pool to authenticate users and then generate a policy document for IAM Role to access resources Here is an example AWS cognito userpools JavaScript SDK get users policy documents Also keep in mind that this solution will be invoking an extra lambda function for each request that you make "
45662047,45631460,"stackoverflow.com",2,"2017-08-13 18:37:30+03","2024-05-17 05:01:50.544588+03","You can use Cognito Auth to Serverside Following would be the steps Implementing Signup and Signin Implement Signup form in the frontend and API Gateway endpoint e g register using Lambda to receive the Signup data which will create user in Cognito using AWS SDK For detailed reference check this link Do similarly for the Signin by creating an frontend API Gateway endpoint e g login Storing and sending the JWT from your Browser and Validating at API Gateway Note Here I have purposefully avoided the IAM Authorization since it will require some additional work from Web App JavaScripts to implement Signature 4 Signing at Browser and also requires to refresh the token frequently which is straightforward with AWS JavaScript SDKs but will become complex if you need to implement it on your own "
45807061,45631460,"stackoverflow.com",1,"2018-05-25 16:05:51+03","2024-05-17 05:01:50.546588+03","Please take a look at this here The example demonstrates various configurations that include custom authorizers cognito lambda dynamoDB etc "
45724950,45703778,"stackoverflow.com",0,"2017-08-17 03:38:52+03","2024-05-17 05:01:52.792989+03","I worked it out finally so in case anyone else encounters this issue here is a summary There seem to be 2 issues Do not create functions in your root folder Create a specific folder for your serverless function i e not in C\Users\nnnnnn but within your regular document storage In Windows 10 it works nicely if you use a OneDrive folder with the benefit that your function s are also then replicated to other dev machines that you might use and are automatically backed up offsite More importantly the serverless framework seems to have an issue if you attempt to deploy to a region other than the default region set in your aws CLI configuration I have no idea why this should be since the credentials I use with the AWS CLI are authorised for all regions I also have no idea why the issue should result in serverless attempting to access a whole series of windows files for which it has no authority but nevertheless In my case I primarily use region apsoutheast2 By default SLS CREATE generates a serverless yml using a default US region If this is left asis there is then a mismatch between the deployment region and your AWS CLI region Not good To avoid the minor pain of having to specify a deployment region in the SLS deploy command just update the deployment region in the serverless yml file to match the CLI region Now works a treat "
46232881,45720046,"stackoverflow.com",14,"2017-09-15 09:21:38+03","2024-05-17 05:01:53.859612+03","I just went through the same ordeal and finally figured it out AWS has horrible documentation on this Sharing my experience to hopefully help you andor others 1 You will need to verify the email you want to send from in SES 2 Once you verify the email you are able to click on it in the SES dashboard and see it is Identity ARN e g arnawssesuswest2MYAWSACCOUNTNUMBERidentity [email protected] This Identity ARN is what you will use in the CloudFormation above for SourceARN under EmailConfiguration 3 Once you click on the verified email in the SES dashboard you will have the option to set Identity Policies Add this snippet there replacing the Resource ARN below with the correct Identity ARN you grabbed from step 2 "
45743071,45741503,"stackoverflow.com",0,"2017-08-17 22:00:33+03","2024-05-17 05:01:54.935206+03","AWS Lambda is not only called by automatic events You can invoke Lambda functions directly via the AWS APISDKCLI and wait for a response You can also place an AWS API Gateway in front of a Lambda function which would then use the response from the Lambda function to generate the API response Your other question regarding the callback function which is specific to NodeJS AWS Lambda functions is a bit vague The callback function is one of the parameters of your NodeJS AWS Lambda function It is passed to your Lambda function handler as a parameter This should be obvious from looking at the documentation on the same page where you copied that quote from "
45745150,45743643,"stackoverflow.com",2,"2017-08-18 00:23:36+03","2024-05-17 05:01:56.093629+03","You have run sudo H pip install sudo ran pip as root and pip changed ownership of some files and directories under your home dir Take the files back"
44707426,44703374,"stackoverflow.com",1,"2017-06-22 22:01:04+03","2024-05-17 05:01:57.481575+03","It does not really sound like you need API Gateway for this problem unless I am missing something Sounds like you need to go into your DNS and create an CNAME entry called clientsite com exampledev com and point that at the S3 bucket directly In that case you would have to create an S3 bucket for each child folder I do not know if that is something you can change But this could all be done with DNS rather than paying for any type of proxy just to map the request path "
44712259,44703374,"stackoverflow.com",1,"2017-06-23 05:45:34+03","2024-05-17 05:01:57.483523+03","This should be possible with S3 behind CloudFront and Lambda once it is out of preview or if you can get in on the preview but there is a wait for that if they are still accepting preview requests because you can read the HTTP headers and modify the request URI in flight prepending part of the host header to the path For the moment though JackKohnAWSs solution can be implemented with one level of simplification you do not have to edit DNS for each site You can create a wildcard CNAME in Route 53 for example com pointing directly to s3website[region] amazonaws com Then all you need to do is create buckets in that region named for the test site you want to create e g client example org example com because after you create the Route 53 wildcard DNS is already set up for any valid bucket name ending in example com in that region This works because of an impliementation detail in S3 the actual CNAME target you use does not actually have to be the web site endpoint for the bucket that is shown in the console it only needs to be the base name for the regional website hosting endpoint The resolution path has no role in which bucket serves the request As long as it arrives at the right regional endpoint the Host header identifies the bucket This is not what you asked for since it is one bucket per client site but once the DNS is configured with the wildcard CNAME any bucket you create with a matching name is already wired up to resolve This should not be impossible in API Gateway but it seems outside the intended usage there Response bodies have a 10 MB payload limit "
44885250,44703374,"stackoverflow.com",0,"2017-07-03 15:10:10+03","2024-05-17 05:01:57.487521+03","One solution would be to install nginx on a host ec2 and use proxypass to proxy the request to s3 content That way you can carry on accessing the content with the friendly url clientsite com exampledev com Your DNS record will now have to point to the host ec2 machine which will use nginx to route the request to appropriate backend s3 content depending on the url accessed You can also have multiple listener for different client To install nginx Nginx config should contain"
44713358,44712890,"stackoverflow.com",3,"2017-06-23 07:58:41+03","2024-05-17 05:01:58.594304+03","Just have it listen to two events "
45555876,44729088,"stackoverflow.com",31,"2018-12-06 22:59:42+02","2024-05-17 05:01:59.536408+03","I spun up the latest aws linux and ran the commands below I scp the dir down into my local and threw it in the package to be zipped and deployed My layout is similar to the aws repo code linked but modified for serverless Lambda code serverless yml I use the artifact and build my own zip If you run into the issue below I suggest you do that httpsgithub comserverlessserverlessissues3215 Ideas grabbed from httpsgist github combensie56f51bc33d4a55e2fc9a httpsgithub comawslabsserverlessimageresizing Edit Might want to also check out lambda layers May only need to do this kind of thing once "
58881623,44729088,"stackoverflow.com",8,"2019-11-16 00:12:23+02","2024-05-17 05:01:59.537997+03","I was struggling on this for a couple of days ended up going through the process myself and it does indeed work ImageMagick is no longer bundled with the Node js 10 x runtime There are 3 options to get ImageMagick working with your Node js 10 x function 1 Package the dependency and include it in your uploaded ZIP file like this one httpsimagemagickexample s3uswest2 amazonaws comimagemagickexample zip httpsgithub comhmagdyimagemagickawslambdaNode js10 xtreemasteroption1_imagemagickexamplezip But with option The deployment package of your Lambda function imagemagickexamplezipdemo is too large to enable inline code editing However you can still invoke your function or 2 Create or use a Lambda Layer that includes ImageMagick to do that That would create a layer zip inside build folder But to save you some time heres a zip file you can use to create a Lambda Layer httpsimagemagicklayer s3uswest2 amazonaws comlayer zip When you create the layer make sure you add Node js 10 x as a supported runtime You can then set your function to use the latest Node js 10 x and add the layer you created The image conversion should then work again Then you can create your aws lambda function like this httpsgithub comhmagdyimagemagickawslambdaNode js10 xtreemasteroption2_imagemagickexamplec_lib_layerindex js 3 NodeJS Runtime Environment npm with AWS Lambda Layers to do that Also if you want to use and got you should follow option 3 httpsgithub comhmagdyimagemagickawslambdaNode js10 xtreemasteroption3_imagemagickexamplenpm_layer Inspired by httpsmedium comanjanava biswasnodejsruntimeenvironmentwithawslambdalayersf3914613e20e"
45562133,44729088,"stackoverflow.com",7,"2021-02-08 08:23:43+02","2024-05-17 05:01:59.540995+03","If you want to tackle image resizing you may also take a look at the serverless sharp image library which uses Sharp a high performance Node js library for image resizing which is about 3x 5x faster compared to GMIM You did not provide enough information to say that it fits your use case requirements but I just wanted to mention it since this library already saved me a lot of AWS Lambda costs so far By the way I am not related to this project but licences are MITApache License 2 0 anyway "
44768948,44729088,"stackoverflow.com",1,"2017-06-27 00:26:07+03","2024-05-17 05:01:59.543995+03","All dependencies can be packed and uploaded as a part of your AWS Lambda function You can mostly use any package you want from AWS Lambda if you can fit it within the allowed size limits and upload the zip file Take a look at the AWS Lambda Deployment Limits section Also heres an example of how to package dependencies for python code httpsstackoverflow coma36093281358013"
46964807,44729088,"stackoverflow.com",1,"2017-10-27 01:29:28+03","2024-05-17 05:01:59.544973+03","For node js you can use nodelambda it simplifies packaging using a docker image The I argument will launch a docker image and launch npm i in your project so it will compile the binary node_modules against the right architecture "
55823033,44729088,"stackoverflow.com",0,"2019-04-24 08:36:21+03","2024-05-17 05:01:59.54697+03","If you install Docker on your local device and add this command into your package json Run npm run dockerbuild before you deploy your app You should change the node version based on the version of your lambda environment "
57550483,44729088,"stackoverflow.com",2,"2019-08-19 07:26:13+03","2024-05-17 05:01:59.547971+03","As I checked the imageMagick is existed already on aws lambda environment So we can use all library graphics images related to imageMagick Refer httpsserverless comblogbuildingaserverlessscreenshotservicewithlambda"
44738534,44738429,"stackoverflow.com",20,"2017-06-24 19:39:46+03","2024-05-17 05:01:59.880197+03","You have identified the issue However you are trying to convert a dict to dict This is what you have The body part as you have rightly identified is what is getting in as str This is what you should have One more step is to make it clientagnostic "
53606192,44738429,"stackoverflow.com",6,"2018-12-04 08:39:15+02","2024-05-17 05:01:59.881197+03","This is because event[body] is not a dict but a str I ran into this problem when decoding an SQS triggered event In case if anyone ran into a problem when a value of json loads event[body] is again not dict but str here is a solution that decodes str to dict recursively Example Usage This should print bucketname "
44738680,44738429,"stackoverflow.com",3,"2017-06-24 19:43:38+03","2024-05-17 05:01:59.884198+03","When dealing with json python provides 2 std functions httpsdocs python org3libraryjson htmljson dumps Serialize obj to a JSON formatted str using this conversion table The arguments have the same meaning as in dump httpsdocs python org3libraryjson htmljson loads Deserialize s a str bytes or bytearray instance containing a JSON document to a Python object using this conversion table What you need here is the latest event[body] is probably a json str so for accessing it s values you fist will need to convert it to a python obj via `json loads"
44757565,44753811,"stackoverflow.com",2,"2017-06-26 13:21:36+03","2024-05-17 05:02:00.815813+03","The issue with your code is that AccessControlAllowOrigin does not accept multiple domains From this answer Sounds like the recommended way to do it is to have your server read the Origin header from the client compare that to the list of domains you would like to allow and if it matches echo the value of the Origin header back to the client as the AccessControlAllowOrigin header in the response So when writing support to the OPTIONS verb which is the verb where the browser will preflight a request to see if CORS is supported you need to write your Lambda code to inspect the event object to see the domain of the client and dynamically set the corresponding AccessControlAllowOrigin with the domain In your question you have used a CORS configuration for two different types Lambda and LambaProxy I recommend that you use the second option so you will be able to set the domain dynamically See more about CORS configuration in the Serverless Framework here "
44763471,44761583,"stackoverflow.com",11,"2017-06-27 00:18:30+03","2024-05-17 05:02:02.716575+03","You need to set an authorizer for your API Gateway This tutorial is a great start point In summary you need to Your serverless yml will look like this with the authorizer configuration You do not need to be restricted to a Cognito authorizer You can use configure an authorizer for Google Facebook etc This setting means that the Lamba function will be triggered only by authenticated users and you can identify what is the User ID by inspecting the event object"
51593007,44777269,"stackoverflow.com",5,"2018-07-30 14:29:49+03","2024-05-17 05:02:03.678138+03",""
44802705,44794464,"stackoverflow.com",4,"2017-06-28 16:01:29+03","2024-05-17 05:02:04.981415+03","So you might what to use a Google Form I do not know what type of Order you are trying to do but in my example I will be assuming that an order is some type of department work order that is submitted and you want it to send a txt message to supervisors so that they can review the work order Here is a poorly drawn overview of how I envision this Overview Image I think that is plenty information for you to get a leaping start into this very interesting project you have If you have any further questions please ask If I do not know the answers I can definitely help point you in the right direction You will first have to start this project before I can get into some detail Hope this helps "
59023451,44812204,"stackoverflow.com",1,"2019-11-25 01:29:41+02","2024-05-17 05:02:05.511355+03","Make sure you have includeModules true setting in the webpack configuration within the custom section in your serverless yml From serverlesswebpack documentation"
49006122,44818114,"stackoverflow.com",13,"2018-02-27 12:12:28+02","2024-05-17 05:02:06.589332+03","Its not currently possible in the core framework because of CloudFormation behavior maybe But you can use this plugin httpsgithub commattfilionserverlessexternals3event After installing serverlesspluginexistings3 by npm install serverlesspluginexistings3 And add plugins to serverless yml Give your deploy permission to access the bucket And use existingS3 event it is not just s3 After sls deploy command You can attach event by using sls s3deploy command it will be added someday in the future httpsgithub comserverlessserverlessissues4241"
58330652,44818114,"stackoverflow.com",12,"2020-06-20 12:12:55+03","2024-05-17 05:02:06.591855+03","This is possible as of serverless version v1 47 0 by adding the existing true flag to your event configuration httpsserverless comframeworkdocsprovidersawseventss3 example from the source The source provides the following caveats IMPORTANT You can only attach 1 existing S3 bucket per function NOTE Using the existing config will add an additional Lambda function and IAM Role to your stack The Lambda function backsup the Custom S3 Resource which is used to support existing S3 buckets "
44823894,44818114,"stackoverflow.com",5,"2017-06-29 14:29:04+03","2024-05-17 05:02:06.593597+03","Unfortunately you cannot specify an existing S3 bucket to trigger the Lambda function because the Serverless Framework cannot change existing infrastructure using Cloud Formation This configuration requires that you create a new bucket You can read more in the following issues that were open on GitHub I would try to configure this trigger using AWS Console or the SDK instead of the Serverelss Framework "
44818623,44818114,"stackoverflow.com",3,"2017-06-29 10:15:19+03","2024-05-17 05:02:06.595018+03","If the bucket was created using Serverless elsewhere in the stack then you could use s3 Bucket Ref serverlesstest Otherwise you will have to construct the name or ARN yourself "
62327800,44818114,"stackoverflow.com",3,"2020-06-11 18:23:20+03","2024-05-17 05:02:06.596021+03","serverless yml seems to be very sensitive to spaces For me this advice was helpful If config looks like this you need to add 2 more spaces to the indent of bucket event rules"
44846139,44842070,"stackoverflow.com",1,"2017-06-30 15:06:28+03","2024-05-17 05:02:07.162835+03","Important Please try this on a nonproduction environment first "
44843641,44842070,"stackoverflow.com",0,"2017-06-30 12:54:21+03","2024-05-17 05:02:07.163835+03","I would consider looking into blue green deployments For DynamoDB you can utilize streams to ensure data is in sync You mentioned server less but it is hard to recommend a solution there without knowing if you are just doing lambda or if you have got an API gateway in front In those cases you might want to look into stage variables "
45178401,44847257,"stackoverflow.com",2,"2017-07-19 01:57:21+03","2024-05-17 05:02:08.154711+03","Modifying the schema requires replacement with cloudformation which will delete and recreate your pool httpdocs aws amazon comAWSCloudFormationlatestUserGuideawsresourcecognitouserpool htmlcfncognitouserpoolschema The update code is not advanced enough to detect that you added a user attribute and call the AddCustomAttributes api it only can modify things accessible to the UpdateUserPool api If you need to add a new attribute you should either use the command line or the console to do it if you have previously created the pool UserPoolClient also requires replacement when some attributes are modified httpdocs aws amazon comAWSCloudFormationlatestUserGuideawsresourcecognitouserpoolclient html Can you provide more details on what you changed if anything when it tried to delete your client A stack arn would be the most useful if you can send it via private message "
44867626,44858622,"stackoverflow.com",1,"2017-07-02 08:04:01+03","2024-05-17 05:02:09.199316+03",""
73194826,44858622,"stackoverflow.com",0,"2022-08-01 16:55:24+03","2024-05-17 05:02:09.200316+03","I have developed a package in npm to generate a serverless base project using httpswww npmjs compackageamcserverlessgenerate"
47578179,44863974,"stackoverflow.com",7,"2017-11-30 18:31:28+02","2024-05-17 05:02:09.90027+03","AWS AppSync was announced this week Nov 29th it supports graphQL subscriptions "
44866491,44863974,"stackoverflow.com",0,"2017-07-02 03:27:49+03","2024-05-17 05:02:09.902271+03","The only serverless service that supports WebSockets on AWS is IoT I know that the name sounds strange but it works fine for web browsers You will need to bundle the AWS IoT SDK in the fronted to subscribe and publish to topics and to use Cognito or IAM to retrieve temporary credentials for your client You can set restrictions so the client will be able to subscribepublish only for a specific topic I do not know exactly how GraphQL subscriptions work but I imagine that your clients will be subscribed to a topic and every time that a data changes one Lambda function will be triggered This Lambda will be able to publish a notification to a topic and all clients subscribed to this topic will receive the message "
48851029,44863974,"stackoverflow.com",0,"2018-02-18 13:15:52+02","2024-05-17 05:02:09.903562+03","I had started using google cloud functions but then moved to zeit now which is serverless but I am also able to use subscriptions Maybe you can have a look there too"
44865131,44864615,"stackoverflow.com",4,"2017-07-01 23:26:27+03","2024-05-17 05:02:10.925484+03","I have dealt with this a bit and found it quite frustrating If you deploy from your setup what does your api look like With individual serverless yaml files you end up with independent api endpoints assuming you are triggering with api calls and not something like s3 I ended up with a structure like this I use the serverless webpack plugin to output the the individual functions into the dist directory The serverless yaml then points to these The webpack config js looks like this After that just make sure you set the individual flag in serverless yml The webpack plugin is quite nice and does most of the heavy lifting With this I can do a single deploy and all the functions end up as individual lambda functions all under one api endpoint You also get the Webpack dev server so you can run serverless webpack serve to test your functions It was a bit of a pain to setup but it is been working pretty well "
43794982,43793888,"stackoverflow.com",21,"2017-05-23 15:02:36+03","2024-05-17 05:02:21.535663+03","The CONTAINS operator is not available in the query API You need to use the scan API for this see this link Try the following Result"
73283910,43793888,"stackoverflow.com",1,"2022-08-08 23:54:09+03","2024-05-17 05:02:21.536663+03","We can use CONTAINS operator in the query API as below "
42448856,42448692,"stackoverflow.com",4,"2017-02-24 23:46:38+02","2024-05-17 05:02:30.807146+03","The only solution is to add a NAT Gateway or NAT instance to your VPC so that resources like your Lambda function that reside in your private subnet will have access to resources outside the VPC "
45931881,33884968,"stackoverflow.com",2,"2017-09-20 11:35:10+03","2024-05-17 05:03:15.573028+03","This may be too late But now you can try ServerlessOffline httpswww npmjs compackageserverlessoffline works like a charm Also this is the github page for it httpsgithub comdheraultserverlessoffline "
43851674,43708167,"stackoverflow.com",2,"2017-05-08 18:33:55+03","2024-05-17 05:02:12.895662+03","Since you want to only focus on the implementation details it is pretty straightforward I am attaching a working example based on your sample files with a couple of adjustments just for completeness This is module that mimics 3rd party dependencies normally in your code that should be something like an AWS lib etc Altered it a bit so that it notifies its caller upon completion The simplest thing you can do is to just stub the required methods at their prototype level and control their behavior accordingly In this example we have stubbed the 3 methods of interest being used inside the lambda so that they return a fabricated result in order to drive the test down the required path Finally since the nature of the lambda is async we enforce it to make use of the callback so that we can gracefully run our assertions upon completion Using this as a skeleton you can leverage all features that Sinon has to offer in order to validate thoroughly your implementation logic Last but not least the article that johni shared should really be used as a reference for more complex scenarios "
43709889,43708167,"stackoverflow.com",1,"2017-04-30 21:30:08+03","2024-05-17 05:02:12.898664+03","I would like to recommend this article which covers very important aspects when unit testing an AWS lambda function I have adopted the way they are doing it "
56417357,43737590,"stackoverflow.com",5,"2019-06-02 20:28:54+03","2024-05-17 05:02:13.980234+03","I found myself with this same question some days ago while structuring a serverless project so I have decided to develop a simple serverless plugin to manage sequelize migrations through CLI With the plugin you can I know this question was posted about two years ago but for those who keep coming here looking for answers the plugin can be helpful The code and the instructions to use it are on the plugin repository on github and plugin page on npm To install the plugin directly on your project via npm you can run"
43744446,43737590,"stackoverflow.com",3,"2017-05-02 21:05:18+03","2024-05-17 05:02:13.982235+03","Lambda functions were designed to be available to run whenever necessary You deploy them when you expect multiple executions Why would you create a Lambda function for a migration task Applying a database migration is a maintenance task that you should execute just one time per migration ID If you do not want to execute the same SQL script multiple times I think that you should avoid creating a Lambda function for that purpose In this case I would use a command line tool to connect with this database and execute the appropriate task You could also run a Node js script for this but creating a Lambda to execute the script and later removing this Lambda sounds strange and should be used only if you do not have direct access to this database "
43747019,43745192,"stackoverflow.com",2,"2017-05-02 23:57:40+03","2024-05-17 05:02:14.892727+03","As I understand your question you want to automatically deploy a new CloudWatch resource to avoid the hassle of manually setting a resource and to avoid forgetting to configure them sometimes In this case the current framework does not provide an automatic solution for that you would need a manual configuration through CloudFormation stacks You would need to add a plugin to keep this task simple and I believe that no one has created one for that yet However instead of keeping your functions warm with CloudWatch you could use a Lambda schedule to trigger all other functions This setting is quite good and there is already a plugin for that You can read detailed instructions in this blog post In summary Install the plugin Reference the plugin in the serverless yml file Add warmup true for all functions that you want to keep warm"
43755407,43753692,"stackoverflow.com",0,"2017-05-03 11:53:10+03","2024-05-17 05:02:15.916053+03","Indeed javascript azure functions run on nodejs so commonjs modules are the natural format Node also natively supports much of ES6 though the Functions version of node might not be the latest however there is a current speed issue with loading all the dependencies in node_modules This is due to file access so a workaround exists to bundle everything into a single script which package json main points to I cant comment on how that fits in with serverless but perhaps this will help clarify "
43761442,43753692,"stackoverflow.com",0,"2017-05-23 15:10:00+03","2024-05-17 05:02:15.918054+03","As far as I know Node js still does not support importexport ES6 syntax for modules See also here Try a new deploy changing from to"
43798643,43763504,"stackoverflow.com",2,"2017-05-05 10:08:09+03","2024-05-17 05:02:16.490203+03","In my experience I can tell you that what you are talking about is also known as nobackend applications resource The principle of this approach is that you can abstract a number of features that traditionally are implemented into the server tier and move them into decoupled services exposed as SaaS As you mentioned a famous example are the smartphone hybrid applications that rely only onto Firebase that provides them authentication authorization and other few backend features If you need another kind of feature like emails you can do it within your frontend code by using a proper email service provider In terms of scalability what you have to do is simply to scale the services that you are using for example using a bigger Firebase plan In terms of security you have to understand that in a web application your code is always visible so all your business logic could be red analized and easly hacked This is why the nobackend approach fits better the mobile application needs since they are wrapped into proper containers designed in order to grant a better level of obscuration about what your application is doing Hope this could help you"
43839336,43763504,"stackoverflow.com",1,"2017-05-08 06:29:21+03","2024-05-17 05:02:16.492203+03","Some of the existing frameworks Search serverless on github to find more AWSbased services seem to scale well but have a look at the competition too"
66334118,43765040,"stackoverflow.com",2,"2021-02-23 15:31:26+02","2024-05-17 05:02:17.587138+03","As author said customizing variableSyntax worked for me httpswww serverless comframeworkdocsprovidersawsguidevariablesusingcustomvariablesyntax"
64807154,43765040,"stackoverflow.com",1,"2020-11-12 17:47:43+02","2024-05-17 05:02:17.5886+03","You can try using the Cloudformation Join function "
68073855,43765040,"stackoverflow.com",1,"2021-11-17 10:39:06+02","2024-05-17 05:02:17.589505+03","Escaping variables is now natively supported by Serverless httpsgithub comserverlessserverlessissues3565 Closing as escaping is supported by a new resolver e g \ self will be taken literally"
43941605,43769891,"stackoverflow.com",3,"2017-05-12 18:38:27+03","2024-05-17 05:02:18.565317+03","Asanka figured it out in the comments Basically facebook was sending multiple events over that I had not accounted for in my code Stuff like message delivered and message read were also events I did not know about I just had to unsubscribe to them in the developer console "
43790256,43771000,"stackoverflow.com",3,"2017-05-08 18:39:06+03","2024-05-17 05:02:19.69067+03","Yes it is perfectly possible to automate such a deployment structure As long as you have code to create a table it should be fairly straightforward to get all of the data from an old table change the data and then upload it all to a new table without any drops in uptime If you write what language you would like to do such a thing in I can help a bit more I have done this before and I have added below a small generified codesample on how you could do this in Java Java method for creating a table given the class of the object type stored in dynamo Java method to delete table I would scan the whole table and plop all of the content into a List and then map through that list converting the objects into your new type and then create a new table of that type but with a different name push all of your new objects and then delete the old table after switching any references you might have of the old table to the new one Unfortunately this does mean that everything consuming your tables are going to need to be able to switch between your two staging tables "
50153275,43818401,"stackoverflow.com",6,"2018-05-03 13:52:46+03","2024-05-17 05:02:21.980027+03","If you are packaging the banckend in NodeJS it should be configured in the webpack config js file and in the typings d ts files so both the editor and the packager knows about these files typings d ts code For this to work a loader such as rawloader for those using Webpack will work This sample has been taken from httpsgithub comapollographqlgraphqltoolsissues273 where there is an active discussion about different approaches Here is a GitHub repo which does roughly the same but with a more powerful solution httpsgithub comwebpackcontribcopywebpackplugin On the other hand if you are packaging the front there is a handy plugin to do the work for you httpswww npmjs compackagewebpackgraphqlloader"
43818552,43818401,"stackoverflow.com",0,"2017-05-06 11:51:59+03","2024-05-17 05:02:21.9822+03","Assuming the graphql files have a JS extension so are processed if they do not export anything or nothing imports them you could try the not recommended import mymodule js"
49467955,43818401,"stackoverflow.com",2,"2019-10-12 15:08:36+03","2024-05-17 05:02:21.983357+03","Another workaround is to copy graphql file to the output folder through automated script for example httpswww npmjs compackagecopy"
43831847,43826710,"stackoverflow.com",0,"2017-05-07 16:06:03+03","2024-05-17 05:02:22.80261+03","From docs you need to create the function role under resources and reference this new role inside your function Example"
46716694,43829577,"stackoverflow.com",6,"2017-10-12 21:37:49+03","2024-05-17 05:02:23.906271+03","You do not need to mess with serverless yml so much Here is the simple way In serverless yml response headers ContentType template and statusCodes are not necessary Then you can just set the statusCode and ContentType in your function So delete this part and replace it with Lambda proxy integration which is the default assembles it into a proper response Personally I find this way simpler and more readable "
43830851,43829577,"stackoverflow.com",1,"2017-05-07 14:20:11+03","2024-05-17 05:02:23.907759+03","you need your lambda to be a proxy type so you set the body property but just try to do that will send the string as result directly or use the callback param"
43968652,43829577,"stackoverflow.com",0,"2017-05-14 23:47:41+03","2024-05-17 05:02:23.909165+03","As mentioned by UXDart you will not be able to do this using the standard integration You should setup a proxy integration with Lambda like here httpdocs aws amazon comapigatewaylatestdeveloperguideapigatewaycreateapiassimpleproxyforlambda htmlapigatewayproxyintegrationlambdafunctionnodejs This will work better with what you are trying to do return xml through api gateway "
44595816,43829577,"stackoverflow.com",0,"2017-06-16 21:29:03+03","2024-05-17 05:02:23.910356+03","Change your serverless yml to this"
45023547,43829577,"stackoverflow.com",0,"2017-07-11 03:29:55+03","2024-05-17 05:02:23.911226+03","Got mine to work with this and"
67316477,43829577,"stackoverflow.com",0,"2021-04-29 14:27:22+03","2024-05-17 05:02:23.912229+03","It works for me "
43925535,43829577,"stackoverflow.com",2,"2017-05-12 00:06:26+03","2024-05-17 05:02:23.912229+03","Checkout the twilio serverless example here httpsgithub comserverlessexamplestreemasterawsnodetwiliosendtextmessage"
43876127,43871759,"stackoverflow.com",8,"2017-05-09 20:19:17+03","2024-05-17 05:02:24.443073+03","Elasticache is not directly accessible from outside AWS environment by default According to their documentation the service is designed to be accessed exclusively from within AWS In your case serverless invoke local times out because the connection itself cannot be established and the lambda function times out So you cannot run invoke locally to test this connection the way you are trying to do To connect to Elasticache redis from your local machine you can use a NAT instance in your public subnet and setup the security groups to open up the correct ports and enable IP forwarding to allow connection to your redis cache cluster The steps are given here However I would just install redis locally and use an environment variable to change the connection string to connect to local redis on local machine and the actual Elasticache cluster when running on lambda "
78439680,43871759,"stackoverflow.com",0,"2024-05-07 04:06:39+03","2024-05-17 05:02:24.445074+03","I am leaving this link here because it helped me when I ran into this issue httpsgithub comawsawssamcliissues318issuecomment377770815 I could not access Elasticache from my Lambda all local with Localstack and this was happening because they could not communicate internally via localhost"
43889368,43888228,"stackoverflow.com",1,"2017-05-10 13:15:46+03","2024-05-17 05:02:25.367946+03","If you want to set the status code headers and body in your code instead of adding them in your configuration you need to use the Lambdaproxy setting See more here Lambdaproxy is the default type So you need to remove the integration lambda in your serverless yml Also remove the cors true setting and modify the handler js to add CORS headers Modify your Lambda function to Modify your serverless yml file to"
43913912,43908662,"stackoverflow.com",9,"2017-05-12 13:05:39+03","2024-05-17 05:02:25.932164+03","From docs You can set the contents of an external file into a variable And later you can use this new variable to access the file variables Or you can use the file directly"
43925438,43908662,"stackoverflow.com",9,"2017-05-12 13:15:32+03","2024-05-17 05:02:25.933165+03","You can also now use remote async values with the serverless framework See httpsserverless comblogserverlessv1 13 0 This means you can call values from s3 or remote databases etc Example serverless yml vars js"
57102575,43908662,"stackoverflow.com",9,"2019-07-20 00:51:24+03","2024-05-17 05:02:25.935166+03","This is how you can separate your environments by different stages serverless yml env test yml During deploy pass stageprod or skip and test project will be deployed Then in your JS code you can access ENV variables with process env VARIABLE1 "
52174855,43908662,"stackoverflow.com",1,"2018-09-05 01:42:44+03","2024-05-17 05:02:25.936166+03","Set Lambda environment variables from JSON file using AWS CLI aws lambda updatefunctionconfiguration profile mfa functionname testapi cliinputjson filedev json"
43927052,43908662,"stackoverflow.com",0,"2017-05-12 02:19:03+03","2024-05-17 05:02:25.937166+03","I had this correct but I was referencing the file incorrectly I do not see this in the docs but passing a file to environment will include the files yaml file and the above structure does work "
42400784,42400269,"stackoverflow.com",4,"2017-02-22 21:41:29+02","2024-05-17 05:02:28.413853+03","The resources schema used by serverless yml is the CloudFormation schema For DynamoDB take a look here To understand DynamoDB concepts and terms I would suggest to start here"
42426822,42415688,"stackoverflow.com",0,"2017-02-24 03:40:56+02","2024-05-17 05:02:28.832112+03","Packaging does not work this way docs You cannot specify the file tree that will be saved inside the zip You can only specify what you will include or exclude in this zip In the docs link you can see an option to use an Artifact In this case you could develop your own code to zip using exactly the rules that you want and output a zip file to be used by the Serverless Framework It is possible but I hope that you do not need this Why do you want to move the contents of the folder function1 to the root level Maybe your true question is How can I reference my Lambda function that is located at another folder If this is the case you could use the following in the serverless yml file The syntax is folderfilename function Another solution would be to create one serverless yml file for each folder The problem with this approach is that you would not be able to access the function2 from the function1 "
42429276,42429275,"stackoverflow.com",19,"2017-02-24 03:36:33+02","2024-05-17 05:02:29.901463+03","The Serverless Framework access functions inside other folders using the following syntax So if we have a file named function1 js with a function handler that we want to execute when our Lambda function is invoked we use the following definition inside the serverless yml file The same would apply for multiple levels of folders"
58620756,42448692,"stackoverflow.com",1,"2019-10-30 09:53:34+02","2024-05-17 05:02:30.809141+03","No need NAT you can do it also with VPC endpoint httpsdocs aws amazon comvpclatestuserguidevpcendpoints html And that is how to do it to Kinesis httpsdocs aws amazon comstreamslatestdevvpc html Works for me and match cheaper Make sure you set the correct security groups sg of the private VPC and not the default VPC If you will read the NAT pricing documentation they are also recommending this httpsaws amazon comvpcpricing read the note at the end"
42456689,42449195,"stackoverflow.com",9,"2017-02-25 15:29:09+02","2024-05-17 05:02:31.693972+03","Take a look at the following documentation Notice that as long as you do not use the attribute as index you do not need to define it DynamoDB is a NoSQL database and is schemaless which means that other than the primary key attributes you do not need to define any attributes or data types at table creation time So in your case the serverless yml should only specify And in your code you can dynamically write into the table attributes which consist of sets maps or even json "
42626162,42457386,"stackoverflow.com",1,"2017-03-06 14:53:14+02","2024-05-17 05:02:32.563891+03","If you are writing it in node it could be that your node version is different than 4 3 2 the one that AWS Lambda uses and that is why testing offline and locally works but not in the AWS Lambda This happened to me before when I was using let You could use nvm to install node v 4 3 2 to test it locally or you could look here to see what features are you allowed to use in your code "
54856747,42457386,"stackoverflow.com",0,"2019-02-24 23:29:14+02","2024-05-17 05:02:32.565893+03","To get API Gateway feeding cloudWatch Now your logs can be viewed in under a name like APIGatewayExecutionLogs_api idapi stage"
45797093,42457673,"stackoverflow.com",0,"2017-08-21 15:44:07+03","2024-05-17 05:02:34.117949+03","Make your yaml code into json this way any mistakes are obvious and will be more likely to be picked up by the parser If anyone comes across this use the referenced config above "
75047188,42457673,"stackoverflow.com",0,"2023-01-08 12:52:53+02","2024-05-17 05:02:34.118949+03","I got a similar error but in my case it was indentation problem Go through your YML file and see if the changes you have made have the correct identation A json parser as mention by others can help narrow down where in the YML file the problem exists"
52406588,42474264,"stackoverflow.com",18,"2021-01-08 15:07:48+02","2024-05-17 05:02:34.339073+03"," Edit As noted in the comments the AWS verbiage I called out in 2018 has been removed That said my thoughts regarding Lambda proxy vs custom integration still hold It looks like AWS recommends choosing Lambda Proxy Integration for new API development Note The Lambda custom integration formerly known as the Lambda integration is a legacy technology We recommend that you use the Lambda proxy integration for any new API For more information see Build an API Gateway API with Lambda Proxy Integration I understand that it is a lot quicker in the short term to spin up an API endpoint and lambda integration using proxy integration rather than the custom integration but I am surprised that it is the recommendation for all API Lambda development going forward"
52389098,42474264,"stackoverflow.com",4,"2018-09-18 17:40:25+03","2024-05-17 05:02:34.341385+03","We also started with Proxy since it really felt fast to get a bunch of functions up and running Soon it was dawning on us that we created a pretty tight coupling to the way the Proxy forces us to read input and write output and that our function shouldn t know about this and should have clearer and simpler interfaces And then we wanted to get started orchestrating a few of those function with AWS Step Functions and that is when we realized we had created functions that really only work with the Proxy integration Not with Step Functions and they are certainly not easily migrated away No Proxy anymore "
60972996,42474264,"stackoverflow.com",4,"2021-06-04 08:31:27+03","2024-05-17 05:02:34.342919+03","I do not like body mapping template because in exported Swagger it is escaped for example And as you understand this is bad to edit such code locally and deploy this swagger file Also when you edit body mapping template in the browser you will not receive errors about your wrong JSON Apache Velocity For example here we have a mistake The mistake is wrong before startDate My backend code in Go is free from such mistakes I do not want to write tests for Apache Velocity Also maybe Proxy Integrations are faster because of missing serviceinthemiddle "
45228564,42474264,"stackoverflow.com",2,"2017-07-21 06:40:19+03","2024-05-17 05:02:34.34439+03","No proxy I have several SLS deployments in production some generating revenue some as internal tools I exclusively use no proxy I dont want to rely on the structures of AWS for my application so if we stop being friends I can migrate without too much pain As to the Pros of the proxy I would consider them as cons as it feels you have too and the con as a pro I have seen it all before lets move super fast Yes we need to be agile and move quickly but not at the cost of thinking I get this with my engineers all the time its one thing to be low on documentationdesign another all together to say nuts to planning lets just code Thats how you corner yourself no matter how fast you get to market With no proxy and some early planning to your project structure and maybe some good DDD thinking its pretty simple to migrate away from AWS should the world burn Further to this I find it very difficult to get new bods up to speed with AWS stuff Once you know it its all gravy but devs are devs not infrastructure engineers those of us who do both are surprisingly rare Abstracting away helps people be productive as they begin their daunting journey down the rabithole Id rather my coder code than need to bother me about CFN every 20 minutes "
68552021,42474264,"stackoverflow.com",1,"2021-08-10 09:27:43+03","2024-05-17 05:02:34.346388+03","As a matter of best practices I personally would choose the proxy integration as distinct from a proxy resource Heres why"
42493572,42492510,"stackoverflow.com",2,"2020-06-20 12:12:55+03","2024-05-17 05:02:35.587832+03","This is an AWS limit for APIs link CreateTableUpdateTableDeleteTable In general you can have up to 10 CreateTable UpdateTable and DeleteTable requests running simultaneously in any combination In other words the total number of tables in the CREATING UPDATING or DELETING state cannot exceed 10 The only exception is when you are creating a table with one or more secondary indexes You can have up to 5 such requests running at a time however if the table or index specifications are complex DynamoDB might temporarily reduce the number of concurrent requests below 5 You could try to open a support request to AWS to raise this limit for your account but I do not feel this is necessary It seems that you could create the DynamoDB tables a priori using the AWS CLI or AWS SDK and use MoonMail with readonly access to those tables Using the SDK example you could create those tables sequentially without reaching this simultaneously creation limit Another option is to edit the sresourcescf json file to include only 10 tables and deploy After that add the missing tables and deploy again Whatever solution you apply consider creating an issue ticket in MoonMails repo because as it stands now it does not work in a first try there are 12 tables in the resources file "
42539177,42532824,"stackoverflow.com",1,"2017-03-01 20:22:54+02","2024-05-17 05:02:36.612059+03","What are you defining in your mydefaultpath You should just need the ARN like role arnawsiam0123456789roleroleInMyAccount"
42540829,42539296,"stackoverflow.com",13,"2017-03-01 21:55:33+02","2024-05-17 05:02:37.136906+03","When creating the Dynamodb table you dont need to mention all the attributes of the table Only two attributes are mandatory while creating the table I e partition key and sort key if available As you have mentioned that the attribute is map definitely it cannot be partition key or sort key because key attributes can only be scalar data type The simple answer is you dont need to define the map attribute document data type in CloudFormation create table script "
68311298,42612499,"stackoverflow.com",28,"2021-08-31 19:14:04+03","2024-05-17 05:02:38.070424+03","This is now supported natively since version 2 3 0 Just reference it via awsaccountId You can also reference region via awsregion Documentation here httpswww serverless comframeworkdocsprovidersawsguidevariablesreferencingawsspecificvariables"
44560487,42612499,"stackoverflow.com",24,"2017-06-15 09:47:29+03","2024-05-17 05:02:38.071915+03","There is a handy serverless plugin httpswww npmjs compackageserverlesspseudoparameters that adds the ability to reference aws parameters such as region and account id that i have just started using to much success "
42614604,42612499,"stackoverflow.com",9,"2017-03-06 00:01:56+02","2024-05-17 05:02:38.073004+03","Serverless itself cannot reference those variables since those are defined within CloudFormation but not exposed in serverless If you need those in the resources section you can directly access them via Refcall AWS CloudFormation Pseudovariables If you need those variable as function environment variables you can overwrite the serverless generated function code with CloudFormation code So to achieve this you must modify you serverless yml by the following pattern "
42614780,42612499,"stackoverflow.com",6,"2021-03-22 11:01:15+02","2024-05-17 05:02:38.074297+03","EDIT this question is probably outdated Consider this comment and this answer AWS CloudFormation offers some variables like AWSAccountId and AWSRegion but you cannot use them in the serverless yml file like AWSAccountId Those are not supported jens answer is right You must use the CloudFormation syntax In the example below I provide another way to use CloudFormation The line is the same of hardcoding the region and account id"
42632451,42631587,"stackoverflow.com",5,"2017-03-06 19:58:18+02","2024-05-17 05:02:38.945388+03","Does Serverless Framework support the ability to deploy the same API to multiple cloud providers AWS Azure and IBM Just use 3 different serverless yml files and deploy each function 3 times and route requests to each provider based on traditional load balancer methods i e round robin or latency The Serverless concept is based on trust you trust that your Cloud provider will be able to handle your traffic with proper scalability and availability There is no multicloud model a single Cloud provider must be able to satisfy your needs To achieve this they must implement a proper loadbalacing schema internally If you do not trust on your Cloud provider you are not thinking in a serverless way Serverless means that you should not worry about the infra the supports your app When you specify a serverless yml file you must say which provider AWS Azure IBM will create those resources Multicloud means that you need one serverless yml file per each Cloud but the source code functions can be the same When you deploy the same function to 3 different providers you will receive 3 different endpoints to access them Now which machine will execute the Load Balance If you do not trust that a single Cloud provides enough availability how will you define who will serve the Load Balance feature The only solution that I see is to implement this loadbalacing in your frontend code Your app would know the 3 different endpoints and randomize the requests If one request returns an error the endpoint would be marked as unhealthy You could also determine the latency for each endpoint and select a preferred provider All of this in the client code However do not follow this path Choose just one provider for production code The SLA service level agreement usually provides a high availability If it is not enough you should still stick with just one provider and have in hand some scripts to easily migrate to another cloud in case of a mass outage of your preferred provider "
42675862,42638236,"stackoverflow.com",0,"2017-03-08 17:50:03+02","2024-05-17 05:02:39.953774+03","Another slick way to accomplish what I want that I am unaware of I think a way the achieve this is to add the scripts to a step in your build server Mavens primary function is to act as a build manager which is slightly different from the deployment use case you are outlining above "
53516028,42638236,"stackoverflow.com",0,"2018-11-28 11:23:27+02","2024-05-17 05:02:39.955774+03","If anyone is still intrigued by this I have started an inspired effort at httpsgithub comolensmarserverless4jtreemastermavenplugin still working on how this would optimally fit in the maven lifecycle ideassuggestions as always welcome "
56476005,42844435,"stackoverflow.com",2,"2019-06-06 13:46:20+03","2024-05-17 05:02:41.875405+03","Serverless framework does provide the ability to reuse a preexisting API in their latest update You can refer httpsserverless comframeworkdocsprovidersawseventsapigatewayshareapigatewayandapiresources for details Let me know if this helps "
42927961,42844435,"stackoverflow.com",0,"2017-03-21 15:02:31+02","2024-05-17 05:02:41.876625+03","One option is to create the Lambda without any eventconfiguration in the serverless yml and then manually configure the existing API Gateway to trigger the created lambda I think it is a fairly good solution in your case since the API Gateway is configured somewhere else anyway "
41283236,41038072,"stackoverflow.com",1,"2016-12-29 11:02:21+02","2024-05-17 05:02:43.182724+03","You need to add events to your functions Have a read through the serverless documentation for events Currently serverless supports lambdas to be invoked by API GateWay Kinesis DynamoDB S3 Schedule SNS and Alexa Skill read more So in this case adding a required events tag should solve your problem Alternatively you can always define all resources and their actions using traditional CloudFormation format within serverless resources node "
41046454,41045135,"stackoverflow.com",2,"2016-12-08 20:32:22+02","2024-05-17 05:02:44.232946+03","make sure you deploy the API once you make any changes like adding CORS I have been bitten by this several time "
41049137,41045135,"stackoverflow.com",1,"2016-12-08 23:30:15+02","2024-05-17 05:02:44.233947+03","If you are using HTTP or Lambda proxy integration the nonOPTIONS method will have to return the relevant CORS headers in this case AccessControlAllowOrigin The two errors you see there in the console are ok if you are using a proxy integration on the GET method Configure the backend to send back the AccessControlAllowOrigin header and try again "
41047741,41045135,"stackoverflow.com",0,"2016-12-08 21:54:45+02","2024-05-17 05:02:44.235947+03","If you are using the Serverless Framework you can easily do it by specifying cors true under your functions http event You can find more details in the docs "
41080777,41070639,"stackoverflow.com",8,"2016-12-11 00:09:46+02","2024-05-17 05:02:45.675225+03","First create different profiles Use cli this works from 1 3 0 will not work in 1 0 0 not sure which you are using since you mention both Then in your serverless yml file you can set the profile you want use If you want to automatically deploy to different profiles depending on the stage you define variables and reference them in your serverless yml file Or you can reference your profile name in any other way Read about variables in serverlessframework You can get the name of profile to use from another file from cli or from the same file like in the example I gave More about the variables httpsserverless comframeworkdocsprovidersawsguidevariables"
41080349,41070737,"stackoverflow.com",1,"2016-12-10 23:08:43+02","2024-05-17 05:02:46.110896+03","From official docs As Serverless 1 x is a complete reimplementation and does not implement all the features that were in 0 x but has a lot more features in general there is no direct update path Basically the best way for users to move from 0 x to 1 x is to go through our guide and the AWS provider documentation that will teach you all the details of Serverless 1 x This should make it pretty easy to understand how to set up a service for 1 x and move your code over We have worked with different teams during the Beta phase of Serverless 1 x and they were able to move their services into the new release pretty quickly Now answering your questions How can I handle many functions under same environment Should I code all functions within one handler js You need to take a look in this blog post about serverless architectures The answer is that you do not need to use only one function You can have multiple functions Also the handler js file do not need to have this name and it do not need to be in the root folder See the following serverless yml example In this example we have two files inside a lib folder photos js and videos js Each file has a function handlePhotos handleVideos that is responsible to handle API gateway events And how can I avoid 50MB limits of each functions Use multiple functions "
56818439,33884968,"stackoverflow.com",2,"2020-06-20 12:12:55+03","2024-05-17 05:03:15.574156+03","An example lambda_function py Example invocation of a local lambda function"
41073303,41070858,"stackoverflow.com",1,"2016-12-10 09:40:06+02","2024-05-17 05:02:47.105931+03","I found the answer of this by myself It is written in document here httpsserverless comframeworkdocsprovidersawseventsapigateway and also I was so careless there are some sample of setting commented out in template This API Gateway setting is written like this So API Gateway deploy setting is part of functionsevents setting "
41175980,41145659,"stackoverflow.com",1,"2016-12-16 03:29:10+02","2024-05-17 05:02:48.520015+03","There is not a way to put request parameters in the resource policy I do not see how that could be secure The policy should only be stating explicitly allowed actions on resources You may have to enumerate several resources in your policy array is allowed "
41207367,41156203,"stackoverflow.com",4,"2017-03-28 13:25:18+03","2024-05-17 05:02:49.046462+03","Lambda is the only option for serverless computing on AWS and Lambda functions run only on Linux machines If you need to run serverless functions in a Windows machine try Azure Functions That is the Lambda equivalent in the Microsoft cloud I am not sure if it runs in a Windows Server 2016 machine and could not find any reference to the platform but I would expect that as a brand new service they are using their own edge tech To confirm if the platform is what you need try this function"
41212905,41156203,"stackoverflow.com",1,"2016-12-18 23:08:40+02","2024-05-17 05:02:49.048103+03","I think yoy can achieve this via combination of CodeDeploy service and AWS CodePipeline Refer to this article httpdocs aws amazon comcodedeploylatestuserguidegettingstartedwindows html to learn how to deploy code via CodeDeploy Later see this article httpdocs aws amazon comcodepipelinelatestuserguidegettingstarted4 html to learn how you can configure aws Pipline to call Code Deploy and later execute your batch job on created windows machine note you will probably want to use S3 instead of Github which is possible with CodePipeline I would consider to bootstrap whole such configuration via script using aws cli this way you can clean up easily your resources like this aws codepipeline deletepipeline name MyJob Of course you can configure the pipeline via aws web console and leave the pipeline configured to run your code on regular basis "
46113189,41156444,"stackoverflow.com",2,"2017-09-08 12:30:57+03","2024-05-17 05:02:49.936131+03","As I have hit this problem and managed to find a solution I thought I would post it here Problem is that he necisary compiled libraries do not exist within Lambda if you include libraries compiled by some other Linux unless they are built with the same compiler and dependencies are not going to work Thankfully a nice chap has figured out the dependencies and built packages for a variety of Python modules that are not included in Lambda including shapely httpsgithub comryfeuslambdapacks download the relevant module from there and copy it into the deployment package removing any you have installed via pip beforehand "
55756672,41156444,"stackoverflow.com",1,"2019-04-19 07:36:28+03","2024-05-17 05:02:49.938131+03","I setup a build script that will produce the Shapely dependencies as a Lambda Layer You can check out my project here httpsgithub combearflagroboticslibgeoslambdabuild but basically it is just using an Amazon image to download and build Shapelys C dependencies for use in Lambda Just drop in the zip file as a layer and run any version of PythonShapely"
63177788,41156444,"stackoverflow.com",0,"2020-07-30 19:49:44+03","2024-05-17 05:02:49.93914+03","I managed to get it running using images from httpshub docker comrlambcilambdatags Make sure to grab the image with the right tag e g python3 7 You can either just run a container off of that directly and compile your libraries and push in that environment or you can follow the instructions here to build your own docker image and automate installing your packages creating a ZIP with your code and uploading the ZIP to lambdas3 "
41196221,41156444,"stackoverflow.com",1,"2016-12-17 09:03:26+02","2024-05-17 05:02:49.94049+03","It is possible Try to follow these steps and see if the problem persists First install GEOS Then Shapely Try to import Shapely"
41187168,41173775,"stackoverflow.com",3,"2016-12-16 17:03:10+02","2024-05-17 05:02:50.879891+03","If you use lambdaproxy you return everything through your lambda function not the configuration It says that it will ignore this part If you want to define requestresponse in your configuration file you need to change your integration to lambda from lambdaproxy "
41194813,41194734,"stackoverflow.com",4,"2017-02-03 18:14:56+02","2024-05-17 05:02:51.953803+03","You are making the request using a request method of json via the type property json is not a valid request method Instead of type json you probably want dataType json The type property can be used to specify the request method eg GET though it was deprecated in favour of the method property as of jquery v1 9 "
41206661,41197983,"stackoverflow.com",15,"2016-12-18 10:29:13+02","2024-05-17 05:02:52.542792+03","I changed my serverless yml file as following and it started reading "
63277583,41214544,"stackoverflow.com",6,"2020-08-06 08:49:17+03","2024-05-17 05:02:53.473826+03","You can Just provide the ARN in the ManagedPolicyArns of a Role resource For policies applied to all functions"
41215956,41214544,"stackoverflow.com",4,"2016-12-19 07:15:11+02","2024-05-17 05:02:53.474827+03","Note the error it expects role instead of policy IAM Policies are documents that define permissions and cannot be attached directly to lambda functions Create an IAM Role and attach the managed policy to the role Think of the role as a container for your policy policies cannot be attached directly to lambda functions but roles can You can freely attach and detach managed and inline policies to your roles Option 1 Fix this error from AWS Console with a predefined policy Option 2 Define actions of AmazonCognitoReadOnly policy in serverless yml This effectively converts the managed policy to an inline policy Warning this is untested Further Reading"
41403233,41393143,"stackoverflow.com",1,"2016-12-31 00:04:29+02","2024-05-17 05:02:54.370173+03","Adding a static file to an S3 bucket is not supported by any official AWS CloudFormation resources but giltcloudformationhelpers has a custom resource Put S3 Objects that can do this See Usage for instructions on installation and usage "
53311363,41413360,"stackoverflow.com",2,"2018-11-15 03:59:43+02","2024-05-17 05:02:55.443007+03","From the code it looks like environmnent is misspelt "
41432786,41413360,"stackoverflow.com",1,"2017-05-23 13:30:36+03","2024-05-17 05:02:55.444007+03","This feature works fine for me I believe that you have mistyped something Could you please create a new project and test the following steps Maybe we can find what is your issue through a MCVE Give me a feedback if this code does not work for you Check your Serverless version expected 1 4 0 Create a new project Use the following serverless yml Use the following handler js Deploy Test HTTP result Log"
41420312,41419941,"stackoverflow.com",2,"2017-01-02 02:46:26+02","2024-05-17 05:02:57.061228+03","You may be able to implement this as a single synchronous operation If the images are less then 10MB then you should be able to create a Lambda function to accept the image upload it to S3 process it and return the data to the user This also assume that you can upload to S3 and process the image within 29 seconds which is the maximum time before API Gateway will timeout the request If that will not work for you then you will have switch to asynchronous processing You can have a Lambda function which response to the S3 upload event and does the processing and writes the results to a table in DynamoDB RDS or Aurora You can then expose an API Gateway method to check for completed processing and get the results The client would then have to poll this API Gateway method "
34132415,33884968,"stackoverflow.com",1,"2015-12-07 13:24:22+02","2024-05-17 05:03:15.575153+03","As 1 4 is not yet released Last week i needed exactly the same thing Therefore i created a small mock server that can be run locally httpsgithub commartinlindenbergJawsLocalServer"
41432543,41419941,"stackoverflow.com",1,"2017-01-02 22:01:24+02","2024-05-17 05:02:57.063234+03","I would recommend an alternate approach This method will result is faster upload better user experience and should be extremely reliable If you really really do want to use API gateway this is possible as binary data is now supported You can receive the data in your function either as binary or base64 encoded You could reply with a response including the base64 encoded processed image and some meta data for example The disadvantages here"
41625409,41452848,"stackoverflow.com",1,"2017-01-13 02:18:09+02","2024-05-17 05:02:57.638765+03","AFAIK there is not a direct way to use that isomorphic starter kit on Lambda AWS Lambda and API Gateway can help you set up a serverless backend And you can host your React app statically on S3 Here is a fork of the starter kit that you linked to that does exactly that React Static Boilerplate There are ways to server side render your React app on Lambda but they are a bit cumbersome and not very straightforward Here is an article outlining some basic aspects of it Rendering React with Amazon Lambda"
39272097,38408493,"stackoverflow.com",0,"2016-09-01 15:53:09+03","2024-05-17 05:02:59.424873+03","A serverless framework projects deploys a single API Gateway So if you want it to be in different API Gateways you need separate serverless framework projects Depending on the size of the services you are making it can make sense or it might not To merge the two API Gateways higher up you can use API Gateway Custom Domains and proxy the requests based on the path to different API Gateways and stages keeping one single domain for them all "
42330428,38408493,"stackoverflow.com",0,"2017-02-19 19:27:51+02","2024-05-17 05:02:59.426874+03","In your example you would want to keep them in the same serverless framework I would create two files player js and game js in srccontrollers to seperate out the logic You can setup serverless with the following YAML file"
42401862,38408493,"stackoverflow.com",0,"2017-02-22 22:42:19+02","2024-05-17 05:02:59.428042+03","One way of doing what you want is to use serverless to deploy the lambdas but to manually set API Gateway to link the endpoints to the lambdas There is a restriction in serverless stated here httpsserverless comframeworkdocsprovidersawsguideservices Where it states Currently every service will create a separate REST API on AWS API Gateway Due to a limitation with AWS API Gateway you can only have a custom domain per one REST API If you plan on making a large REST API please make note of this limitation Also a fix is in the works and is a top priority In our experience we manage to have Services with different API and a routing object in our clients To decide if they should be in the same serverless service you need to get into Modeling In our case we answer this questions When you change games are you going to change players etc This link can help you with that answer httpsmartinfowler comarticlesmicroservices html"
38466084,38456057,"stackoverflow.com",3,"2016-07-19 21:36:26+03","2024-05-17 05:03:00.0132+03","To return status 304 in your API you would need to throw an error from your Lambda function It is possible to return the LastModified value in the error message from your Lambda function and route that to the LastModified header in the API response For details have a look at Option 2 here Thanks Ryan"
38650053,38650052,"stackoverflow.com",12,"2017-03-06 09:04:38+02","2024-05-17 05:03:01.085641+03","After ask to Amazon Web Services Unfortunately the mapping of the Authorizer is not currently configurable and every returned error from a lambda function will map to a 500 status code in API gateway Moreover the mapping is performed on an exact string match of the output so in order to return the intended 401 Error to the client you should execute a call to context fail Unauthorized Finally I change to and work fine Sharing to whom may encounter this "
78377770,38650052,"stackoverflow.com",0,"2024-04-25 07:53:28+03","2024-05-17 05:03:01.087642+03","If your custom auth lambda is not able to return the policy document your APIs will return 500 error Use async await properly to get the right response Assuming your code is in Node js The code I was trying to execute was returning before promises getting fulfilled You have to make sure that the code returns after the promises are fulfilled only Test the authorizer from the test feature of API authorizers to get the exact error causing your custom authorizer API gateway pair to fail "
44191861,38713469,"stackoverflow.com",9,"2018-10-29 01:57:58+02","2024-05-17 05:03:01.734886+03","Serverless has changed a bit since you asked this question but if you are using the lambda_proxy method of integration you can use a handler like Note I have used ES6 features so you will want to use Node 6 or higher to directly copypaste this example "
38729237,38713469,"stackoverflow.com",3,"2016-08-02 22:46:15+03","2024-05-17 05:03:01.736886+03","If you cannot fix it on the Lambda function you could probably do a replaceAll in the API Gateway mapping template I think this could work just to replace the escaped double quotes Edit So the swagger would be if I got the escaping right "
39980757,38919753,"stackoverflow.com",2,"2016-10-11 18:18:17+03","2024-05-17 05:03:03.32341+03","In serverless 1 0 0RC2 you can set the authorizationType as follows"
39149770,38919753,"stackoverflow.com",1,"2016-08-25 18:54:23+03","2024-05-17 05:03:03.324411+03","Checkout the following link for the closest I found on help with authentication httpsgithub comserverlessserverlessblob85f4084e6b0fd4a6d763ace8cd0db82817bbc712libpluginsawsdeploycompileeventsapiGatewayREADME mdhttpsetupwithcustomauthorizer I have not used AWS_IAM but here is how I define a CUSTOM authentication which should indicate the overall format Therefore I think you need to add the authorizer section I cannot test this but let me know if it does not work this way"
42822241,39097572,"stackoverflow.com",46,"2017-03-22 03:04:39+02","2024-05-17 05:03:04.812657+03","Define in serverless yml Access customerId in code"
39097729,39097572,"stackoverflow.com",3,"2016-08-23 13:06:53+03","2024-05-17 05:03:04.814659+03","change path name Change your handler js file"
39098216,39097572,"stackoverflow.com",3,"2016-08-23 13:08:20+03","2024-05-17 05:03:04.814659+03"," Solution path customercustomerId customerId input params customerId Now the path param customerId is passed through to the AWS Lambda function as your JSON event"
39134130,39120750,"stackoverflow.com",3,"2016-08-25 01:46:37+03","2024-05-17 05:03:05.571348+03","Setting acceleration on an S3 bucket is not yet supported by CloudFormation These things typically lag by a few weeksmonths Updates are usually announced on the what is new page and in the docs that you already linked I feel bad for the CFN team they are always playing catchup "
39143544,39143243,"stackoverflow.com",2,"2016-08-25 14:03:16+03","2024-05-17 05:03:06.460974+03","Solved by myself The problem is that I was making the createConnection outside of the handler when I declared the conn constant Moving the createConnection declaration inside the handler function works as expected in every call Hope this helps Thanks "
39269510,39171675,"stackoverflow.com",1,"2016-09-01 13:48:26+03","2024-05-17 05:03:07.378133+03","First off see why the IamRoleLambda cannot be created Go to AWS web console and go to CloudFormation Click on your stack probably it says it is failed Scroll to the failed step and see why It usually specify exactly what failed Second you do not need your lambda execution role to have permissions to the stream Try to remove the part from the iam policy about the stream So it looks like this instead"
40412344,33884968,"stackoverflow.com",2,"2020-06-20 12:12:55+03","2024-05-17 05:03:15.571674+03","The Bespoken sevrerless plugin makes your local Lambdas externally accessible It is very useful both for local testing with Postman as well as for Webhookbased services like Alexa Slack Twilio etc The architecture is shown here To use it you just install the plugin then run You can then start sending requests to your service locally We think it is a very useful tool for testing with serverless "
39270814,39194776,"stackoverflow.com",0,"2016-09-01 14:50:57+03","2024-05-17 05:03:08.44662+03","The IAM statements for the Lambda Execution Role ie the role that the Lambda function assumes when executing is not specified in the resources So if you need permissions to do something from inside your Lambda function you need to give the assumed role permission This is specified in the provider section So in your case I just copied something from your link you will have to change it to what you need it will be something like this assuming nodejs runtime Bonus For general resources specified in cloud formation json format use an online converter from json to yaml Much easier to get the started that way "
39257665,39244928,"stackoverflow.com",30,"2018-06-02 22:34:03+03","2024-05-17 05:03:09.561823+03","How to Debug With the following trust policy In API Gateway console for your APIs region Go to settings Enter in the ARN of the API GatewayCloudWatch logging role click Save Go to the stage of your API Under CloudWatch Settings select Enable CloudWatch Logs Set Log level to INFO Select Log full requestsresponses data Redeploy your API to that stage Go to the Resources tab for your API Select Actions Deploy API Make requests wait a few minutes and see what the logs say in CloudWatch The Error The Cause Once I enabled Invoke with caller credentials using Credentials arnawsiamuser the callers IAM role did not have access to invoke the lambda function This resulted in the 500 error Once I gave the callers IAM role access everything started to work properly "
39411201,39244928,"stackoverflow.com",1,"2016-09-09 15:05:10+03","2024-05-17 05:03:09.563373+03","What does the API Gateway Log show does it show Invalid permissions on Lambda function I think you will need to include the permission creation resource in the CloudFormation Template Here is one of mine"
39423355,39244928,"stackoverflow.com",0,"2016-09-10 09:41:31+03","2024-05-17 05:03:09.564371+03","In case some one in need Following link explain how to enable cloudwatch logs for debugging api gateway issues httpskennbrodhagen net20160723howtoenableloggingforapigateway"
43286871,39374581,"stackoverflow.com",9,"2017-04-07 23:47:43+03","2024-05-17 05:03:10.412935+03","I have got this working with serverless by emulating http ClientRequest and using a form parser tool like formidable I am using lambdaproxy for the API Gateway event configuration "
39420345,39374581,"stackoverflow.com",2,"2016-09-10 01:06:54+03","2024-05-17 05:03:10.414937+03","Well I couldnt make it as multipartformdata so I used base64 string action js RequestTemplate"
39451607,39450504,"stackoverflow.com",1,"2016-09-12 16:34:46+03","2024-05-17 05:03:12.169079+03","You can put multiple functions in one serverless yml"
39458754,39450504,"stackoverflow.com",1,"2016-09-13 00:03:42+03","2024-05-17 05:03:12.171079+03","I cannot speak directly to Serverless framework support but this is certainly possible in API Gateway You can maintain multiple Swagger files for each sub API and use importmodemerge to import both definitions into the same API See httpdocs aws amazon comapigatewaylatestdeveloperguideapigatewayimportapi html Thanks Ryan"
39672906,39450504,"stackoverflow.com",1,"2016-09-24 08:33:56+03","2024-05-17 05:03:12.171894+03","What I have done with my own code is to pull all code out of the handler js files and put it inside modules These modules would be required into the handler js files and a simple function would then be called usersModule js usershandler js This way you can place the meat of your code wherever you would like to have it organized in whatever way makes sense to you You would however still have to have a single API defined Or use the answer from RyanGAWS to merge the APIs If you would still like to keep code and API definitions separate you could create the users API and the products API separately You would then have another combined API that would call either of these APIs So this way you would have a single service with a single base URL that you would call You can do this with integration type HTTP I have not tried this so I do not know how well it would work "
39460014,39450504,"stackoverflow.com",0,"2016-09-13 02:07:02+03","2024-05-17 05:03:12.173587+03","You can use Custom Domain Names httpdocs aws amazon comapigatewaylatestdeveloperguidehowtocustomdomains html setup the custom domains name you will need a SSL certificate httpmyapi com then map your apis Just call your functions like this "
42715091,39450504,"stackoverflow.com",0,"2017-03-10 11:43:08+02","2024-05-17 05:03:12.174585+03","I came up with my own solution to this problem I abstracted the integration points of my application so that I have specific Integration services API S3 SNS etc which respond to events and then process those events and delegate them to separate microservices I wrote an article on it with code examples "
39728141,39451505,"stackoverflow.com",35,"2016-10-18 20:43:05+03","2024-05-17 05:03:12.83466+03","The link you posted is sadly the only real answer at this time API Version 20120810 PutItem may return items just before they were updated or none at all The ReturnValues parameter is used by several DynamoDB operations however PutItem does not recognize any values other than NONE or ALL_OLD In short the only reliable way to retrieve your inserted object is to GetItem just as you surmised "
39460047,39451505,"stackoverflow.com",27,"2016-09-13 02:11:36+03","2024-05-17 05:03:12.836662+03","Just pass the params Item in the callback Pass the err in the callback too "
64064781,39451505,"stackoverflow.com",17,"2020-09-25 16:06:52+03","2024-05-17 05:03:12.837661+03","You can use UpdateItem instead of PutItem UpdateItem creates a new item if it does not exist already plus you can set ReturnValues to ALL_NEW to return the whole item as it appears in the database after the operation Using GetItem right after PutItem is not a good idea unless you are using strong consistency in DynamodDB If you have eventual consistency default then GetItem right after PutItem can still return empty "
39451889,39451505,"stackoverflow.com",3,"2016-09-12 17:02:03+03","2024-05-17 05:03:12.838996+03","Note that this is the item you are inserting which you already have access to You could simply change your code to this and then you would have the inserted item in the item variable already You seem to be confused about what you are inserting because you start your question by showing an object you say you are inserting but the code you posted is inserting a slightly different object "
33993652,33837797,"stackoverflow.com",1,"2015-12-02 08:05:28+02","2024-05-17 05:03:14.533508+03","This is not currently supported as of v1 4 It is a known issue that is being discussed httpsgithub comjawsframeworkJAWSissues295 and something similar is in the roadmap For now you will have to create a separate awsm for each route method"
33927964,33884968,"stackoverflow.com",19,"2016-04-14 21:27:25+03","2024-05-17 05:03:15.566656+03","It does not look like there is way to do this right now but version 1 4 0 is about to be released and among other things it should include a new command jaws serve which should address your problem Heres the PR httpsgithub comjawsframeworkJAWSpull269 UPDATE you can now use the new serverlessserve plugin for this UPDATE 2 serverlessserve has not been updated in a while it looks like serverlessoffline is a much better option now to emulate Lambda functions "
52760555,33884968,"stackoverflow.com",4,"2018-10-11 15:52:30+03","2024-05-17 05:03:15.568476+03","You can now use lambdalocal Install it like this Add your parameters as a JSON object to a file in this example event json and call the index js file like this"
35190688,33884968,"stackoverflow.com",3,"2016-02-04 02:59:32+02","2024-05-17 05:03:15.569619+03","I am not sure if this question is still relevant or not but I am using DEEP Framework to test the code locally andor deploy it on AWS Lambda Check this out Disclosure I am one of the contributors to this framework"
55853037,33884968,"stackoverflow.com",3,"2019-04-25 18:40:12+03","2024-05-17 05:03:15.570639+03","The serverless framework now provide a way to invoke functions locally With that you can create queries in json files like If your function is described in serverless yml file you can then invoke it locally with"
36532078,33884968,"stackoverflow.com",0,"2016-04-10 19:04:17+03","2024-05-17 05:03:15.576153+03","As of the date of this post you can run functions locally by doing sls function run [nameoffunction] Any json used in your functions event json will be passed into your function For testing your endpoints you can also use Serverless Offline which is a fork of the serverlessserve project "
34051023,34050300,"stackoverflow.com",1,"2015-12-02 21:11:20+02","2024-05-17 05:03:16.616303+03","You could use xmlhttprequest to define a global XMLHttpRequest so you can use le_js in Node"
34423520,34359675,"stackoverflow.com",7,"2015-12-22 21:56:51+02","2024-05-17 05:03:17.437102+03","in short ftp will not work with lambda since they use ephemeral ports sftp will work nicely with lambda i tested using java code via jsch with no issues tho i cant see how it wouldnt work with any js sftp lib "
34360242,34359675,"stackoverflow.com",3,"2015-12-18 18:45:14+02","2024-05-17 05:03:17.439103+03","It is possible tested just now Make sure ur timeout is set to be long enough and you are calling context succeed on process termination"
34360334,34359675,"stackoverflow.com",1,"2015-12-18 18:51:25+02","2024-05-17 05:03:17.440103+03","By default Lambda functions only have 3 seconds to complete If it takes longer than that you will get the error you are seeing You can adjust the timeout to anything up to 5 minutes To change it using the aws CLI run"
37648639,34751377,"stackoverflow.com",3,"2016-06-06 05:03:36+03","2024-05-17 05:03:18.366707+03","You can put the rdsInstanceName in the environment section of a functions sfunction json file then access it using the process env MyRdsInstanceName within Lambda and reference this stageregion specific variable in your Lambda using something like Hope this helps"
35093604,35067236,"stackoverflow.com",5,"2020-06-20 12:12:55+03","2024-05-17 05:03:19.268602+03","see doc httpdocs aws amazon comAmazonS3latestAPIRESTObjectHEAD html If you have the s3ListBucket permission on the bucket Amazon S3 will return a HTTP status code 404 no such key error If you dont have the s3ListBucket permission Amazon S3 will return a HTTP status code 403 access denied error my code was trying to run headobject on a nonexistent item so the error that i got was forbidden which was correct since i didnt have the listbucket permission for neither the s3 bucket nor the lambda "
35255775,35252815,"stackoverflow.com",20,"2017-07-16 20:52:55+03","2024-05-17 05:03:21.162985+03","EDIT updated the answer for Serverless Framework 1 x The solution is to set the iamRoleStatements to allow Lambda to access the DynamoDB resources Note the credentials used by the Serverless Framework must have permission to the same DynamoDB resources add the iamRoleStatements in your serverless yml deploy the changes To give permissions in a function level instead of allowing all functions to access DynamoDB see my other answer here "
35254733,35252815,"stackoverflow.com",4,"2016-02-07 16:32:23+02","2024-05-17 05:03:21.164986+03","While I am not familiar with the way Serverless works what you are looking for is an IAM Role You can assign a role to an EC2 instances or AWS Lambda functions so that code that you write that uses the AWS SDK will automatically be able to retrieve AWS credentials with the permissions associated with that role For AWS Lambda and your use case you will want to grant the role you assign AWS Lambda access to the DynamoDB tables it requires to run This can be deceivingly simple to use you simply do not provide credentials and it just works as long as the role has the correct permissions The AWS SDK takes care of everything for you by automatically retrieving credentials that are associated with the Role From the link you provided the specific question that references this under the best practice is Credentials from IAM Roles for EC2 Instances where it refers to EC2 instances but this also applies to AWS Lambda "
36221435,35382007,"stackoverflow.com",8,"2016-03-25 16:02:19+02","2024-05-17 05:03:22.10057+03","As of Serverless v0 5 this is pretty easy Environment variable handling blends Serverless Project Variables You define the Project variables in _metavariables in a perstage perregion way for instance in svariablesdevuseast1 json Then in the sfunction json file s where that bucket is used you define the environment variables that function needs and reference the Project Variable in a templatelike way It will then appear just like any other environment variable so in Node So far the Serverless docs have not caught up with this change but I expect they should soonish "
35756417,35744288,"stackoverflow.com",1,"2016-03-05 03:40:55+02","2024-05-17 05:03:23.079552+03","The obvious solution is to update customRole within sfunction json to include the stage variable Unfortunately this functionality is currently incomplete in the released version of Serverless but is expected to be available in v0 5 A GitHub issue has been opened that includes this specific functionality 20160304 Update Serverless v0 5 has now been released for beta testing You can install it using the following command"
35943369,35943185,"stackoverflow.com",7,"2016-03-11 17:15:07+02","2024-05-17 05:03:24.157389+03","Authentication is done via Basic Authentication hence you can use the u flag in curl Using the userpass version as you used it should work aswell To invoke an action you have to use POST hence XPOST Also the API expects applicationjson as the ContentType Data is sent via the d flag in curl You also have a typo in your url You need to use actions instead of action the whole API uses plurals All in all your request should look like this There is a blog article covering this topic For blocking actions just add blockingtrue as parameter "
35945762,35943185,"stackoverflow.com",4,"2016-03-11 18:55:05+02","2024-05-17 05:03:24.15931+03","The wsk CLI has also the very handy v option which shows you the HTTP request and headers so if you do you will see the actual REST API call "
42739879,35943185,"stackoverflow.com",0,"2017-03-11 21:53:05+02","2024-05-17 05:03:24.160367+03","Authentication is done via Basic Authentication Add an HTTP Header for Authorization Use the string where base64Encoded str is a method to base64 encrypt a string You can find the username and password for your OpenWhisk account on Bluemix via httpsconsole ng bluemix netopenwhisklearncli or via wsk cli wsk property get it contains the whisk auth property which is in the format usernamepassword"
37647653,35945251,"stackoverflow.com",3,"2016-06-06 02:43:43+03","2024-05-17 05:03:25.375184+03","To store the Arn from your CloudFormation Template sresourcecf json add some items into the Outputs section The FnGetAtt is a function in CF to get a reference from another resource being created When you deploy the CF Template using serverless resources deploy s dev r euwest1 the Kinesis Stream is created for that StageRegion and the Arn will be saved into the region properties file _metaresourcesvariablessvariablesdeveuwest1 json Note the initial capitalisation change insertVariableNameForLaterUse You can then use that in the functions sfunction json as insertVariableNameForLaterUse such as the environment section and reference this variable in your Lambda using something like CloudFormation happens before Lambda Deployments Though you should probably control that with a script rather than just using the dashboard Hope that helps "
36005922,35945251,"stackoverflow.com",0,"2016-03-15 10:26:36+02","2024-05-17 05:03:25.377184+03","What are the steps of deployment you are following here from Serverless For the first part of your ask I believe you can do a sls resources deploy to deploy all CF related resources and then you do a sls function deploy OR sls dash deploy to deploy the lambda functions So technically resource deploy CF does not actually deploy lambda functions For the second part of your ask if you have a usecase where you want to use the output of a CF resource being created as of now this feature has been addedmerged to v0 5 of Serverless which has not yet been released "
74679901,53797848,"stackoverflow.com",2,"2022-12-04 21:26:13+02","2024-05-17 05:12:38.154709+03","I was encountring same problem and solve it this way That is not very clean but as per now variables syntax collides with AWS params syntax See this for more details httpsgithub comserverlessserverlessissues2601"
36010035,35969178,"stackoverflow.com",1,"2016-03-15 13:28:16+02","2024-05-17 05:03:26.765253+03","I am writing answer of my own question make database js file in componentlib folder code of database js created object like this in componentlibindex js file Can use connection variable to write mysql query like this in componentlibindex js"
36005600,35969178,"stackoverflow.com",0,"2016-03-15 10:07:37+02","2024-05-17 05:03:26.76678+03","I believe you have a Component created in your Serverless Framework based project that contains multiple lambda functions And now you want to write the MySQL connection code such that this code block is available for reuse in all your lambda functions of that component If this is the ask then Serverless does provide a lib folder inside your Component directory which you can utilize to write common code logic to be reused Since you have a NodeJSbased runtime for your component there should be an index js file inside your Component folder The first thing you want to do is to add the MySQL connection code logic to a functionmethod in index js Serverless should have already included for you this entire lib folder in all your lambda functions handler js code like this Therefore the nextfinal thing you want to do is reuse your connection functionmethod in all the lambda functions belonging inside your Component like this Hope this helps let me know how it goes "
36167871,35969178,"stackoverflow.com",0,"2016-04-08 00:07:29+03","2024-05-17 05:03:26.768772+03","To build off of Normal Goswamis answer You have specified the database here in the connection My lambdas each need different databases so in the connection code just leave off the database And then use an oddly named function to change the database in each lambda function You could also look into using a database connection pool "
39835399,35969178,"stackoverflow.com",0,"2016-10-03 18:41:04+03","2024-05-17 05:03:26.769684+03","you have to make connection out of function as we are doing it with mongodb we are making mongodb connection out side of Lambda Function my code snippet from httpsgithub commalikasinger1serverlespracticetreemastermongodbconnection in your cace it will be something like this most probably"
59032879,35969178,"stackoverflow.com",0,"2019-11-25 15:24:20+02","2024-05-17 05:03:26.770879+03","I am assuming you are using serverless framework on AWS Although you can create a connection and assign it to a frozen variable it is not guaranteed that your lambda will not create a new connection Here is why The best way so far in my personal opinion is to create a separate lambda function for db related operations and invoke this function through other lambdas Here is the flow client registerUserLambda dbLambda DATABASE However the thing about lambdas is that when there are too many requests there will be new containers created to handle other requests That is new connections will be created Therefore the concept of connection pools does not work well for now on serverless lambdas "
37095241,36016455,"stackoverflow.com",2,"2017-05-23 15:07:01+03","2024-05-17 05:03:27.182755+03","As already answered in comments bucket names are globally unique If you created a project using one AWS account you cannot use another AWS account to add a new stage in the same project because the bucket name will not be available even if it is in another AWS Region As per docs Amazon S3 bucket names are globally unique regardless of the AWS region in which you create the bucket This is a big problem when hosting static websites in S3 because the bucket name must match the domain name If the bucket name is already taken you cannot host it there "
36049913,36044074,"stackoverflow.com",1,"2016-03-17 03:24:37+02","2024-05-17 05:03:27.760692+03","I have just taken to updating the sfunction json file to set the method of an sls functionendpoint Same goes for setting specific statusCode requestTemplates and so on Heres an example of a POST functionendpoint sfunction json config file "
36087832,36087745,"stackoverflow.com",2,"2016-03-18 17:17:19+02","2024-05-17 05:03:28.8158+03","I found it in the OpenWhisk documentation does the trick It streams the activation log to my shell"
32707971,32707970,"stackoverflow.com",2,"2015-10-21 17:12:02+03","2024-05-17 05:03:29.908858+03","The JAWS AWS Module system aka awsm allows for this functionality via an attribute in the awsm json metadata file that specifies how the lambda code is packaged Specifically the lambda package optimize exclude and lambda package optimize includePaths defined in the lambda configuration options section of the awsm json spec The exclude attribute specifies node modules that should be excluded from the optimization process under the covers it is just using browserify exclude In the exclude attribute you simply specify [awssdk] You must then tell JAWS to include awssdk in the zip by putting its path in includePaths The includePaths attribute is a list of paths relative to the back directory that are pulled in asis into the zip file before deployment For a complete example see the awsm sample"
33002028,33001798,"stackoverflow.com",4,"2015-10-07 23:39:00+03","2024-05-17 05:03:30.950225+03","I am not sure you can protect your front end from people calling it more than they should since that is extremely hard to determine However for real DDoS or DoS protection you would probably want to use the features of API Gateway check the question about threats or abuse or AWSs new WAF I know WAF has the ability to block ranges of IP addresses and the like "
33003310,33001798,"stackoverflow.com",0,"2015-10-08 01:13:31+03","2024-05-17 05:03:30.951225+03","what Boushley said you may want to checkout Cloudflare httpswww cloudflare comddos"
36532008,33001798,"stackoverflow.com",0,"2016-04-10 18:57:39+03","2024-05-17 05:03:30.9526+03","Actually Amazon API Gateway automatically protects your backend systems from distributed denialofservice DDoS attacks whether attacked with counterfeit requests Layer 7 or SYN floods Layer 3 "
47248332,33001798,"stackoverflow.com",0,"2017-11-12 13:37:16+02","2024-05-17 05:03:30.953584+03","In your serverless yml you can now provide a provider usagePlan property assuming you are using AWS While this does not mean that you cannot be DDoSed as mrBorna mentioned AWS tries to prevent this by default it should mean that if you are DDoSed you will not be significantly affected from a financial perspective "
33853558,33062564,"stackoverflow.com",1,"2015-11-22 15:32:03+02","2024-05-17 05:03:32.001361+03","For now on 1 3 3 there is no automation for generating resourcescf json from the awsm json as I expected it to be as well From their Gitter channel resources part of awsmmodule can update main resources cf file on the first install through the post install step but that is it for now There is no tooling at the moment to do the same thing when you add changes to your module manually For now such changes have to be moved by hand to the appropriate resourcescf file Future versions of JAWS should bring support for commands that allow rebuilding resourcescf file from information stored inside the awsmmodules as some people requested that feature "
33731044,33062564,"stackoverflow.com",0,"2015-11-16 10:27:55+02","2024-05-17 05:03:32.003941+03","I ran across this question when I had a similar problem In my case setting the region and stage specifically deployed successfully Are you sure that you have not deployed the resources to a different stage andor region and that is why AWS is giving you the validation error i e in that regionstage your stack is already up to date Heres what I ran jaws deploy resources staging useast1 After editing my cloudformationstaginguseast1resourcescf json file Then I confirmed via the AWS Web Console that the changes were indeed in effect "
36868230,36853719,"stackoverflow.com",0,"2016-04-26 17:41:37+03","2024-05-17 05:03:39.828953+03","If I add the r useast1 option to serverless function run then it works fine Interestingly serverless does not need the r option when it is run remotely if there is a single region in the project but does when run locally To summarize successfully reads variables from the appropriate svariables file but does not In contrast works fine without a region being explicitly specified presumably because I only have one region in my project "
35986397,33261828,"stackoverflow.com",3,"2016-03-14 13:32:40+02","2024-05-17 05:03:32.997351+03","In general the problem you encounter is the disability of the serverlessoptimizerplugin to handle dynamically loaded NPM modules or globals correctly e g when using the mysql NPM package So you have to exclude it from optimization The solution heavily depends on the serverless version and the Node version you use so I will list the different solutions below Severless v4 Node v4 Set the excludes in your scomponent json as follows Serverless v5 Node v4 Components have been obsoleted and removed in this serverless version favoring functions instead So apply the optimizer configuration directly to your sfunction json configuration files Node v5 The NPM executable included with Node v5 internally does dependency optimization and dependency module flattening This is not yet really compatible with the current serverlessoptimizerplugin The solution here is to add the dependencies that are already optimized by NPM as proposed by Masatsugu Hosoi in his answer above like this"
33932504,33261828,"stackoverflow.com",1,"2015-11-26 09:09:02+02","2024-05-17 05:03:32.999351+03","edit awsm json httpsgithub comfelixgenodemysqlissues1249"
70369695,33261828,"stackoverflow.com",1,"2021-12-15 22:06:33+02","2024-05-17 05:03:33.000672+03","For any coming in the future what worked for me to add the following in the webpack config js file Mysql does not seem to like the minification"
33289056,33261828,"stackoverflow.com",0,"2015-10-22 22:35:52+03","2024-05-17 05:03:33.001624+03","I just had this exactly same problem The problem is with the browserify and the mysql module Unfortunately I could not find a real solution By reading the code the browserify is the only available option as builder httpsgithub comjawsframeworkJAWSblobmasterlibcommandsdeploy_lambda js You can set the builder as false This will simply zip all your files before sending them to Amazon Unfortunately again just doing this will not work For some reason all files are inside a node_module folder to work you must take the files out before upload the package Still all this is manual edit There is already an open issue about this last part httpsgithub comjawsframeworkJAWSissues239"
33676601,33633891,"stackoverflow.com",0,"2016-01-31 15:42:59+02","2024-05-17 05:03:33.660457+03","atm there is no way to do this via Serverless Framework one thing i found out was that u can omit values in the url so itll be considered blank ex so this considers option2 as blank so this kinda solves the issue except user would need to add the additional s"
36167743,36122250,"stackoverflow.com",7,"2017-05-23 14:50:32+03","2024-05-17 05:03:35.092678+03","It looks like the requestParameters in the sfunction json file is meant for configuring the integration request section so I ended up using This ended up adding them automatically to the method request section on the dashboard as well I could then use them in the mapping template to turn them into a method post that would be sent as the event into my Lambda function Right now I have a specific mapping template that I am using but I may in the future use Alua Ks suggested method for mapping all of the inputs in a generic way so that I do not have to configure a separate mapping template for each function "
36130339,36122250,"stackoverflow.com",2,"2016-03-21 13:53:55+02","2024-05-17 05:03:35.094679+03","You can pass query params to your lambda like In lambda function access querystring like this event querystring"
61146122,36122250,"stackoverflow.com",2,"2020-04-10 21:07:35+03","2024-05-17 05:03:35.095685+03","First you need to execute a putmethod command for creating the Method Request with query parameters After this you can execute the putintegration command then only this will work Otherwise it will give invalid mapping error"
36142383,36122250,"stackoverflow.com",0,"2016-03-21 23:56:22+02","2024-05-17 05:03:35.0971+03","Make sure you are using the right end points as well There are two types or some such in AWS friend of mine got caught out with that in the past "
36139199,36127475,"stackoverflow.com",6,"2017-03-20 06:41:55+02","2024-05-17 05:03:35.385027+03","The Serverless Framework requires Node js v4 0 or higher Any version of Node js within the v4 x or v5 x lines should work You are correct that AWS Lambda currently only supports Node js v0 10 36 The decision to build the Serverless Framework on Node js v4 0 was done in anticipation that AWS Lamabda would eventually support Node js v4 0 or higher When developing code for AWS Lambda you should continue to only use features compatible with Node js v0 10 36 If you make any contributions to the framework you can use Node js features available in v4 0 Another option is to use Babelify to transform your ES2015 code uploading to AWS Lambda This allows you to develop in ES2015 without having to wait for AWS Lambda to officially support it This can be done automatically each time you deploy with the Serverless Framework using the Optimizer Plugin Update A new option now exists the Serverless Babel Runtime This goes one step beyond what Optimizer does and uses Babel inside the runtime itself Update 2 AWS Lambda now supports Node js v4 3 "
37364924,36127475,"stackoverflow.com",1,"2016-05-22 18:16:19+03","2024-05-17 05:03:35.387557+03","If you are starting out with a brand new fresh project I would highly avoid starting with node 0 10 x That version is just receiving important security fixes at this point and only for another five months until October 2016 The Node js has adopted the common Long Term Support LTS pattern to keep releases timely and stable Here is the current LTS plan for node I would highly recommend starting out with version 4 or 5 depending on your appetite for change and keeping your project up to date You will be able to take advantage of numerous new features over the 0 100 12 releases as well as better prepare you for ES6 "
36555765,36551410,"stackoverflow.com",3,"2016-04-11 22:22:04+03","2024-05-17 05:03:36.342575+03","For the lambda part You can use event json file and then run sls function run According to docs if do not specify any stage function will run locally if you do specify a stage function will run deployed code in corresponding stage BUT the docs seems outdated you also need to pass d flag like This command will invoke your deployed lambda function with parameters from your local event json file Here is the source code for function run options For APIG integration There are some samples in documentation If you do not want to use templates you can just insert related code in your sfunction json inside the endpoint description Syntax is as described in API Gateway Accessing the input Variable doc "
36586788,36559256,"stackoverflow.com",1,"2016-04-13 04:09:06+03","2024-05-17 05:03:37.020416+03","This issue was fixed with my PR here httpsgithub comserverlessserverlessoptimizerpluginpull41"
36754695,36748324,"stackoverflow.com",3,"2016-04-20 23:37:52+03","2024-05-17 05:03:38.025781+03","Your mapping template should look like this For mysite compathjson7B22val223A2017D this mapping template will result in the following JSON If you want the querystring JSON to be passed on the root level to your Lambda function use this as a mapping template"
36812870,36783641,"stackoverflow.com",2,"2016-04-23 18:42:40+03","2024-05-17 05:03:38.956498+03","There is a plugin called meta sync that should solve your problem httpsgithub comserverlessserverlessmetasync"
36860774,36853719,"stackoverflow.com",0,"2016-04-26 12:28:12+03","2024-05-17 05:03:39.82697+03","I have to do a bit of guessing here because I am lacking some info from your question So I will assume that that you have a sfunction json file that has this in it environment myEnvironmentVar myEnvironmentVar if this is the warning you are getting Serverless WARNING This variable is not defined region If this is the case then the problem is that you have not defined this variable any of the these two files Also if you add the value in svariablescommon json and then also add it to svariablesstage json then svariablesstage json will override the value of svariablescommon json"
54786044,54314930,"stackoverflow.com",1,"2019-02-20 14:07:43+02","2024-05-17 05:13:12.90054+03","Import syntax is not available in Node v8 x x Use require instead or compile it with babel just like Bogdan Alexandru Militaru did "
40757983,36853719,"stackoverflow.com",0,"2016-11-23 09:01:45+02","2024-05-17 05:03:39.830349+03","First of all run following command to create your stageregionss variables json file in _meta folder then provide your stage name and the region you have to provide your amazon access key and the secret key Then the following json file will be created If you want to add the variable manually you can do it as follows Those variables will be added to above svariables your_region your_stage json file and then give the key and the value or in one line as follows Hope this helps "
52556438,36877647,"stackoverflow.com",2,"2018-09-28 16:26:06+03","2024-05-17 05:03:40.590368+03","You can specify a list of API keys to be used by your service Rest API by adding an apiKeys array property to the provider object in serverless yml You will also need to explicitly specify which endpoints are private and require one of the api keys to be included in the request by adding a private boolean property to the http event object you want to set as private API Keys are created globally so if you want to deploy your service to different stages make sure your API key contains a stage variable as defined below When using API keys you can optionally define usage plan quota and throttle using usagePlan object Heres an example configuration for setting API keys for your service Rest API For more info read the following doc httpsserverless comframeworkdocsprovidersawseventsapigateway"
36973626,36902720,"stackoverflow.com",2,"2016-05-02 03:47:55+03","2024-05-17 05:03:41.694986+03","I created an issue at MongoDB JIRA server and I was confirmed the driver as of today does not run in the browser Of course I was not trying to run it in the browser but in Node but as my friend dvlsg pointed in the comments of my question it is possible browserify shims some stuff important for the correct implementation of MongoDB driver So if you are out there pulling your hair off as of today try a workaround which does not include bundling up mongodb driver with browserify The driver maintainer confirmed he was working on a version that will work in the browser which will possibly work the way I needed but it is not ready even for alpha release hopefully if you are reading this in the future "
36944792,36944330,"stackoverflow.com",9,"2016-04-29 21:00:28+03","2024-05-17 05:03:42.696045+03","Here are a few steps that should make it work in handler py add the following That is it Let me know if it worked "
41634501,36944330,"stackoverflow.com",4,"2017-01-13 14:28:01+02","2024-05-17 05:03:42.697046+03","I followed below steps to deploy with dependencies Created a directory for dependencies in project root mkdir vendor Add dependencies in requirements txt file manually or use pip freeze requirements txt Update serverless yml file package include vendor import sys sys path insert 0 vendor Now serverless deploy will upload function with dependencies "
46000668,36944330,"stackoverflow.com",4,"2017-09-01 15:51:12+03","2024-05-17 05:03:42.699047+03","I would recommend using the serverlesspythonrequirements plugin to include packages installed via pip"
37096489,37094695,"stackoverflow.com",40,"2016-05-08 09:04:06+03","2024-05-17 05:03:43.827002+03","I have now solved my own problem and I hope I can be of help to someone experiencing this problem in the future There are two major considerations when connecting to a database like I did in the code above from a Lambda function Heres my updated code Note that I have also put my Redis configuration into a separate file so I can import it into other Lambda functions without duplicating code This works exactly as it shouldand it is blazing fast too "
37108651,37108120,"stackoverflow.com",2,"2017-05-23 15:07:56+03","2024-05-17 05:03:44.482337+03","You can set environment variables in the environment section of a functions sfunction json file Furthermore if you want to prevent those variables from being put into version control for example if your code will be posted to a public GitHub repo you can put them in the appropriate files in your _metavariables directory and then reference those from your sfunction json files Just make sure you add a _meta line to your gitignore file For example in my latest project I needed to connect to a Redis Cloud server but did not want to commit the connection details to version control I put variables into my _metavariablessvariables[stage][region] json file like so and referenced the connection settings variables in that functions sfunction json file I then put this redis js file in my functionslib directory Then in any function that needed to connect to that Redis database I imported redis js For more details on my ServerlessRedis setup and some of the challenges I faced in getting it to work see this question I posted yesterday "
37118498,37108120,"stackoverflow.com",2,"2016-05-09 21:35:45+03","2024-05-17 05:03:44.484337+03","update CloudFormation usage has been streamlined somewhat since that comment was posted in the issue tracker I have submitted a documentation update to httpdocs serverless comdocstemplatesvariables and posted a shortened version of my configuration in a gist It is possible to refer to a CloudFormation output in a sfunction json Lambda configuration file in order to make those outputs available as environment variables sresourcecf json output section sfunction json environment section Usage in a Lambda function old answer Looks like a solution was found implemented in the Serverless issue tracker link To quote HyperBrain To have your lambda access the CF output variables you have to give it the cloudformationdescribeStacks access rights in the lambda IAM role The CF loadVars promise will add all CF output variables to the process environment as SERVERLESS_CF_OutVar name It will add a few ms to the startup time of your lambda Change your lambda handler as follows"
37175100,37169377,"stackoverflow.com",28,"2016-05-12 02:56:29+03","2024-05-17 05:03:45.528114+03","I think what you are experiencing is the same as what I was experiencing recently I could install npm packages in my application root directory but nothing would get deployed to lambda My understanding is that serverless deploys everything under each component directory subdirectory under the application root In your case under functions I could not find much in the serverless documentation around this but what I did was define a package json file under my functions folder and then run an npm install in that subdirectory Then after deploying to lambda the node_modules under this directory got deployed too meaning that my function code could require any of these npm modules So your folder structure should now look like this The benefit here as well is that you can only deploy the npm dependencies that your functions need without those that serverless needs to deploy your resources Hopefully that helps once again not sure this is best practise just what I do because this is not documented anywhere that I could find on the Serverless repository or in any example code "
47652701,37169377,"stackoverflow.com",8,"2017-12-05 13:37:59+02","2024-05-17 05:03:45.530153+03","For me best solution was Serverless plugin serverlesspluginincludedependencies serverlesspluginincludedependencies"
56707601,37169377,"stackoverflow.com",6,"2019-06-21 19:53:40+03","2024-05-17 05:03:45.531718+03","You can do the following Reference document"
69922491,37169377,"stackoverflow.com",0,"2021-11-12 08:12:49+02","2024-05-17 05:03:45.532571+03","If someone runs into this trouble and none of the answers above are helping try this one worked for me "
40656647,40656603,"stackoverflow.com",57,"2020-06-20 12:12:55+03","2024-05-17 05:04:31.754607+03","The UI does not directly support renaming a Function but you can work around this using the following manual steps Github Issue for renaming Azure Function To anyone like myself that arrived here looking to rename their function despite this being the previously correct answer there is now a much smoother CMD based process as detailed in this answer by SLdragon and an even smoother GUI based process detailed in this answer by Amerdeep below "
46841668,40656603,"stackoverflow.com",33,"2017-10-20 05:45:30+03","2024-05-17 05:04:31.756608+03","Now 2017 10 we can use console to rename the Azure Function name"
37307593,37297339,"stackoverflow.com",0,"2016-05-19 18:29:19+03","2024-05-17 05:03:46.349275+03","Another asynchronous model many customers have used This setup has some advantages for high workload APIs as fetches from the Kinesis stream can be batched and do not require a 1to1 scaling of both your API Gateway limits and Lambda limits Update To answer your questions about scalability Kinesis Kinesis scales by adding what it calls shards to the stream Each shard handles a portion of your traffic based on a partition key Each shard scales up to 1000 rps or 1MBps see limits Even with the lower default 25 shards this would support up to 25000 rps or 25MBps with an evenly distributed partition key API Gateway API Gateway has a default account level limit of 500 rps but this can easily be extended by requesting a limit increase We have customers in production that are using the service at limits above your current suggested scale "
37302126,37297339,"stackoverflow.com",0,"2016-05-18 18:01:29+03","2024-05-17 05:03:46.352276+03","If you want a fast response from the API and not have to wait for the processing of data you could"
37365203,37297339,"stackoverflow.com",0,"2017-05-23 15:23:59+03","2024-05-17 05:03:46.353277+03","You should first run some tests to see what type of real world response times you are getting from having your lambda function complete all the logic If the times are above what you feel are acceptable for your use case here is another asynchronous solution utilizing an SNS Topic to trigger a secondary Lambda function "
37363867,37347003,"stackoverflow.com",2,"2016-05-21 16:59:28+03","2024-05-17 05:03:47.569354+03","Your demo repo does not appear to be including the AWS SDK setting the region as noted in the Getting Started guide I e Note that dynamodoc was deprecated almost a year ago You may want to try the DynamoDB DocumentClient instead This updated API has much more clear errorhandling semantics that will probably help point out where the problem is "
37667932,37403141,"stackoverflow.com",5,"2016-06-07 01:42:31+03","2024-05-17 05:03:48.663287+03","You have go two issues The 13 second delay This is expected and welldocumented when using Lambda As Nick mentioned in the comments the only way to prevent your container from going to sleep is using it You can use Lambda Scheduled Events to execute your function as often as every minute using a rate expression rate 1 minute If you add some parameters to your function to help you distinguish between a real request and one of these ping requests you can immediately return on the ping requests and then you have worked around your problem It will cost you more but we are probably talking pennies per month if anything Lambda has a generous free tier The 30 second delay is unusual I would definitely check your CloudWatch logs If you see logs from when your function is working normally but no logs from when you see the 30 second timeout then I would assume the problem is with API Gateway and not with Lambda If you do see logs then maybe they can help you troubleshoot Another place to check is the AWS Status Page I have seen sometimes where Lambda functions timeout and respond intermittently and I pull my hair out only to realize that there is a problem on Amazons end and they are working on it Heres a blog post with additional information on Lambda Container Reuse that while a little old still has some good information "
37472055,37423542,"stackoverflow.com",2,"2016-05-27 01:27:52+03","2024-05-17 05:03:49.163749+03","This is a complex question that would require some knowledge of your app and architecture but I will try to give you a high level answer on an approach that you can use as a starting point If you want this to be a fully automated solution which is the correct way to think about it then you will need to add some form of build tools into your process Some examples of these tools would be either gulp or grunt Here is a quick overview of what you might be able to do "
51165075,37423542,"stackoverflow.com",1,"2018-07-04 05:39:44+03","2024-05-17 05:03:49.16575+03","I faced this exact same issue and solved it by writing a custom plugin to build the frontend with the serverless yml environment variables The plugin is called serverlessbuildclient if you would like to use it Then I used serverlessfinch to upload the build to S3 serverlessbuildclient is pretty simple it just iterates over the environment variables and adds them to process env This is the important bits I have 2 separate serverless projects one for the frontend and one for the backend When you deploy the backend the stack outputs the ServiceEndpoint which is the base endpoint that you will want to reference In the serverless yml file I have something similar to this Then in the frontend I reference the environment variable to make requests My deployment script executes commands in this order"
37453416,37426088,"stackoverflow.com",0,"2016-05-26 09:44:45+03","2024-05-17 05:03:50.094458+03","Check if you have code running after you are calling callback See here aws lambda By default the callback will wait until the Node js runtime event loop is empty before freezing the process and returning the results to the caller You can set this property to false to request AWS Lambda to freeze the process soon after the callback is called Also if you are using an older version of node with Lambda you should call context succeed and context fail error to end the function "
37620785,37612655,"stackoverflow.com",3,"2016-06-06 17:03:45+03","2024-05-17 05:03:50.984443+03","I have solve my problem I was trying to upload pdf before I generate pdf I have solve this problem using the following code"
37613623,37612655,"stackoverflow.com",0,"2016-06-03 14:48:15+03","2024-05-17 05:03:50.985986+03","I have done similar kind of thing before I want a few clarifications from you and then I will be able to help you better 1 In your code server side you have mentioned in the callback function that actual aws link is getting returned Are you sure that your file is getting uploaded to Amazon s3 I mean did you check your bucket for the file or not 2 Have you set any custom bucket policy on Amazon s3 Bucket policy play an important role in what can be downloaded from S3 3 Did you check the logs to see exactly which part of code is causing the error Please provide me this information and I think the I should be able to help you "
37918384,37612655,"stackoverflow.com",0,"2016-06-20 12:00:06+03","2024-05-17 05:03:50.987984+03","if we do not want to upload at s3 just return generated file from awslambda "
37682270,37669198,"stackoverflow.com",3,"2016-06-07 17:35:20+03","2024-05-17 05:03:52.34865+03","This is how to fix In the component_asfunction json replace with in the function_1 js call the util js like from the Serverless documentation The handler property gives you the ability to share code between your functions By default the handler property is handler handler that means it is only relative to the function folder so only the function folder will be deployed to Lambda If however you want to include the parent subfolder of a function you should change the handler to be like this functionNamehandler handler As you can see the path to the handler now includes the function folder which means that the path is now relative to the parent subfolder so in that case the parent subfolder will be deployed along with your function So if you have a lib folder in that parent subfolder that is required by your function it will be deployed with your function This also gives you the ability to handle npm dependencies however you like If you have a package json and node_modules in that parent subfolder it will be included in the deployed lambda So the more parent folders you include in the handler path the higher you go in the file tree "
43712663,40656603,"stackoverflow.com",29,"2017-05-01 03:08:59+03","2024-05-17 05:04:31.757609+03","Create a new function and you will have an option to name it then delete the default one HttpTriggerCSharp1 I know it is not renaming but the easiest option around "
51055783,40656603,"stackoverflow.com",25,"2018-06-27 12:00:11+03","2024-05-17 05:04:31.758992+03","Go to Function Apps Click on platform features Click on app service editor Right click on your default function nameselect"
38040477,37748423,"stackoverflow.com",1,"2016-06-26 19:35:41+03","2024-05-17 05:03:53.653138+03","It looks like the serverless projects libEndpoint js does not include an entry for requestModels but the project is actively maintained so perhaps you can raise an issue on GitHub for them to add support I thought it might be useful to share an AWS CLI approach to this in the meantime You create request models the same way you create response models but having created them there is not a simple command equivalent to aws apigateway putmethodresponse to associate request models with the method This seems a missing feature of AWS CLI However I got it working using aws apigateway updatemethod You need to have created the model for the request first and then this command adds it to the method Note the odd application1json construct is to stop the slash in applicationjson being interpreted as part of the path Incidentally I tried and failed to get the JSON file argument to patchoperations working If anyone can shed any light on why this file leads to the error below I would love to hear about it "
38028627,37779324,"stackoverflow.com",2,"2021-09-10 12:56:50+03","2024-05-17 05:03:54.212378+03","I imagine that Serveless Framework is using your aws cli configuration There is a file on your home path homexxx awscredentials that stores yor AWS credentials Probably your credentials dont have permissions to use cloud formation Serveless uses Cloud formation extensively cloudformationDescribeStackResources You have to give permission adding an asws policy to your user to perform action on cloud formation "
38091784,37815711,"stackoverflow.com",2,"2016-06-29 09:22:10+03","2024-05-17 05:03:55.287731+03","I use following request template It will wrap data path headersparamsquery into a JSON object and pass it to the function You can refer Apache Velocity Templates to get better understanding about the inner syntax such as foreach header in "
37819930,37815711,"stackoverflow.com",1,"2016-06-14 21:51:09+03","2024-05-17 05:03:55.288732+03","Have you tried util parseJson It takes the json as a string and turns it into a traditional json object "
38091690,37822230,"stackoverflow.com",1,"2016-06-29 09:15:45+03","2024-05-17 05:03:56.576986+03","I use following response template If the message returned from the lambda function matches the selectionPattern specified in the response template it will return the correct status code "
37901148,37883864,"stackoverflow.com",1,"2017-05-23 14:52:30+03","2024-05-17 05:03:57.404316+03","As solved in comments the issue occurred when a old Node version was used and Serverless requires Node version 4 x or greater In this case the let declaration was not recognized because it is a ES6 specification and support started only in Node 4 Kangax table "
37936014,37935040,"stackoverflow.com",4,"2016-06-21 08:09:06+03","2024-05-17 05:03:58.431195+03","I guess you need to init after cloning it from your Git Reference httpdocs serverless comdocsprojectinit"
38162410,38156623,"stackoverflow.com",1,"2016-07-02 19:49:31+03","2024-05-17 05:03:59.498933+03","As of this writing the author has an answered his own question but there are some issues impacting the longterm stability of the solution The correct solution is to sort the keys usually in lexical order before encoding or after decoding the object and assemble a hash or perhaps better an HMAC of the canonical data sorted keys and values This makes signing and verification genuinely deterministic Using the wrong content type to make something work seems a little sketchy and fragile Also it should be possible to eliminate the problem entirely by expecting specific certificates to be presented by the application server certificate pinning in a sense A malicious user with an MITM proxy and forged SSL certificate would have a computationallyimpractical time impersonating your application server in that case JSON Web Tokens also seem promising but perhaps not within the constraints of the question "
38156743,38156623,"stackoverflow.com",0,"2016-07-02 08:16:44+03","2024-05-17 05:03:59.50085+03","After a few hours of work the fix was actually fairly simple I needed to change the response type to texthtml and then stringify before returning With serverless I set the following In my code then"
42601298,38206954,"stackoverflow.com",0,"2017-03-04 22:55:16+02","2024-05-17 05:04:00.400331+03","httpsgithub comstackiatest2doc js I am working on this project which enables generating documents currently only API blueprint from BDD tests just exactly what you need Test code example"
38243445,38238496,"stackoverflow.com",7,"2020-06-20 12:12:55+03","2024-05-17 05:04:01.842855+03","Heres the quote in question Initialize external services outside of your Lambda code When using services like DynamoDB make sure to initialize outside of your lambda code Ex module initializer for Node or to a static constructor for Java If you initiate a connection to DDB inside the Lambda function that code will run on every invoke In other words in the same file but outside of before the actual handler code When the container starts the code outside the handler runs When subsequent function invocations reuse the same container you save resources and time by not instantiating the external services again Containers are often reused but each container only handles one concurrent request at a time and how often they are reused and for how long is outside your control Unless you update the function in which case any existing containers will no longer be reused because they would have the old version of the function Your code will work as written but is not optimized The caveat with this approach that arises in current generation Node js Lambda functions Node 4 x6 x is that some objects notably those that create literal persistent connections to external services will prevent the event loop from becoming empty a common example is a mysql database connection which is holding a live TCP connection to the server by contrast a DynamoDB connection is actually connectionless since it is transport protocol is HTTPS In this case you need to either take a different approach or allow lambda to not wait for an empty event loop before freezing the container by setting context callbackWaitsForEmptyEventLoop to false before calling the callback but only do this if needed and only if you fully understand what it means Setting it by default because some guy on the Internet said it was a good idea will potentially bring you mysterious bugs later "
39791686,39774436,"stackoverflow.com",12,"2017-05-23 15:26:19+03","2024-05-17 05:04:10.093933+03","create requirements txt pip freeze requirements txt create a folder with all the dependencies pip install t vendored r requirements txt Note that in order to use these dependencies in the code you will need to add the following See httpsstackoverflow coma369447921111215 for another example UPDATE Instead of the bullet 2 and the code above you can now use the serverlesspythonrequirements plugin install the plugin and add the plugin to your serverless yml Do not forget to make sure you have a requirements txt file That is it once sls deploy is called the plugin will package the dependencies with the code For a full sample take a look at the serverlesspythonsample "
41634572,39774436,"stackoverflow.com",0,"2017-05-23 15:02:38+03","2024-05-17 05:04:10.095934+03","I had similar problem took these steps to deploy with dependencies httpsstackoverflow coma416345012571060"
39952026,39793242,"stackoverflow.com",2,"2019-09-04 12:56:01+03","2024-05-17 05:04:11.173349+03","Serverless V1 currently not working with status code But there is a serverless plugin for use status code 1 install plugin 2 Add plugin name in serverless yml file 3 Add this custom code in serverless yml file you can change this code according to your requirement Add redirect url in your handler file module exports home function event context context succeed Location httpstest com Define function and its responses in serverless yml file responses selfcustom redirectResponses "
54297364,54285038,"stackoverflow.com",1,"2019-01-21 22:30:33+02","2024-05-17 05:13:07.394239+03","It basically says that Ref is expecting a value You have defined it but not assigned any value to it If there is no use then you should remove this part from your code"
38272659,38271477,"stackoverflow.com",0,"2016-07-08 20:47:35+03","2024-05-17 05:04:02.308022+03","The internet is build on the Internet Protocol suite This suite has 5 different layers of protocols The physical layer the link layer the network layer the transport layer and the application layer Each depends on the one before If you just use the browser by default HTTP application layer is used which relies on TCP transport layer which relies on IP v4 or v6 network layer which relies on ethernet link layer which finally relies on the actual cable that is plugged into your computer for WiFi the first three are the same but the last two differ if I am not mistaken Now to your question Is there any protocol API or software in existence that can send dataIMetc directly from one device to another with no server Yes there is I suggest you start looking at protocols that are in the application layer To highlight a few standards next to HTTP S FTP is for file transfer IMAP is for emails clients SMTP is for email servers and SSH is a secure shell which can also be used to tunnel data through For your specific case I think either FTP FTPS if you want it over SSL or SSH can be a solution but it is hard to know for sure without the specifics The only thing that these protocols have in common is that one of the two computers will act like server and the other computer as client This has as downside that portforwarding might be necessary If you have chosen the protocol you would like to use then you are up for the next step selecting a program that can do that for you For HTTP S I would recommend Apache If you are using Linux and chose SSH then you are in luck there is a build in SSH server in Linux you can use that For other protocols you might just want to search yourself as I do not have any suggestions I hope this answers most of your questions Desirius"
38275439,38271477,"stackoverflow.com",0,"2016-07-09 00:10:01+03","2024-05-17 05:04:02.311024+03","In browser context WebRTC is probably what you are looking for It allows user to user communications httpsen wikipedia orgwikiWebRTC httpswebrtc org"
39615669,38327946,"stackoverflow.com",1,"2016-09-21 14:45:07+03","2024-05-17 05:04:03.384073+03","Starting with Serverless v1 0Beta 1 you are already able to use python You can create a python service this way Then you will get a serverless yml that will have the provider and runtime configured like this"
38773392,38327946,"stackoverflow.com",0,"2016-08-04 20:10:33+03","2024-05-17 05:04:03.385073+03","Unless it is changed very recently only Node js is supported in the alpha At the moment we only support Node js in this alpha but other languages will follow httpblog serverless comserverlessv10alpha1announcement EDIT v1 0 went beta August 3rd so you should be good to go now "
38382315,38363618,"stackoverflow.com",3,"2016-07-14 22:06:01+03","2024-05-17 05:04:03.833477+03","So the error in my sfunction json was due to the authorizer field in the custom authorization function not in the endpoint or function that implements the auth code I needed to add this to the sfunction json For anyone struggling with the same issue here is the completed sfunction json for custom authorization I have not figured out how to add a Token Validation Expression yet and I understand there is issues with the console reflecting the region and name but otherwise it works perfect "
38445917,38393872,"stackoverflow.com",0,"2016-07-19 00:00:49+03","2024-05-17 05:04:04.926925+03","All of the output from your lambda function is returned in the response body so you will need to map part of the response body to your API response header will produce payload message test LastModified In this case you would use integration response body LastModified as the mapping expression As a side note naming things body and header in your response body may make the mapping expressions confusing to read Thanks Ryan"
39541785,39500865,"stackoverflow.com",0,"2016-09-17 03:40:15+03","2024-05-17 05:04:05.289549+03","Please post exported Swagger for your API I can confirm there is nothing special about AccessControlAllowCredentials header so you should be able to map this according to your needs Command"
39595088,39507004,"stackoverflow.com",19,"2016-09-28 11:02:38+03","2024-05-17 05:04:06.313642+03","You need to create the custom domain first and upload the certificates This should not be part of this code or your deployment of this piece of software After you have a custom domain you need to think in terms of CloudFormation Like with everything else in Serverless Framework you can leverage CloudFormation templates to administer resources in AWS So your question becomes how can you add your API Gateway to your Custom Domain in CloudFormation hint there is much more help on Google if you search for CloudFormation instead of Serverless Framework By creating a AWSApiGatewayBasePathMapping in CloudFormation is the answer This is done in the Resources in your serverless yml file Like this for example This requires you to have variables for the domainName and stageName in the serverless env yml file for the stages you use Edit For versions of Serverless Framework 1 before rc1 you have to add DependsOn IamPolicyLambda to the pathmapping resource This was fixed in httpsgithub comserverlessserverlesspull1783 Before rc1 you should use RestApiApigEvent instead of ApiGatewayRestApi"
57855415,39507004,"stackoverflow.com",2,"2019-09-09 16:51:43+03","2024-05-17 05:04:06.316643+03","I think this topic deserves an update so I will give it a try Be sure to first create a certificate with Certificate Manager Then be sure your serverless user has the right admin permissions to modify Route53 record sets Then add the following to your serverless yaml Before you deploy run this can take a while Source and additional options can be found here "
39741799,39618450,"stackoverflow.com",0,"2016-09-28 11:19:51+03","2024-05-17 05:04:07.416845+03","An option would be to package all your functions together that is with a serverless yml file at the root only and declare your functions with full path like This will create a unique archive with all your directory structure starting at root At this point you can just access say your Commons source files via relative paths "
39647934,39627727,"stackoverflow.com",2,"2016-09-22 23:01:42+03","2024-05-17 05:04:08.096394+03","API Gateway does not support sending binary responses As an alternative you can have your Lambda store binary data in S3 and return HTTP redirect to the S3 object location via the Location header "
39628534,39627727,"stackoverflow.com",1,"2016-09-22 03:40:33+03","2024-05-17 05:04:08.098394+03","API Gateway does not natively support binary data Some of our customers have had success base64 encoding the data in Lambda including in a JSON reponse and using response mapping template to decode the data to respond to the client "
39706966,39676532,"stackoverflow.com",1,"2016-09-26 18:38:38+03","2024-05-17 05:04:09.050827+03","As mentioned in comments Serverless does not support this by default You should look to add the appropriate resources to your CloudFormation template as a custom resource or create it using the AWS CLI or another SDK "
39721244,39676532,"stackoverflow.com",0,"2016-09-27 12:39:18+03","2024-05-17 05:04:09.052391+03","Used the AWS CLI with the following command"
71958203,39676532,"stackoverflow.com",0,"2022-04-21 19:58:18+03","2024-05-17 05:04:09.053303+03","The above answers are outdated Usage plans and API keys are now supported See here httpswww serverless comframeworkdocsprovidersawseventsapigatewaysettingapikeysforyourrestapi Here is the example from the above document"
40641007,40640433,"stackoverflow.com",3,"2016-11-16 21:47:34+02","2024-05-17 05:04:30.685906+03","AFAIK it is impossible to configure deployment to multiple regions via serverless yml However you can do it via the cli one region at a time You may want to automate it using your own script implement it as a plugin or submit a feature proposal "
46927970,40656603,"stackoverflow.com",10,"2017-10-25 12:14:09+03","2024-05-17 05:04:31.759969+03","Below worked for me I wanted to rename my azure function from HttpTriggerCSharp1 to mynewfunc1 Go to Run below commands Now restart the application NOTE The function code query param changes by doing this "
39856067,39854690,"stackoverflow.com",5,"2016-10-04 18:18:31+03","2024-05-17 05:04:11.815469+03","You will have to place the Lambda function inside the VPC that the ElastiCache cluster resides in Of course once you do that the Lambda function only has access to resources that exist inside the VPC so it will no longer have access to DynamoDB The solution to that is to add a NAT gateway to the VPC which will allow the Lambda function to access resources outside the VPC I would think that setting up the VPC and NAT gateway would fall outside the Serverless framework but I am not an expert in that framework I would suggest looking into configuring that manually via the AWS console or doing it through something like CloudFormation and then simply specifying in your Serverless framework configuration the VPC that it needs to use "
41864245,39854690,"stackoverflow.com",2,"2017-01-26 02:40:52+02","2024-05-17 05:04:11.81747+03","While it is not properly documented you can actually configure VPC directly in the serverless config file see link Version 0 5 Version 1 0"
72544655,39854690,"stackoverflow.com",0,"2022-06-08 14:07:41+03","2024-05-17 05:04:11.818469+03","Adding summary of how I setuped this create a new VPC create 3 private subnets and 2 public subnet create a security group create a new IGW create a new NAT we need 2 route tables lambda configuration references httpsaws amazon compremiumsupportknowledgecenterinternetaccesslambdafunction httpsdocs aws amazon comlambdalatestdgserviceselasticachetutorial html"
39884477,39867904,"stackoverflow.com",9,"2022-08-25 11:35:28+03","2024-05-17 05:04:12.904245+03","I understand your frustration I had the same feeling until I looked deeper into the new version and formed a better understanding One thing to note though is the new version is not completely finished yet So if something is completely missing you can file an issue and have it prioritized before 1 0 is out You are supposed to define multiple functions under the same service under the functions section of serverless yml To package these functions individually exclude code for other functions you will have to set individually true under package section You can then use include and exclude options at the root level and at the function level as well There is an upcoming change that will let you use glob syntax in your include and exclude options example fn js You can find more about packaging here httpswww serverless comframeworkdocsprovidersawsguidedeploying Not sure how to use different roles for different functions under the same service How did you do it with 0 5"
40472151,39867904,"stackoverflow.com",1,"2016-11-07 20:34:27+02","2024-05-17 05:04:12.905825+03","I was trying to find a solution for individual iam roles per function as well I could not find a way to do it but while I was looking through the documentation I found the line Support for separate IAM Roles per function is coming soon on this page so at least we know they are working on it "
64234539,39867904,"stackoverflow.com",1,"2020-10-07 03:26:49+03","2024-05-17 05:04:12.906823+03","The IAM Roles Per Function plugin for Serverless allows you to do exactly what it says on the tin specify roles for each function You can still use the providerlevel roles as well By default function level iamRoleStatements override the provider level definition It is also possible to inherit the provider level definition by specifying the option iamRoleStatementsInherit true EDIT You can also apply a predefined AWS role at both the provider and function level "
39964416,39937095,"stackoverflow.com",1,"2016-10-10 21:20:24+03","2024-05-17 05:04:13.854219+03","Your approach sounds good And as for 3 you can just have your CI runs the local tests deploy to dev or test environment and run lambdaTests You need to make sure lambdaTest command handles failures the throw errors when a lambda request fails or returned unexpected value "
39959109,39956794,"stackoverflow.com",0,"2016-10-10 16:11:04+03","2024-05-17 05:04:14.811133+03","If I understand correctly you are trying to pass a path argument In this case use the following In serverless yml In handler js "
40386678,39978877,"stackoverflow.com",13,"2016-11-02 19:57:37+02","2024-05-17 05:04:15.620285+03","This has been added in a recent release of Serverless however that release breaks the deploy functionality on certain operating systems so the release to go with is 1 1 0 This is done inside the serverless yml file by adding deploymentBucket as a field under provider Example"
59458070,39978877,"stackoverflow.com",1,"2019-12-23 18:29:54+02","2024-05-17 05:04:15.622286+03","This is a functionality added to the serverless framework but there is an a npm module that helps you to implement this feature check it out "
40103231,40070216,"stackoverflow.com",1,"2017-05-23 15:00:43+03","2024-05-17 05:04:16.306769+03","You can use the serverlesspluginwriteenvvars plugin to define environment variables in serverless yml like this Notice the variables must be defined under custom and writeEnvVars The plugin will create a env file during the code deployment to AWS Lambda In the code using pythondotenv you can read the environment variables by loading the env file Just in case if you need help with deploying and loading external dependencies look at httpsstackoverflow coma397916861111215"
50027031,40071125,"stackoverflow.com",31,"2018-07-13 11:24:39+03","2024-05-17 05:04:17.320965+03","You need to install serverlesspythonrequirements and docker Then add the following to your serverless yml Make sure you have your python virtual environment active in CLI Install any dependencies with pip note that in CLI you can tell if venv is active by the venv to the left of the terminal text Make sure you have opened docker then deploy serverless as normal What will happen is that serverlesspythonrequirements will build you python packages in docker using a lambda environment and then zip them up ready to be uploaded with the rest of your code Full guide here"
45947431,40071125,"stackoverflow.com",6,"2017-08-29 22:59:28+03","2024-05-17 05:04:17.323082+03","Now you can use serverlesspythonrequirements It works both for pure Python and libraries needing native compilation using Docker A Serverless v1 x plugin to automatically bundle dependencies from requirements txt and make them available in your PYTHONPATH Requires Serverless v1 12"
40103298,40071125,"stackoverflow.com",0,"2017-05-23 13:30:45+03","2024-05-17 05:04:17.324079+03","The Serverless Framework does not handle the pip install See httpsstackoverflow coma397916861111215 for the solution"
70267365,40174530,"stackoverflow.com",1,"2022-05-13 05:06:40+03","2024-05-17 05:04:18.296593+03","Have you tried defining a cachePolicy under provider as per Serverlesss reference YML and then linking the cachePolicy via a functions CloudFront event Straight from Serverless Frameworks sample YML like this"
40995938,40216890,"stackoverflow.com",0,"2016-12-06 15:03:01+02","2024-05-17 05:04:19.202632+03","Return an error object and it should work Also use callback instead of context to return an error that is the default behavior now context fail works just for legacy support callback new Error Your message "
45177726,40282468,"stackoverflow.com",4,"2018-02-02 18:13:44+02","2024-05-17 05:04:20.040015+03","It is possible to deploy Angular app on serverless environment I could say more it is possible with serverside rendering Angular Universal Check out this boilerplate repo httpsgithub commaciejtrederangularuniversalpwa and live demo httpswww angularuniversalpwa maciejtreder com The app is deployed using Serverless framework httpsserverless com"
40288387,40282468,"stackoverflow.com",3,"2016-10-27 18:17:40+03","2024-05-17 05:04:20.041014+03","Do not think so It says you need to use angular2webpack and that starter guide says it requires node version 5 AWS lambda supports only 4 3 2 Do not know about google functions "
72910271,42880987,"stackoverflow.com",0,"2022-07-08 13:44:44+03","2024-05-17 05:05:06.836726+03","You have issue with your ts files as serverlessoffline plugin cannot find equivalent js file it throws error module not found There is a work around to it to install serverlessplugintypescript Only issue with the plugin is that it creates a new build dist folder with transpiled js files"
40330134,40323897,"stackoverflow.com",0,"2021-01-08 11:16:37+02","2024-05-17 05:04:21.052389+03","You can use your Custom Authorizer function with the oauth2 token Alternatively you can use Cognito with the corresponding IAM roles to manage user access to your AWS resources The second approach which you describe in the flow picture does not need of an auth lambda in front of every request and lets you control access to other AWS resources from your frontend like IoT i e Once you get the secretKey accessKey sessionToken and region you only need to sign every request to APIG You do not need the AWS SDK to do this If you do not want to use the exported API you have to sign the requests yourself It is implemented in the sigV4Client js so it is fairly easy to copy You are going to need few dependencies anyway so why not use the exported API You are using React already which is so big "
40337417,40334566,"stackoverflow.com",3,"2016-10-31 08:18:04+02","2024-05-17 05:04:22.028945+03","Finally figured out by finding these docs httpswebpack github iodocsconfiguration htmlexternals Had to have output libraryTarget commonjs set"
40346670,40335932,"stackoverflow.com",2,"2016-10-31 18:44:27+02","2024-05-17 05:04:23.039945+03","The code you linked does not seem to call the send function on your res object Have you tried adding an end function to it that calls the callback function "
40384980,40384979,"stackoverflow.com",29,"2016-11-02 18:28:09+02","2024-05-17 05:04:23.652431+03","The issue for me was that I was missing the following directory in my Windows PATH env variable Once I added that I could run serverless sls etc "
46538035,40384979,"stackoverflow.com",7,"2017-10-03 08:58:23+03","2024-05-17 05:04:23.653431+03","This is probably a stupid answer but might help someone i hope I was facing the same problem even after adding the path I found that my AppData folder was hidden somehow making it available fixed the issue for me also after making the changes do not forget to open a new CMD D "
54883514,40384979,"stackoverflow.com",6,"2019-02-26 12:36:08+02","2024-05-17 05:04:23.654432+03","I had the same issue when i run the Command Prompt as Administrator it works fine "
70394369,40384979,"stackoverflow.com",1,"2021-12-17 16:03:54+02","2024-05-17 05:04:23.655432+03","I have a stupid answer too I had used nvm to change my node version last week I had changed back to the same version in the meantime but you still have to install serverless again then "
76264538,40384979,"stackoverflow.com",1,"2023-05-16 18:01:11+03","2024-05-17 05:04:23.656432+03","I just run below command and its working fine for me and then npm install g serverless"
41084534,40386505,"stackoverflow.com",31,"2016-12-13 22:12:34+02","2024-05-17 05:04:24.473995+03","EDIT TO YOUR EDIT I did some digging it seems like serverless will automatically disable the schedule if it is not a string Meaning if your entire event is schedule rate 10 minutes it will be enabled But if you have other properties you HAVE TO enable it because it will be disabled by default So your current yml should look like this You can use same input and inputPath in your serverless yml file just like you would do with cloudwatch event rule The only difference from cloudwath rules is that you can actually pass an object and serverless will stringify it for you automatically Example This will be equal to cloudformation event rule with input key1value1key2value2 thus passing json instead of matched event Noticed just now that the question was asked on November 2nd At that time it was not possible to do it but it was implemented soon after the question was asked httpsgithub comserverlessserverlesspull2567"
51728473,40442288,"stackoverflow.com",1,"2018-08-07 16:51:31+03","2024-05-17 05:04:26.673938+03","You are exceeding number of tables created in parallel the easiest way to fix it is using DependsOn You can set a table dependency into another table so this table will not be created until the creation of the depending table In this example table 1 will not be created until table 2 is already created so they are not created in parallel "
40465261,40463255,"stackoverflow.com",0,"2016-11-07 16:29:09+02","2024-05-17 05:04:26.874698+03","You might want to take a look at serverlesspluginwriteenvvars This plugin will create a env file upon deployment with configurations you specify in serverless yml You can still keep your configuration in separate files yml or json and refer to them from serverless yml For example Let us say configconf1stage1config yml consist of You can refer to it from serverless yml For local testing you will need to have a script to create the env file yourself A python example"
41529904,40476193,"stackoverflow.com",1,"2017-01-08 07:50:18+02","2024-05-17 05:04:27.912993+03","Just a note about the accepted answer It is not recommended to release a db connection using the finally block In case something breaks in the lifecycle of the promise chain there is a chance finally may not be called leading to resource leaks This is documented in bluebird docs httpbluebirdjs comdocsapiresourcemanagement html"
40491099,40476193,"stackoverflow.com",0,"2016-11-08 22:21:42+02","2024-05-17 05:04:27.914994+03","Bluebird definitely works I think there might be a problem with ending connection using disposer I use postgresql promise library so cannot really try with mysql but I rewritten it to use disposer and I get same timeout Try ending connection in your finally block EDIT Ok actually releasing connections is bad in lambdas you should be ending them mysqljs docs say When you are done with a connection just call connection release and the connection will return to the pool ready to be used again by someone else You do not really want that That is why you get a timeout When your promise chain ends the connection returns to the pool and WAITS to be used again thus lambda timeouts since it is never ending Use connection end or connection destroy "
40577708,40569505,"stackoverflow.com",1,"2016-11-13 21:06:55+02","2024-05-17 05:04:28.916794+03","I think you are passing an image as JSON format so you can use input body to access the entire body in the template Also you should put single single quote around the parameter name instead of double single quote Like multipartformdata principalId context authorizer principalId body input body boardId input params boardId FYI API Gateway does not support binary data currently I recommend you to send a base64 encoded string of the image to API Gateway then you base64 decoded before you put into S3 in you Lambda function "
47167739,40596182,"stackoverflow.com",17,"2017-11-07 23:45:31+02","2024-05-17 05:04:29.523303+03","I know this is old but I have had this problem recently and solved it so I thought I would put what I found here This answer is based on this forum post which required a bit of context for me to get working httpsforum serverless comtusinganexistingapikey770 Using the resources section it is possible to add custom CloudFormation configs into your deployment This includes adding in a custom usage plan with specific api keys enabled httpsserverless comframeworkdocsprovidersawsguideresources The structure is roughly as follows with explanations below Each of these Resources are named after the key you give them Serverless gives you the name of the serverlessgenerated Resource names in case you wish to overwrite parts of them or reference them You can name them pretty much anything though as long as it matches CloudFormation naming requirements Serverless does add a few variables though Additionally some behaviour about usage plans and usage plan keys You may be interested in creating your auth structure outside of any one api deployment and using CloudFormations via Serverless Outputs service to get the ARN andor ID of each of the resources you have created httpdocs aws amazon comAWSCloudFormationlatestUserGuideoutputssectionstructure html Outputs uses the same format as Resources and an example can be seen in the example aws serverless yml This will allow you to change the usage plans independent of the apis themselves and maintain that separately You can save those outputs for use by your apis using a javascript variable reference to add only the plans that should be enabled on a perstage perapi basis tldr Use the resources structure to make raw CloudFormation configs "
40707990,40692799,"stackoverflow.com",3,"2016-11-20 20:48:45+02","2024-05-17 05:04:32.749899+03","Sorry but you cannot achieve what you want at least not in this way When you call another function using request post you must provide a callback and you must wait for it to finish What you could do is to call request abort but this is a hacky solution Why is this hacky Because you cannot abort immediately You need to wait a few milliseconds for the request to be received in the other server but there is no way to know exactly when it happens If you know that the second Lambda will run for 2 minutes you could add a setTimeout function to abort the request after 5 seconds and avoid having the functionOne running for a long time in a idle state But again this is a bad solution Why do not you place the functionTwo code inside functionOne to avoid starting a second Lambda Another workaround would be to call a SNS topic from functionOne and configure this topic to trigger functionTwo "
68498088,40692799,"stackoverflow.com",0,"2021-07-23 13:56:53+03","2024-05-17 05:04:32.752899+03","In your handler you can use flag callbackWaitsForEmptyEventLoop false"
40711648,40709990,"stackoverflow.com",10,"2016-11-21 03:46:28+02","2024-05-17 05:04:33.611834+03","If you are using S3 you must prerender the pages before uploading them You cannot call Lambda functions on the fly because the crawler will not execute JavaScript You cannot even use Prerender io with S3 Suggestion E g the address from example comaboutus must be mapped as a us html file inside a folder about in your bucket root Now your users and the crawlers will see the exactly the same pages without needing JavaScript to load the initial state The difference is that with JavaScript enabled your framework Angular will load the JS dependencies like routes services etc and take control like a normal SPA application When the user click to browse another page the SPA will reload the inner content without making a full page reload Pros Cons The crawler will see the old content but the user will probably see the current content as the SPA framework will take control of the page and load the inner content again You said that you are using S3 If you want to prerender on the fly you cannot use S3 You need to use the following Route 53 CloudFront API Gateway Lambda Configure Set the API Gateway endpoint as the CloudFront origin Use HTTPS Only in the Origin Policy Protocol CloudFront The Lambda function must be a proxy In this case your Lambda function will know the requested address and will be able to correctly render the requested HTML page Pros Cons "
40954802,40709990,"stackoverflow.com",9,"2016-12-04 04:41:24+02","2024-05-17 05:04:33.614835+03","If you are willing to use CloudFront on top of your S3 bucket there is a new possibility to solve your problem using prerender on the fly Lambda is a new feature that allows code to be executed with low latency when a page is requested With this you can verify if the agent is a crawler and prerender the page for him 01 Dec 2016 announcement Lambda Preview Just last week a comment that I made on Hacker News resulted in an interesting email from an AWS customer Heres how he explained his problem to me In order to properly get indexed by search engines and in order for previews of our content to show up correctly within Facebook and Twitter we need to serve a prerendered version of each of our pages In order to do this every time a normal user hits our site need for them to be served our normal front end from Cloudfront But if the user agent matches Google Facebook Twitter etc we need to instead redirect them the prerendered version of the site Without spilling any beans I let him know that we were very aware of this use case and that we had some interesting solutions in the works Other customers have also let us know that they want to customize their end user experience by making quick decisions out at the edge This feature is currently in preview mode dec2016 but you can request AWS to experiement it "
53430201,40709990,"stackoverflow.com",2,"2020-06-20 12:12:55+03","2024-05-17 05:04:33.617189+03","Heres a solution that uses and is approved by prerender cloud httpsgithub comsanfrancescoprerendercloudlambdaedge This uses Lambda to prerender your app via a make deploy command Taken from the repos README Serverside rendering prerendering via Lambda for singlepage apps hosted on CloudFront with an s3 origin This is a serverless project with a make deploy command that"
63588566,40709990,"stackoverflow.com",0,"2020-08-26 02:43:18+03","2024-05-17 05:04:33.619186+03","There are actually couple of options Mostly will require Cloudfront and Lambda One possible way is to add some logic to your lambda function to check the useragent header of the request to differentiate between requests from crawlers and regular users If request is from crawler you can present a crawler friendly response with meta tags optimized for such request This will definitely require some extra work and it means a lambda execution with almost every request I hope that AWS give us an option to differentiate based on header on the future "
40750604,40750387,"stackoverflow.com",1,"2016-11-22 22:01:03+02","2024-05-17 05:04:34.676851+03","Your structure looks good if a bit flat I like putting code flows together There are usually multiple functions to get to a result Those should be grouped Common functions that cross flows but do not cross projects go into a common folder in project I base my repo organization on overall ideas If lambdas cross projects they go in a common repo Project specific stay in their repo Many times the hardest part of using a serverless architecture is finding the code being called With a good logical grouping you will save yourself many headaches later "
40824191,40823889,"stackoverflow.com",4,"2016-11-27 01:21:51+02","2024-05-17 05:04:35.389847+03","Never mind my syntax was wrong Changin it to above fixed the errors "
41497838,41474358,"stackoverflow.com",21,"2017-02-12 15:41:58+02","2024-05-17 05:04:36.2931+03","From docs you need to create the function role under resources and reference this new role inside your function Example"
74925045,41474358,"stackoverflow.com",0,"2022-12-27 03:15:25+02","2024-05-17 05:04:36.294101+03","Using the module serverlessiamrolesperfunction you can write iamRoleStatements under the name of each function like the questioner Hexy wrote "
41537375,41537178,"stackoverflow.com",0,"2017-01-08 22:14:44+02","2024-05-17 05:04:37.104029+03","You can implement Amazon API Gateway Custom Authorizers which will allow you to implement authentication in whatever manner you would like to see it work within a Lambda function that is called from api gateway "
41560401,41537178,"stackoverflow.com",0,"2017-01-10 04:55:27+02","2024-05-17 05:04:37.10603+03","You can use Cognito User Pools with API Gateway that will allow you to control the user that has access to your API easily Walkthrough Sign up a firsttime user to a specified user pool Sign in a user to the user pool Get the users identity token Call API methods configured with a user pool authorizer supplying the unexpired token in the Authorization header or another header of your choosing When the token expires repeat Step 24 Identity tokens provisioned by Amazon Cognito expire within an hour "
56796267,41544241,"stackoverflow.com",17,"2019-07-03 22:03:42+03","2024-05-17 05:04:38.042603+03","I ran into this issue as well but with Python For me there were a few problems that had to be ironed out It makes sense that sslTrue needed to be set but the connection was just timing out so I went round and round trying to figure out what the problem with the permissionsVPCSG setup was "
58636855,41620437,"stackoverflow.com",1,"2019-10-31 06:19:44+02","2024-05-17 05:04:43.73189+03","If test env you can go to AWS console delete existed table So If you want to create multiple lambda functions share some tables you should create one serverless only handle Dynamodb and remaining services do not contain any Dynamodb config If you want all lambda same API Gateway you can add apiGateway below provider as example below for example serverless A DynamoDB and public endpoint API Gateway have restApiIdxxxxx restApiRootResourceIdyyyyyy serverless B User service and public endpoint users serverless C Vehicle service and public endpoint vehicle"
75557514,41544241,"stackoverflow.com",1,"2023-02-24 16:05:27+02","2024-05-17 05:04:38.044605+03","In my case I had TransitEncryptionEnabled true with AuthToken xxxxx for my redis cluster I ensured that both my lambda and redis cluster belonged to the same private subnet I also ensured that my securityGroup allowed traffic to flow on the desired ports The major issue I faced was that my lambda was unable to fetch data from my redis cluster And whenever it attempted to get the data it would throw timeout error I used Node Js with NodeRedis client Setting option tls true worked for me This is a mandatory setting if you have encryption at transit enabled Here is my config Hope this answer is helpful to those who are using Node Js with NodeRedis dependency in their lambda "
42811650,41544241,"stackoverflow.com",0,"2017-03-15 15:47:28+02","2024-05-17 05:04:38.046533+03","As Tolbahady pointed out the only solution was to create an NAT within the VPC "
77656233,41544241,"stackoverflow.com",0,"2024-03-20 20:22:55+02","2024-05-17 05:04:38.047531+03","feus4177 is right With encryption enabled you need to set tls true"
41571893,41569982,"stackoverflow.com",1,"2020-06-20 12:12:55+03","2024-05-17 05:04:38.786611+03","According to the Serverless IAM documentation By default one IAM Role is shared by all of the Lambda functions in your service An IAM Policy is also created and is attached to that Role Also by default your Lambda functions have permission create and write to CloudWatch logs and if you have specified VPC security groups and subnets for your Functions to use then the EC2 rights necessary to attach to the VPC via an ENI will be added into the default IAM Policy To add specific rights to this servicewide Role define statements in provider iamRoleStatements which will be merged into the generated policy To invoke a Lambda function from another function you just need to add the lambdaInvokeFunction action to the existing IAM permissions Serverless already provides So an example serverless yml service should have a iamRoleStatements section that looks like this In reference to the other answer cited"
41600501,41570810,"stackoverflow.com",4,"2017-01-11 22:53:41+02","2024-05-17 05:04:39.888456+03","Not enough rep to add a comment have you tried adding a DependsOn attribute to the Lambda Permission resource Explicitly setting that property will result in CloudFormation waiting until the Lambda Function resource is created before creating this permission Also if you were not already aware the serverless folder that gets created in the root of your project contains the CloudFormation templates used by serverless which can be helpful when troubleshooting unexpected CloudFormation behavior "
50755622,41570810,"stackoverflow.com",3,"2019-05-24 00:03:16+03","2024-05-17 05:04:39.889564+03","By default Serverless creates your custom resources first which makes sense as you usually put S3 buckets etc there that your functions rely on In the end though Serverless translates everything to a Cloudformation template which you can see in the serverless directory What you will notice there is that your function names are suffixed with LambdaFunction So if you named your function Foo this is translated to FooLambdaFunction By that name you can reference the function in a custom resource which makes Cloudformation wait for the function before it creates the resource E g "
41574518,41572237,"stackoverflow.com",12,"2017-01-10 19:15:16+02","2024-05-17 05:04:40.80472+03","There is no official AWS CloudFormation resource that will manage adddelete an individual S3 Object within a Bucket but you can create one with a Custom Resource that uses a Lambda function to call the PUT Object DELETE Object APIs using the AWS SDK for NodeJS Heres a complete example CloudFormation template You should be able to also use these resources within a serverless yml configuration file though I am not positive about how exactly Serverless integrates with CloudFormation resourcesparameters "
57177131,41572237,"stackoverflow.com",4,"2021-12-06 22:36:56+02","2024-05-17 05:04:40.806721+03","Have you looked into the serverlesss3sync plugin It allows you to upload a folder to S3 from your local machine as part of the deployment "
57294874,41572237,"stackoverflow.com",3,"2019-07-31 19:02:32+03","2024-05-17 05:04:40.807722+03","If you are doing this to deploy a website you can use serverlessfinch and it will create the bucket for you automatically if it does not already exist "
41576816,41576552,"stackoverflow.com",4,"2017-01-10 21:58:35+02","2024-05-17 05:04:41.857341+03","You can use if variable to check if a variable is not null In your use case you can try to put the null check around the authorization header like this "
41628839,41590483,"stackoverflow.com",4,"2017-01-15 12:03:55+02","2024-05-17 05:04:42.621222+03","The reason the default hello Lambda function is not listed in the AWS Lambda console is because your Lambda function was uploaded to the default region useast1 while the Lambda console displays the functions of another region To set the correct region for your functions you can use the region field of the serverless yml file Make sure the region property is directly under the provider section With 24 spaces indent Like this Alternatively you can specify the region at deployment time like so"
41591839,41590483,"stackoverflow.com",0,"2017-01-11 15:28:47+02","2024-05-17 05:04:42.623223+03","Duh I had made a super silly mistake So I was looking for a lambda function in the wrong region of course it could not be found Before deploying one must make sure to set the correct region UPDATE Well actually I had set the region in serverless yml by providing However for some reason the default region was not overwritten and the function was deployed to the wrong region Odd that Anyway one easy way around this issue is to provide the region at deployment time"
56163745,41620437,"stackoverflow.com",12,"2019-05-16 11:04:39+03","2024-05-17 05:04:43.726997+03","Backup the table and delete it Restore once deployed I had the same issue when I renamed my serverless project and tried to deploy I had a table tanks that was being used again It failed with the following I got it solved by making a backup of the tanks table then deleting it After deleting one must wait for five minutes because the caches are cleaned periodically and not immediately After that I tried serverless deploy and it worked After this you will need to restore the backed up database "
54287691,41620437,"stackoverflow.com",11,"2019-01-21 12:13:28+02","2024-05-17 05:04:43.728159+03","This question is fairly old but still shows up in the top 5 results in Google so here is a bit of an insight It is a good idea to split up your serverless stack into multiple services each having its own serverless yml file One for your API and lambdas one for your DynamoDB This way you can deal with them separately and update your lambdas without touching your DB Here is a great guide on that"
41992040,41620437,"stackoverflow.com",4,"2017-02-02 14:07:53+02","2024-05-17 05:04:43.729533+03","Its not possible atm with [email protected] I just open an issue on github httpsgithub comserverlessserverlessissues3183 Please everyone that need this feature join this discussion I will update here with news "
63563832,41620437,"stackoverflow.com",2,"2020-08-24 18:15:28+03","2024-05-17 05:04:43.730676+03","have struggled with this for 2 weeks together with other deployment issues now i would say table alredy exists or queue does not exist and similar is all about you are trying to violate stack security since i redeployed all from scratch and restored dbs i have almost always stable deployment with changes in resources till some colleague does something manually so to not have such issues would better"
45257807,41620437,"stackoverflow.com",0,"2017-07-22 21:20:48+03","2024-05-17 05:04:43.733345+03","I had same problem and remove the xxxDynamoDbTable in serverless yml that item already exists in your Dynamodb could work you do not have to Resources it again D"
73215257,42880987,"stackoverflow.com",1,"2022-08-03 03:58:50+03","2024-05-17 05:05:06.837726+03","For anyone developing python lambda functions with serverlessoffline and using a virtual environment during local development deactivate your environment delete it entirely and recreate it Install all python requirements and try again This worked for me "
77222960,41620437,"stackoverflow.com",0,"2023-10-03 16:59:40+03","2024-05-17 05:04:43.734342+03","If you really want to avoid deleting the table then it is best to stop managing the AWS Resource as part of your lambda function stack Instead you can import the production table resource from some external source The serverless team Will not Fixed a feature request to fix this What we deal with here is not a limitation of a Framework per se but limitation of CloudFormation through which Framework deploys configured services While what is being requested seems now kind of possible with CloudFormation via combination of handling of DeletionPolicy and introduced not far ago import resources capability Trying to tackle this generically seems far from trivial It may require tons of work and new issues to fight with as already observed by kennu Due to implied complexity this does not seem as right direction It seems more reasonable to agree that resources as configured with a Serverless service are inseparable part of a service and are meant to also be removed with service when we remove it with sls remove For cases when we do not find that acceptable we should rather configure resources in question externally Note that Framework in many places allows to attach to existing created and configured externally resources "
60697883,41620437,"stackoverflow.com",1,"2020-03-15 23:35:00+02","2024-05-17 05:04:43.736343+03","Check to make sure you are not accidentally creating a new instance of the CloudFormation stack perhaps with a different name If you are deploying using the same name as a stack that already exists it should simply be updating everything However if you are inadvertently creating a new stack because the name changed for example myappprod vs myappproduction this failure could occur because your myappprod already created the tables and myappproduction is trying to recreate those same tables which would fail since they already exist "
41683457,41633800,"stackoverflow.com",26,"2017-01-16 21:11:35+02","2024-05-17 05:04:44.281735+03","The AWS SDK does not include a local plugin for using Cognito User Pools at this time however we have heard this request from other customers and will consider it in future releases "
58973957,41633800,"stackoverflow.com",5,"2019-11-21 13:30:17+02","2024-05-17 05:04:44.282735+03","Recently localstack released a pro version that actually allows to run the cognito locally It also has several other services that normally are used with cognito so this is the perfect solution for me httpsgithub comlocalstacklocalstack"
70046770,41633800,"stackoverflow.com",4,"2021-11-20 16:21:11+02","2024-05-17 05:04:44.284736+03","Also there is an open source contributor created node packagedocker image emulating some parts of Cognito but not all httpsgithub comjagregorycognitolocal Have not used it personally but looking forward to it "
41657535,41640394,"stackoverflow.com",2,"2017-01-15 05:29:15+02","2024-05-17 05:04:44.999714+03","and I found the answer of my question by myself For changing header serverless yml setting must be and responding code must be One confusing point is in response template and integration response body in setting of header mapping are same meaning So body is same with integration response body body To Changing Status code we should use Error object "
41656823,41640394,"stackoverflow.com",1,"2017-01-15 03:15:17+02","2024-05-17 05:04:45.001714+03","To get path_info setting must use lambda integration You can access the path parameters using Lambda Proxy integration by using Just remember to check event[pathParameters] null first if there is a chance your Lambda can be called without any path parameters "
41667190,41657302,"stackoverflow.com",3,"2020-06-20 12:12:55+03","2024-05-17 05:04:45.947647+03","If you want to force the response as a binary response you can set CONVERT_TO_BINARY to the contentHandling on integration response via AWS CLI or via API Currently we are lack of this option on the console "
41664843,41664708,"stackoverflow.com",43,"2017-01-15 20:41:19+02","2024-05-17 05:04:47.564056+03","Yes Serverless v1 5 support to Cognito user pool authorizer If you use previous version of serverless you have to update v1 5 or later For the userpool authorization of api end point you have to specify pool arn More details read this article "
50808067,41664708,"stackoverflow.com",31,"2018-06-12 04:23:31+03","2024-05-17 05:04:47.565057+03","If you want to set the authorizer to a Cognito User Pool you have declared in your resources you must use CloudFormation to create the authorizer as well "
55886896,41664708,"stackoverflow.com",7,"2019-04-28 06:58:10+03","2024-05-17 05:04:47.566057+03","Serverless 1 35 1 In case someone stumbles across this how I did Here is my working solution Wherever you create the user pool you can go ahead and add ApiGatewayAuthorizer Then when you define your functions"
41710083,41683730,"stackoverflow.com",4,"2017-01-18 03:55:16+02","2024-05-17 05:04:48.030038+03","Unfortunately you cannot use commas in Lambda environment variables This is an AWS limitation and not a Serverless issue For example browse the AWS console and try to add a environment variable that contains a comma When you save you will get the following error 1 validation error detected Value at environment variables failed to satisfy constraint Map value must satisfy constraint [Member must satisfy regular expression pattern [^]] The error message says that the regex [^] must be satisfied and what this small regex explicitly says is to not ^ accept the comma Any other char is acceptable I do not know why they do not accept the comma and this is not explained in their documentation but at least their error message shows that it is intentional As a workaround you can replace your commas by another symbol like to create the env var and replace it back to comma after reading the variable or you will need to create multiple env vars to store the endpoints "
41779299,41779051,"stackoverflow.com",3,"2017-01-21 14:23:45+02","2024-05-17 05:04:48.939678+03","Following the same link your mistake is related with this restriction So fix it using"
41967464,41789192,"stackoverflow.com",4,"2017-01-31 23:04:13+02","2024-05-17 05:04:49.922673+03","This is an interesting question and the answer is that you can certainly build a website using Azure functions You can create a http trigger that returns a static html file that you include in your function If you want you can add Javascript or jQuery logic to that file For even more functionality you can create more functions that will serve as an API which you can query from your html file The API could e g utilize Azure storage where you can cheaply store data in tables or blobs Using this pattern you could in theory build complex web apps on a serverless architecture I would guess however that you would probably find it a bit cumbersome to work with as the tooling is not really great yet On the other hand you would never need to think about hostingscaling so that is a big plus Just to prove my theory I implemented a small function app with two c http trigger functions One that serves an html file another one for the api that returns a number It is just thrown together quickly to prove the point so I apologize if the code is messy HttpTriggerCSharp1 HttpTriggerCSharp2 index html Note that you will have to add the index html file to your function files collection You can try it out at httpstestfunctionwebsite azurewebsites netapiHttpTriggerCSharp1"
41789607,41789192,"stackoverflow.com",1,"2017-01-22 12:06:25+02","2024-05-17 05:04:49.924728+03","For storing static files you can use Azure Storage that is comparable to AWS S3 httpsblogs msdn microsoft commake_it_better20160809simplewebsitesusingazurestorageblobservice For storing data there are many options but if you are looking for NoSQL style DB implied by you reference of AWS SimpleDB then you can use Azure DocumentDB httpslearn microsoft comenusazuredocumentdbdocumentdbintroduction Both components are serverless and pay as you go style billing "
71129903,42880987,"stackoverflow.com",0,"2022-02-15 18:21:59+02","2024-05-17 05:05:06.835725+03","My case was configuring params for creating an AWS lambda function The right string for a handler was last row Where myFileName is the name of a file that I use as the zip file "
51344939,41789192,"stackoverflow.com",0,"2018-07-15 06:15:06+03","2024-05-17 05:04:49.926536+03","Azure storage accounts now support hosting static websites currently in preview to provide serverless websites This works for static websites like Angular and React SPA applications but does not provide any server side compute like ASP NET MVC It can be used with Azure Functions Proxies for additional features I wrote a post highlight the some features and how to set one up httpwww deliveron comblogserverlesswebsitesazurenewstorageaccountstaticwebsites"
41863125,41860539,"stackoverflow.com",13,"2017-01-26 16:35:29+02","2024-05-17 05:04:50.446411+03","You can define the AllowedOrigin with the following statement Ref ApiGatewayRestApi references the internal name of the generated API "
41876249,41860851,"stackoverflow.com",5,"2017-01-26 16:54:54+02","2024-05-17 05:04:51.145747+03",""
41886270,41860851,"stackoverflow.com",5,"2017-01-27 04:36:48+02","2024-05-17 05:04:51.146747+03","You can set DeletionPolicy Retain when creating your DynamoDB tables to prevent them from being accidentally deleted by Cloud Formation If your Lambda is invoked by SNS then you could simply fail when the SES limit is exceeded SNS would then reattempt delivery using backoffs "
42357864,41860851,"stackoverflow.com",3,"2017-02-21 05:00:56+02","2024-05-17 05:04:51.148747+03","My approach at the moment is to create the dynamodb in a separate process So my serverless setup is readonly no db creation Because I do not think I would be recreating my db that often "
41935428,41931892,"stackoverflow.com",22,"2017-01-30 14:01:09+02","2024-05-17 05:04:52.107391+03","From docs So try this cron expression for 1130 PM at the last day of the month"
41954275,41948509,"stackoverflow.com",6,"2017-04-17 11:31:53+03","2024-05-17 05:04:53.853807+03","It has to do with your ANY proxy method As stated here httpdocs aws amazon comapigatewaylatestdeveloperguidehowtocors html Note When applying the above instructions to the ANY method in a proxy integration any applicable CORS headers will not be set Instead you rely on the integration back end to return the applicable CORS headers such as AccessControlAllowOrigin So you will have to make your backend API return the appropriate CORS headers "
43906841,41948509,"stackoverflow.com",0,"2017-05-11 07:58:36+03","2024-05-17 05:04:53.855726+03","You need to have the header on your server as well as the api gateway See this sample The cors header is applied to the static bucket website httpswww codeproject comArticles1178781ServerlessArchitectureusingCsharpandAWSAmazo"
48264295,41948509,"stackoverflow.com",0,"2018-01-15 15:47:49+02","2024-05-17 05:04:53.856729+03","For the APIs to work properly two things must be done 1 The options method must be correctly setup usually done using a mock method on the API gateway 2 The HTTP method implementations in your code must return the CORS header correctly There are quite a few articles about this if you search For me the problem was Point 1 using the API Gateway Enable CORS button did not work for me when I was developing APIGateway Lambda integration using NET Core I also did not find a way to add creation of the options method in the serverless template file Heres another way to do it after publishing the lambdas from CLI or VisualStudio fire a PUT request on the API endpoint and pass a swagger definition which contains the options method defs and ensure you set the query param modemerge You can use PostMan to do this or You use a DotNet utility which does the same thing explained here httpsbytestream pythonanywhere comblogEnablingAPIGatewayCORS The source code is available on GitHub too "
42047635,41961359,"stackoverflow.com",1,"2017-02-05 03:30:24+02","2024-05-17 05:04:54.891655+03","It is possible but I have not estimated the latency considerations involved in starting Titan in a Lambda function For high request rates write loads may not be appropriate as each lambda container will try to secure one range of ids from the titan_ids table and you may run out of ids quickly If your requests are readonly one way to reduce Titan launch time is to open the graph in readonly mode In readonly mode Titan does not need to get an id range lease from titanids either "
41979068,41974569,"stackoverflow.com",12,"2017-02-01 14:47:33+02","2024-05-17 05:04:55.99061+03","If you want to find the API Gateway URL that triggered the Lambda function you need to check the event variable that your Lambda function receives If you want to build the API Gateway URL example httpsabcdefghij executeapi useast1 amazonaws comdevmyserviceresource use Complete test example Note if you test this directly in the AWS Lambda Console it may throw an error because the event object will be empty and without the headers and requestContext properties So try this using the API Gateway console or browsing the URL directly "
44601210,42085694,"stackoverflow.com",14,"2017-06-17 08:41:56+03","2024-05-17 05:04:59.05103+03","OK I have figured out how to do this with real Chrome Devtools with the excellent node inspect This is much better than node inspector because it uses the latest built in chrome devtools more info on node inspect node debugbrk inspect which serverless invoke local f myfunctionname I ran that but my function was not loaded yet probably some lazy loading in the serverless code So I added a debugger to the top line of my function and everything seems to be working great In my case I also needed some test data so I passed that through like this I am working on a Mac and I heard there might be some problems with which serverless on windows But someone give it a shot and let me know "
52136432,42085694,"stackoverflow.com",8,"2018-09-03 00:27:36+03","2024-05-17 05:04:59.052727+03","I was able to setup my PHPStorm debugger configuration in a way that now I can step through my functions locally using serverlessoffline plugin I am triggering functions via http requests using Postman See below steps to achieve this 1 2 3 4 "
44446337,42085694,"stackoverflow.com",2,"2017-06-09 01:08:52+03","2024-05-17 05:04:59.053724+03","I have gotten this to work in IntelliJ so it should work in Webstorm too You will need the serverlessoffline plugin httpsgithub comdheraultserverlessoffline In your Run Configuration change your Application parameters to offline s dev and add the environment variable SLS_DEBUG serverlessoffline will start a server that the IntelliJ Node debugger can hook into "
43357707,42085694,"stackoverflow.com",1,"2017-04-12 02:15:02+03","2024-05-17 05:04:59.055724+03","Use this it emulates lambda and serverless httpsgithub comdheraultserverlessofflinedebugprocess"
42188499,42034563,"stackoverflow.com",9,"2017-02-12 16:03:30+02","2024-05-17 05:04:59.060725+03","I was trying to solve the same problem and this question and the self answer were very helpful However I want to add another answer with more details and a working example to help future readers There are two things that you may need 1 Start a State Machine 2 Invoke one specific function from a State Machine usually for testing purposes The following demo uses both cases First we need to configure the serverless yml file to declare the State Machine the Lambda functions and the correct IAM permissions Define the Lambda functions inside the handler js file Deploy executing Test using"
42182291,42034563,"stackoverflow.com",1,"2017-02-12 16:04:25+02","2024-05-17 05:04:59.062512+03","Solved using serverless environment variables In the function"
47140308,42034563,"stackoverflow.com",0,"2017-11-06 17:25:38+02","2024-05-17 05:04:59.063526+03","Here is how you solve it nowadays In your serverless yml define your stepFunctions and also Outputs Then you can set your state machine ARN as an environment using selfresources Outputs fipeStateMachine Value "
42097077,42095258,"stackoverflow.com",3,"2017-02-07 20:17:04+02","2024-05-17 05:04:59.116546+03","The Reference to an emptystring in your iamRoleStatements section Ref is likely causing the Unresolved resource dependencies [] error Remove this line from your template since it seems to be unnecessary "
65897007,43302843,"stackoverflow.com",1,"2021-01-26 08:54:09+02","2024-05-17 05:05:19.592111+03","I was able to solve this issue by using Chocolaty Instead of npm install serverless use choco install serverless for windows Ref httpswww serverless comframeworkdocsgettingstarted"
42124132,42124131,"stackoverflow.com",1,"2017-02-08 23:39:23+02","2024-05-17 05:04:59.162991+03","The obvious solution is adding s local to the command This was starting to get too verbose for my liking however and it also increases the probability of accidentally deploying to a new stage called local which is obviously undesirable So I created this helper bash function Usage invoke stage function_name [payload_name] Examples will invoke the function locally with the payload at testspayloadsmyFunctiondefault json while applying the local stage env will invoke the deployed function with stage dev and payload testspayloadsmyFunctionmy_payload json the stage env will be correct if the deployed service has the appropriate serverless yml file This is clearly an opniniated implementation but feel free to modify it to your liking "
42196921,42158768,"stackoverflow.com",0,"2017-02-13 06:58:38+02","2024-05-17 05:05:01.541582+03","It depends on how the request mapping template is defined In API GW console testing please check the CW logs to see what is sent from API GW to Lambda under Endpoint request body after transformations "
42199189,42197896,"stackoverflow.com",5,"2017-02-13 10:00:32+02","2024-05-17 05:05:01.620635+03","You can use greedy path variables like myservicesum proxy and the lambda proxy integration used by the serveless framework to solve this use case "
42216318,42197896,"stackoverflow.com",3,"2017-02-14 03:31:39+02","2024-05-17 05:05:01.621635+03","You can use the standard API Gateway proxy features no need to use the serverless framework although the framework is great for other reasons In the console it is pretty easy this guide should take you through the setup httpdocs aws amazon comapigatewaylatestdeveloperguideapigatewaycreateapiassimpleproxyforlambda html"
42285893,42285813,"stackoverflow.com",11,"2017-02-17 00:28:05+02","2024-05-17 05:05:02.040552+03","On the first line after your functions module definition add the following line callbackWaitsForEmptyEventLoop You can set this property to false to request AWS Lambda to freeze the process soon after the callback is called even if there are events in the event loop AWS Lambda will freeze the process any state data and the events in the Node js event loop any remaining events in the event loop processed when the Lambda function is called next and if AWS Lambda chooses to use the frozen process More details read this article"
42298730,42285813,"stackoverflow.com",0,"2017-02-17 15:04:25+02","2024-05-17 05:05:02.042552+03","You may use the old context done function to return immediately or more specifically context succeed context fail This function is still available on Node 4 Though it does not abruptly ends the running Lambda but gives a response to the caller like API Gateway and keep running on the background if needed for at most 15 seconds Funny extra if you schedules a function to run a little later using setTimeout you have those 15 seconds to run free of charge because Lambda only holds charge to explicitly asynchronous functions calls "
42300056,42299974,"stackoverflow.com",5,"2017-02-17 16:13:36+02","2024-05-17 05:05:03.66056+03","I change callback first parms to null and work fine Ref"
42395096,42299974,"stackoverflow.com",0,"2017-02-22 17:03:27+02","2024-05-17 05:05:03.66156+03","This is a common pattern in Node js and its called ErrorFirst Callbacks Basically if you pass a first argument into your callback it will be considered and handled as an Error As you mentioned once you put a callback null response it all worked as expected since the first argument is null "
42336176,42302798,"stackoverflow.com",12,"2017-04-21 23:50:48+03","2024-05-17 05:05:04.219006+03","I think there are two issues here The function name is createMovie so you need to say serverless invoke f createMovie l Check Cloudwatch log for the internal server error I am guessing it is the dynamodb table name You have created moviesTwo but you have TableName movies Hope this helps "
52708435,42302798,"stackoverflow.com",11,"2018-10-08 21:52:00+03","2024-05-17 05:05:04.220647+03","You can try to view the functions that are deployed so you can debug the problem further "
42519647,42302980,"stackoverflow.com",0,"2017-03-01 00:08:55+02","2024-05-17 05:05:05.154206+03","Faced the same issue You need to connect to AWS explorer in visual studio before creating a project Create a separate account in AWS console portal and try again "
68820493,42877521,"stackoverflow.com",14,"2021-08-17 18:53:13+03","2024-05-17 05:05:06.077664+03","UPDATE DEC 2019 AWS now also offers Provisioned Concurrency httpsaws amazon comblogsawsnewprovisionedconcurrencyforlambdafunctions Basically you pay around 10month for a 1GB Lambda per instance that you want to keep warm "
46704752,42877521,"stackoverflow.com",10,"2017-10-12 11:21:06+03","2024-05-17 05:05:06.078665+03","BBC has published a nice article on iPlayer engineering where they describe a similar issue They have chosen to call the function every few minutes using CloudWatch Scheduled Events So in theory it should just stay there except it might not So we have set up a scheduled event to keep the container warm We invoke the function every couple of minutes not to do any processing but to make sure weve got the model ready This accounts for a very small percentage of invocations but helps us mitigate race conditions in the model download We also limit artificially how many lambdas we invoke in parallel as an additional measure "
42881471,42880987,"stackoverflow.com",50,"2017-03-19 02:20:54+02","2024-05-17 05:05:06.824998+03","Did you npm install in your working directory before doing your serverless deploy The awssdk node module is available to all lambda functions but for all other node dependencies you must install them so they will be packaged with your lambda when you deploy You may find this issue on the serverless repository helpful httpsgithub comserverlessserverlessissues948 "
52641534,42880987,"stackoverflow.com",10,"2018-10-04 10:44:03+03","2024-05-17 05:05:06.825998+03","I fixed this error when in package json I moved everything from devDependencies to dependencies Cheers"
73136555,42880987,"stackoverflow.com",3,"2022-07-27 13:47:58+03","2024-05-17 05:05:06.826999+03","I was doing some stupidity But still wanted to put here so any beginner like me should not struggle for it I copied the serverless xml from example where handler value was But my index js was in src folder Hence I was getting file not found It worked after I change the handler value to"
45853238,42880987,"stackoverflow.com",2,"2017-08-24 07:55:51+03","2024-05-17 05:05:06.827999+03","I have the same problem with serverless framework to deploy multiple lambda functions I fixed by the following steps This should solve your problem"
66977498,42880987,"stackoverflow.com",2,"2021-04-07 01:56:15+03","2024-05-17 05:05:06.828999+03","I do not if it applies to this answer but in case someone just needs a brain refresh I forgot to export my handler and was exporting the file with was looking for a default export that did not exist changed from this handler foldernameexports to this handler foldernameexports handler"
74448621,42880987,"stackoverflow.com",1,"2022-11-15 17:57:58+02","2024-05-17 05:05:06.831+03","I went to cloud watch and looked for the missing packages Then npm i missing package and do sls deploy The missing packages are needed in the dependencies in my case there was some on the devDepencies and another missing"
42882089,42880987,"stackoverflow.com",0,"2017-05-23 15:25:47+03","2024-05-17 05:05:06.831674+03","You need to do the package deployment in case you have external dependencies Please see this answer AWS Node JS with Request Reference httpdocs aws amazon comlambdalatestdgnodejscreatedeploymentpkg html"
68759677,42880987,"stackoverflow.com",0,"2021-08-12 17:51:09+03","2024-05-17 05:05:06.832728+03","For me the issue was that the handler file name contained a dot mainhandler graphql js caused serverless Error Cannot find module main when I changed the file to mainhandlergraphql js everything worked "
69942509,42880987,"stackoverflow.com",0,"2021-11-12 13:52:01+02","2024-05-17 05:05:06.833725+03","In several cases do not forget to check global serverless installation mine solved by reinstalling"
69985285,42880987,"stackoverflow.com",0,"2021-11-16 09:31:53+02","2024-05-17 05:05:06.834725+03","In my case what worked was switching to node 10 via nvm I was using a newer version of node v15 14 0 than was probably supported by the packages "
42894326,42894202,"stackoverflow.com",7,"2017-03-20 03:02:00+02","2024-05-17 05:05:07.960502+03","You can create nested stacks that would also have the advantage of simpler testing improving reuse and using different roles Common practice is to separate out different layers into different stacks For example build the VPC in one stack deploy backend in another stack and the frontend in another stack See Use Nested Stacks to Create Reusable Templates and Support Role Specialization"
65582935,42894202,"stackoverflow.com",3,"2021-01-05 18:41:21+02","2024-05-17 05:05:07.961502+03","The resource limit is now 500 if that helps AWS CloudFormation now supports increased limits on five service quotas"
50581946,42894202,"stackoverflow.com",1,"2018-05-29 13:21:37+03","2024-05-17 05:05:07.962503+03","I have to face the same problem in the serverless framework What I do Create microservices for each module like Authentication Usermanagement SMS Gateway Notification etc that helps to manage code and AWS resources At the end expose API to create AWS custom domain and assign cloud formation to it I follow this blog it is help and Serverless also suggest flow link "
42914317,42910561,"stackoverflow.com",5,"2017-03-20 23:31:18+02","2024-05-17 05:05:08.426045+03","I would recommend using the serverlesswebpack plugin It is hard to tell without seeing the entire serverless yml file but I would assume that serverless is trying to deploy the functions listed under functions which in your case are written in a syntax not understood by the Node js 4 3 runtime on AWS lambda A good walk through on how to set up a project using the serverlesswebpack plugin has been detailed by Serverless Stack "
55129089,42910561,"stackoverflow.com",3,"2019-08-19 18:58:28+03","2024-05-17 05:05:08.428046+03","node version serverless plugin install name serverlesswebpack"
42924617,42910561,"stackoverflow.com",1,"2017-03-21 12:35:30+02","2024-05-17 05:05:08.429046+03","One possible solution to the error is to remove the src from the handler of the function in serverless yml file This approach has the side effect that when automatically creating tests with serverlessmochaplugin the src is no longer considered and it must be added manually in const mod require srcuser js There may be other side effects absence of evidence is not evidence of absence So I am still looking for a solution without side effects "
42965101,42924784,"stackoverflow.com",0,"2017-03-23 02:29:23+02","2024-05-17 05:05:09.495967+03","As mentioned OpenWhisk is one option but another would be Iron ios Functions platform which is opensource httpopen iron io Ultimately many of the solutions you will likely encounter will include the use of Docker So you might just want to start with Docker and see how it works for you The challenges then become orchestration for which there is a few players in the space including Kubernetes You may also want to look into some of the products and services Datawire offers httpswww datawire io Once you are running Docker you can do pretty much whatever you want with regard to hosting When you reach for services like AWS Lambda or ECS then you will be a bit more stuck However you will need to take care of many more details yourself "
42949461,42938365,"stackoverflow.com",2,"2017-05-23 15:10:16+03","2024-05-17 05:05:10.385544+03","It seems that your problem is that you do not handle the OPTIONS verb See this answer to understand more about this requirement Before making the PUT request the browser will preflight an OPTIONS request as a safety measure to ensure that this kind of request is expected by the server If you do not support OPTIONS it will not make the PUT request To fix this change your serverless yml file to add OPTIONS support Modify your handle with something like this"
62017048,42938365,"stackoverflow.com",0,"2020-05-26 10:47:43+03","2024-05-17 05:05:10.387545+03","You can try by increasing the time out in lambda This fixes CORS problem of mine "
43056793,42950758,"stackoverflow.com",3,"2017-04-26 11:18:34+03","2024-05-17 05:05:11.515638+03","I made it work using sharp0 17 3awslinuxx64node6 10 1 tar gz tarball that was created on AWS EC2 instance running Nodejs 6 10 1 The tarball contains node_modules directory with sharp system binaries libvips library specific to the Lambda execution environment To avoid conflicts between my local node_modules Nodejs 7 5 on Mac and node_modules inside the tarball Nodejs 6 10 on Linux I am creating my Lambda service under a subdirectory Project structure looks as follows Most of the dependencies I need are for development and testing purpose These are maintained inside root package json file also includes sharp but compiled for my Nodejs 7 5 environment offering to test image manipulations locally My servicehandler js and serviceutils contains ES6 compatible source code with Lambda function handler it is transpiled from src directory If I need other dependencies for production besides sharp I install them to servicespackage json using prefix option But not awslambda neither awssdk they are globally installed within Lambda meaning no need to include them in deployable zip file It ensures installation of lodash version compatible with Lambda environment because servicepackage json defines Nodejs version to rely on However there is a nuance other production dependencies does not have to be environment dependent If so they will not work because you install them from your local machine which is not equal to Lambdas one Since Lambda requires zip archive I compress contents of my service directory And my Lambda functions works Everything is ES6 compatible sharp has Lambda environment binaries and my other production dependency versions correlates with Nodejs 6 10 1 Additionally I would suggest to use Serverless I use it too It dramatically simplifies Lambda functions development and deployment "
43382405,42950758,"stackoverflow.com",1,"2017-04-13 05:06:42+03","2024-05-17 05:05:11.519547+03","Niks answer definitely helped me to get to a working solution One thing that I would add is that the people behind serverlesssharpimage updated their package so the tarball works with node v6 10 now so I do not see a reason to have two different node environments referenced I do everything in v6 10 httpsgithub comadieuadieuserverlesssharpimagetreemasterlib"
54973557,42950758,"stackoverflow.com",1,"2019-03-03 22:44:32+02","2024-05-17 05:05:11.521245+03","Had similar problem and managed to install binaries for Linux x64 platform by Then just upload Lambda as usual and it works just fine Above works on Mac as well as windows and details are in documentation at httpsharp pixelplumbing comenstableinstallawslambda"
68758038,42950758,"stackoverflow.com",1,"2021-08-12 16:04:17+03","2024-05-17 05:05:11.522366+03","For anyone stumbling upon this post now I have accomplished this by copying my package json file into an AWS Cloud9 IDE and simply running npm install From there just download the node_modules folder "
43695656,42963184,"stackoverflow.com",40,"2017-04-29 15:20:46+03","2024-05-17 05:05:12.645474+03","If you select input option Constant JSON text an input box should appear just below You need to enter the json ping true Your function will get the json as the event object and you can access event ping just like your code If you are using the serverless framework instead of doing this in AWS console you can add an schedule event for your function This would be in addition to your existing http event You can add the ping true parameter under the input section of the scheduled event like this This will create and enable the cloudwatch log event with the specified schedule and send the input parameter ping in the event object "
43523777,43520629,"stackoverflow.com",2,"2017-04-20 18:24:35+03","2024-05-17 05:05:28.108981+03","The stack trace suggests that the rc module is causing the error That module is used by a module called getproxy to determine if there are any HTTPHTTPS proxies configured It is trying to read the NPM configuration which is typically stored in a file called npmrc The error suggests there is an issue with that file "
54337327,54314930,"stackoverflow.com",0,"2019-01-24 01:36:19+02","2024-05-17 05:13:12.90154+03","I succeed to deploy my code to lambda function running Node 8 1 using Babel version 6 I had exacty the same problem as you mention above The solution for me"
42996349,42995058,"stackoverflow.com",2,"2017-03-24 12:07:45+02","2024-05-17 05:05:13.515798+03","You need to add packaging configuration In your serverless yml file add It is useful to remove the AWSSDK modules because if you do not upload them Lambda will use what AWS offers which is better and to remove dev modules like testing frameworks However all other modules are dependencies and will be needed to be uploaded for your function to work properly So configure the package settings to includeexclude exactly what you need Regarding your other question why to upload node_modules again if it have not been updated It is not a limitation of the Serverless Framework It is a limitation of the AWS Lambda service You cannot make a partial upload of a Lambda function Lambda always requires that the uploaded zip package contains the updated code and all required modules dependencies If your deploy is taking too long maybe you should consider breaking this Lambda function into smaller units "
43023295,43018630,"stackoverflow.com",5,"2017-03-26 01:54:21+02","2024-05-17 05:05:14.41877+03","Do you need sync or async communication A good approach would be to use events because awslambda is designed as an event based system So you could use SNS or SQS to decouple your services If you just want to make calls from one service to another you could invoke the lambda function directly via the awssdk see docs So you would not add an API Gateways endpoint and your lambdas would stay private To better anwser your question you should give a short overview of your application and and an example of an interservice call you would make "
43025252,43018630,"stackoverflow.com",2,"2017-03-26 08:08:32+03","2024-05-17 05:05:14.421771+03","As I understand it you intend to make the various functions in a given a service private In doing so each service will likely have serverless yml file that resembles the following Image shows the setup for api keys used with a serverless framework rest api While this is a suitable approach it is less desirable than using Custom Authorizers Custom Authorizers allow you to run an AWS Lambda Function before your targeted AWS Lambda Function This is useful for Microservice Architectures or when you simply want to do some Authorization before running your business logic If you are familiar with the onEnter function when using ReactRouter the logic among Custom Authorizers is similar Regarding implementation since different services are leveraged to deploy various functions consider deploying the function to AWS and noteing the ARN of the Lambda function Follow these links to see the appropriate setup for the custom authorizer These images show the serverless yml file for using custom authorizers when the authorizers are not part of the service but rather deployed on lambda already The following github project awsnodeauth0customauthorizersapifrontend is a good example of how to implement Custom Authorizers when the authorizer funciton is in the same service as the private function Note your situation differs slightly yet you should expect their authorizer function logic to be simliar only the project structure should differ"
58512269,43033730,"stackoverflow.com",3,"2019-10-23 16:46:08+03","2024-05-17 05:05:15.480424+03","I would like to make explicit what Dancrumb already wrote in a comment to the downvoted answer What seemed to work for him and me was to replace finegrained S3 permissions with a single wildcard permission I can only speculate for the reason In an issue report for the Serverless project on Github I saw people mention that they were using a user that is tied to an account that has MFA enabled which applies in my case too the account has MFA not the user If s3 is too permissive for you you may be able to restrict it after the fact with an additional deny segment Unfortunately I do not have the time to verify this theory "
69869580,43033730,"stackoverflow.com",0,"2021-11-07 05:24:05+02","2024-05-17 05:05:15.481424+03","For me I accidentally had a conflicting managed policy it seems"
43034869,43033730,"stackoverflow.com",4,"2017-03-27 00:38:06+03","2024-05-17 05:05:15.482424+03","Change the resource to be This will permit the given actions for any resource Your actions are all related to S3 therefore they will be permitted to operate on any Amazon S3 buckets If you want to permit it to only operate on one bucket use"
43166498,43166190,"stackoverflow.com",5,"2017-04-02 11:57:23+03","2024-05-17 05:05:16.361312+03","You do not need to store credentials in your lambda functions All funtions run with a role the role you set when you created the function Since the lambda function has a role you can add or remove permissions from this role as needed without changing the function itself Manage Permissions Using an IAM Role Execution Role Each Lambda function has an IAM role execution role associated with it You specify the IAM role when you create your Lambda function Permissions you grant to this role determine what AWS Lambda can do when it assumes the role There are two types of permissions that you grant to the IAM role If your Lambda function code accesses other AWS resources such as to read an object from an S3 bucket or write logs to CloudWatch Logs you need to grant permissions for relevant Amazon S3 and CloudWatch actions to the role If the event source is streambased Amazon Kinesis Streams and DynamoDB streams AWS Lambda polls these streams on your behalf AWS Lambda needs permissions to poll the stream and read new records on the stream so you need to grant the relevant permissions to this role httpdocs aws amazon comlambdalatestdgintropermissionmodel html"
43196119,43187686,"stackoverflow.com",4,"2017-04-04 13:04:15+03","2024-05-17 05:05:17.46358+03","The Serverless Framework is a CLI that offers the following When I look to this CLI there is nothing I cannot do with Kudu and GitHub integration That is great If you have already stablished a development workflow to organize and deploy your code maybe you do not need the Serverless Framework The framework was created to help new users to deploy stuff but it it based on Azure tools so there is no magic happening there Just some people trying to make things easier than using the Azure CLI Serverless Framework at this stage is more useful for AWS Lambdas Maybe Depends if you think that Azure CLI is better or not than AWS CLI I have tried to implement my own code to deploy AWS Lambda functions and I know how hard it is There are many configuration steps and things to learn which are very trivial to configure when using the Serverless Framework "
43235375,43230808,"stackoverflow.com",2,"2017-04-05 18:20:22+03","2024-05-17 05:05:18.507796+03","I put save here because these librairies are not in the package json of MoonMail "
43302844,43302843,"stackoverflow.com",42,"2017-04-09 19:05:04+03","2024-05-17 05:05:19.588109+03","It seems that PowerShell has a commandcmdlet called SelectString which has an alias of sls The PowerShell alias sls seems to take precedence over the node js serverless command of sls One way to remove the PowerShell sls alias is by running the following in PowerShell This change only applies to the current PowerShell session To permanently the sls PowerShell alias you can change Microsoft PowerShell_profile ps1 file From PowerShell open your profile page in Notepad with the following command Add the following to the file and save The profile can be reloaded by running the following from Powershell In my file you will see that I have removed the aliases of curl and sls Now I see what I expect when entering sls in PowerShell How do I permanently remove a default Powershell alias Update A more simple option is to use the command serverless instead of sls "
59754495,43302843,"stackoverflow.com",6,"2020-01-18 00:53:57+02","2024-05-17 05:05:19.591111+03","For future visitors in current version if serverless is installed by npmyarn ^1 61 there is a secondary shortcut option and that shortcut works flawlessly in Powershell Imo it is the simplest way to avoid problems with SelectString alias collision and do not require any change in Powershell session or config If serverless is installed by chocolatey as mentioned by Nick Cox in comment slss approach will fail and you can use approach suggested by him sls exe"
43329928,43328619,"stackoverflow.com",3,"2017-04-11 00:57:57+03","2024-05-17 05:05:20.57556+03","There is no GetObject event Please follow this link for a list of supported events S3 will only notify you or trigger a Lambda function when an object is created removed or lost due to reduced redundancy So it is not possible to do exactly what you want but you have a few alternatives Use Lambda to intercept your calls to a CloudFront distribution that uses a S3 as Origin This interceptor could be able to send another file if the requested one is missing This is not a good solution since you would increase latency and costs to your operation Instead of offering a S3 endpoint to your clients offer a API Gateway endpoint In this case ALL image requests would be processed by a Lambda function with the possibility to give another file if the requested one is missing This is not a good solution since you would increase latency and costs to your operation And the best option that may work but I have not tried is to configure a S3 bucket Redirection Rule This is a common use case for static website hosting where a page not found status code 404 redirects to another page like pagenotfound html In your case you could try to redirect to an address of a default image This solution would not use Lambda functions "
45337443,43333376,"stackoverflow.com",1,"2017-07-27 00:06:29+03","2024-05-17 05:05:21.582728+03","I think what you are asking is If I or a colleague checks out the serverless code from git on a different machine will we still be able to deploy and update the same lambda functions and the same API gateway endpoints And the answer to that is yes Serverless keeps track of all of that for you within their files Unless you run serverless destroy no operation will create a new lambda or api endpoint My team and I are using this method we commit all code to a git repo and one of us checks it out and deploys a function or the entire thing and it updates the existing set of functions properly If you setup an environment file that is all you need to worry about really And I recommend leaving it outside of git entirely "
54155974,43333376,"stackoverflow.com",0,"2019-01-12 03:19:07+02","2024-05-17 05:05:21.584728+03","For AWS Serverless Framework keeps track of your deployment via Cloudformation CF parametersidentifiers which are specific to an accountregion The CF stack templates are uploaded to an autogenerated S3 bucket so it is already backed up for you So all you really need to have is the original deployment code in a git repo and have access to your keys Everything else is already backed up for you "
43381463,43381418,"stackoverflow.com",0,"2017-04-13 03:26:15+03","2024-05-17 05:05:22.62162+03","Thats a big buzz word for not managing your own servers and infrastructures No big deal whatsoever For node maily there is httpsserverless com At the end of the day it is the same thing for your web app and serverside Just managed differently Pretty much like the cloud we treat it like if it was mythical beast "
43401667,43387296,"stackoverflow.com",0,"2017-04-14 00:03:10+03","2024-05-17 05:05:23.677106+03","Looking at your question I would advise to align your priorities first why do you want to move away from the Java backend that you are running on now Which problems do you want to overcome You are combining the microservices architecture and the concept of serverless infrastructure in your question Both can be used in conjunction but they do not have to A lot of companies are using microservices even bigger enterprises like Uber on NodeJS but serverless infrastructures like Lambda are really just getting started I would advise you to read up on microservices especially e g here are some nice articles You will also find answers to your question about performance and joins When considering an architecture based on Lambda do consider that there is no state whatsoever possible in a Lambda function This is a step further then stateless services that we usually talk about they generally target client state that does not exist anymore But a Lambda function cannot have any state so e g a persistent DBconnection pool is not possible For all the downsides there is also a lot of stuff you do not have to deal with which can be very beneficial especially in terms of scalability "
43403814,43401141,"stackoverflow.com",1,"2017-04-14 03:55:31+03","2024-05-17 05:05:24.670509+03","Although there is a published documentation for the package command it is not available yet in version 1 11 It is planned to ship only in the next version 1 12 In v 1 11 the deploy command always executes the package command under the the hood but you do not have control about this packaging process You could use the noDeploy option to create local files with the selected stageregion but running deploy again will override the package ignoring what was previously set It will deploy to the stageregion of the options parameters if provided or look into the serverless yml file Another problem is that v 1 11 does not support the path option so you would not be able to select where to save the package nor to select where to find the package to deploy Again you need to wait for the next version to use those features "
49570494,43425065,"stackoverflow.com",30,"2018-03-30 10:46:37+03","2024-05-17 05:05:25.805176+03","On newest versions of serverless currently 1 26 1 you can provide the argument awsprofile ex sls awsprofile suku deploy httpsserverless comframeworkdocsprovidersawsguidecredentialsusingtheawsprofileoption However in your case another option could be to use per stage profile httpsserverless comframeworkdocsprovidersawsguidecredentialsperstageprofiles"
43435934,43425065,"stackoverflow.com",9,"2017-04-16 12:52:06+03","2024-05-17 05:05:25.807177+03","The profile argument is not currently supported You can set the environment by setting the AWS_PROFILE environment variable as appropriate To run a command using a particular profile other than the default one in the system settings you can do On Linux OSX On Windows"
53425049,43425065,"stackoverflow.com",4,"2018-11-22 08:29:29+02","2024-05-17 05:05:25.808177+03","Ensure that your aws profile is setup in awscredentials you can the run export AWS_PROFILEselectedAccount For the rest of your shell session selectedAccount will be your default profile You can now run sls deploy and it will deploy to selectedAccount"
77191597,43425065,"stackoverflow.com",0,"2023-09-28 05:19:31+03","2024-05-17 05:05:25.809148+03","You must use awsprofile not profile "
43445639,43445570,"stackoverflow.com",5,"2017-04-18 11:11:03+03","2024-05-17 05:05:26.564441+03","Just run And you will get all the information on the deployed endpoints and functions "
43450026,43449289,"stackoverflow.com",1,"2017-04-17 13:51:26+03","2024-05-17 05:05:27.62504+03","if you read the NodeJS require documentation you are highly encouraged to use local node_modules folders even if your packages require have the same dependencies httpsnodejs orgapimodules htmlmodules_loading_from_node_modules_folders Alternatively your can work with NODE_PATH notice that require will first look into local node_modules then start looking into other folders so try to delete local node_modules If the NODE_PATH environment variable is set to a colondelimited list of absolute paths then Node js will search those paths for modules if they are not found elsewhere Note On Windows NODE_PATH is delimited by semicolons instead of colons Since webpack exposes many module resolving options httpswebpack github iodocsresolving html Resolving a module path For resolving a module Resolve first gathers all search directories for modules from Webpacks context directory This process is similar to the node js resolving process but the search directories are configurable with the configuration option resolve modulesDirectories In addition to this the directories in the configuration option resolve root are prepended and directories in the configuration option resolve fallback are appended "
53927130,52546563,"stackoverflow.com",0,"2018-12-26 05:24:55+02","2024-05-17 05:11:45.738488+03","The problem is libraries that use lowlevel language usually in C And when serverless generates packets those languages are not sent The solution Enable docker packaging through the serverlesspythonrequirements plugin "
43527950,43527150,"stackoverflow.com",0,"2017-04-20 22:09:53+03","2024-05-17 05:05:29.087711+03","API Gateway does not support using JSON payload as cache key You will need to disable caching for POST under stage settings by expanding stage and resource path in the console UI Alternatively you may want to enable caching only for GET under stage settings "
43539215,43538747,"stackoverflow.com",1,"2017-04-21 12:38:30+03","2024-05-17 05:05:30.159196+03","Follow these steps to start the platform in a virtual machine in your local environment If this works you should see the following output If you encounter problems with these steps please open an issue in the Github repository for the project "
49699283,43538747,"stackoverflow.com",0,"2018-04-06 21:47:45+03","2024-05-17 05:05:30.160196+03","Deploy OpenWhisk using the ansible playbook in highavailability mode "
43574523,43540198,"stackoverflow.com",0,"2017-04-23 20:49:18+03","2024-05-17 05:05:31.200971+03","The integration response for the OPTIONS route returned AccessControlAllowMethods OPTIONSANY If you are enabling CORS by creating OPTIONS resource via CloudFormation template then you should make AccessControlAllowMethods header to return DELETEGETHEADOPTIONSPATCHPOSTPUT instead of OPTIONSANY This is how API Gateway console would do if you enable CORS from console That should fix it "
43958266,43540198,"stackoverflow.com",0,"2017-05-14 00:41:46+03","2024-05-17 05:05:31.202972+03","Updating Serverless to a version higher than 1 10 0 fixed it They have fixed this bug in this issue "
43558945,43542901,"stackoverflow.com",1,"2017-04-22 14:58:45+03","2024-05-17 05:05:32.131908+03","I do not think you can do it via serverless yml you can do it from the handler like this As it currently stands any and all lambda invocations that fail and do not specify a particular status code or does not return a response correctly to the API gateway is assumed to be a Http status 500 Internal Server Error But assuming you want a custom http code to be returned then you can use somthing like the following hope this helps "
43585421,43580664,"stackoverflow.com",3,"2017-04-24 13:15:11+03","2024-05-17 05:05:32.877273+03","After a couple of hours of debugging and going through AWS documentation it seems that there is currently no way of getting exponential back of from AWS SNS for anything else apart from HTTPHTTPS sources You can checkout the this As quoted in the documentation When a user calls the SNS Publish API on a topic that your Lambda function is subscribed to Amazon SNS will call Lambda to invoke your function asynchronously Lambda will then return a delivery status If there was an error calling Lambda Amazon SNS will retry invoking the Lambda function up to three times After three tries if Amazon SNS still could not successfully invoke the Lambda function then Amazon SNS will send a delivery status failure message to CloudWatch Since there is a async invocation of the Lambda SNS will not care what the exit status of the lambda is Hence from the point of view of SNS a successful invocation of the lambda is success enough and will not provide a failure event hence no customised back off For now it seems adding an HTTP endpoint is the only option "
47292108,43654084,"stackoverflow.com",1,"2017-11-14 19:42:59+02","2024-05-17 05:05:33.928851+03","The file is inside a new folder created when you ran sls project init c n yourlowercaseprojectname For example if you stage is dev and your region is useast1 you should see this file"
43674278,43654084,"stackoverflow.com",1,"2017-04-28 10:18:16+03","2024-05-17 05:05:33.929852+03","apiHost is the URL that points to your API Gateway so you have got to set it after deploying at least one endpoint"
43658932,43658561,"stackoverflow.com",2,"2017-05-23 15:17:50+03","2024-05-17 05:05:35.012313+03","With Google Cloud Functions you would typically use a hosted database such as the Firebase Realtime Database for the persistent storage In that scenario you would use GCF as an API gateway to the Firebase Database You could also spin up your own VM that runs a database of your liking That would then take the role of the Firebase Database in the previous example and you would still use GCF as the API gateway In these two approaches you have two microservices the database itself is a service and the GCF function s are a service that wraps the database A final option is to deploy a database into the GCF container that your functions run in You could then connect to this database from your functions code without having to connect to an external service See this answer for some additional information on deploying a custom binary into the GCF container "
43960842,43959961,"stackoverflow.com",1,"2018-11-09 06:29:26+02","2024-05-17 05:05:37.725382+03","If you use the Lambda proxy integration you need to use a specific format for http events that directly invoke your Lambda functions e g if you use serverless invoke from the CLI and not the AWS API Gateway Management Console where you can invoke the Lambda function through your API using the Test button With a Lambda proxy in this example you need to create a json file with a body property and stringify the value like this The reason is With the Lambda proxy integration API Gateway maps the entire client request to the input event parameter of the backend Lambda function httpdocs aws amazon comapigatewaylatestdeveloperguideapigatewaysetupsimpleproxy htmlapigatewaysimpleproxyforlambdainputformat"
43960286,43959961,"stackoverflow.com",0,"2017-05-14 07:10:50+03","2024-05-17 05:05:37.728373+03","In my event json files I had to put my json like this"
43965921,43962148,"stackoverflow.com",6,"2017-05-14 18:49:38+03","2024-05-17 05:05:38.162543+03","When you create lambda functions inside a VPC the elastic network interfaces of the lambda functions are assigned only a private IP address But to connect to a resource in the internet you need a public IP address If your mongo instance is accessed over the internet your lambda function would not be able connect to it You need to setup a NAT gateway to get internet access to the lambda function Go to the below link and check under the topic Internet Access for Lambda Functions to see steps httpdocs aws amazon comlambdalatestdgvpc html"
43969112,43962148,"stackoverflow.com",5,"2024-02-16 13:45:19+02","2024-05-17 05:05:38.164544+03","You do right by attaching the Lambda to the VPC for database traffic to be transmitted over a private network It is an unnecessary security compromise otherwise and slower over the Internet The previous answer is correct you now have an ENI attached to your Lambda Function which means it has a private IP connection on your VPC Subnet I am guessing that your MongoDB instance is in your VPC too if it was elsewhere on the internet you should have kept it as publicly connected Some relevant info Design Consideration A combination of patterns that I use for similar scenarios Hope this helps "
44551098,44223282,"stackoverflow.com",1,"2017-06-14 20:25:20+03","2024-05-17 05:05:48.422499+03","At my company we have been building 56 microserivces with Lambda API Gateway DynamoDB mostly and came up with the some guidelines and template httpsgithub combalmbeeslambdamicroservicetemplate For Environment Variables I recommend to do provider name aws runtime nodejs4 3 environment file env optstage yml in serverless yml and just use process env which means putting environment variables in git repo such as envproduction yml you can write your on CICD script to inject those variables when doing deployment only but we just decided to do this since git repo is private anyway For Development Guideline Highly recommend it seriously helpful to keep everything working without writing crazy amount of test code to just make sure you do not make compilecatchable mistakes if you are thinking of making Restful API checkout this also"
44243038,44239296,"stackoverflow.com",4,"2017-05-29 15:54:36+03","2024-05-17 05:05:48.916246+03","AWS docs does not mention any way you could name your cloudwatch logs stream for lambda they are always named awslambdafunction name so that might be the reason why there is no such option in Serverless What your yaml does is hook to an event published to updatePermission log stream which is a different thing entirely "
44028356,44024567,"stackoverflow.com",11,"2017-05-17 18:23:01+03","2024-05-17 05:05:39.231775+03","You can deploy to different environments using stages with the serverless framework The deploy command has the stage option which you can specify using stage or s The option for region is region or r Heres an example This option can also be used to deploy an individual lambda function to a specific environment You might also want to use serverless variables to make your environment configuration dynamic You can access environment variables using the syntax envSOME_VAR There is also a way to make variables stageregion specific using nested variables From the docs Making your variables stageregion specific serverless env yml allowed you to have different values for the same variable based on the stageregion you are deploying to You can achieve the same result by using the nesting functionality of the new variable system For example if you have two different ARNs one for dev stage and the other for prod stage you can do the following env optstage _arn This will make sure the correct env var is referenced based on the stage provided as an option Of course you will need to export both dev_arn and prod_arn env vars on your local system Links to serverless documentation Deploy httpsserverless comframeworkdocsprovidersawsclireferencedeployfunction Workflow recommendations httpsserverless comframeworkdocsprovidersawsguideworkflowusingstages Serverless Variables httpsserverless comframeworkdocsprovidersawsguidevariables"
73088664,44024567,"stackoverflow.com",1,"2022-07-23 09:55:44+03","2024-05-17 05:05:39.234776+03","This question was asked 5 years ago and I assume back to the time there was not much support for multienvironment deployment using serverless framework Nowadays there are few ways to achieve your goals Use of separate serverless yml Assume you have two serverless yml serverless dev yml and serverless prod yml You can run serverless deploy configserverless dev yml and serverless deploy configserverless prod yml to use different yaml files For more read this Use of stage in serverless yml You can provide stage variable by using s or stage in your serverless command like serverless deploy s dev You can then reference to this value in serverless yml using optstage in this example it becomes dev You can use this inline with the other string like optstage UserTable becomes devUserTable Use of param in serverless yml If using stage does not satisfy you you can use param to pass a keyvalue pair to the serverless yml For example you have param TABLE_NAME in yaml you can then set the value by for example serverless deploy paramTABLE_NAMEdevUserTable Like Method 2 the value is replaced by what you give in the command I think method 2 and 3 are the most suitable ways to do staging because you only maintain one serverless yml you only need method 1 in case you need multiple API Gateways in your service You could imagine if you use serverless prod yml as stable version and serverless dev yml as development when there is a release you have to replace everything from serverless dev yml to serverless prod yml which is not a good practice in terms of DevOps use your VCSs branching function instead Hope this help "
44036642,44032664,"stackoverflow.com",3,"2017-05-18 03:13:37+03","2024-05-17 05:05:39.60753+03","Currently there is no shortcut to declare permissions for a Lambda function You need to give permissions based on its ARN Below follows a working example serverless yml handler js"
51863609,44032664,"stackoverflow.com",3,"2018-08-15 20:30:48+03","2024-05-17 05:05:39.608529+03","You can also use serverlessiamrolesperfunction plugin which allow you to attach individual IAM role per lambda function and in combination with serverlesspseudoparameters you can reference the other lambda "
44032997,44032667,"stackoverflow.com",2,"2017-05-17 22:09:56+03","2024-05-17 05:05:40.938512+03","Form docs there is a very similar example Invoke a Lambda function every 10 min MonFri cron 010 MONFRI You could try"
44033048,44032667,"stackoverflow.com",1,"2017-05-18 00:14:09+03","2024-05-17 05:05:40.939512+03","This is the exact syntax that answers my question"
44058299,44052476,"stackoverflow.com",0,"2017-05-19 00:49:45+03","2024-05-17 05:05:41.702488+03","We figured it out We removed the Router from Root and just returned HomePage because we do not need the singlepageapp router since we are statically serving all the views The Router was breaking the app because it tried to reroute the app after the static content already finished loading So instead of this We have this"
44113435,44113266,"stackoverflow.com",1,"2017-05-22 15:55:59+03","2024-05-17 05:05:43.845863+03","You can server HTML pages using Lambda functions You just need to set the headers See this example Or you could return a JSON object as Lambda functions are commonly used and this JSON object would have a HTML string set as one of the properties "
44271457,44113266,"stackoverflow.com",0,"2017-05-30 23:35:11+03","2024-05-17 05:05:43.846863+03","You can use a JavaScript based HTML templating library such as UnderscoreJS to compile and upload HTML templates in the form of JavaScript dependancies along with Lambda code This allows replacing parts in your HTML using placeholders which is easy to maintain "
44159407,44150905,"stackoverflow.com",1,"2017-05-24 16:10:49+03","2024-05-17 05:05:44.605952+03","The messaging package relies on a microservice that relays Kafka messages as OpenWhisk trigger fires You can find instructions on how to build and deploy the microservice as well as the actions in the messaging package here httpsgithub comapacheincubatoropenwhiskpackagekafkaissues99issuecomment300536379"
44186038,44154025,"stackoverflow.com",0,"2017-05-25 20:13:15+03","2024-05-17 05:05:45.64868+03","First you need the Cognito identity of the caller to be known to the Lambda function The request context in API Gateway includes the Cognito id which you can put into the payload that is sent to the Lambda function or use the Lambda proxy integration and have it included automatically Once you have the Cognito id in the Lambda you can use it to retrieve an associated dataset from Cognito Sync You can use an IAM policy like AmazonCognitoReadOnly to give your Lambda function permission to call the ListRecords API on Cognito Sync which gives you access to the dataset "
44159145,44156935,"stackoverflow.com",2,"2017-05-24 15:59:32+03","2024-05-17 05:05:46.68608+03","The way forward I think is to create a cross account role that has access to that table then have your lambda function assume that role Create the cross account role in the account that has the Dynamodb table and give it access to that httpdocs aws amazon comIAMlatestUserGuidetutorial_crossaccountwithroles html Inside your lambda function have it assume that role and get temporary credentials to access that table httpdocs aws amazon comSTSlatestAPIReferenceAPI_AssumeRole html"
44226107,44223282,"stackoverflow.com",6,"2017-05-28 13:15:23+03","2024-05-17 05:05:48.420499+03","I would recommend using Serverless Framework to improve developer productivity Few of the practices we follow Keep all the infrastructure changes in Serverless Framework generated CloudFormation stack template Create different API Gateway stages for each developer Utilize Serverless Plugins E g Serverless Offline Severless DynamoDB Local etc Use a NodeJS proxy if you plan to setup hybrid development environment e g Use Serverless Offline plugin emulating API Gateway and Lambda localy S3 with Cognito in AWS Use a task runner like Gulp to automate starting web servers deployment etc Use environmental variables to store environment specifics Apart from this its better to use a seperated AWS account for production You can configure AWS organizations to simplify managing multiple accounts "
44944347,44944226,"stackoverflow.com",1,"2017-07-06 12:03:00+03","2024-05-17 05:06:13.91739+03","Include the query also since the first error is regarding MySQL syntax About callback is not a function it is not passed as a parameter in below code as far as I see Above is the corrected one "
45007178,45005202,"stackoverflow.com",2,"2017-07-10 11:27:27+03","2024-05-17 05:06:14.900806+03","As you are already using Dynamodb in your solution one possible solution I could think"
44343583,44336417,"stackoverflow.com",0,"2020-06-20 12:12:55+03","2024-05-17 05:05:49.7153+03","the way Node uses require is so much more than I thought First Node js looks to see if the given module is a core module Node js comes with many modules compiled directly into the executable binary ex http fs sys events path etc These core modules will always take precedence in the loading algorithm If the given module is not a core module Node js will then begin to search for a directory named node_modules It will start in the current directory relative to the currentlyexecuting Javascript file in Node and then work its way up the folder hierarchy checking each level for a node_modules folder read more here I will try out Putting all modules in a separate folder each with it is own Folder prefixed with FunctionName_ so I know where each module is used and the test file package json file Then if I need a module I can require it from the functions with shallow nesting which would look not so bad with package json and have a separate folder Shared where I keep the shared Modules I am still open for better ideas "
44370538,44370048,"stackoverflow.com",2,"2017-06-06 15:58:37+03","2024-05-17 05:05:50.834421+03","The POST verb preflights an OPTIONS verb that you do not support So you need to create a method for OPTIONS that will return status code 200 success and with the expected headers For both the OPTIONS and POST try the following headers you may fine tune the headers later to allow just what you need"
66775748,44396427,"stackoverflow.com",7,"2023-11-06 07:53:37+02","2024-05-17 05:05:51.923205+03","according to the documentation port is no longer available use httpPort and lambdaPort instead like below code or Any of the CLI options can be added to your serverless yml For example from official documentation"
46729084,44396427,"stackoverflow.com",6,"2017-10-13 14:38:46+03","2024-05-17 05:05:51.924214+03","If you want to run two or more Serverless API Gateways at the same time locally you can easily do it with port parameter Basically open two command line windows and in the first window go to your first service directory and run sls offline start port 3001 in the other window go to your second service and run sls offline start port 3002 This way you you will have two services listening on two ports in this examples httplocalhost3001 and httplocalhost3002 There is one catch at the moment if you also use serverlessdynamodblocal plugin If you do not use DynamoDB plugin then you are okay and can stop reading now DynamoDB plugin is using the same port parameter and that causes java net BindException Address already in use See this issue httpsgithub com99xtserverlessdynamodblocalissues135 The workaround for this is to keep serverlessofflinelocal plugin enabled in only one service if you have two or more Example In myservice1 you keep all dynamodb config in serverless yaml file and start this service with default port sls offline start migrate true In the next service let us call it myservice2 you remove serverlessdynamodblocal from plugins in serverless yaml there is no need for any other changes and then you can start the service with sls offline start port 3001 First service will start DynamoDB and the second one will be able to use it "
51627736,44396427,"stackoverflow.com",4,"2019-12-16 08:03:39+02","2024-05-17 05:05:51.927212+03","What I do is create another service that has all the functions of other services Below is my folder structure You can remove serverlessoffline plugin on the service1 and service2 Update I have develop a script for generating offline serverless yml Check out the example here httpsgithub comNecromancerxserverlessofflinetemplate"
67949821,44396427,"stackoverflow.com",3,"2021-06-12 17:33:35+03","2024-05-17 05:05:51.928211+03","Check this package out httpsgithub comedisslsmultigateways It simulates AWS API Gateways API mappings on your local environment It allows you to run multiple API gateways under a single port on your local forwarding requests to each service based on base paths you configure For example if you have two serverless apps one for authentication and one for products You can assign a different base path for each serverless app So when you receive a request to localhost3000auth it sends the request to the auth serverless app And when you receive a request to localhost3000products it sends the request to the products serverless app "
44420317,44419465,"stackoverflow.com",2,"2017-06-07 21:33:54+03","2024-05-17 05:05:52.851701+03","AccessControlAllowHeaders is returned in a preflight response from OPTIONS by the server in this case API Gateway for specifying which headers the client can use when making a request AccessControlExposeHeaders is specified in your handlers response indicating which headers you want the browser to expose to the client I added AccessControlExposeHeaders XAmznRemappedAuthorization to my handlers response and everything is now working as expected "
44425158,44423438,"stackoverflow.com",0,"2017-06-08 04:17:35+03","2024-05-17 05:05:53.719017+03","This is a known issue with node inspector Take a look here Since 0 9 0 we use httpsgithub combenderjsbrowserlauncher2 to start the browser and make sure it is ChromeChromiumOpera i e the browsers that can properly render node inspector we detect installed browsers in the system and choose the most appropriate one earlier we used opener module which just delegated opening the browser to the OS which would open the defaul browser which could have been e g Firefox and this could be the reason why the behavior has changed browserlauncher2 actually does a bit more than just launching a browser for instance it creates a new profile for Chrome in a subfolder of this is probably the issue that CalvinScott reported i e Chrome that was opened was the new profile created by browserlauncher not your original profile you should be able to open your original profile of Chrome normally Also you may consider this Since version 6 3 Node js provides a buitin DevToolsbased debugger which mostly deprecates Node Inspector see e g this blog post to get started The builtin debugger is developed directly by the V8Chromium team and provides certain advanced features e g longasync stack traces that are too difficult to implement in Node Inspector "
44471420,44462723,"stackoverflow.com",1,"2017-06-10 11:53:59+03","2024-05-17 05:05:54.776755+03","httpsserverless comframeworkdocsprovidersawsguidevariables Should be able to create an environment variable so you can swap between two cloud formation references depending on the setting "
44468065,44467080,"stackoverflow.com",17,"2017-06-12 23:22:18+03","2024-05-17 05:05:56.053021+03","NotAuthorizedException SignUp is not permitted for this user pool exception is thrown when the user pool only allows administrators to create the users via the AdminCreateUser API With this setting enabled SignUp API cannot be called and will throw this error If you are calling this from a lambda trigger you can use AdminCreateUser API or disable this setting so your user pool allows SignUp API calls "
59319402,44467080,"stackoverflow.com",6,"2021-08-02 09:24:45+03","2024-05-17 05:05:56.054021+03","As Chean Mehta pointed out you can disable the AdminCreateUser setting for SignUp API to work for that you have to set AllowAdminCreateUserOnly to false in your serverless cognito configuration or you can disable this by following these steps"
50471640,44493807,"stackoverflow.com",3,"2018-05-22 18:40:37+03","2024-05-17 05:05:56.488989+03","Added in resources section of serverless yml as below"
44494289,44493807,"stackoverflow.com",1,"2017-06-12 11:02:41+03","2024-05-17 05:05:56.49099+03","Good question I believe you can do it in the sfunction json file like so Sources httpsgithub comserverlessserverlessissues587 httpsgithub comserverlessserverlessissues463"
45029233,45029129,"stackoverflow.com",2,"2017-07-11 11:26:54+03","2024-05-17 05:06:16.977504+03","Serverless allows you to define a default IAM role for all functions which should have CloudWatch access by default Also by default your Lambda functions have permission to create and write to CloudWatch logs You can also fine tune the IAM role for all functions or even provide finegrained permissions for each function using the role attribute The reference is here"
44500409,44497896,"stackoverflow.com",0,"2017-06-12 16:14:27+03","2024-05-17 05:05:57.499321+03","As I understood your Facebook bot would receive the user input 6 AM and it would need to request the backend a Lambda function to schedule some operation for this time At 6 AM another Lambda function would be triggered to execute the task for this user I see that you have tagged this question with the Serverless Framework It does support scheduling Lambda functions docs like in this example But currently you cannot call the Serverless Framework programatically You can follow the discussion here and here In this case I guess that you would need to use the AWS SDK to configure and schedule Lambda functions "
44555958,44497896,"stackoverflow.com",0,"2017-06-15 01:55:30+03","2024-05-17 05:05:57.501297+03","Your best bet would be to expose CRUD functions so your bot can manage users schedules into AWS Those functions will store schedule records in DynamoDB or whatever persistence strategy you choose Then have a separate function on a cron schedule that periodically scans the schedule records and spawns messages to whatever other function sends the messages "
44560948,44497896,"stackoverflow.com",0,"2017-06-15 10:11:13+03","2024-05-17 05:05:57.503295+03","Have you considered CloudWatch scheduled events They can be created programmatically and can have cron expressions You can configure them to trigger lambdas or publish to a stream It seems that this might be exactly what you need Be sure to check CloudWatch limits though "
44538758,44534099,"stackoverflow.com",5,"2017-06-14 10:55:21+03","2024-05-17 05:05:58.817322+03","The hostname is no directly exposed by cloudformation so you have to build it from the outputted values of the created resources All resources of the serverless framework follow a certain naming convention AWS Resources With this information you can create your hostname through the following statement"
44545245,44544392,"stackoverflow.com",1,"2017-06-14 15:46:39+03","2024-05-17 05:05:59.875197+03","Java is one of the supported programming languages of AWS Lambda It is possible to run an application using Java you just have to take the warmup time into consideration if that fits your usecase then use it You could also use SNS and a hook to your lambda to keep it warm if you do not receive requests"
46660491,44544392,"stackoverflow.com",1,"2017-10-10 12:44:57+03","2024-05-17 05:05:59.877197+03","Personally I would AVOID using java runtime for AWS lambda as much as possible I understand that it is very tempting to use java assuming that you are looking into migrating an existing implementation into microservices But you are always going to pay the penalty of slow warmup time compared to other runtimes You may also miss out on Java compiler optimisations as the lambda may not be invoked enough number of times to trigger C1 and C2 compilations My preference would be only to use java for lambda if you are planning to write a lean implementation means no spring hibernate etc etc "
46660587,44544392,"stackoverflow.com",0,"2017-10-10 10:10:16+03","2024-05-17 05:05:59.879198+03","Using Java with AWS lambdas is perfectly fine but Lambdas are functions not applications So you should avoid to use a framework like Spring because you do not need that The question is what do you want to achieve in your function and why do you need a framework to execute such small amount of code What is your use case"
44546675,44545847,"stackoverflow.com",3,"2017-06-14 16:47:42+03","2024-05-17 05:06:00.536775+03","You need to set the headers to specify the content type to HTML and the response must be a full valid HTML page with the html tag and the rest Example "
44683541,44611695,"stackoverflow.com",2,"2017-06-21 21:23:38+03","2024-05-17 05:06:01.689311+03","If the API is responding quickly to your curl request then the problem is not on AWS end Try matching when you send your request via Facebook to your app and when your app recieves it If it is getting held up on Facebooks end Im afraid there isnt much you can do to solve it "
45138299,44611695,"stackoverflow.com",0,"2017-07-17 10:28:03+03","2024-05-17 05:06:01.690073+03","Another issue could be the datacenter your lambda is running in versus where facebook is For example using chkutil com you can see facebook com seems particularly slow from the AsiaPacific datacenters "
45260028,44611695,"stackoverflow.com",0,"2017-07-23 02:01:15+03","2024-05-17 05:06:01.691223+03","As it turns out Facebook was experiencing DNS issues and has since remedied the issue "
44627891,44626308,"stackoverflow.com",33,"2017-06-19 13:17:29+03","2024-05-17 05:06:02.631096+03","You can add CloudFormation resources to the resources section For ElasticSearch this would look something like this "
56803982,44626308,"stackoverflow.com",4,"2019-06-28 12:05:20+03","2024-05-17 05:06:02.632097+03","to add to Jens answer you might want the output you can add this to your serverless yml config"
44653034,44635309,"stackoverflow.com",8,"2017-06-20 15:25:10+03","2024-05-17 05:06:04.087956+03","After some more research I have found a way to debug the deployment process The gcloud tool comes with a set of options which are not easily to spot but offer features like verbosity And that is what you need for debugging the deployment process Find all the options here httpscloud google comsdkgcloudreferencealphafunctionsdeploy bottom of the page In my case the issue was a private NPM repo which for sure could not be checkout due to lack of permissions gcloud beta functions deploy NAME stagebucket BUCKET verbosity debug"
44647631,44635309,"stackoverflow.com",1,"2017-06-20 11:22:38+03","2024-05-17 05:06:04.089657+03","I just had this same issue and it was caused by a typo in a module name in package jsondependencies "
44670736,44669027,"stackoverflow.com",5,"2017-06-21 15:27:37+03","2024-05-17 05:06:04.523034+03","If you want to respond with an error you have to use the success callback with an error response construct If you are using the context fail callback AWS will assume that the Lambda technically failed and respond with the default mapping present in your API Gateway Sample error response"
44714533,44669027,"stackoverflow.com",3,"2017-06-23 09:32:50+03","2024-05-17 05:06:04.524569+03","This way I can change API Gatway I can manage my API response to using stemplates json to adding this code base This way I return my response with 400 statusCode and a valid message "
44680654,44678952,"stackoverflow.com",4,"2017-06-21 18:42:03+03","2024-05-17 05:06:05.793999+03","Although they say that Moonmail supports version 0 5 2 or higher it is not true You need to use version 0 5x of the Serverless Framework and you cannot use any version 1 x because 1 x is a complete rewrite Unfortunately the solution is to uninstall the current Serverless Framework and install an old version "
44684113,44683761,"stackoverflow.com",5,"2017-06-21 21:54:59+03","2024-05-17 05:06:06.697194+03","You probably just missed some indentation in your serverless yml file The section under s3 needs an extra indentation otherwise the event source is not recognized "
73084713,44683761,"stackoverflow.com",1,"2022-07-22 21:34:34+03","2024-05-17 05:06:06.698195+03","For framework version 3 if name does NOT respect s3 naming rules sls deploy works yet as you have mentioned aws will not show it I would consider this a silent failure and not intuitive If you were to do the same thing with the resource sls would not do the cloudformationdeployment S3 bucket name rules"
45036288,45035906,"stackoverflow.com",9,"2017-07-11 16:43:44+03","2024-05-17 05:06:18.064401+03","Almost immediately after I posted my question I think I was able to answer it The problem with my identity pool is that I needed to reference the provider name in the following way I needed to use the special amazon function FnGetAtt in order for this to work "
77740311,45234813,"stackoverflow.com",0,"2023-12-31 22:20:45+02","2024-05-17 05:06:26.573375+03","I believe Lambda Extensions provide a means for capturing finalization logic Heres a Stack Overflow Answer that discusses this option It points to this section of the AWS documentation Presumably you could shutdown db connections inside a finalization callback Im curious about this as it appears to be a cleaner shutdown method however I have not yet tested it "
58123361,54711268,"stackoverflow.com",0,"2019-09-26 22:15:17+03","2024-05-17 05:13:32.401289+03","Dont forget to add the s3 policy and bucket to your dependsOn list"
44702964,44701914,"stackoverflow.com",6,"2021-10-07 11:46:00+03","2024-05-17 05:06:07.507163+03","Browsers will not accept a cookie for a completely different domain than the request was sent to So if you are redirecting to a new domain you cannot set a cookie for that new domain The domain part of the cookie works only for domains that have the same root domain but differ only in subdomain This is taken from RFC 6265 The user agent will reject cookies unless the Domain attribute specifies a scope for the cookie that would include the origin server For example the user agent will accept a cookie with a Domain attribute of example com or of foo example com from foo example com but the user agent will not accept a cookie with a Domain attribute of bar example com or of baz foo example com So to recap You cannot set a cookie from your server for a completely different domain and the domain attribute in the cookie will not save you either Under specialized circumstances you can set a cookie for a different domain that is different only in subdomain e g shares the same root domain FYI if your cookie every did get set appropriately you would have to change this to this to properly serialize your object "
44703617,44701914,"stackoverflow.com",0,"2017-06-22 18:26:57+03","2024-05-17 05:06:07.509164+03","You are setting your cookie equal to data[Object object] not the actual information You need to serialize your data object in that string "
44918263,44894202,"stackoverflow.com",2,"2017-07-05 09:13:53+03","2024-05-17 05:06:08.373231+03","Here are a couple of options When calling updateEmail update the email in your database first before calling updateEmail THowever if an error occurs you need to catch it and undo that change in your db When a user wants to updateEmail send their id token and new email to your server or firebase function http endpoint There you verify the ID token with the admin SDK then use the client SDK require firebase using the uid from the ID token admin auth createCustomToken uid then using client SDK firebase auth signInWithCustomToken customToken You can then call user updateEmail newEmail on the backend and save the email Always save the uid only and just use Admin SDK admin auth getUser uid to look up the user and get their email This guarantees you get the users latest email as you will not be able to catch the email revocation if the user chooses to do so No need to save anything Use the CLI SDK to download all your users and their emails Check httpsfirebase google comdocscliauthauthexport This is also better as you will always be able to get the latest email per user even if they revoke the email change "
45987560,44898431,"stackoverflow.com",7,"2018-08-29 06:57:00+03","2024-05-17 05:06:09.528297+03","I just started searching for the answer to this very question Before I post some of my preliminary data have you found a solution to this Here is what I have found so far BitBucket httpsgithub comeflabsstashhookmirror GitHub httpsgithub comgitbucketgitbucketissues833 httpshelp github comarticlesaboutwebhooks JGit httpsfancybeans com20120824howtouses3asaprivategitrepository Gitlab How to create a Gitlab webhook to update a mirror repo on Github especially the part here I successfully used the mirror feature to automatically pull a GitHub Repository into GitLab using the mirror feature offered by GitLab com I also found httpsgithub comlambcilambci which is serverless CI and looks promising I believe the answer lies in a combination of AWS SNS Topic monitoring of Webhooks GitLab and others and pass the response to Lambda which spins up an instance which has either git or jgit to run the command look through the LambCI Repository for details The only problem with this solution is that the AWS Lambda instance will clone the mirrored repository to its local storage and then push to AWS Code Commit and this will be repeated each time an instance gets started so for LARGE repositories or VERY ACTIVE repositories this may not be a good idea and you would be better off spinning up a nano EC2 instance with a CRON job to mirror the repository to AWS Code Commit "
49780266,44898431,"stackoverflow.com",3,"2018-04-11 19:31:11+03","2024-05-17 05:06:09.531298+03","AWS Quickstart provide a solution that copies your repo to s3 using lambda and webhooks This solution could quite easily be modified to copy to CodeCommit rather than s3 "
44898598,44898431,"stackoverflow.com",2,"2017-07-04 09:39:24+03","2024-05-17 05:06:09.532298+03","It seems migrating directly from Github to AWS Codecommit is not available in the AWS documentation The following links have some scripts that can do the migration using a local machine Please check httpwww paulkearney com201509migratingfromgithubtoawscodecommit html httpsgist github compaulkearney6042561c56654a15af3c"
45369399,44899374,"stackoverflow.com",2,"2017-07-28 12:10:26+03","2024-05-17 05:06:09.944779+03","I think this issue has to do with what you are returning and how you returning it In the production environment if you do not properly return a promise your function will be killed since it assumes your work is done locally that will probably not happen so the bug is invisible Look here for an example httpsgithub comfirebasefunctionssamplesissues78"
44931064,44930572,"stackoverflow.com",8,"2017-07-05 19:00:06+03","2024-05-17 05:06:10.582484+03","You need to exit the lambda through a success or error callback Otherwise the engine stays on until a timeout occurs The easiest way to exit your lambda would be to call context succeed done after you code is done Here some basic introduction to the topic Lambda Function Handler Node js "
49932473,44930572,"stackoverflow.com",5,"2018-04-20 04:40:20+03","2024-05-17 05:06:10.583484+03","The accepted solution doesn t work for me I m connecting to a RDS instance and if I send context succeed done the callback is not called I m connecting to an Amazon RDS instance inside a lambda running nodejs 8 1 SOLUTION In order to exit of Nodejs event Loop you must add the next code to invoke the callback"
44936968,44936360,"stackoverflow.com",2,"2017-07-06 01:27:34+03","2024-05-17 05:06:11.51321+03","In your code you are not adding callbacks for when there is an error in the request And you may want to add a trycatch to handle any other issues with your code Also you do not need to set callbackWaitsForEmptyEventLoop because you are not adding extra events besides your main request Try this"
54692764,44937666,"stackoverflow.com",3,"2019-02-14 16:29:25+02","2024-05-17 05:06:12.403019+03","You can write something like this in the serverless yml file Now you can call serverless with optional commandline options stage andor region to override the defaults defined above e g In your code you can then use the environment variable REST_API_URL node js python Java"
44945262,44937666,"stackoverflow.com",1,"2017-07-06 12:40:49+03","2024-05-17 05:06:12.404578+03","The serverless framework has a documentation page on how they generate names for resources See AWS CloudFormation Resource Reference So the generated RestAPI resource is called ApiGatewayRestApi "
70614210,44937666,"stackoverflow.com",0,"2022-01-07 00:08:07+02","2024-05-17 05:06:12.405679+03","Unfortunately the documentation does not mention it"
78169924,44937666,"stackoverflow.com",0,"2024-03-16 00:25:20+02","2024-05-17 05:06:12.406677+03","You need to define an API resources Resources HttpApi and then you can use the alias HttpApi to get the ID Note I am using Typescript to write my Serverless files so you will have to remove the "
44941653,44941218,"stackoverflow.com",1,"2017-07-06 09:49:32+03","2024-05-17 05:06:12.886685+03","Seems like your credentials are not completely set Setting up your Azure credentials Once the serverlessazurefunctions plugin is installed it expects to find your Azure credentials via a set of wellknown environment variables These will be used to actually authenticate with your Azure account so that the Serverless CLI can generate the necessary Azure resources on your behalf when you request a deployment see below The following environment variables must be set with their respective values For details on how to create a service principal andor acquire your Azure accounts subscriptiontenant ID refer to the Azure credentials documentation "
45084548,45075395,"stackoverflow.com",14,"2018-04-17 18:19:55+03","2024-05-17 05:06:19.216174+03","awssdk is in the neighborhood of 24MB and you do not need it since it is already available to lambda functions One option is to move it to devdependencies and then put your devdependencies in package json of a parent directory There are also some tools that can help serverlesspluginincludedependencies plugin thought I am not sure how well it works if the exclude function is broken httpsgithub comdougmoscropserverlesspluginincludedependencies Webpack can also be used with serverlesswebpack plugin to control dependencies Webpacks dead code elimination can make a pretty substantial difference Not ideal but you can also run npm prune production before deployments You will need to run npm install again after "
45075527,45075395,"stackoverflow.com",4,"2017-07-13 13:03:13+03","2024-05-17 05:06:19.218175+03","You can use the serverless ability to exclude some packages you do not need for example you are guaranteed to here the manual Unfortunately seems that since version 1 16 there is a problem that those excludes are ignored version 1 15 1 created much smaller zips and since 1 16 the zip include anything in your node_modules I opened an issue but it is still unanswered "
64270458,45075395,"stackoverflow.com",1,"2020-10-08 23:54:49+03","2024-05-17 05:06:19.219448+03","I used layers for this Layers are used to pull in additional code and content in the form of layers Which will keep your lambda functions small and will pull dependencies from layers in runtime Step 1 Create a folder somewhere called nodejs Step 2 In this folder install your dependencies which are large in size For example Step 3 Zip it give name of your choice Step 4 Login to aws console go to lambda service and then choose create layer Then upload zip file you created As it says you can upload this zip file to S3 in case its larger than 10 mb At this step you can assign layer to lambda by going into lambda function and add custom layer which you created from above steps In case you are interested to add layer is yml config"
52689159,45075395,"stackoverflow.com",1,"2018-10-07 16:52:52+03","2024-05-17 05:06:19.221445+03","To solve this problem I used a plugin called serverlesspluginscripts and wrote a shell script that runs on afterpackagecreateDeploymentArtifacts hook The script unpacks deployment package removes node_modules directory runs npm install onlyprod and packs everything back For some reason npm prune production did not work well for me at all This is the configration serverless yml "
45283102,45083814,"stackoverflow.com",0,"2017-07-24 17:24:28+03","2024-05-17 05:06:19.535917+03","Have you tried the serverlessawsdocumentation plugin "
74846011,45083814,"stackoverflow.com",0,"2022-12-19 05:21:02+02","2024-05-17 05:06:19.537912+03","If you are using REST API API Gateway v1 you can simply write the following in your serverless yml file But I do not have a solution if you are using HTTP API API Gateway v2 "
45156025,45125273,"stackoverflow.com",0,"2017-07-18 04:28:11+03","2024-05-17 05:06:21.536608+03","I am not sure using serverless framework a right choice for this But technically you should be able to add a CloudFormation to create the datapipeline to the resources section in serverless yml"
45132080,45130789,"stackoverflow.com",78,"2017-07-16 21:33:54+03","2024-05-17 05:06:22.320307+03","Try running and then"
60187956,45130789,"stackoverflow.com",13,"2020-02-12 14:05:29+02","2024-05-17 05:06:22.321069+03","If the above options are not working due to insufficient access or sudo access following one will definitely work as it is saving the serverless into your local Reference link httpsserverless comframeworkdocsprovidersawsguideservices"
55809254,45130789,"stackoverflow.com",12,"2019-04-23 13:31:32+03","2024-05-17 05:06:22.322066+03","Was getting the same error serverless command not found but instead of NPM was using YARN To fix it had to execute or better add to your bash_profile then if not already installed"
52740440,45130789,"stackoverflow.com",9,"2018-10-10 15:36:25+03","2024-05-17 05:06:22.323067+03","my recomendation here is to allways install the serverless framework as a dev dependency npm install serverless savedev specially if you are working in a team where each member can have its own version of the framework After that you can call the framework using npm scripts For example you can create a new entry in the scripts section like this deploy serverless deploy and call it using npm run deploy "
52472538,45130789,"stackoverflow.com",4,"2018-09-24 07:03:17+03","2024-05-17 05:06:22.325067+03","Try with the following order"
65133494,45130789,"stackoverflow.com",1,"2020-12-03 22:16:54+02","2024-05-17 05:06:22.325686+03","Another option following this post is to try npx serverless "
45198065,45198064,"stackoverflow.com",2,"2017-07-19 21:11:29+03","2024-05-17 05:06:23.388181+03","The value as defined in the YAML file is being interpreted as a number Enclose it in single quotes to make it explicit that it is a string"
50781927,45213362,"stackoverflow.com",1,"2018-06-10 11:34:05+03","2024-05-17 05:06:24.362638+03","I would consider using AppEngine or containers for your task rather than CloudFunctions There are some suggestions for how to do it here httpcode markedmondson me4waysschedulerscriptsongooglecloudplatform and on Compute Engine here httpscloud google comsolutionsrunningratscale"
45225160,45222993,"stackoverflow.com",1,"2017-07-21 04:17:28+03","2024-05-17 05:06:25.417906+03","You should always use asynchronous methods and function calls in Node However Node js always runs require synchronously and module being required can require other needed modules which is an expensive process Even for the context of Lambda this remains the same since if you define require outside of the function it will be initiated upon Cold start of Lambda and not rerun for subsequent Hot start calls More information on Lambda container reuse reference "
45227243,45222993,"stackoverflow.com",0,"2017-07-21 03:42:49+03","2024-05-17 05:06:25.419974+03","require is a synchronous operation that happens in the moment that it is executed In your example if the function is not executed then the module will not be added to the memory You may reduce the memory usage with your strategy but will it be significant Be careful to not worry too much with microoptimizations because the best practice or at least the standard practice in Node is to require modules at the beginning and not in the middle of the code "
45245651,45234813,"stackoverflow.com",24,"2021-05-17 22:40:28+03","2024-05-17 05:06:26.569374+03","The short answer is that there is no such event to know when the container is stopped UPDATE I have not used this library but httpswww npmjs compackageserverlessmysql appears to try to solve just this problem PREVIOUS LONG ANSWER The medium answer after speaking with someone at AWS about this I now believe you should scope database connections at the module level so they are reused as long as the container exists When your container is destroyed the connection will be destroyed at that point Original answer This question touches on some rather complicated issues to consider with AWS Lambda functions mainly because of the possibility of considering connection pooling or longlived connections to your database To start with Lambda functions in Node js execute as a single exported Node js function with this signature as you probably know The cleanest and simplest way to handle database connections is to create and destroy them with every single function call In this scenario a database connection would be created at the beginning of your function and destroyed before your final callback is called Something like this NOTE I have not tested that code I am just providing an example for discussion I have also seen conversations like this one discussing the possibility of placing your connection outside the main function body The theory here is this because AWS Lambda will try to reuse an existing container after the first call to your function the next function call will already have a database connection open The example above should probably check the existence of an open connection before using it but you get the idea The problem of course is that this leaves your connection open indefinitely I am not a fan of this approach but depending on your specific circumstances this might work You could also introduce a connection pool into that scenario But regardless you have no event in this case to cleanly destroy a connection or pool The container process hosting your function would itself be killed So you would have to rely on your database killing the connection from it is end at some point I could be wrong about some of those details but I believe at a high level that is what you are looking at Hope that helps "
45637685,45236308,"stackoverflow.com",2,"2017-08-11 17:29:56+03","2024-05-17 05:06:27.625221+03","Unsigned Thanks for the comment Although your recommendation to delete and redeploy did not work the link you posted did mention having S3 privileges I added Full S3 Access to my code build role and kboom it worked "
65113206,45273443,"stackoverflow.com",0,"2020-12-02 19:32:10+02","2024-05-17 05:06:28.589741+03","We have recently had a similar requirement and the answer is EFS Lambdas have a 250mb ceiling and Gatsby is much larger than this 500mb By installing Gatsby in your EFS instance and mounting that instance to your Lambda you will be building pages in no time There is a few other caveats however I put together an overview of how to do it here httpswww jameshill devarticlesrunninggatsbywithinawslambda"
45281037,45279437,"stackoverflow.com",2,"2017-07-25 12:34:49+03","2024-05-17 05:06:29.578349+03","Issue resolved when I specifically required the jquerydeferred module which can be found here httpswww npmjs compackagejquerydeferred"
45279595,45279437,"stackoverflow.com",1,"2017-07-24 14:36:13+03","2024-05-17 05:06:29.579349+03","You have confused jQuery with jQuery UI In your model js replace With Deferred is a jQuery method and not a jQuery UI feature "
45304969,45303121,"stackoverflow.com",48,"2017-07-25 16:35:03+03","2024-05-17 05:06:30.462343+03","I needed to delete the stack from cloud formation once done I was able to rerun serverless deploy successfully "
45785345,45303121,"stackoverflow.com",4,"2017-08-20 21:08:36+03","2024-05-17 05:06:30.464343+03","Yes the serverless works like that only When you deleted the s3 bucket it was not deleted from the stack entry hence it was failed We should delete Stack entry as well from it if we are deleting the S3 bucket From error handling we can also check if bucket exists or not "
68596478,45303121,"stackoverflow.com",2,"2021-07-30 22:56:20+03","2024-05-17 05:06:30.465343+03","For production use this plugin httpswww serverless compluginsserverlessdeploymentbucket and set the bucket it will create if it doesnt exist "
76304725,45303121,"stackoverflow.com",0,"2023-05-22 12:25:39+03","2024-05-17 05:06:30.466343+03","I had a similar error and here is my solution Run the following commands in you terminal from the project root directory serverless remove or sls remove serverless deploy or sls deploy"
45339543,45324276,"stackoverflow.com",3,"2017-07-27 03:40:15+03","2024-05-17 05:06:31.529633+03","It appears that your design is API Gateway Lambda Database Amazon API Gateway is highly scalable and will meet your needs easily You could also implement throttling rules to make sure that clients are not abusing your API AWS Lambda is also highly scalable There is a default limit of 1000 concurrent Lambda function executions but they will automatically queue if this amount is exceeded eg if your function takes several seconds to complete In general SQL databases have no guarantee of performance A query can be simple or complex and it is hard to predict the performance without running lots of load tests However Amazon DynamoDB is a NoSQL database where you can configure exactly how many Reads and Writes per second you require It also allows limited burst above this limit and the ability to Auto Scale to vary performance throughout the day The downside is that it is difficult to performance analysis on data in a NoSQL database so you would want to export it to either a traditional SQL Database eg MySQL or into an ELK stack on Amazon Elasticsearch Service "
45368224,45348580,"stackoverflow.com",42,"2020-08-11 16:25:44+03","2024-05-17 05:06:32.126796+03","well I found the answer The settings in my response object are fine I just had to manually change the settings in API Gateway for this to work in the browser I have added to binary media types under the binary settings in API Gateway console API GATEWAY FRONTEND opening the api url in new tab target_blank Probably the browser is handling the encoded base 64 response In my case with chrome the browser just opens the pdf in a new tab exactly like I want it to do "
53667160,45348580,"stackoverflow.com",18,"2018-12-07 11:58:16+02","2024-05-17 05:06:32.128816+03","After spending several hours on this I found out that if you set Content handling to Convert to binary CONVERT_TO_BINARY the entire response has to be base64 I would otherwise get an error Unable to base64 decode the body Therefore my response now looks like callback null buffer toString base64 The Integration response The Method response And Binary Media Types"
45361640,45348580,"stackoverflow.com",7,"2017-07-28 00:54:01+03","2024-05-17 05:06:32.130176+03","If you have a gigantic PDF then it will take a long time for Lambda to return it and in Lambda you are billed per 100ms I would save it to S3 first then let the Lambda return the S3 url to the client for downloading "
71150771,45348580,"stackoverflow.com",1,"2022-02-17 01:53:15+02","2024-05-17 05:06:32.131179+03","I was having a similar issue where pdf where downloaded as base64 and started happening when changed the serverles yml file from to The issue is because the way AWS implemented this feature From aws documentation here When a request contains multiple media types in its Accept header API Gateway honors only the first Accept media type If you cannot control the order of the Accept media types and the media type of your binary content is not the first in the list add the first Accept media type in the binaryMediaTypes list of your API API Gateway handles all content types in this list as binary Basically if the first media type contained in the accept request header is not in your list in binaryMediaTypes then you will get base64 back I checked the request in the browser and the first media type in the accept header was texthtml so I got it working after changing my settings to Hope this helps anyone with the same issue "
55871106,45348580,"stackoverflow.com",0,"2019-11-27 01:12:19+02","2024-05-17 05:06:32.133174+03","Above solution is only for particular contenttype You cannot more content type Follow only below twostep to resolve multiple content type issue API gateway API method integration request Create your response as Note It is not secure"
75133900,45348580,"stackoverflow.com",0,"2023-01-16 13:59:57+02","2024-05-17 05:06:32.134314+03","Instead of doing all this It is better to use serverlessapigwbinary plugin in your serverless yaml file Add Hope that will help someone "
45412370,45349124,"stackoverflow.com",0,"2017-07-31 12:41:58+03","2024-05-17 05:06:32.898637+03","Build it up using CloudFormation syntax in the resources section "
59075612,45349124,"stackoverflow.com",0,"2019-11-27 19:33:36+02","2024-05-17 05:06:32.899638+03","Below is an example IAM Policy that allows a Lambda function to write to an AWS ElasticSearch database I suggest you employ the use of the popular plugin serverlesspseudoparameters to fill in the AccountID Region etc Ref httpsdocs aws amazon comelasticsearchservicelatestdeveloperguideesac html"
47730721,45437205,"stackoverflow.com",23,"2018-03-07 04:19:56+02","2024-05-17 05:06:33.944509+03","I attempted to follow the same article and experienced the same error Adding outFiles did not help although it did change my error message to I cannot explain why VSCode has a problem with the executable in node_modules bin but if I point at node_modulesserverlessbin instead things work as expected Here is my full working configuration where my test event JSON exists in sampleevent json in the project root Using Serverless ^1 26 1 Node 8 9 4 LTS VSCode 1 20 1"
45521894,45437205,"stackoverflow.com",2,"2017-08-05 15:28:14+03","2024-05-17 05:06:33.946115+03","To get debugging to work with TypeScript I needed to add outFiles set to the folder where my compiled code goes I have not tried to debug straight JS but I would assume it is something like this "
56157066,45437205,"stackoverflow.com",2,"2019-05-15 23:41:27+03","2024-05-17 05:06:33.947118+03","None of the solutions worked for me so here is my modification as a resource Also multiple coworkers were able to attack just by flipping autoattach to on and using the invoke local keywords Below is a snippet featuring the launch json that eventually worked for me w comments for clarity where my function is named Processor function or f The name of the function in your service that you want to invoke locally path or p The path to a json file holding input data to be passed to the invoked function as the event This path is relative to the root directory of the service stage or s The stage in your service you want to invoke your function in npm 5 6 0"
65030074,45437205,"stackoverflow.com",2,"2020-11-27 00:43:28+02","2024-05-17 05:06:33.94892+03","Do what the other guides state and setup your project with a launch json file The problem I had was that the supposed file program workspaceRoot node_modules binsls threw an error I changed it to workspaceRoot node_modulesserverlessbinserverless and it worked Heres the full file vscodelaunch json Be aware that the argument hello is the name of the function I want to debug I think the intended use case must be that you change that file name for whatever function you want to invoke Maybe someone could create a VSCode plugin to do this"
45444721,45443678,"stackoverflow.com",1,"2017-08-03 17:14:03+03","2024-05-17 05:06:35.058868+03","By design the current version of serverless framework does not support this feature But the version 0 5 6 had been designed to deploy multiple stages into api single api gateway project Following open issues are related to this Some have suggested few workarounds AWS API Gateway stages and names are separated Issue below mainly highlight different services into same api gateway But it also has some comments related to your query as well Deploy many micronano services to one API Gateway Update In the recent release of serverless v1 19 they have mentioned that they have started working on a solution for this Serverless v1 19 Changelog"
47644083,45453680,"stackoverflow.com",0,"2017-12-05 02:14:34+02","2024-05-17 05:06:36.164329+03","Simply use a custom domain name httpdocs aws amazon comapigatewaylatestdeveloperguidehowtocustomdomains html You can map base path to an API so you could map service1 to the first API and service2 to the second API Clients would be hitting a custom domain name rather than the default executeapi endpoint that we provide "
56720505,45453680,"stackoverflow.com",0,"2019-06-23 05:02:09+03","2024-05-17 05:06:36.16633+03","In main service In secondary service Source httpsserverlessstack comchaptersapigatewaydomainsacrossservices html"
45472757,45453680,"stackoverflow.com",1,"2017-08-03 02:58:04+03","2024-05-17 05:06:36.167331+03","httpsserverless comframeworkdocsprovidersawsguideservices Note Currently every service will create a separate REST API on AWS API Gateway Due to a limitation with AWS API Gateway you can only have a custom domain per one REST API If you plan on making a large REST API please make note of this limitation Also a fix is in the works and is a top priority The inprogress fix can be tracked here The relevant forum thread has a few suggestions Probably the easiest is to have an extra API gateway that delegates to the real gateways either through None of those options are without flaws though "
47644038,45458993,"stackoverflow.com",3,"2017-12-05 02:09:27+02","2024-05-17 05:06:36.646312+03",""
45473364,45458993,"stackoverflow.com",3,"2017-08-03 04:29:34+03","2024-05-17 05:06:36.647313+03","I also have faced latency issues with api gateway but I do not have exact figures on that However as of now by design api gateway has a latency issue and it is an ongoing httpsforums aws amazon comthread jspathreadID225458 According to the engineers I think 700ms is bit more high Just in case there is another concern with lambda as hot start and cold start when you first call lambda latency is bit high And there are couple of workarounds to overcome this as mentioned in the following blog post httpsserverless comblogkeepyourlambdaswarm"
62673128,45471447,"stackoverflow.com",1,"2020-07-01 11:51:01+03","2024-05-17 05:06:38.316742+03","when I ran the stack I kept my deployed API stageName same as the BasePathMapping stageName and able to successfully create the Mapping Additionally I have provided the specific certificate for the Domain while the creation "
64598285,45471447,"stackoverflow.com",1,"2020-10-29 21:50:46+02","2024-05-17 05:06:38.31814+03","First you need a resource AWSApiGatewayStage This resource depends on AWSApiGatewayDeployment Then make AWSApiGatewayBasePathMapping to depends on AWSApiGatewayStage Here is my tested solution"
73056097,45471447,"stackoverflow.com",0,"2022-07-20 20:46:51+03","2024-05-17 05:06:38.319272+03","Both of the current answers are correct but not necessarily usable in all instances The more general answer is to either Heres a link to a serverless issue that discusses the cause of this and a lot of good discussion on whys and workarounds httpsgithub comserverlessserverlessissues2233"
45753692,45751946,"stackoverflow.com",9,"2017-08-18 15:05:44+03","2024-05-17 05:06:39.015467+03","You have to configure a NAT instance or a managed NAT Gateway to provide internet access to your Lambdas inside the VPC You may have to use the resource section of your serverless yml file to create the NAT Gateway NAT Instance resource Have a look at the resources section of the Serverless Framework documentation These resources will be added to the cloudformation stack upon serverless deploy You can overwriteattach any kind of resource to your CloudFormation stack You can add Resources Outputs or even overwrite the Description You can also use Serverless Variables for sensitive data or reusable configuration in your resources templates So you can add the Cloudformation template for a NAT Gateway inside the resource section For Example Here is a link to a complete CloudFormation snippet of Lambda inside VPC "
48365526,45751946,"stackoverflow.com",5,"2018-01-21 11:45:43+02","2024-05-17 05:06:39.018467+03","I created an example of lambda with vpc eslaticache and NAT gateway You can check at httpsgithub comittusawslambdavpcnatexamples"
45763504,45755167,"stackoverflow.com",6,"2017-08-18 22:04:53+03","2024-05-17 05:06:39.661288+03","In AWS you can setup a distributed architecture using other services to filter requests before hitting your serverless functions You can use AWS WAF to protect your API Gateway Endpoints Currently you have to proxy your API through CloudFront Check the following diagram and refer the AWS documentation for more details Diagram Credits AWS answers for AWS WAF Security Automations"
45768917,45768908,"stackoverflow.com",1,"2017-08-19 10:27:26+03","2024-05-17 05:06:41.089081+03","OK everyone forgets to mention that the thing you add should be named not"
45769466,45768908,"stackoverflow.com",0,"2017-08-19 11:32:42+03","2024-05-17 05:06:41.090415+03","You may want to try using precompiled Functions with new VS2017 15 3 tooling Then you will not have to create any folders manually just reference the NuGet packages that you need "
45784337,45771213,"stackoverflow.com",0,"2018-01-16 15:43:31+02","2024-05-17 05:06:41.859611+03","I am not sure what your issue is but I thought I would share some of my learning working with Twilio and AWS Lambda Not sure if this is what you already know or not however API Gateway in order to expose an HTTP endpoint via a Lambda you require a API Gateway configured to point to the AWS Lambda Lambda Request the only payload that Lambda will receive is a JSON payload Twilio will only produce a FORMURLEncoded format You have to configure API Gateway to transform the FormURLEndcoded to JSON Lambda Reply JSON in JSON out To convert the reply from JSON to TwiXML you have to transform the message from JSON to TwilXML AWS Lambda IMO does not hold a candle to Azure Functions Do yourself a favour and try Azure Functions it does not have any the restrictions of AWS Lambda and does not have to go through a translation layer like API Gateway In addition and likely the best feature you can run it locally without having to create your own framework Why AWS would not prioritize a local development environment is beyond me other than perhaps being first to market HTH"
47988507,45775676,"stackoverflow.com",0,"2017-12-27 10:04:23+02","2024-05-17 05:06:43.357537+03","Store refresh token in local storage is not a good idea because user can see local storage in developer tools or can run Javascript function locally which fetch data from local storage on your sitewebapp You can setup lambda function for this purpose and store refresh token in HttpOnly cookie You can set response header from your lambda function nodejs example This cookie will be sent on each request from your client to lambda HttpOnly will insure you that this cookie cannot be accessible from javascript "
45819830,45807461,"stackoverflow.com",12,"2017-08-22 17:03:01+03","2024-05-17 05:06:44.493177+03","Either move the mongoose connect code inside the handler or stop calling db close You currently have a single database connection being reused by multiple invocations of your Lambda function but you are closing it after the first invocation completes "
73109438,45807461,"stackoverflow.com",3,"2022-07-25 15:41:58+03","2024-05-17 05:06:44.494177+03","Lambda will executes your handleCreateUser function in each invocation but everything outside handleCreateUser will be executed only on cold start Lambda will cache that values for further invocations and will not execute in every invocation So i think you should move this code in handleCreateUser function "
68434149,45807461,"stackoverflow.com",1,"2021-07-19 03:58:20+03","2024-05-17 05:06:44.495176+03","My usage is Python but you will do this in your preferred language Best solution for lambda considering pay per time Run this globally before lamba function i do it in a config class Up to you how you implement make_connection Do not use db close at all AWS calls loads your lambda function and runs you global functions once After that at every call it ONLY RUNS the lambda which it keeps loaded for a while from some tests 20min to 50min The connection will be closed by the db driver on an internal timeout Advantages you only open connection once in a long time saving you time for every lambda run Disadvantages you hold on connection all the time lambda is in memory In my opinion it is worth it "
74414633,45807461,"stackoverflow.com",1,"2022-11-12 18:28:19+02","2024-05-17 05:06:44.497177+03","If you create the DB connection outside your handler it will be setup in the Init phase of the Lambda container Then it can be used by one or more invocations Invoke phase So closing the connection inside your handler will result in the following invocations having a nonusable closed connection To properly close your connection rather than awaiting the timeout you can use a hook that triggers on container shutdown As copied from here "
45837346,45837105,"stackoverflow.com",4,"2017-08-23 13:35:46+03","2024-05-17 05:06:45.22048+03","Try removing the stack first using serverless remove and then redeploy "
72145101,45837105,"stackoverflow.com",1,"2022-05-06 20:03:46+03","2024-05-17 05:06:45.221638+03","It is not the exact same issue but this github issue gives an alternative soulution Cannot rename Lambda functions 108 I commented the function definition I wanted to rename and the resources with references to it then sls deploy uncommented and sls deploy again The problem with this is that the first deploy will delete the function so you have to take into account this down time "
45847065,45847009,"stackoverflow.com",3,"2017-08-23 21:32:54+03","2024-05-17 05:06:46.212533+03","You cannot do socket io like bidirectional communication only using AWS Lambda since it is stateless and asynchronous You can use AWS IOT Web Sockets for this Optionally with Lambda "
45855776,45855775,"stackoverflow.com",3,"2017-08-24 10:45:11+03","2024-05-17 05:06:47.067647+03","This can be achieved by adding HttpVersion http2 below DistributionConfig property See full example below "
75661278,45855775,"stackoverflow.com",0,"2023-03-07 13:15:53+02","2024-05-17 05:06:47.068647+03","Although the OP asked for HTTP2 support AWS announced HTTP3 support for CloudFront in August 2022 So in addition to enabling HTTP2 support see Cninrohs answer you can also enable HTTP3 as suggested in the AWS official docs Allowed values http1 1 http2 http2and3 http3 HTTP3 HTTP2 and HTTP3"
46780884,45872046,"stackoverflow.com",1,"2020-06-20 12:12:55+03","2024-05-17 05:06:49.272737+03","As the official documentation states Note When you write your Lambda function code do not assume that AWS Lambda always reuses the container because AWS Lambda may choose not to reuse the container Depending on various other factors AWS Lambda may simply create a new container instead of reusing an existing container It seems like a bad architecture design to try to keep a Lambda Container up but apparently it is a normal scenario your warmed container not being used when a different event source triggers a new container "
46001289,45898048,"stackoverflow.com",2,"2017-10-31 16:40:55+02","2024-05-17 05:06:50.802533+03","Use a cron schedule with appropriate fields and the maxTriggers parameter set to 1 The maxTriggers parameter ensures that after a single event the trigger will not be called again Oneoff triggers can be created from the commandline Heres an example for a trigger to run once at 1st January 0000 There is an open issue in the opensource repo to provide better support for these kind of events httpsgithub comapacheincubatoropenwhiskpackagealarmsissues89"
45990391,45907122,"stackoverflow.com",9,"2017-09-01 00:32:34+03","2024-05-17 05:06:51.316593+03","Things I would look at Try comparing what happens between the two SLS_DEBUGtrue sls deploy verbose and SLS_DEBUGtrue sls deploy function f myFunction verbose Check your serverless config packaging etc against your project structure One red flag is that the function deploy is as big as the service deploy This could be a misconfiguration problem Use serverless package to see how the package s are zipped It can provide some clues Are you using any plugins which may have altered the way your package is created How many node_modules directory do you have Do you have only one for the entire service or one for each function"
45928297,45907122,"stackoverflow.com",2,"2017-08-29 01:42:03+03","2024-05-17 05:06:51.317593+03","You can make the deploy process more verbose by passing the verbose argument to the deploy function Either sls deploy verbose or sls deploy v will do the trick "
46012605,45907122,"stackoverflow.com",2,"2020-06-20 12:12:55+03","2024-05-17 05:06:51.319188+03","I was not able to figure out why function deployment as opposed to service deployment would hang I may have misconfigured my serverless yml file But no big deal I can do without sls deploy function myFunction Because my expectations were wrong I thought deploying a function would be way faster than deploying a service by somehow not redeploying the node_modules directory But there is no partial function deployment in AWS when a function is deployed all necessary node modules must be deployed as well for the function to work As explained in serverless doc The Framework packages up the targeted AWS Lambda Function into a zip file The Framework fetches the hash of the already uploaded function zip file and compares it to the local zip file hash The Framework terminates if both hashes are the same That zip file is uploaded to your S3 bucket using the same name as the previous function which the CloudFormation stack is pointing to I had naively hoped that only the updated handler would be uploaded to S3 But as the function is packaged before deployment it does need all of its modules and dependencies So the way I see it function deployment would save time as opposed to service deployment only if the service has multiple functions and the service functions do not use many common nodejs modules And if sls deploy function f myFunction does not hang that is So to increase development speed the trick is to use offline emulation with a tool like serverless offline serverless offline provides a local server and lambda function myFunction becomes accessible locally by calling httplocalhost3000myFunction in Postman or the browser In most cases sls deploy can be called only once after the handler has been thoroughly tested offline "
45911483,45911191,"stackoverflow.com",0,"2017-08-28 06:37:13+03","2024-05-17 05:06:52.084678+03","No A connection could be reused It does not need to start a new connection on every request If you use the redis creatClient to create a connection you could use this connection always in your app And it has reconnect mechanism if the connection is broken So in your app development you do not need to care the connection problem just get a global connection and always use it "
55122220,54762237,"stackoverflow.com",1,"2019-03-12 15:07:16+02","2024-05-17 05:13:33.906853+03","Finally it works using LambdaProxy integration adding a http event to the function"
45926464,45911450,"stackoverflow.com",1,"2017-08-28 22:57:22+03","2024-05-17 05:06:53.188202+03","As I understand it the Node SDK for Realm is only for the Professional and Enterprise editions of the Realm Object Server ROS The ROS can only be deployed on your own Linux instance Please see this for more details httpsrealm iodocsrealmobjectserverpeee"
46150859,45911450,"stackoverflow.com",1,"2017-09-11 11:02:27+03","2024-05-17 05:06:53.189699+03","Exactly as Clifton Labrum says the professional and enterprise editions of the Realm Mobile Platform has a node js SDK to be used on the server to listen for changes in Realms At every change in a Realm you will get an event which you can then process as you like For instance you can store objects in DynamoDB You can also just leave the objects in Realms "
46031163,45918399,"stackoverflow.com",1,"2017-09-04 09:07:39+03","2024-05-17 05:06:54.251959+03","The answer was hiding in transform runtime plugin for babel Correct babelrc file with the addition of babelpolyfill in the srcpackage json since serverless needs to be deployed with this Alternatives such as using webpack in conjunction with serverless are highly disrecommended "
45938766,45926811,"stackoverflow.com",0,"2017-08-29 15:07:39+03","2024-05-17 05:06:55.347113+03","Your table key is both id and vuelta but you are only providing vuelta in the delete request Modify the key in your delete request so it contains both the id and vuelta Also depending on your client library may need to specify"
45954104,45952948,"stackoverflow.com",1,"2017-08-30 10:07:32+03","2024-05-17 05:06:57.485924+03","As per the documentation Note There is no query hook for remove only for documents If you set a remove hook it will be fired when you call myDoc remove not when you call MyModel remove If you rewrite your query to use findOneAndRemove you can add a middlewarehook for that Also take into account Shubhams answer regarding arrow function expressions "
45953740,45952948,"stackoverflow.com",0,"2017-08-30 09:46:46+03","2024-05-17 05:06:57.486924+03","lambda function implements this as lexical this so it will not work use old style httpsgithub comgetifyYouDontKnowJSblobmasteres6202620beyondch2 mdnotjustshortersyntaxbutthis"
45962944,45962924,"stackoverflow.com",4,"2017-08-30 17:20:23+03","2024-05-17 05:06:58.692869+03","I would try removing those keys from the config and see what happens "
46063910,45973694,"stackoverflow.com",2,"2017-09-06 22:45:26+03","2024-05-17 05:06:59.098069+03","I ran into this problem and did some digging It turns out that v1 21 0 of serverless broke packaging of binaries httpsforum serverless comtserverless1210breakssharplibrary2606 httpsgithub comserverlessserverlessissues4182 The recommended fix is to upgrade to v1 21 1 "
45981320,45973694,"stackoverflow.com",0,"2017-09-03 21:08:35+03","2024-05-17 05:06:59.100069+03","Since Lambda runs on a Linux container you should be running serverless deploy from a Linux machine This way your native modules will be compiled for your target architecture which is Linux To check the deployment package that serverless creates you can use sls package or sls deploy noDeploy for older versions and inspect the serverless directory that it creates You will see a zip file in here extract its contents and test the code from there If the contents of this zip is not what you expect not the same as when you manually copy them then maybe something is wrong with your file structure andor serverless yml "
46033679,45990013,"stackoverflow.com",1,"2017-09-04 12:02:43+03","2024-05-17 05:07:01.175713+03","kubeless also creates services for functions so you should able to just do a http get to httpbikesearch8080 if your DNS setup is working and your application is in the same namespace If you are in another namespace you need to use a more qualified name e g bikesearch functionnamespacesvc cluster local If you want to call the function from outside the k8s cluster you might want to create an Ingress with kubeless ingress create "
46028425,45990013,"stackoverflow.com",0,"2017-09-04 01:41:36+03","2024-05-17 05:07:01.177713+03","If DNS is correctly configured for your cluster you can also directly access the Kubernetes master under the kubernetes DNS name Another way is environment variables Kubernetes itself is registered as a service so you can use the KUBERNETES_SERVICE_HOST environment variable For newer Kubernetes versions you have to authenticate so have a look at how to access the API server from within a pod "
51473763,45991897,"stackoverflow.com",18,"2018-07-23 10:38:53+03","2024-05-17 05:07:02.152929+03","I had this issue and the following is a summary of the steps I took to resolve Good luck "
52568547,45991897,"stackoverflow.com",1,"2018-09-29 15:51:14+03","2024-05-17 05:07:02.154124+03","Try to change IAM DB Authentication Enabled to YES on your database and apply changes immediately so you do not wait for maintenance That solved my problem "
46216561,46003563,"stackoverflow.com",20,"2017-11-23 11:17:24+02","2024-05-17 05:07:02.916466+03","Solved by using cfstackName outputKey "
46137559,46003563,"stackoverflow.com",3,"2017-09-10 08:24:13+03","2024-05-17 05:07:02.917848+03","It appears that you are using the ImportValue shorthand for CloudFormation YAML My understanding is that when CloudFormation parses the YAML and ImportValue actually aliases FnImportValue According to the Serverless Function documentation it appears that they should support the FnImportValue form of imports Based on the documentation for FnImportValue you should be able to reference the your export like Hope that helps solve your issue "
55137611,46003563,"stackoverflow.com",3,"2019-03-13 10:46:03+02","2024-05-17 05:07:02.919205+03","I struggled with this as well and what did trick for me was Note that intrinsic functions ImportValue is on a new line and indented otherwise the whole event is ignored when cloudformationtemplateupdatestack json is generated "
74408706,46003563,"stackoverflow.com",0,"2022-11-12 00:23:44+02","2024-05-17 05:07:02.920385+03","I could not find it clearly documented anywhere but what seemed to resolve the issue for me is For the Variables which need to be exposedexported in outputs they must have an Export property with a Name subproperty In serverless ts Once this is deployed you can check if the exports are present by running in the terminal using your associated aws credentials Then there should be a Name property in a list And then if the above is successful you can reference it with FnImportValue like so e g "
51663943,46029454,"stackoverflow.com",1,"2018-08-03 03:51:54+03","2024-05-17 05:07:03.939289+03","I have check with the latest version I believe serverless lambda application can not use normal methods to communicate with DynamoDB thus I am had to use SaveAsync "
55311739,46029454,"stackoverflow.com",0,"2020-06-20 12:12:55+03","2024-05-17 05:07:03.94129+03","The reason the NET Core version only has async is because the underlying http client available in NET Core only supports async operations We debated about keeping the sync methods in the NET Core version and have them just call the async versions and then block The problem with that is the SDK would not be following best practice for the platform and more importantly it could mask a potential performance problem httpsgithub comawsawssdknetissues480issuecomment257382757 AWS team recommends more details at httpsdocs aws amazon comlambdalatestdgdotnetprogrammingmodelhandlertypes htmldotnetasync"
46572304,46568278,"stackoverflow.com",4,"2017-10-04 22:00:55+03","2024-05-17 05:07:24.196991+03","When you create an API Gateway it will create an underlying CloudFront distribution on your behalf You may not see it in your AWS account but it is there Also domains used for CloudFront distributions must be globally unique Which means that if someone else already has a CloudFront distribution for api example com then you cannot use it as well So you already said that you do not have an existing CloudFront distribution visible in your AWS account If you already have an API Gateway for api example com then you cannot create a second one If this is the case you need to use a different domain or delete the original one If you do not have an API Gateway for that domain then someone else probably already has one If this is the case then you will need to use a different domain "
56797323,46043872,"stackoverflow.com",7,"2019-06-27 22:08:06+03","2024-05-17 05:07:04.515583+03","For anybody else finding this through google hope this helps What is causing it AWS has a limit of 75GB on the size of all the deployment packages that can be uploaded per region This includes all of your Lambda functions and all their historical versions combined in a given region The error could happen if you have a large number of Lambda functions that have been deployed many times Each deployment creates a version and this can add up over time Solution 1 If you do not need to version your Lambda functions you can turn off Lambda versioning by setting it in your serverless yml Solution 2 Alternatively you can manually remove older Lambda versions You can use the serverlesspruneplugin to automate the process for you The plugin can be used to do a onetime clean up or can be configured in your serverless yml to auto prune older Lambda versions after each deployment Heres more details about this error httpsseed rundocsserverlesserrorscodestoragelimitexceeded"
46044801,46043872,"stackoverflow.com",2,"2017-09-05 01:43:05+03","2024-05-17 05:07:04.518583+03","httpdocs aws amazon comlambdalatestdglimits html Every Lambda function is allocated with a fixed amount of specific resources regardless of the memory allocation and each function is allocated with a fixed amount of code storage per function and per account Lambdas have invocation limits but also deployment limits which is what your problem is Look through the limits and work out which one has been breached "
48281733,46043872,"stackoverflow.com",0,"2018-01-16 14:51:45+02","2024-05-17 05:07:04.519752+03","Lambda creates a version of your functions on each deployment so frequent deploys can cause storage problems Your solution is correct however you can also remove other unused versions of your functions by writing a simple script First you would like to get the versions of your functions then decide which ones you want to delete"
46047920,46047478,"stackoverflow.com",7,"2019-02-27 00:35:00+02","2024-05-17 05:07:05.595596+03","With CloudFormation that is pretty straight forward All you need is to define an AWSRDSDBInstance AWS also provides some example templates for that httpsdocs aws amazon comAWSCloudFormationlatestUserGuidesampletemplatesservicesuseast1 htmlw2ab2c23c42c13c27 As you can include CloudFormation resources in Serverless you can add that directly to your serverless yml so it gets deployed by Serverless without the need to use a separate CloudFormation deployment httpsserverless comframeworkdocsprovidersawsguideresources More complete example of all available options for RDS including Aurora httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawsresourcerdsdbcluster html"
46047961,46047478,"stackoverflow.com",2,"2017-09-05 09:04:08+03","2024-05-17 05:07:05.598597+03","Yes you can definitely create your rds instances with cloudformation templates This json snippet will create your RDS instance for complete reference to create your RDS and Elastic beanstalk refer httpsgithub comsatterlyAWSCloudFormationsamplesblobmasterElasticBeanstalk template Hope it will help "
46082505,46058826,"stackoverflow.com",0,"2017-09-06 21:58:03+03","2024-05-17 05:07:06.697276+03","Error is fixed now it is the serverless framework issue They have fixed it on 1 21 1 release httpsgithub comserverlessserverlessreleasestagv1 21 1 "
46230228,46082516,"stackoverflow.com",2,"2017-09-15 04:05:42+03","2024-05-17 05:07:07.380422+03","I think the problem is that the apolloclient library uses rollup to produce a UMD and webpack is having trouble with the UMD Great what do you do This Webpack plugin might fix it If not can you share the code that uses the apollo client"
46186408,46098047,"stackoverflow.com",5,"2017-09-13 01:50:26+03","2024-05-17 05:07:08.484365+03","if you want complete freedom full transparency in Lambda then you might want to look at Lambda proxy integration and in template"
46098721,46098047,"stackoverflow.com",1,"2017-09-07 18:12:37+03","2024-05-17 05:07:08.485858+03","Please use input params key Please create a body mapping template in API gateway integration response and try input params YourQueryStringKey Example Then in Lambda function you can get value like below"
48283029,46098173,"stackoverflow.com",14,"2018-01-16 16:18:45+02","2024-05-17 05:07:09.432454+03","I have encountered the same issue and I spent hours on it Finally I found a solution do NOT ref the bucket Change to Or even simpler"
46369091,46358050,"stackoverflow.com",3,"2017-09-22 19:16:22+03","2024-05-17 05:07:10.486816+03","When you get Process exited before completing request it means that the node process has crashed before Lambda was able to call callback If you go to Cloudwatch logs there would be an error and stack trace of what happened You should connect to the MongoDB instance inside your handler and before you call callback disconnect first It would be like this "
58601791,46358050,"stackoverflow.com",1,"2019-10-29 07:20:39+02","2024-05-17 05:07:10.488537+03","Here is an article explaining with details how lambda work with node and an example of how to implement DB connection Differently of dashmug suggested you should NOT disconnect your DB since connecting every time will decrease your performance "
46362248,46361980,"stackoverflow.com",1,"2017-09-22 13:13:19+03","2024-05-17 05:07:11.588356+03","I think you can see the execution logs in Cloudwatch Log Groups LogGroupFunctionName"
46375173,46371287,"stackoverflow.com",2,"2017-09-23 04:37:31+03","2024-05-17 05:07:12.642022+03","You can do it via external plugins and the solution is discussed in detail here httpsgithub comserverlessserverlessissues1918"
56331760,46371287,"stackoverflow.com",2,"2019-05-27 22:52:25+03","2024-05-17 05:07:12.644023+03","It is now supported by Serverless framework and there is no need for a plugin You need to add the following configuration to they serverless yml file Notice that it was added in version 1 42 0 "
46395097,46394171,"stackoverflow.com",39,"2017-09-27 00:56:43+03","2024-05-17 05:07:13.905232+03","I got it The key was just adding a list under the key Resource but I also learned that it is better to just use the logicalIDs you use when provisioning the tables Full example to follow"
46436496,46394171,"stackoverflow.com",7,"2017-11-08 09:40:46+02","2024-05-17 05:07:13.907233+03","I would like to put my updates since I spend time and learn a lot from this question The currently accepted answer is not fully functioned What I added 1 Make sure in your handler there is an environment TABLE_NAME or another name you can adjust accordingly as below it is referring the lambda functions environment variables 2 update serverless yml to nominate table name to each function or Depend on which table the function targets The full serverless yml is updated here Refer serverless environment variables"
66345167,46394171,"stackoverflow.com",0,"2021-02-24 07:28:02+02","2024-05-17 05:07:13.909233+03","If your intention is to provide access to all tables in the stack that is being deployed you can use This way the lambdas in your stack are limited to tables in your stack and you will not have to update this every time you add a table "
46407920,46400550,"stackoverflow.com",1,"2017-09-25 17:39:26+03","2024-05-17 05:07:14.757933+03"," i have administrator access to all aws services Take note that the Lambda function is NOT running under your user account You are supposed to define its role and permissions in your YAML In the provider section in your serverless yaml add the following Reference httpsserverless comframeworkdocsprovidersawsguideiam"
46401370,46400550,"stackoverflow.com",0,"2017-09-25 15:27:17+03","2024-05-17 05:07:14.759928+03","You are not authorized to perform this operation This means you have no permission to perform this action client describe_instance_status There some ways to make your function can get right permission Read more from this httpdocs aws amazon comlambdalatestdgintropermissionmodel html"
46416289,46405045,"stackoverflow.com",21,"2017-09-26 04:01:13+03","2024-05-17 05:07:15.776707+03","According to the documentation there is a few ways to attach existing roles to a function or entire stack Note that the role you use must have additional permissions to log to cloudwatch etc otherwise you will not get logging "
52758537,46568278,"stackoverflow.com",3,"2018-10-11 14:07:15+03","2024-05-17 05:07:24.199145+03","Heres how I have encountered and solved this problem"
46514376,46412929,"stackoverflow.com",5,"2017-10-04 14:11:33+03","2024-05-17 05:07:16.706317+03","By default Serverless enables proxy integration between the lambda and API Gateway What this means for you is that API Gateway is going to pass an object containing all the metadata about the request into your handler as you have noticed Mon Sep 25 204522 UTC 2017 Endpoint request body after transformations resourcepricingtestpathpricingtesthttpMethodGETheadersnullqueryStringParameters nameciao surnamebonjour pathParametersnullstageVariablesnull This clearly does not map to your model which has just the fields name and surname in it There are several ways you could go about solving this Your attempt with the HelloWorldRequest class does actually work if you make your class a proper POJO by making the fields mutable and thus creating the setters for them AWS Lambda documentation states The get and set methods are required in order for the POJOs to work with AWS Lambdas built in JSON serializer Also keep in mind that Scalas Map is not supported If you do not need the metadata then instead of changing your model you can make API Gateway pass only the data you need into the lambda using mapping templates In order to do this you need to tell Serverless to use plain lambda integration instead of proxy and specify a custom request template Amazon API Gateway documentation has an example request template which is almost perfect for your problem Tailoring it a little bit we get This template will make a JSON out of the query string parameters and it will now be the input of the lambda Endpoint request body after transformations name ciao Which maps properly to your model Note that disabling proxy integration also changes the response format You will notice that now your API returns your response model directly statusCode200body \message\\Hello ciao\ headers xfoocoucou base64Encodedtrue You can fix this by either modifying your code to return only the body or by adding a custom response template This will transform the output into what you expect but will blatantly ignore the statusCode and headers You would need to make a more complex response configuration to handle those Instead of extending RequestHandler and letting AWS Lambda map the JSON to a POJO you can instead extend RequestStreamHandler which will provide you an InputStream and an OutputStream so you can do the de serialization with the JSON serializer of your choice "
46417237,46417182,"stackoverflow.com",10,"2017-09-26 06:09:20+03","2024-05-17 05:07:17.341927+03","When debugging serverless s packaging process use sls package or sls deploy noDeploy for old versions You will get a serverless directory that you can inspect to see what is inside the deployment package From there you can see if node_modules is included or not and make changes to your serverless yml correspondingly without needing to deploy every time you make a change "
69783699,46417182,"stackoverflow.com",2,"2021-10-31 05:13:37+02","2024-05-17 05:07:17.343896+03","Serverless will exclude development packages by default Check your package json and ensure your required packages are in the dependencies object as devDependencies will be excluded "
74303854,46417182,"stackoverflow.com",0,"2022-11-03 15:27:22+02","2024-05-17 05:07:17.345119+03","I was dumb to put this in my serverless yml which caused me the same issue you are facing "
46427367,46423346,"stackoverflow.com",2,"2017-09-26 19:41:59+03","2024-05-17 05:07:18.285334+03","Do not use str r as it will turn it into a stringified representation of a Python dictionary "
46440458,46439875,"stackoverflow.com",5,"2018-07-23 18:45:15+03","2024-05-17 05:07:18.706357+03","Based from this answer and this AWS documentation page I was able to figure out how to solve it The solution is to add the following in my serverless yml "
46453979,46449906,"stackoverflow.com",2,"2017-10-01 19:40:54+03","2024-05-17 05:07:19.545849+03","I cannot see a way a great and clean way of implementing an express like middleware using lambda functions it is a different concept I guess it would be a better approach to Fore more httpdocs aws amazon comapigatewaylatestdeveloperguideusecustomauthorizer html"
46458862,46449906,"stackoverflow.com",0,"2017-09-28 02:52:30+03","2024-05-17 05:07:19.54785+03","Sounds like a standard VETO pattern verify enrich transform operation As Tom Melo mentioned you can hook a cusom authorizer to an API gateway if your setup has one If you want a single place to sanitise and transform the request you can use a proxy pattern in the API gateway to alter the message then route to the real operation via SNS based on the path parameters etc If you do not have an API Gateway then you can chain everything together using SNS "
46473985,46473585,"stackoverflow.com",1,"2017-09-28 19:23:23+03","2024-05-17 05:07:20.463875+03","There is a configuration problem somewhere Basically it is sending a request to productiongraphql when it should have been sending it to graphql "
47699320,46491769,"stackoverflow.com",0,"2017-12-07 18:16:31+02","2024-05-17 05:07:21.555868+03","I have seen this error and it is nondescriptive I believe it means an exception is being thrown that is not being caught It also appears when there is a runtime syntax error like using an undefined variable Here is my workflow which is different than yours but allows me to set a breakpoint and step through the code From this I can identify the bad line Install serverlessoffline so you can run your project locally Install Visual Studio Code so you have an IDE to debug in I have posted my launch json to Breakpoints not being hit when debugging Serverless in vscode Set a breakpoint then hit your enpoing with a browser or curl "
46499239,46498399,"stackoverflow.com",3,"2017-09-30 06:14:14+03","2024-05-17 05:07:22.252411+03","When you are developing severless projects locally there are several tools you can leverage to improve the productivity We have shared solutions to similar issues as listed below which contains development setup to emulate Web Frontend Serverless API Gateway Lambda and DynamoDB locally with automation using Gulp e g"
46503483,46500957,"stackoverflow.com",2,"2017-09-30 17:07:46+03","2024-05-17 05:07:23.890383+03","Have you considered getting small EC2 instance and then set up cron jobs there It can then publish events to SNS or directly call required tasks And you should be able to schedule new jobs dynamically as well "
46545809,46500957,"stackoverflow.com",2,"2017-10-03 16:30:54+03","2024-05-17 05:07:23.891638+03","You can use DynamoDB with TTL DynamoDB Streams and AWS Lambda for this Since the schedule is dynamic and coming from the user you can save those items in a DynamoDB table with its TTL set to the scheduled execution time When the TTL is reached for an item it will create a DynamoDB Stream which you can then use to trigger a Lambda function References"
46510173,46500957,"stackoverflow.com",1,"2017-10-01 10:26:47+03","2024-05-17 05:07:23.892636+03","As a workaround why not have the lambda wake on a Cloudwatch alert then check for tasks every 5 seconds until 55 seconds have elapsed"
50590143,46500957,"stackoverflow.com",1,"2018-05-29 20:49:45+03","2024-05-17 05:07:23.893636+03","You likely already found a solution to this but my service httpsposthook io may be good fit for your use case It lets you schedule hooks with an API call like this Then from your lamdba function you can either use the data you passed in as data or the hook s unique ID to look something up in your database and do the needed work A free account allows you to schedule 500 of these requests a month "
46557238,46500957,"stackoverflow.com",1,"2018-05-31 06:37:55+03","2024-05-17 05:07:23.895196+03","Other solutions seem promising but there are another solution I found using step functions wait state httpdocs aws amazon comstepfunctionslatestdgamazonstateslanguagewaitstate html I cannot use it in my region yet because my region is singapore and it cannot be used across region Currently now I would try to see a dynamodb solution above As of 2018 step function was generaly available and work as expected"
50616246,46500957,"stackoverflow.com",0,"2018-05-31 06:35:47+03","2024-05-17 05:07:23.900196+03","As of 2018 there was Azure Logic Apps An equivalence service to aws step function on azure It contains delay connector that can schedule delay time httpslearn microsoft comenusazureconnectorsconnectorsnativedelay"
78157304,46500957,"stackoverflow.com",0,"2024-03-14 01:18:29+02","2024-05-17 05:07:23.901401+03","Another solution would be to use EventBridge scheduler httpsdocs aws amazon comschedulerlatestUserGuidescheduletypes htmlonetime"
46575821,46568278,"stackoverflow.com",16,"2017-10-05 02:49:36+03","2024-05-17 05:07:24.195991+03","Thanx to Matts guidance I found what caused the phantom CloudFront distros Within the AWS console go to Amazon API Gateway Custom Domain Names Delete all the conflicting domains there and redo the deployment steps again "
71104189,71091145,"stackoverflow.com",31,"2022-02-13 21:35:42+02","2024-05-17 05:29:23.329398+03","You can fix it by setting the name of the service directly to service property like this The nested notation is no longer supported "
68352398,46568278,"stackoverflow.com",0,"2021-07-12 21:35:26+03","2024-05-17 05:07:24.200235+03","The phantom cloudfront distro is not visible because it is in an account owned by AWS itself and used for deploying distros used by Edgeoptimized instances of API gateway As Daniel mentioned in his answer above delete any custom domain names associated with the certificate in the console but if they are not visible if they were already deleted for example try deleting them using the AWS CLI since it appears to do a better job of fully cleaning up the links to those AWS Cloudfront distros aws apigateway deletedomainname region INSERT REGION domainname INSERT DOMAIN NAME As CGreg encountered with the sls_delete_domain command you may encounter errors I was trying to delete an old certificate in ACM that was associated with one of these distros via a custom domain name which I would already deleted I ran the above command twice per custom domain name in any region I thought I might have originally deployed to The first time I got a domain name not found error and the second time I got an operation timed out max retries error but the association to the Cloudfront distribution was removed and I was able to delete the old cert Not sure if just once would do it but since I got different error responses something different must have been happening under the hood so no harm trying it "
46611850,46577881,"stackoverflow.com",1,"2017-10-06 21:27:27+03","2024-05-17 05:07:25.357837+03","I figured out what I was doing wrong The Execution failed due to configuration error Malformed Lambda proxy response error was referencing the response Lambda was trying to send back through API Gateway Serverless automatically uses Lambda Proxy Integration When I was looking at the response I noticed extra escaping at the base response level for instance \isBase64Encoded\ false \statusCode\ 200 which I believe was triggering the error Sure enough when I dug in deeper I realized that I was dumping JSON json dumps after creating the response which was throwing the error Thanks for the responses Mindless mistake on my part "
46596621,46579421,"stackoverflow.com",5,"2017-10-06 03:29:10+03","2024-05-17 05:07:26.276271+03","I do not believe Serverless has this functionality Your options are"
46622928,46579421,"stackoverflow.com",2,"2017-10-07 20:22:13+03","2024-05-17 05:07:26.277271+03","I found this sample config file and it contains a bucket name prefixed by a serverless module path This is a complete shot in the dark but if you want to write to fooBAR maybe this setting would work for you EDIT Does changing the package name affect which key the deployment is written to"
46713424,46579421,"stackoverflow.com",0,"2017-10-12 18:26:32+03","2024-05-17 05:07:26.279272+03","Here is example with putting index html I am not that much familiar with serverless framework so hope it helps "
46595653,46595148,"stackoverflow.com",3,"2017-10-06 01:23:35+03","2024-05-17 05:07:27.187529+03","Create an API key for your nonproduction API Gateway A different key from any that you use in production Give that key to your dev team then they will be able to call your nonproduction API Gateway endpoints while the API will be inaccessible to anyone without the key "
46602662,46602254,"stackoverflow.com",2,"2017-10-06 12:33:02+03","2024-05-17 05:07:28.166906+03","The templates Serverless uses to deploy are available in two places"
46608138,46603227,"stackoverflow.com",3,"2017-10-06 17:32:44+03","2024-05-17 05:07:29.268197+03","You can use AWS_IAM as your Authorization for your endpoint You can then create an IAM Policy like this which allows you to specify the IP Address Article httpbenfoster ioblogawsapigatewayiprestrictions"
46607943,46603227,"stackoverflow.com",2,"2017-10-06 18:48:35+03","2024-05-17 05:07:29.26945+03","The real problem is not about white listing using AWS SAM template but rather how do you do it all Once you finalise a method in general it can be generalised into a template I personally think creating a separate VPC for your API gateway should be a good solution Or you could block traffic from within your application logic or AWS WAF EDIT Therefore No I do not think it could be done just via a SAM template for API gateway resource "
46689819,46619746,"stackoverflow.com",9,"2017-10-11 16:43:30+03","2024-05-17 05:07:30.276592+03","The ability to configure user pool with the new SignUp flow options is not yet supported through CloudFormation The parameter that is used to specify the email or phone number only options is UsernameAttributes We will add this as a 1 to the feature request to support this with CloudFormation "
46620152,46619746,"stackoverflow.com",1,"2017-10-07 15:31:56+03","2024-05-17 05:07:30.278593+03","You need to set the AliasAttributes AWSCognitoUserPool AliasAttributes Here a sample CloudFormation template"
58282670,46619746,"stackoverflow.com",1,"2019-10-08 11:25:40+03","2024-05-17 05:07:30.280593+03","The ability to configure user pool with the new SignUp flow options is now supported through CloudFormation AWSCognitoUserPool UsernameAttributes like so"
75899407,46619746,"stackoverflow.com",0,"2023-03-31 17:00:08+03","2024-05-17 05:07:30.281593+03","Works for me even on updating UserPool via CloudFormation template More configuration options available in the official AWS docs "
47699137,46668483,"stackoverflow.com",0,"2017-12-07 18:07:01+02","2024-05-17 05:07:30.989173+03","Here is my launch json which allows me to use breakpoints I am using the serverlessoffline to run locally I also am using webpack and babel The skipCacheInvalidation is for that I hope this points you in the right direction "
46697547,46689554,"stackoverflow.com",10,"2022-05-10 21:26:32+03","2024-05-17 05:07:32.114386+03","If you want to respond with HTTP errors in that case you have to encode the HTTP error as successful Lambda response This type of errorhandling is specific to API Gateway Just like in traditional Node web servers e g express you can throw any error using throw new Error Invalid Payload and the middleware usually converts it into an HTTP response with the correct response status In API Gateway Lambda this can be written like this Basically it is a handled error The lambda function succeeded but the request failed maybe 400 404 or 500 You should be handling errors always or else if your handler crashes due to a runtime error or syntax error or any unhandled error your user will get an unexpected response a 500 or a 502 which you probably do not want Please remember that Lambda is not only used for API Gateway callback error is used for nonAPI Gatewaytriggered Lambdas For example if you have an SNStriggered Lambda you can return callback Any error message here and it will let SNS know that it failed and so SNS can retry the invocation "
46696074,46690906,"stackoverflow.com",2,"2017-10-11 22:28:40+03","2024-05-17 05:07:32.781801+03","lambda is automatically encoding to base64 so I had to remove it and directly send the buffer "
74310506,46690906,"stackoverflow.com",0,"2022-11-09 05:46:39+02","2024-05-17 05:07:32.783802+03","I came across similar problem while using serverless Express AWS Gateway needs a http response like this It is useless to put isBase64Encoded in the Http Response Header AWS API Gateway only checks if the isBase64Encoded is outside the Http header If it is true then decode the body before sending it to a HTTP client Express its response object does not seem to allow to add something outside HTTP Header in a Http Response If you do not want to give up using Express the workaround is to do the decoding in the browser which is pretty easy"
60968571,46694083,"stackoverflow.com",0,"2020-04-01 13:15:46+03","2024-05-17 05:07:33.898953+03","AWS Lambda is intended to process distinct events given as an object from AWS Api Gateway or any other Lambda invocation What you could do instead"
64522293,46698545,"stackoverflow.com",1,"2020-10-25 11:42:53+02","2024-05-17 05:07:34.615374+03","I know this is super old but in case someone else comes across this I built basically a params factory I have a function that returns my params and my ExpressionAttributeValues object and UpdateExpression string only comprise of the variables that pass my validation"
75908312,71091145,"stackoverflow.com",3,"2023-04-08 16:42:07+03","2024-05-17 05:29:23.330399+03","This is working for me just remove name in server yaml Use service auctionservice service name auctionservice"
46700076,46699410,"stackoverflow.com",1,"2017-10-12 04:40:22+03","2024-05-17 05:07:35.701644+03","I will avoid dealing with local filesystem when dealing with serverless Why not use S3 to download your binary file If you still want to download a binary file and pipe it to filesystem It can download and get it to the filesystem Reference httpsgithub comrequestrequest Hope it helps "
46719449,46711598,"stackoverflow.com",1,"2017-10-13 00:53:17+03","2024-05-17 05:07:36.724215+03","If you specify a particular deployment bucket instead of letting serverless create its own then you may encounter this bug For now you can hack the deployed server less code and comment out the check then wait for the bug to be fixed "
46763522,46762906,"stackoverflow.com",0,"2017-10-16 08:34:37+03","2024-05-17 05:07:37.768323+03","The aws SDK lib uses https to connect to various services like s3 sqs etc Since you are running performance tests its likely to invoke new lambda containers in multiple configured subnets I guess you are getting the ECONNREFUSED while sending connecting to outside world via HTTPHTTPS protocols My suggestion is to check the flow logs for the subnets that you are using for lambdas Search for the REJECT OK for 443 port and verify if the errors are originating for lambdas running in specific subnets "
74295277,46762906,"stackoverflow.com",0,"2022-11-02 22:26:04+02","2024-05-17 05:07:37.770324+03","AWS api gateway by default uses HTTPS make sure to send requests using HTTPS and not HTTP "
77168058,46762906,"stackoverflow.com",0,"2023-09-24 19:51:07+03","2024-05-17 05:07:37.772324+03","The callback url or Endpoint should be DOCS httpsdocs aws amazon comapigatewaylatestdeveloperguideapigatewayhowtocallwebsocketapiconnections html"
46770460,46765894,"stackoverflow.com",1,"2017-10-16 15:34:07+03","2024-05-17 05:07:39.376519+03","httpsserverless comframeworkdocsprovidersawseventss3 e g "
46976421,46971822,"stackoverflow.com",1,"2017-10-27 16:43:55+03","2024-05-17 05:07:41.031417+03","First of all user insert returns a Promise and calling db closeConnections right after it may close the connection when you still need it To achieve what you want to do db closeConnections should be called just before the callback parameter Maybe you could call db closeConnections in your helper functions responseHelper success and responseHelper error before the execution of the callback parameter I guess these functions are written only once and shared by all your lambda handler "
47206938,47047539,"stackoverflow.com",0,"2017-12-01 04:12:00+02","2024-05-17 05:07:42.026981+03","You can use dot notation in your UpdateExpression in order to set values for nested properties "
47078830,47078759,"stackoverflow.com",5,"2017-11-02 17:21:19+02","2024-05-17 05:07:42.966855+03","You either need to remove the Lambda function from your VPC if it does not need VPC resource access then adding it to the VPC only introduces performance issues anyway or you need to make sure the Lambda function is in a private subnet of your VPC and that subnet has a route to a NAT Gateway "
47169642,47127849,"stackoverflow.com",0,"2017-11-08 02:29:13+02","2024-05-17 05:07:43.393089+03","You can export the model and save it in a config file to be used later This is not necessarily what you want but it gives you version control over the standard models and lets you use them across services Example using serverlessawsdocumentation in serverless yml in the referenced models yml Note that the schema field seems to be optional I believe at least my tests deployed and ran without it I just kept it in because AWS template uses it That will give you local control over the model This will mean a bit of extra work on your end to synchronize models across stacks Now if you want to use an existing model in another service by reference and not by redeploying it I am not entirely sure if that is possible CloudFormation Model objects seem to only be identified by their model name able to be associated with a single ApiGatewayRestApi object at a time When I try to deploy the same model name across stacks it happily obliges and the old model remains unchanged There may be a way I am missing though"
51430429,47127849,"stackoverflow.com",0,"2018-08-16 22:46:27+03","2024-05-17 05:07:43.395084+03","Update The solution that I originally posted worked in some cases but not in others I have now posted a solution that will generally work and is much simpler I just ran into the same problem how to keep the API ID in those model references not hardcoded This question is already somewhat old but since it has taken me several hours to find the answer I will post it here for posterity Say you have a request model called CreateUserRequest and you want to factor out the User schema then put the following into your serverless yaml The JSON schema goes into the user json file in the models subfolder If you want to reference other JSON schemas from within the user json file there is a special way to do that in serverlessawsdocumentation And in your serverless yaml file you will need to make those referenced schema files known as models too Note that the references within these JSON schema files are not valid JSON schema references so you for example cannot load them with a JSON schema validator at least not directly It is just what serverlessawsdocumentation expects Would be much nicer if it was able to support true JSON schema references but then I also do not have the time to contribute such an enhancement "
47203238,47155378,"stackoverflow.com",4,"2021-03-03 22:25:24+02","2024-05-17 05:07:44.450718+03","Yes indeed there is no actual REST service backing up Google Cloud Functions It uses out of the box HTTP triggers To hustle the way around I am using my request payload to determine which action to perform In the body I am adding a key named path For example consider the Function USER To add a user To remove a user If your operations are purely CRUD you can use request method which offers verbs like GET POST PUT DELETE to determine operations "
68750152,47155378,"stackoverflow.com",3,"2021-08-12 02:59:43+03","2024-05-17 05:07:44.452719+03","You could use Firebase Hosting to rewriting URLs In your firebase json file Keep in mind this is a workaround and it has a major drawback you will pay for double hit Consider this if your app has to scale "
72559555,47155378,"stackoverflow.com",2,"2022-06-09 17:05:13+03","2024-05-17 05:07:44.453719+03","You can write your functions in different runtimes The Node js runtime uses the Express framework So you can use its router to build different routes within a single function Add the dependency The following example is using typescript Follow these guidelines to initiate a typescript project to deploy run"
47186339,47155378,"stackoverflow.com",0,"2017-11-08 20:24:07+02","2024-05-17 05:07:44.454694+03","Currently in google allows only one event definition per function is supported For more"
66465939,47155378,"stackoverflow.com",0,"2022-10-04 08:35:02+03","2024-05-17 05:07:44.457634+03","Express can be installed with npm i express then imported and used more or less as normal to handle routing If Express is not an option for some reason or the use case is very simple a custom router may suffice If parameters or wildcards are involved consider using routeparser A deleted answer suggested this app as an example The Express request object has a few useful parameters you can take advantage of Heres a simple proofofconcept to illustrate Usage If you run into trouble with CORS andor preflight issues see Google Cloud Functions enable CORS"
47223830,47185826,"stackoverflow.com",0,"2017-11-10 15:25:06+02","2024-05-17 05:07:45.429331+03","You need to handle warmup events as a special case in your handler Basically your handler should return right away when it is invoked via a warmup event In the example given in serverlesswarmupplugin you can do it this way Notice that there is an if statement at the beginning to check if it is a warmup event If it is return successfully right away This check should be at the beginning so that it does not have to connect to MongoDB at all "
55383608,55366403,"stackoverflow.com",0,"2019-03-27 19:49:34+02","2024-05-17 05:14:12.440876+03","is there any chance you are using template exported from an old stack By the way What happens if you just It will create the template for you and the stack and deploy it "
47222889,47217486,"stackoverflow.com",1,"2017-11-10 14:32:50+02","2024-05-17 05:07:46.138992+03","You need to specify which value from env yml you want to use In your example if you want to get the value of keyonetwo you would use Which would yield valueonetwo Also checkout the documentation and how they reference environment variables You need to set each environment variable so you would need"
47222829,47217486,"stackoverflow.com",1,"2018-02-24 13:05:34+02","2024-05-17 05:07:46.140992+03","Environment variables cannot be an object They are simply keyvalue pairs where value should be of primitive types i e stringnumberbooleannull Your keyone variable is an object which is why it throws the error Variables must be an object with String or simple type properties "
47249458,47249256,"stackoverflow.com",4,"2017-11-12 15:38:24+02","2024-05-17 05:07:46.86403+03","In your serverless yml you have not given the Lambda function any permissions to access S3 The examples in your template are commented out Lambda functions use IAM roles for permissions to access AWS resources In the Amazon Management Console select your Lambda function Scroll down and look for Execution role This will show you what your template created for your function Manage Permissions Using an IAM Role Execution Role Each Lambda function has an IAM role execution role associated with it You specify the IAM role when you create your Lambda function Permissions you grant to this role determine what AWS Lambda can do when it assumes the role There are two types of permissions that you grant to the IAM role If your Lambda function code accesses other AWS resources such as to read an object from an S3 bucket or write logs to CloudWatch Logs you need to grant permissions for relevant Amazon S3 and CloudWatch actions to the role If the event source is streambased Amazon Kinesis Streams and DynamoDB streams AWS Lambda polls these streams on your behalf AWS Lambda needs permissions to poll the stream and read new records on the stream so you need to grant the relevant permissions to this role IAM Policies for AWS Lambda"
49985128,47249256,"stackoverflow.com",3,"2018-04-23 18:49:17+03","2024-05-17 05:07:46.867029+03","I had the permissions already but adding the following resources solved the issue for me"
47275306,47253027,"stackoverflow.com",1,"2017-11-14 01:37:01+02","2024-05-17 05:07:47.731303+03","You could do it at deploy time but certificates expire so it is best to make this a recurring thing When I was faced with this problem I created a Lambda to upsert the SSL certificate and install it it needs a long timeout but that is fine it does not need to run often The key it needs can be given as secure environment variables Then I use serverlesswarmupplugin to set up a daily trigger to check whether the certificate is due to be refreshed The plugin is configurable to warm up relevant lambdas on deployment too which allows me to check for expired or missing SSL certs on each deploy Maybe you could do something similar "
47365528,47253027,"stackoverflow.com",0,"2017-11-18 13:30:19+02","2024-05-17 05:07:47.733304+03","Using Step Functions would be the best choice for this particular use case Since you have 3 distinct steps they would be three seperate Lambda functions that Step Functions can pass inputsoutputs between as well as including wait times and retries "
47283121,47282928,"stackoverflow.com",4,"2017-11-14 12:18:58+02","2024-05-17 05:07:48.982033+03","The problem is that returns a bytes object not a string This is because the encoding may require more than a single character Now you are using split on a bytes object but you cannot split it using a string you need to split using another bytes object Specifically should fix your error but depending on what you are expecting maybe decoding is more appropriate before the split"
47288679,47286970,"stackoverflow.com",6,"2017-11-16 07:33:33+02","2024-05-17 05:07:50.041145+03","Ok so I figured out the problem There was a key statement missing in the question that I posted because I thought it was not relevant but which turned out to the problem I was just stringifying the results from the query using The above works for a get but for a query it needs to be result Item s Silly on my part Thank you for the help P S I have added the statement to the original question to be clear"
47464349,47301055,"stackoverflow.com",5,"2017-11-24 01:08:23+02","2024-05-17 05:07:51.170997+03","Including a file In the example given error json can contain any valid schema So something as simple as this is fine typeobjectproperties message typestring It is also fine to include attributes like schema and title This is especially handy when you have models already defined in AWS but you do not have the serverless yaml to build them You can simply copy the schema out of the AWS console paste the json into a file and use the schema file syntax mentioned in the question As far as I can tell anything that you can get the AWS console to accept will work DRY I do not know of a way to reference models from within other models in a serverless file but you could use the same approach as the plugin authors and just put anything you need to reuse outside of the models and somewhere it is easier to reuse The plugin authors use commonModelSchemaFragments So if you have some fragments like so You can reference those fragments in models like this Marking attributes optional You can accomplish this by marking attributes as required Simply provide a list of all attributes except the ones you want to be optional The json schema for that looks like this which you would build by using yaml like this in your serverless file Note on request validation Marking attributes required or optional only matters if request body validation is turned on I do not know of a way to turn on request validation using any special serverless syntax It looks like you can do this in the resources section but I have not tried it Source "
47437891,47301055,"stackoverflow.com",1,"2017-11-22 17:03:57+02","2024-05-17 05:07:51.175998+03","Just a guess posting it as an answer to preserve formatting your toplevel entity in the schema should be an object not an array something like this"
47331528,47327765,"stackoverflow.com",7,"2017-11-16 15:56:00+02","2024-05-17 05:07:53.979176+03","There should be some separation between two resources i e two DynamoDB tables Note You can define only key attributes while creating the DynamoDB table In other words you do not need to define all other nonkey attributes Try this Read and Write capacity units You specify throughput capacity in terms of read capacity units and write capacity units One read capacity unit represents one strongly consistent read per second or two eventually consistent reads per second for an item up to 4 KB in size If you need to read an item that is larger than 4 KB DynamoDB will need to consume additional read capacity units The total number of read capacity units required depends on the item size and whether you want an eventually consistent or strongly consistent read One write capacity unit represents one write per second for an item up to 1 KB in size If you need to write an item that is larger than 1 KB DynamoDB will need to consume additional write capacity units The total number of write capacity units required depends on the item size Read and write capacity units"
54701281,47327765,"stackoverflow.com",3,"2019-02-15 19:11:43+02","2024-05-17 05:07:53.981177+03","Read and Write Capacity Units are the max size of data the db is allowed to processes per second if you go over this amount in any second your request would throttle It might be easier to just use DynamoDB OnDemand and pay for the Db table usages rather than calculating WCU and RCU Here is an example of 3 tables added in a formatted manner and without semiquotes Back to ReadWrite Capacity Mode Example1 imagine you foresee a traffic of writing 10KB of data per second into the db Using the WCU formula you would need 10KB 1KB 10WCU Example2 Expecting writing traffic of 7 5KB of data to the db we would need 7 5KB 1KB 8WCU Strongly Consistent mode Round up DataSize 4KB Eventually Consistent mode Round up DataSize 4KB 2"
57829621,47356928,"stackoverflow.com",5,"2019-09-07 02:48:48+03","2024-05-17 05:07:54.507168+03","This gives the following"
47381364,47377205,"stackoverflow.com",12,"2017-11-19 21:56:32+02","2024-05-17 05:07:55.380906+03","Ok I found the answer In the API Gateway under custom domains there is a section called Base Path Mappings This MUST be set to one of your functions with the default path of or just enter nothing for the path and then the destination to your lambda service This seemed to make it work for me "
47385697,47385177,"stackoverflow.com",21,"2017-11-20 07:47:16+02","2024-05-17 05:07:56.447252+03","AWS DynamoDb is a NOSQL type database and no need to define all the keys during the Table creation Also from the AWS documentation it is clear that in Attribute Definition you have to specify the Key schema and indexes An array of attributes that describe the key schema for the table and indexes Please edit your code as below For More CreateTable"
52197801,47385994,"stackoverflow.com",17,"2018-09-06 09:30:33+03","2024-05-17 05:07:57.238774+03","The Serverless Framework documentation says the following about sls remove The sls remove command will remove the deployed service defined in your current working directory from the provider So sls remove is not the way to go Just remove the visitsTable resource from your serverless yaml and run sls deploy again The Serverless Framework uses AWS CloudFormation under the hood So deleting things manually is no good idea Just keep in mind Resources created as part of an AWS CloudFormation stack must be managed and modified through stack updates Maybe this is a good read httpsvirtualbonzo com20171211didyoumanuallydeletearesourcecreatedbyawscloudformation"
53329218,47385994,"stackoverflow.com",5,"2018-11-16 01:18:00+02","2024-05-17 05:07:57.240984+03","Set DeletionPolicy to Retain on the resources you do not want removed upon stack deletion and the rest of them will be removed when you run sls remove httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawsattributedeletionpolicy html"
52745465,47385994,"stackoverflow.com",2,"2018-10-10 20:08:35+03","2024-05-17 05:07:57.241987+03","The Serverless Framework generates a CloudFormation template When you run a CloudFormation template for the first time it creates all of the resources When you run it a future time it compares the new version with the previous version and generates a plan to make up the difference That may be adding or removing resources There are some exceptions to this normally data resource types like S3 buckets which need to be force deleted So to answer your question you should remove the resources that you do not want anymore from the serverless yml file and do a sls deploy again "
47400819,47400276,"stackoverflow.com",0,"2017-11-20 22:51:00+02","2024-05-17 05:07:58.32296+03","Was able to resolve with kichiks help What I missed was the section about omitting packages in the serverlesspythonrequirements docs Specifically By default this will not install the AWS SDKs that are already installed on Lambda So in my serverless yml I added Once I deployed it was using my packaged versions of boto3botocore"
47412588,47400464,"stackoverflow.com",0,"2017-11-21 13:57:21+02","2024-05-17 05:07:59.206096+03","can you change to"
47433933,47400464,"stackoverflow.com",0,"2017-11-23 14:48:40+02","2024-05-17 05:07:59.207096+03","I forgot to edit the version of buildspec yml Now i changed it to version 0 2 and it works fine "
48469862,47404025,"stackoverflow.com",15,"2018-04-17 16:45:38+03","2024-05-17 05:08:00.092474+03","I am having the same issue in the following scenario Serverless Webpack Babel In recommended webpack config devdependencies are excluded Babelruntime depends of babel normally added as devdependency is a devdependency too If you add babelruntime to your common dependecies now serverless using webpack will package babelruntime with full information about the package in this case the version required or You can avoid these warnings "
74397677,47404025,"stackoverflow.com",0,"2022-11-11 05:18:35+02","2024-05-17 05:08:00.094469+03","If the warning is not causing an issue I always regret trying to fix up npm as you always end up down a time wasting rabbit hole of conflicting versions and broken serverless deployments "
47421333,47414559,"stackoverflow.com",5,"2017-11-21 21:38:08+02","2024-05-17 05:08:00.525837+03","I would suggest that your algorithm is destined to give you issues because the query string is a set of keyvalue pairs with no intrinsic ordering There should not be an expectation that it will pass through any particular system in any particular order The same is true of request headers Some libraries that build HTTP requests store query string parameters in an intermediate dictionaryhash structure so even absent the issue you see here which I suspect to be API Gateway since CloudFront claims to preserve the ordering which is arguably a suboptimal design since colorredsizelarge is again arguably but pretty compellinglyso exactly the same thing as sizelargecolorred My guess would be that API Gateway may be optimizing its ability to perform caching which does not actually use the CloudFront cache it has its own implementation by canonicalizing the query string ordering But as I suggest above your algorithm should require a binary lexical sort case sensitive rather than alphabetical which might be assumed to be case insensitive of the query parameters on the sending end and the same thing again on the receiving end This seems like unnecessary complexity but this is almost certainly why the various AWS signing algorithms require the query string and header for the same reason keys and values be sorted before signing because you simply cannot rely on client libraries proxies or other entities to handle them consistently "
47455088,47453599,"stackoverflow.com",0,"2017-11-23 14:00:45+02","2024-05-17 05:08:01.192602+03","The only way DynamoDB will return ordered results is if your table has a sort key If you wanted to query on an id and order by date you could use id as your partition key and a date as number attribute as your sort key DynamoDB keys can only be StringNumber or Binary attributes You could have an attribute that was a number and represented the date in YYYYMMDDHHMMSS format which would naturally sort in the correct order You could then use DynamoDB ScanIndexForward function and limit your result set to a given number of items Alternatively you could just query by id and sort by date using application code "
47464660,47464573,"stackoverflow.com",3,"2017-11-24 01:52:21+02","2024-05-17 05:08:02.235208+03","The returned string looks like a simple query string so you could just parse the query string in vanilla js or try to find another method to use instead of toString to get the value you need "
54043268,47557222,"stackoverflow.com",5,"2019-01-04 19:07:12+02","2024-05-17 05:08:09.288091+03","Unfortunately Chen Dachaos answer fails with An error occurred ApiGatewayResourceExperimentExperimentVarPsizeVar Resources path part only allow azAZ09 _ and curly braces at the beginning and the end The current workaround to this is adding http handlers for each optional variable in the path like so"
50441289,47557222,"stackoverflow.com",1,"2018-05-21 05:40:41+03","2024-05-17 05:08:09.289845+03","Add after the params can make it work "
54530182,47557222,"stackoverflow.com",0,"2019-02-05 10:18:17+02","2024-05-17 05:08:09.290842+03","If you want itemstatus to be optional then you must set it to false in your serverless request definition like so"
73685711,47557222,"stackoverflow.com",0,"2022-09-12 10:19:59+03","2024-05-17 05:08:09.291843+03","I have used the following option and it worked with and without the parameters"
47792393,47778415,"stackoverflow.com",1,"2017-12-13 13:54:39+02","2024-05-17 05:08:10.125812+03","Similiar to Dunedans suggestion In these scenarios I typically rely on DynamoDB Local httpsdocs aws amazon comamazondynamodblatestdeveloperguideDynamoDBLocal html I like this for a few reasons"
47798212,47778415,"stackoverflow.com",0,"2017-12-13 18:43:30+02","2024-05-17 05:08:10.127813+03","With Xunit you can share the IAmazonDynamoDB instance across tests"
47868837,47819015,"stackoverflow.com",0,"2017-12-18 14:46:36+02","2024-05-17 05:08:11.969378+03","If your goal is to include the models1 ts and models2 ts you can use this plugin httpsgithub comwebpackcontribcopywebpackplugin if you want those models to be included inside the generated bundle we need to make sure that your import resolution on webapck is well configured "
47482626,47479757,"stackoverflow.com",7,"2018-03-24 02:31:06+02","2024-05-17 05:08:03.488787+03","My understanding for serverless framework and other similar serverlessfocus tools Both Serverless Framework and Terraform support different cloud providers Use AWS as sample in next statements Compare Serverless framework to Terraform serverless framework is serverless specialist Terraform is GP terraform is fully Infrastructure as Code which covered most resources Serverless framework is a middle layer only to generate Cloudformation template which mostly for serverless related resources in aws focus on lambda api gateway dynamodb etc You can write all in Cloudformation template directly but the template file will be huge it is hard to maintain by its JSONYaml template as well With a few dozen lines in serverless yml serverless framework can generate a thousand or several thousand lines of cloudformation template It saves a lot of time to deal with the cloudformation coding By the way if you already knew cloudformation syntax you can easily put the same cloudformation yaml codes directly in the resources part serverless template knows how to create them You can write Terraform codes to manage lambda api gateway Dynamodb as well Maybe you can get less codes than Cloudformation template but still too complicated In another way it does not make sense to let serverless framework deal with all AWS resources that other tools do most well already such as EC2 VPC ECS etc Serverless framework is still in developing because of its popularity many developers are involved to add features into it daily Maybe one day you can get what you need but now you have to mix serverless framework with Cloudformation or Terraform or other tools together in some case "
47482022,47479757,"stackoverflow.com",4,"2017-11-25 04:09:03+02","2024-05-17 05:08:03.492788+03","The most obvious difference is that serveless architecture focuses on functionasaservice and terraform is designed to manage infrastracture including your EC2 and ECS instances Serveless com might be more suited for AWS Lambda deployments and it is equivalents in other clouds but it cannot manage your virtual machines load balancers and all the other "
53048873,47479757,"stackoverflow.com",1,"2018-10-29 17:32:20+02","2024-05-17 05:08:03.493791+03","I find that Serverless just handles the configuration of Lambda and API Gateway better The deployment is less of a hassle as well In fact I have a few projects where Serverless handles Lambda functions endopints and any other resource is handled by Terraform You could even use Terraform to deploy Serverless projects using localexec commands "
66108405,47479757,"stackoverflow.com",1,"2021-02-08 21:53:26+02","2024-05-17 05:08:03.494795+03","2021 update the Serverless team explained the difference themselves in this blog post The gist of it is that the two can be used together From the blog post For example If you have a shared database and two Serverless applications that create tables in it the database should be managed by Terraform The specific tables should be created and destroyed by the Serverless Framework during the app deployment and teardown process "
47513572,47511942,"stackoverflow.com",0,"2017-11-27 16:42:27+02","2024-05-17 05:08:05.14561+03","I found out that this can be done using FnGetAtt to pass the value as environment variable"
47553824,47515464,"stackoverflow.com",1,"2017-11-29 15:19:23+02","2024-05-17 05:08:06.74107+03","I put prefixes in my Parameter Store variables e g Then in my serverless yml I can do You should be able to do something similar "
77070136,47515464,"stackoverflow.com",0,"2023-09-17 03:15:15+03","2024-05-17 05:08:06.743071+03","I created a serverless plugin for this because I was facing the same issue and did not find a plugin for it You can see the plugin here httpsgithub comrobinthomasserverlessawssecrets The idea is that you shall have environment variables that are secrets and hence having dummy values that are already loaded by Serverless Framework will be replaced by the secret values from AWS Secrets Manager by the plugin So that means you need to have different environment variables for different environments can use different env files Then use different secret keys in AWS Secrets Manager for each environment This way I do not have to keep the secret in my GitHub repo and still have secret support in my serverless deployments Run below command to install the plugin Add the plugin to serverless yml Now if you have environment variables that looks like below the plugin shall search for secrets MYSQL_USERNAME and MYSQL_PASSWORD you can configure the plugin and then replace it within the deployment package "
47939021,47533166,"stackoverflow.com",0,"2017-12-22 11:29:58+02","2024-05-17 05:08:07.360113+03","The easiest way is to use the plugin serverlesspackagepythonfunctions So simply define in the serverless yml Important is to use useDocker true this is spinning up a docker locally based on the AWS AMI therefore you get the right dependencies After that create your function in serverless yml Inside your testfolder place the requirements txt This file will be used for deploying the service with the right packages let me know if you have further questions"
47539611,47536542,"stackoverflow.com",7,"2019-05-28 05:46:04+03","2024-05-17 05:08:08.511992+03","There is no problem related to test2 For test1 you are fine to sls deploy many times But if you run sls remove when the dynamodb is set to Retain in serverless yml the dynamodb table is not deleted So you cannot create it again with sls deploy because the resource with same name is exist This is the design in aws cloudformation You found the open ticket already for a new feature to skip resources We have to wait for the feature to be developed and merged I am waiting for the same solution as well Go there to vote it up With current situation you have to backup the dynamodb destroy it and run sls deploy and restore it if it is really matter I normally manage with variable such as in custom for different environments"
51359804,47536542,"stackoverflow.com",1,"2018-07-16 13:33:13+03","2024-05-17 05:08:08.513455+03","Just to clarify the point despite you have 2 serverless yml files as the service name is the same for both sandboxcore the deployment of test1 or test2 will affect the same cloud formation template This means that when you deploy test2 you are deliberately removing the track of the Dynamo Tables from the template and in a subsequent deploy of test1 Cloud Formation will be unable to create a resource with the same name as you already deleted from the template If you want to avoid data loss setting the policy to Retain should do the trick but you need to merge both serverless yml into one Then DynamoDB tables will never being removed from the template What can help you to solve the issue as the tables are already created with data is to create a backup of your tables deploy the joint serverless yml files as one unique service with the tables included manually remove the tables from the console and restore the backups with exact the same name as the ones created by Cloud Formation This will ensure that your template still has the reference to the ARNs of the tables "
68200888,47536542,"stackoverflow.com",0,"2021-06-30 22:55:04+03","2024-05-17 05:08:08.515738+03","The proper way to fix cloudformation dynamodb retained tables You can import already existing resources in AWScloudformationstacksmystack Copy the Template that you have deployed in the stack Template tab run sls package with args stage stage etc so you can get the generated serverlesscloudformation_template_update_stack json for your master version of the project find the missing resources already existing easy way filter events with DELETE_SKIPPED Copy the resources from serverlesscloudformation_template_update_stack json to the Template found in point 1 Stack Actions import resources in to stack Upload template file add the table names in the gaps they are just in the resources themselves Validate that the actions to do are just importing the missing tables to the stack and press enter See that the Events with the imports"
47867243,47833344,"stackoverflow.com",21,"2019-01-10 11:33:58+02","2024-05-17 05:08:13.038327+03","Even though dashmug s answer is correct I found that the way I was trying to make it work was quite close to a valid solution too As explained in this github comment it is possible to reference other files in the resources section Provided that each those files defines a Resources key of its own like What I did not manage is to have a mixed approach such as So I just have a resourcesbase yml for that instead "
51038805,47833344,"stackoverflow.com",17,"2019-08-28 12:13:50+03","2024-05-17 05:08:13.040291+03","I cannot comment but I would like to extend Jesuspcs answer There is a way to achieve that mixed approach in serverless yml In this case files firstcfresources yml and secondcfresources yml must have the next structure"
47835813,47833344,"stackoverflow.com",4,"2017-12-15 17:54:23+02","2024-05-17 05:08:13.041289+03","Take note that the resources property has to be an object containing a Resources property NOT an array of resources like what you wanted in your code snippet So to use external file references you can do something like Reference Reference variables in other files"
47875183,47872662,"stackoverflow.com",1,"2020-06-20 12:12:55+03","2024-05-17 05:08:13.934277+03","According to the Serverless Framework documentation To let the Serverless Framework access your AWS account we are going to create an IAM User with Admin access which can configure the services in your AWS account Note In a production environment we recommend reducing the permissions to the IAM User which the Framework uses Unfortunately the Frameworks functionality is growing so fast we cannot yet offer you a finite set of permissions it needs we are working on this In the production environment IAMFullAccess AWSLambdaFullAccess and CloudFormationFullAccess should be enough "
73532377,47872662,"stackoverflow.com",1,"2022-08-29 20:13:29+03","2024-05-17 05:08:13.936277+03","2022 update Serverless com now has an [at least partial] solution to determine a set of privileges that a diven deploy will require httpswww serverless comblogabcsofiampermissionsmanagingpermissionsfortheserverlessframeworkuser tldr"
47873592,47873257,"stackoverflow.com",1,"2017-12-18 19:34:21+02","2024-05-17 05:08:14.757241+03","There are some steps in the tutorials that you skipped You do not have the file package json in your working directory That is why you have the warning npm WARN saveError ENOENT no such file or directory open Redactedpackage json You want to launch the node server but you do not have the module server js resulting in the error Error Cannot find module Redactedserver According the documentation you should first run An then you follow correctly the appropriate steps to create your service or to deploy the server "
47908622,47898748,"stackoverflow.com",3,"2017-12-20 16:48:59+02","2024-05-17 05:08:15.879336+03","If you are using some kind of env variable to determine the state dev and prod you could have something like this serverless yml then you can have extra confirgurations for the provider devprovider yml prodprovider yml"
55686464,47904989,"stackoverflow.com",5,"2019-04-15 12:45:19+03","2024-05-17 05:08:16.94697+03","This is my structure and in my function First you tell serverless you want to exlude everything and you say that each function will include their own files Inside each function I include everything inside a specific folder as dist and then to exclude specific files as files ending with map or for example the awssdk library inside node modules "
47905834,47904989,"stackoverflow.com",3,"2019-01-13 09:30:26+02","2024-05-17 05:08:16.948971+03","You can use the package and exclude configuration for more control over the packaging process Add this to your serverless yml For more information on includingexcluding folders httpsserverless comframeworkdocsprovidersawsguidepackaging"
54578975,47904989,"stackoverflow.com",3,"2019-02-07 19:26:16+02","2024-05-17 05:08:16.950972+03","change your serverless yml file like this and in your function"
47937923,47918269,"stackoverflow.com",0,"2017-12-22 10:07:35+02","2024-05-17 05:08:17.997008+03","I would suggest going for a reactangularvue frontend hosted e g on S3CDN that uses serverless for backend queries only instead of rendering dynamic HTML through Lambdas The standard approach allows you to build apps that are much more responsive and can benefit from e g CDNs See e g httpswww slideshare netmitocgroupserverlessmicroservicesreallifestoryofawebappthatusesangularjsawslambdaandmore or httpsserverlessstack com"
48789655,47919233,"stackoverflow.com",2,"2018-02-14 16:32:07+02","2024-05-17 05:08:18.944702+03","For your example it is enough to use one service serverless yml You can use one lambda in 1 service to handle users and accounts requests Or you can create 2 lambdas one per entity "
47926207,47919233,"stackoverflow.com",1,"2017-12-21 15:40:23+02","2024-05-17 05:08:18.946198+03","It really depends on the architecture you want for your app Take a look here I think it might help you decide what you want really want If you have a lot of endpoints at one point you might need 2 services because you will reach the resources limit You can always set the pathmapping if you want to have one single url for your app "
52455238,47919233,"stackoverflow.com",1,"2018-09-22 12:17:02+03","2024-05-17 05:08:18.947196+03","In few words as you wish Technically you can aggregate several functions into one and invoke specific one base on event parameter attribute Moreover you can run expresskoa server inside AWS lambdas or other FaaS without any pain as a bonus you can use ANY and any But in general depends on situation If you invoke specific endpoint very often then it is better to move it to separated lambda If you know that bench of endpoints invoked seldom it is better to aggregate them under one lambda Especially if you use warmup "
56830769,47919233,"stackoverflow.com",0,"2019-07-01 09:13:23+03","2024-05-17 05:08:18.949196+03","One thing I like about architecture is that there is no right answer but the most suitable for your problemsituation On top of that bestgood practices are a guideline to help you on all of that In my case I added the complexity to the code so my CRUD paths are very simple cms entity or cms entity id Entity means my collection in my BackEnd so I know which model to use Applying this to your question you would have something like With this solution you do not have to create a function for each new entity you create in your DB It also has the OpenClosed concept of the SOLID principles "
47951525,47947413,"stackoverflow.com",4,"2017-12-23 12:00:37+02","2024-05-17 05:08:19.857081+03","That template is not really complicated Having it as a SAMtemplate would not make it more readable at all because the ressources it defines either cannot be defined using SAM or would not make it significantly shorter As far as I know there are also no tools available which would do conversion to SAMServerless automatically I always find such templates way more readable in YAML than in JSON so what you could do is to convert it from JSON to YAML using awscfntemplateflip edit it and convert it back afterwards or even use it as YAML "
76273530,47947413,"stackoverflow.com",0,"2023-05-17 18:03:59+03","2024-05-17 05:08:19.859082+03","to convert json to yaml just use this site or one like it httpswww json2yaml com"
48020851,47990383,"stackoverflow.com",4,"2017-12-29 12:41:46+02","2024-05-17 05:08:20.51308+03","Upload your json with the secret key just as you are doing then do this On your lambda configuration set GOOGLE_APPLICATION_CREDENTIALS as environment variable key and your credentials json file name as value It works on all my lambdas that use google api "
51172261,48451715,"stackoverflow.com",3,"2018-07-04 14:27:17+03","2024-05-17 05:08:40.006237+03","In AWS SES Home click on the verified email then click on Identity Policies Check there is CognitoSESPolicy is there or not If it is not there then click on CreatePolicy Custom policy and paste this code 2 Use IdentityARN in the Cloud Formation template for SourceARN under EmailConfiguration "
55404629,55390461,"stackoverflow.com",3,"2019-03-28 20:32:04+02","2024-05-17 05:14:13.375589+03","You are likely missing two things here I hope this helps "
47995764,47990383,"stackoverflow.com",1,"2017-12-29 10:26:20+02","2024-05-17 05:08:20.515081+03","While giving the right permissions would probably have worked according to Tom Melos comment and this github issue I wanted to put the secret into an environment variable because this is described as best practice First I needed a way to get the token so I ran this that needs the file client_secret json which you can download from the google api console following this guide The resulting three strings I put into the environment following this guide and then was able to do this For more details personally I just learned the basics of Oauth2 I have documented here what happens in these requests and why we need the refresh_token here "
53520348,47990789,"stackoverflow.com",2,"2018-11-28 15:37:04+02","2024-05-17 05:08:21.502177+03","EDIT If your tables were created in another Serverless service you can skip steps 1 4 and 8 If your tables were created in a standard CloudFormation Stack edit this stack to add the outputs from step 2 and skip steps 1 4 and 8 Stuck with the same issue I came up the following workaround Create a new serverless service with only tables in it you want to make a copy of your existing tables setup Optional You can use serverlessdynamodbautoscaling to configure autoscaling from the serverless yml Set up the stack to output the tables name Arn and StreamArn Deploy the stack Copy the data from your old tables to the newly created ones To do so I used this script which works well if the old and new tables are in the same region and if the table are not huge For larger tables you may want to use AWS Data Pipeline Replace your hardcoded references to your tables in your initial service with the previously outputed variables Deploy those changes"
48030712,48002587,"stackoverflow.com",17,"2017-12-30 05:39:20+02","2024-05-17 05:08:22.653143+03","I think the issue is that you are mixing the short form of the HTTP event http GET with the long form that adds additional options Try using this The main changes are 1 Adding method and path keys on the http event object and 2 Indenting the cors object another level It was previously at the top level of the http event Let me know if this helps "
70965232,48002587,"stackoverflow.com",2,"2022-02-03 04:25:20+02","2024-05-17 05:08:22.654521+03","The problem is how the Prefilght request working in my experiencie this process does not work well only configuring the custom header on GET method so just add on POST method too and fix the CORS problem This CORS and API Gateway survival guide and Fixing Common Problems with CORS and JavaScript help me to understand the problem and the solution Finally this is my JS code"
48016267,48010627,"stackoverflow.com",1,"2017-12-29 03:10:38+02","2024-05-17 05:08:23.958646+03","Is there a way to map all parametersheaders and body to the other http endpoint does it require a special template Yes use HTTP_PROXY integration type In the console this is a checkbox in the Integration Request page "
48013113,48010627,"stackoverflow.com",0,"2017-12-28 21:16:53+02","2024-05-17 05:08:23.960647+03","I was able to find a workaround to have this working seems more a workaround than the correct solution I had to set the Integration RequestParameters in the resources of the serverless yml to achieve this "
50749624,48014067,"stackoverflow.com",0,"2020-11-23 16:28:09+02","2024-05-17 05:08:24.160819+03","UPDATE I got a similar problem when I was trying to create triggers programmatically using my own AWS Lambda I got stuck on this when I saw that the problem was with my trigger that which had no permission to invoke the published Lambda function So I needed to add the permission for the trigger first with the method addpermission This is not clearly written on the AWS docs So before I added the the trigger on the Lambda I used the following method in node js I tested the same function for the Serverless framework and Shazam my triggers were published We can do something like this for now while the Serverless code is not updated In this way this problem will need to be solved on the Serverless sourcecode and I will try to do it ASAP From what I checked this is the default behavior for the AWS Lambda functions so there is no issue with the Serverless framework Every time I publish a Lambda function there is no way to create the trigger events automatically For further information we can read the documentation of Versioning aliases "
48084953,48018338,"stackoverflow.com",0,"2018-01-03 22:52:12+02","2024-05-17 05:08:25.136001+03","I needed to do something similar recently I did it with a configuration like this Then create the handler at the specified location and return the result you wish In this case the handler would be at custom_errorserrors invalid_path In my case I returned an HTTP 404 with a custom message saying that an id was required The method any line will catch all HTTP verbs so you will only need to add three lines for each endpoint that you want this protection on instead of a group for each individual HTTP verb "
48216253,48018338,"stackoverflow.com",0,"2018-01-11 23:41:25+02","2024-05-17 05:08:25.137586+03","I accomplished this using the proxy catchall feature of ApiGateway like so Note this MUST be the last http endpoint in your serverless file so that other valid endpoints are not caught You can read more about it here httpsaws amazon comblogsawsapigatewayupdatenewfeaturessimplifyapidevelopment"
54558785,48047990,"stackoverflow.com",0,"2019-02-07 17:42:14+02","2024-05-17 05:08:25.73971+03","I do not know how to make serverless use the domain name of your choice However it is possible to reference the domain generated using Ref LogSimpleDBTable syntax E g to pass the domain name to lambda making it available as process env SDB_DOMAIN_NAME variable Or reference it in IAM role statements"
48087604,48082561,"stackoverflow.com",1,"2018-01-05 16:34:00+02","2024-05-17 05:08:26.483648+03","How about using a parameter Otherwise SelectSplit can be done neatly"
48181038,48137499,"stackoverflow.com",9,"2018-01-10 07:53:02+02","2024-05-17 05:08:28.312684+03","You cannot use the latest version You have to use a specific version as the documentation you linked states You must specify the ARN of a function version you cannot specify a Lambda alias or LATEST If you are creating the Lambda function in your template you can also create a version and use that Note that when updating the stack a new version will not be created when the Lambda function code changes You have to manually create and use a new version by changing the name of BasicAuthLambdaFunctionVersion to BasicAuthLambdaFunctionVersion2 or something else To automate this you can edit the template with a script before using it If you are using the Serverless Framework take a look at httpsgithub comsilvermineserverlessplugincloudfrontlambdaedge httpsgithub comserverlessserverlessissues3944"
59509983,48171076,"stackoverflow.com",9,"2019-12-28 12:20:35+02","2024-05-17 05:08:29.338938+03","The description in the question focuses on getting all resources in a single resource file and use different such files per stage This works but has the limitation that you must put at least one resource in each stage And also forces you to group together resources Another way to include optional resources that I used in serverless yml is as follows Then you only create optionalresource prod yml to hold your productiononly resource When generating the template for dev stage serverless resolves the optional reference to an empty element because the file does not exist and then just ignores it Note I used the stage variable just as example but it can be any other variable e g region or a custom variable "
48191030,48171076,"stackoverflow.com",3,"2018-01-10 17:39:37+02","2024-05-17 05:08:29.340892+03","Replacing this with this Fixed my issue "
53150076,48612240,"stackoverflow.com",5,"2018-11-05 09:33:11+02","2024-05-17 05:08:40.938907+03","I ran into the same issue I tested incognito and the site worked fine in inco after doing a cache invalidation the same way that Michael stated in the first comment It looks like it is browser caching alongside the Cloudfront caching I was able to resolve the issue by clearing browser cookiesdata from the last day "
48312237,48216035,"stackoverflow.com",1,"2018-01-18 02:47:25+02","2024-05-17 05:08:30.548722+03","The comment from speshak eventually lead me to the answer I did not need to filter by Failed state but rather Deleted This allowed me to see the logs for the nested stack that was created then deleted with more specific messaging What this ended up showing me is that the updatestack process was applying the nested stacks to my current set up before removing all the resources from what would become the root stack So the true problem was that I was accidentally trying to create duplicate resources AWS saw a resource in a nested stack that matched the root stack and kicked out with a validation error even though the resource would have been removed from the root stack eventually "
48238453,48218421,"stackoverflow.com",5,"2018-01-13 11:11:50+02","2024-05-17 05:08:31.459444+03","Some npm modules use native binaries that are being compiled when you do npm install These compiled binaries only run on the OSplatform where they were compiled Because of the above those native dependencies that you compiled on your MacOS will NOT work once you upload them on AWS Lambda since Lambda runs on Linux To solve your problem you would need to create your Lambda deployment package including npm install on a Linux machine You have several ways to do this Use a Linux Virtual Machine e g Virtualbox or Parallels and do your npm install from inside that VM Use vagrant Same as number 1 Use docker Still very similar to number 1 and 2 Or simply use pure JS dependencies and you will not have the above problem in the first place Many native npm modules now have pure JS alternatives "
48255769,48255681,"stackoverflow.com",12,"2018-01-15 03:05:42+02","2024-05-17 05:08:31.507738+03","I found out the answer process env IS_LOCAL will detect if you are running locally Missed this on their website somehow "
56143302,48255681,"stackoverflow.com",6,"2019-10-25 10:29:04+03","2024-05-17 05:08:31.509209+03","If you are using AWS Lambda it has some builtin environment variables In the absence of those variables then you can conclude that your function is running locally httpsdocs aws amazon comlambdalatestdglambdaenvironmentvariables html This method works regardless of the framework you use whether you are using serverless Apex UP AWS SAM etc "
48559154,48255681,"stackoverflow.com",1,"2018-02-01 11:19:55+02","2024-05-17 05:08:31.510452+03","You can also check what is in process argv process argv[1] will equal usrlocalbinsls process argv[2] will equal invoke process argv[3] will equal local"
48278184,48272249,"stackoverflow.com",2,"2018-01-16 11:40:19+02","2024-05-17 05:08:32.327204+03","The answer is not exactly but I created a really nice workaround as long as you are okay with the database being publicly accessible Modify your resources file to something like this My strategy here is to connect a Route53 record that takes the unique endpoint ID that is impossible to predict and make it a domain name you will understand In my case I create it with the stage name so my functions later will always know the database is located at databasedev yourdomain com Using this you can now essentially know the HOST for all your lambda and or nodejs functions Any better solutions to this are of course welcome this is just what I came up with for now "
48284264,48277558,"stackoverflow.com",9,"2018-01-16 17:07:15+02","2024-05-17 05:08:33.385585+03","Using the GetAtt short form is even more readable "
48278060,48277558,"stackoverflow.com",6,"2018-01-16 11:34:27+02","2024-05-17 05:08:33.387586+03","Found the answer its this I did not know I could use the brackets like that in YAML "
74507061,48277558,"stackoverflow.com",0,"2022-11-20 11:48:16+02","2024-05-17 05:08:33.388586+03","What I have done is I have created an env variable to store the ref to the rds instance endpoint and used it as required Now if i wanted to use it i can use it as"
49334692,48304178,"stackoverflow.com",4,"2018-04-05 18:59:46+03","2024-05-17 05:08:34.448056+03","I had a SPA single page application written in React communicating to a REST JSON API written in nodejs and hosted on Heroku as a monolith I migrated to AWS Lambda and split the monolith into 3 AWS Lambdas micro services inside of a monorepo The following project structure is good if your SPA requires users to login to be able to do anything I used a single git repository where I have a folder for each service Inside of each of the services folders I have a serverless yml defining the deployment to a separate AWS Lambda Each service maps to only 1 function index which accepts all HTTP endpoints I do use 2 environments staging and production My AWS Lambdas are named like this I use AWS Api Gateways Custom Domain Names to map each lambda to a public domain www example com server side rendered landing pages apistaging example com You can define the domain mapping using serverless yml Resources or a plugin but you have to do this only once so I did manually from the AWS website console My com domain was hosted on GoDaddy but I migrated it to AWS Route 53 since HTTPS certificates are free app service The app service contains a folder src with the single page application code The SPA is built locally on my computer in either bundlesproduction or bundlesstaging based on the environment The built generates the js and css bundles and also the index html The content of the folder is deployed to a S3 bucket using the serverlesss3deploy plugin when I run serverless deploy v s production I defined only 1 function getting called for all endpoints in the serverless yml I use JSON instead of YAML The handler js file returns the index html defined in bundlesstaging or bundlesproduction I use webpack to build the SPA since it integrates very well with serverless with the serverlesswebpack plugin api service I used awsserverlessexpress to define all the REST JSON API endpoints awsserverlessexpress is like normal express but you cannot do some things like express static and fs sendFile I tried initially to use a separate AWS Lambda function instead of awsserverlessexpress for each endpoint but I quickly hit the CloudFormation mappings limit www service If most of the functionality of your single page application require a login it is better to place the SPA on a separate domain and use the www domain for serverside rendered landing pages optimized for SEO BONUS graphql service Using a micro service architecture makes it easy to experiment I am currently rewriting the REST JSON API in graphql using apolloserverlambda"
48367138,48304178,"stackoverflow.com",2,"2018-01-21 15:12:16+02","2024-05-17 05:08:34.453057+03","I have done pretty much the same architecture and hosted the single page app on s3 What you can do is set up cloudfront for api gateway and than point api yourDomain com to that cloudfront Than you will also need to enable cors on your api This plugin handles setting up domain and cloudfront for you httpsgithub comamplifyeducationserverlessdomainmanager I am not sure about your project requirements but if you want to serve static files faster setting up domaincloudfronts3 might be a wise choice "
59279457,48304178,"stackoverflow.com",0,"2019-12-11 07:37:04+02","2024-05-17 05:08:34.455057+03","Here is something meaningful and worthy to read about the Serverless Architecture and Azure Serverless services httpsspscloudarchitect blogspot com201912whatisserverlessarchitecture html"
58957407,48310072,"stackoverflow.com",2,"2019-11-20 17:03:04+02","2024-05-17 05:08:35.363164+03","Now it is available as part configuration and detailed out here"
48365038,48364481,"stackoverflow.com",0,"2018-01-21 10:36:01+02","2024-05-17 05:08:36.414688+03","Not exactly what you are looking for but these are links to some CRUD tutorials with react Simple CRUD App by Sophie Step by Step React CRUD FullStack ReactGraphql CRUD APP"
48397950,48397748,"stackoverflow.com",0,"2018-01-23 11:13:32+02","2024-05-17 05:08:37.450377+03","Your screenshot shows that you already have a distribution using www runway supply This is the S3 bucket name that you previously created and then made into an S3 website "
48419582,48418581,"stackoverflow.com",2,"2018-01-24 12:01:21+02","2024-05-17 05:08:38.038278+03","This is because by default the lambda callback waits for empty event loop before freezing the process doc You can change this behavior by setting context callbackWaitsForEmptyEventLoop to false On subsequent calls in case of hot start your lambda should be able to reuse the pool You can use middy middleware or serverless plugin to warmup your lambda and prevent cold start Also lambdas never run forever the maximum execution duration per request is 300 seconds doc and of course you can set your own lower timeout That being said it is a risky path and should be used with caution "
48528888,48450603,"stackoverflow.com",2,"2018-04-27 17:04:51+03","2024-05-17 05:08:38.960258+03","I was finally able to find what was causing this thread My scenario is slightly different but essentially i have 2 options 1 Could add permission Lambda addPermission to the lambda that updates the user pool 2 Add the policy to the triggered lambda to allow being invoked by the user Pool after i deploy it right now option 2 seems better to me so i am going with this one "
65799844,48612240,"stackoverflow.com",3,"2021-01-19 23:34:20+02","2024-05-17 05:08:40.940169+03","I would recomend anyone who is uploading directly to AWS S3 bucket to clear the CloudFront edge cache Using AWS CLI this can be done with the folowing line In order to find the CloudFront Distribution Id navigate to cloudFront in AWS console Read more here Invalidating Files"
71542368,48612240,"stackoverflow.com",0,"2022-03-19 23:50:54+02","2024-05-17 05:08:40.941173+03","In my case my CloudFront distribution was blocking access to all static files Creating a CF behavior that whitelisted that path resolved the issue "
55390713,48612240,"stackoverflow.com",1,"2019-03-28 07:27:36+02","2024-05-17 05:08:40.942172+03","I faced a similar issue I was not using serverless AWS lambda What was happening was that inside my buildindex html somehow it was failing at the links hrefs and scripts src tag So I had link hrefstaticcssmain 866f5359 chunk css relstylesheet and I changed it to link hrefhttpss3uswest2 amazonaws comfullthrottlelabsreacttaskstaticcssmain 866f5359 chunk css relstylesheet similarly for scripts as well So instead of giving relative paths in buildindex html giving an absolute path did the trick for me "
48670265,48617237,"stackoverflow.com",6,"2018-02-07 19:47:30+02","2024-05-17 05:08:41.885481+03","You can load JSON files using require Or better yet convert your JSON into JS "
55390862,48617237,"stackoverflow.com",2,"2019-03-28 07:42:22+02","2024-05-17 05:08:41.887481+03","I recommend looking at copywebpackplugin httpsgithub comwebpackcontribcopywebpackplugin You can use it to package other files to include with your Lambda deployment In my project I had a bunch of files in a templates directory In webpack config js to package up these templates for me it looks like"
55614469,48617237,"stackoverflow.com",2,"2019-04-10 16:58:47+03","2024-05-17 05:08:41.888524+03","For working with nonJSON files I found that process cwd works for me in most cases For example"
48617313,48617237,"stackoverflow.com",0,"2018-02-05 09:03:17+02","2024-05-17 05:08:41.889625+03","fs readFileSync cannot find file when deploying with lambda Check the current directory and check target directory content in deploy environment Add appropriate code for that checking to your programscript "
48665369,48636053,"stackoverflow.com",0,"2018-02-07 15:42:42+02","2024-05-17 05:08:42.855744+03","The YAML that you have here looks good but there may be some other problems Just to get you started Try to add a simple function that would only print some logs when it is run and try to add a trigger for that function manually If it works then try to do the same with the serverless command line but start with a simple function with just one log statement and if it works then go from there See also this post for more hints S3 trigger is not registered after deployment"
56796229,48646923,"stackoverflow.com",7,"2019-07-03 22:03:12+03","2024-05-17 05:08:43.883954+03","I ran into this issue as well For me there were a few problems that had to be ironed out It makes sense that sslTrue needed to be set but the connection was just timing out so I went round and round trying to figure out what the problem with the permissionsVPCSG setup was "
53314614,48646923,"stackoverflow.com",0,"2018-11-15 09:48:14+02","2024-05-17 05:08:43.884772+03","Try having the lambda in the same VPC and security group as your elastic cluster"
48654428,48649037,"stackoverflow.com",10,"2018-02-07 03:07:17+02","2024-05-17 05:08:44.783549+03","I do not think this is clearly documented but based on observations these UUIDs do not appear to be randomly generated which is good for uniqueness Instead they look like they are a variant of Type 1 UUIDs where most of the bytes actually represent a timestamp so it would appear to be a safe assumption that they are both spatially and temporally unique When the digit M in xxxxxxxxxxxxMxxxxxxxxxxxxxxxxxxx is set to 1 the UUID should be a Type 1 and should represent a high resolution timestamp and node identifier although in this case the node component seems to carry no meaningful information but the timestamps seem close to real time though not precisely so "
49197004,48663465,"stackoverflow.com",1,"2018-03-09 17:33:15+02","2024-05-17 05:08:45.852038+03","The answer to this question is made of two parts 1 It is possible to copy the Google credentials json file into the zip bundle using the serverlesswebpack plugin and the webpackplugincopy serverless yml webpack config js 2 The DialogFlow node client use this gRPC client which has a c native module dependency This is also true for all other node clients for Google Cloud Platform products like Datastore You will need to build the native c modules on amazonlinux instance on your computer through Docker or an EC2 C Addons as AWS Lambda functions Using Packages and Native nodejs Modules in AWS Lambda REST JSON API instead of gRPC Since native c module are annoying to build and all the Google Cloud Platform node clients also add 30mb to your serverless zip bundle you might want to avoid the gRPC client and findwrite a HTTP client that calls the REST JSON API instead JSON over HTTP has a higher latency than gRPC but this is not significant unless you have many layers of micro services calling each other In the future the node gRPC clients might work without c modules using javascript and weight far less than 30mb but at the time of writing there is no sign of commitment except for an alpha stage submodule on the gRPC node client "
48672043,48670635,"stackoverflow.com",1,"2018-02-07 21:44:39+02","2024-05-17 05:08:46.667161+03","It is not very clear what you are asking about The Serverless framework was created partly because to group many functions in a single project If you prefer splitting the project into multiple projects then of course you can do it and it may even have some advantages for example to do the versioning separately to have independent pull requests in different repos etc I would not split the project just for the sake of having one function per project but there may be other reasons To answer your question Does it matter Yes it does because it means a different project and a different workflow You did not tell us what is your main concern and why do you want to split the project so it is hard to tell you anything more not knowing with respect to what aspect does it matter "
48710139,48691576,"stackoverflow.com",1,"2018-02-09 18:27:49+02","2024-05-17 05:08:48.662673+03","To save this or anything else really in a cookie you need to either respond to a browser request with SetCookie HTTP header see or to set it with clientside JavaScript see How you do it exactly depends on Remember that cookie is clientside state and all you tell us about is how you get the data that you want to be stored in a cookie which does not matter and not how the clientserver interaction is done which is what matters here "
48710861,48709277,"stackoverflow.com",22,"2023-03-27 16:36:07+03","2024-05-17 05:08:49.335383+03","Heres a list of things to check when presigned URLs do not work Even if your IAM credentials do not permit access to the S3 object s it is possible to use those credentials to create a presigned URL a purely local computation but that URL will not actually allow you to access the object because the credentials underpinning the presigned URL do not have access to the object you can tell this is a local computation and does not involve any calls into AWS by presigning an object such as s3notmybucketcat png That will work and generate a presigned URL but it will not actually be usable to retrieve that object "
73977525,48709277,"stackoverflow.com",0,"2022-10-06 19:58:44+03","2024-05-17 05:08:49.337383+03","httpsaws amazon compremiumsupportknowledgecenters3bucketownerfullcontrolacl As access to a bucket is given to other accounts if they put an object in the bucket the account owner does not get automatic access to those put files To fix this you need to add this to your commands Such as or Otherwise you leave the ACL for the object hyper specific to the other account and user that pushed the file Hope that helps "
56927093,56926793,"stackoverflow.com",0,"2019-07-08 03:24:52+03","2024-05-17 05:15:31.851258+03","The first entry in ResultsByTime has an empty Group Therefore it is not possible to get Keys from it Instead use This will return an empty string if Keys does not exist "
48751318,48732348,"stackoverflow.com",3,"2018-02-12 18:39:36+02","2024-05-17 05:08:51.005963+03","Amazon has its own authentication service integrated with AWS that is called Cognito This is usually the first choice of handling authentication since if you are using AWS Lambda then you already have Cognito available with no additional services needed but there is nothing to stop you from using any other authentication service like Firebase Authentication Auth0 Stormpath Okta etc This may be an advantage for you if you are already using it you have users there etc It is marginally more complicated that Cognito if you are using AWS because you need to handle additional API credentials So the answer to your question Can I integrate firebase authentication with aws lambda is yes you can But you do it on the application level in your code not in the AWS admin console You do it very similarly like you would do with any traditional backend framework like Express Hapi Restify etc but using the Serverless framework And just like with other frameworks it depends whether you want to do the authentication serverside or clientside For serverside solutions there are The clientside authentication is explained at"
48770587,48766225,"stackoverflow.com",6,"2018-02-13 17:49:10+02","2024-05-17 05:08:51.533406+03","The Serverless framework is a tool that handles all of the timeconsuming tasks that you would otherwise need to perform manually using the web console and lets you quickly install the handler functions on the infrastructure of a given provider but it does not actually convert those functions in any way which you can see when you see those functions in the AWS or Azure web console I assume that is also the case with IBM but I have not used that provider yet It means that whenever there are any differences between providers like different context objects different event data different ways to respond to requests etc you currently need to handle those differences yourself This is something that surprised me as well when I first found out about it and my idea was to write a simple abstraction layer that would handle those differences between AWS and Azure and write handlers to that common abstraction instead of the target provider If that is something useful for others then I think I might open source that microframework for the Serverless framework But the bottom line is that to my best knowledge at the time of this writing the Serverless framework itself does not handle the differences between functions deployed to different providers Please correct me in the comments if I am wrong "
49255229,48773229,"stackoverflow.com",10,"2018-03-13 13:40:22+02","2024-05-17 05:08:52.495798+03","This feature has been recently added to serverless Documentation is available here Essentially the apiGateway to be used in a serverless file can be configured through a config option inside providers This feature was introduced by this pull request and is available from serverless version 1 26 "
48807798,48806535,"stackoverflow.com",0,"2018-02-15 14:53:23+02","2024-05-17 05:08:53.211152+03","The general idea is to check if there is an error in the connection the insert and so on Take at look at this error checking There are more sophisticated methods but for your case this should do the work This is a example of how it show look your script And you should have an output like this If your output is not like this well the missing log would point the place where you have your error "
48817341,48816456,"stackoverflow.com",10,"2018-02-16 00:28:44+02","2024-05-17 05:08:53.93804+03","I needed to set the same region for both the serverless function as well as the ssm variable assignment"
52740513,48816456,"stackoverflow.com",6,"2018-10-10 15:39:56+03","2024-05-17 05:08:53.94004+03","if the parameter is a SecureString you need to add true after the path to the parameter on the serverless yml file as explained here httpsserverless comframeworkdocsprovidersawsguidevariablesreferencevariablesusingthessmparameterstore This will tell the framework to decrypt the value Make sure that you have permissions to use the key used to encrypt the parameter "
59207180,48816456,"stackoverflow.com",6,"2021-10-22 00:58:35+03","2024-05-17 05:08:53.942041+03","Check your IAM policy To get the parameters the user doing the deployment needs access to SSM This offers full access See the docs to narrow it down a bit ie GetParameters GetParameter "
63491577,48816456,"stackoverflow.com",2,"2020-08-19 19:52:41+03","2024-05-17 05:08:53.943041+03","Add this to the provider section in serverless yml file"
58207457,48816456,"stackoverflow.com",3,"2019-10-02 21:37:03+03","2024-05-17 05:08:53.94403+03","to use SSM variables you need to prefix awsreferencesecretsmanager example"
48819793,48818839,"stackoverflow.com",2,"2018-02-16 05:50:36+02","2024-05-17 05:08:54.864816+03","Not sure if my understand is right there is no need extra serverless plugin to do the job Use nodejs as sample Suppose you have below setting in serverless yml Then define the environment variable dynamoDbTableName to one lambda function in serverless yml Then you should be fine to reference this variable by below way in handler js"
48870606,48865505,"stackoverflow.com",7,"2018-02-19 18:47:27+02","2024-05-17 05:08:56.751968+03","The object keyname value is URL encoded and this was causing the issue This behaviour is documented here The s3 key provides information about the bucket and object involved in the event Note that the object keyname value is URL encoded For example red flower jpg becomes redflower jpg When dealing with filenames that contains Unicode characters please see this answer from Alastair McCormack You need to convert the URL encoded Unicode string to a bytes str before unurlparsing it and decoding as UTF8 "
52025747,48869655,"stackoverflow.com",15,"2018-08-26 14:23:35+03","2024-05-17 05:08:57.734365+03","httpswww npmjs compackageserverlessofflinetokenauthorizers Serverlessoffline will emulate the behaviour of APIG and create a random token that is printed on the screen With this token you can access your private methods adding xapikey generatedToken to your request header All api keys will share the same token To specify a custom token use the apiKey cli option Command will look like this"
69089251,48869655,"stackoverflow.com",2,"2021-09-07 16:37:35+03","2024-05-17 05:08:57.736365+03","For local dev use this inside serverless yml Or this inside serverless ts"
74625905,48869655,"stackoverflow.com",0,"2022-11-30 12:04:50+02","2024-05-17 05:08:57.737365+03","Given latest changes this configuration worked for me with serverless offline httpsgithub comdheraultserverlessofflineissues963"
77100633,48869655,"stackoverflow.com",0,"2023-09-14 00:34:58+03","2024-05-17 05:08:57.738875+03","serverlessoffline removed apiKey as a cli parameter as part of v11 0 0 The documentation on httpswww serverless compluginsserverlessoffline is currently out of date To specify a custom token use the apiKey cli option needs to be removed MIGRATION if you want to specify the apiKey value yourself please define it under provider apiGateway apiKeys in the serverless config There is a discussion occurring in issue issues1608 dnalborczyks response should provide more insight httpsgithub comdheraultserverlessofflineissues1608issuecomment1306200311 "
48889171,48887000,"stackoverflow.com",0,"2018-02-20 19:19:48+02","2024-05-17 05:08:58.488216+03","Well The format of Policy object is wrong Version and Statement should be covered by policyDocument "
48897580,48896014,"stackoverflow.com",1,"2018-02-21 05:06:03+02","2024-05-17 05:08:59.045963+03","According to the documentation you can reference environment variables with envVARIABLE You can then do something like"
49293665,48903072,"stackoverflow.com",0,"2018-03-15 09:25:55+02","2024-05-17 05:08:59.882284+03","This is not really an answer just a link to the issue I brought up in the serverless github repo httpsgithub comserverlessserverlessissues4761"
48945900,48945389,"stackoverflow.com",78,"2021-09-03 19:51:36+03","2024-05-17 05:09:02.547661+03","I have had success streaming data to S3 it has to be encoded to do this If the data is in a file you can read this file and send it up"
77047013,71091145,"stackoverflow.com",0,"2023-09-05 21:44:12+03","2024-05-17 05:29:23.332399+03","if you are using node14 then go for serverless it will work for your"
61825511,48945389,"stackoverflow.com",14,"2020-05-15 21:34:14+03","2024-05-17 05:09:02.548661+03","My response is very similar to Tim B but the most import part is 1 Go to S3 bucket and create a bucket you want to write to 2 Follow the below steps otherwise you lambda will fail due to permissionaccess I have copied and pasted it the link content here for you too just in case if they change the url move it to some other page a Open the roles page in the IAM console b Choose Create role c Create a role with the following properties Trusted entity AWS Lambda Permissions AWSLambdaExecute Role name lambdas3role The AWSLambdaExecute policy has the permissions that the function needs to manage objects in Amazon S3 and write logs to CloudWatch Logs Copy and past this into your Lambda python function"
72801242,48945389,"stackoverflow.com",0,"2022-06-29 15:13:15+03","2024-05-17 05:09:02.550661+03",""
50476808,48947073,"stackoverflow.com",1,"2018-05-23 00:37:26+03","2024-05-17 05:09:03.365766+03","Option 1 Presuming that you are developing a Node Serverless application deploying a new set of code with the same git commit ID and packagelock jsonyarn lock should result in the same environment This can be achieved by executing multiple deploy commands to different stages e g There are various factors that may cause the deployed environments to be different but the risk of that should be very low This is the simplest CICD solution you can implement Option 2 If you would like to avoid the risk from Option 1 at all cost you can split the package and deployment phase in your pipeline Create the package before you deploy from the codebase that you have checked out Archive as necessary then to deploy Option 3 This is an improved version of Option 2 I have not tried this solution but it should theoretically be possible The problem with Option 2 is that you have to execute the package command multiple times which might not be desirable YMMV To avoid the need of packaging more than once first create the package Then to deploy If you have the following resource in buildcloudformationtemplateupdatestack json for example The result of the script you execute before sls deploy should modify the CF resource to This option of course will imply that you cannot have any hardcoded resource name in your app every resource names must be injected from serverless yml to your Lambdas "
48962588,48952567,"stackoverflow.com",4,"2018-02-24 21:41:18+02","2024-05-17 05:09:04.514158+03","Why does serverless require you to be very specific with your dynamodb settings That is AWS requirement not serverless The resources section is directly passed by serverless to AWS CloudFormation so it should follow CloudFormations syntaxrules Is not DynamoDB supposed to be schemaless It is And just like ALL schemaless databases it needs a key for it to work Other databases e g MongoDB just simply create the key for you if you do not provide one DynamoDB only asks you to explicitly set the key in exchange for speed and scalability but the rest of the values are all up to you and it is schemaless Why is throughput provisioning done here This config is supposed to be committed right What is the idea behind including specific config details The whole point of using the resources section of the serverless yml file is to manage your infrastructure through code By defining the infrastructure that your applicationweb service needs in code it makes it so much easier to create update or delete those resources I do not think anyone would enjoy doing any infrastructure change e g increasing provisioned throughput to five DynamoDB tables in each of ten different regions manually through the AWS DynamoDB web console But unlike other variables throughput provisioning depends on demand and requirements can change When demand and requirements change you change it in your serverless yml Again the whole purpose of infrastructureascode is that you should never have to touch AWS console itself How do devs usually enter this into serverless yml Also would not it reset every time it deploys It will reset That is why you have to make the change in serverless yml not the AWS console If you are referring to different environments with different demands and requirements you can set set different values for each of them "
53978809,48953841,"stackoverflow.com",4,"2018-12-30 17:18:49+02","2024-05-17 05:09:05.553334+03","In your lambda function you have to return a json object like that then you can use serverlessapigwbinary plugin to config APIGateway Binary Support or you can do it by manualy Change APIGateway setting use applicationpdf instead of my image mime types "
51203418,49006352,"stackoverflow.com",0,"2018-07-06 08:14:43+03","2024-05-17 05:09:06.326947+03","Try installing Maven or ensure that mvn is on your PATH For me on Mac brew install maven did the trick "
49052380,49045256,"stackoverflow.com",3,"2018-03-01 16:55:24+02","2024-05-17 05:09:07.272773+03","Assuming all your function code are provider agnostic Each provider have their own specific way of defining and configuring things so you would expect that the lowlevel details of the serverless yml file for each would be different That being said the highlevel properties of the serverless yml are pretty much common for most if not all providers This would allow you to have one serverless yml for all providers that simply references other YAML files depending on an environment variable Assuming you have serverlessaws yml serverlessazure yml and serverlessgoogle yml for your providerspecific configuration you should be able to use this in your serverless yml Whenever you deploy you can choose which provider to use by specifying the PROVIDER environment variable "
53715393,49045256,"stackoverflow.com",1,"2018-12-11 01:45:35+02","2024-05-17 05:09:07.274775+03","dashmugs answer should work but does not If you try to include the entire provider section it does not get evaluated i e srs print just spits out the unevaluated expression Trying to parameterize each key does not work because it changes to order which seems to cause the deploy to fail Results in this I ended up just maintaining separate serverless yml files and deploying with a little bash script that copies the appropriate file first Really wish you could just specify the config file as a deploy option as requested here httpsgithub comserverlessserverlessissues4485"
61798760,49065665,"stackoverflow.com",1,"2020-05-14 16:36:25+03","2024-05-17 05:09:07.892174+03","Came here looking for a simple straightforward answer and did not want to add plugins as suggested in Request validation using serverless framework If you set parameters as required and want to validate them you must add a request validator to your serverless yml The method you want to validate will be named something like ApiGatewayMethodGet Post Patch Put Delete You can look the name up when you package your serverless functions in the created template files Courtesy for this solutions goes to httpsgithub comserverlessserverlessissues5034issuecomment581832806"
64943962,49065665,"stackoverflow.com",0,"2020-11-21 16:08:31+02","2024-05-17 05:09:07.894097+03","For those of you failing to see this like I also did This is what you need to do in plain english Turn ApiGatewayMethodNameOfYourApiLookItUpInYourTemplate to APIGatewayMethod12 In my case it was APIGatewayDealsGet The thing I was looking at was my handler name in serverless Alternatively if this does not work check the s3 bucket mine was called xxxxxxxapserverlessdeploymentbuck1epdp60eqveqr and go to serverless yyyyyyyyyyy aaaa timestamp compiledcloudformationtemplate json And look for the name of your method in there example mine was"
49068899,49068548,"stackoverflow.com",3,"2018-03-02 14:11:07+02","2024-05-17 05:09:08.712455+03","This only supported for AWS RDS aurora databases lets read this article "
49147918,49111504,"stackoverflow.com",0,"2018-03-07 11:07:29+02","2024-05-17 05:09:09.798071+03","I resolved the problem by changing the environnement I did not know that python is not portable I developed the lambda function in my Windows environment but lambda is usually run in a Linux environment in AWS and that is why my lambda was not working So the solution is to change the environnement and use Linux instead of windows"
51219197,49111504,"stackoverflow.com",1,"2018-07-07 04:21:28+03","2024-05-17 05:09:09.800071+03","I compiled my code on MACOSX environment and was caught in the same problem these days and even I put my code on ec2 and installed all the dependencies in linux or ubuntu environment the error is the same httpsdocs aws amazon comlambdalatestdgcurrentsupportedversions html docs here gave me the hint that the current lambda execution environment is Public Amazon Linux AMI version AMI name amznamihvm2017 03 1 20170812x86_64gp2 please pay attention to the specific version of AMI even you use Amazon Linux as your ec2 instance the newer or older version can make problem So after I changed the version of my ec2 instance recompiled my code on it and store the zipped files on s3 than the AWS lambda run successfully "
56332035,49133294,"stackoverflow.com",36,"2022-02-11 08:55:29+02","2024-05-17 05:09:10.82615+03","This is now supported by Serverless framework so there is no need to use external plugins To enable requests validation one need is to add the following to the serverless yml Instead of keeping the file location directly under applicationjson you can also keep the name of the model after defining it under serverless yml files apiGateway section link to documentation Kindly note that as of Feb2022 serverlessoffline plugin is not validating the http request schemas in your local Though they are supporting deprecated version http request schema "
49143822,49133294,"stackoverflow.com",35,"2018-03-07 05:53:22+02","2024-05-17 05:09:10.828478+03","To implement request validation using serverless you need to do a couple of things Include your modelheader definitions in your stack and then tell API gateway to use them for request validation You will need to install the following packages And then you will need to include them in your serverless yml Note below is only a quick rundown of how to incorporate the packages Visit the packages documentation pages for more comprehensive examples Provide API gateway with a description of your models headers You can import json schemas for your models and declare http headers using the serverlessawsdocumentation plugin Heres how you would add a model to your serverless yml And heres how you would reference the model in your lambda definition You can also declare request headers against your lambda definition like so Tell API gateway to actually use the models for validation This part makes use of the serverlessreqvalidatorplugin package and you need to add AWSApiGatewayRequestValidator resources to your serverless yml file You can specify whether you want to validate request body request headers or both And then on individual functions you can make use of the validator like so Put all together your lambda definition would end up looking a little like this"
61504119,49133294,"stackoverflow.com",4,"2020-04-29 17:22:33+03","2024-05-17 05:09:10.830812+03","As Ivan indicated there is no need for external plugins as this is supported by the Serverless framework However I think the way to configure this has changed This example was taken from httpswww serverless comframeworkdocsprovidersawseventsapigatewayrequestschemavalidators"
64682617,49133294,"stackoverflow.com",2,"2020-11-04 17:05:53+02","2024-05-17 05:09:10.831809+03","In case you are like me and you do not want to add plugins as suggested in httpsstackoverflow comquestions49133294requestvalidationusingserverlessframework If you set parameters as required and want to validate them you must add a request validator to your serverless yml The method you want to validate will be named something like ApiGatewayMethodGet Post Patch Put Delete You can look the name up when you package your serverless functions in the created template files Courtesy for this solutions goes to httpsgithub comserverlessserverlessissues5034issuecomment581832806"
65027837,49133294,"stackoverflow.com",1,"2020-11-26 21:37:41+02","2024-05-17 05:09:10.833827+03","Request validation using serverless"
49433719,49430008,"stackoverflow.com",1,"2018-03-22 18:24:29+02","2024-05-17 05:09:11.316537+03","You might have other things that are wrong but for sure you have wrong syntax in the require should be"
49523130,49506341,"stackoverflow.com",0,"2018-03-28 01:01:42+03","2024-05-17 05:09:11.859076+03","So the answer is that you should call clientend method after publishing this single message This might look cumbersome but in case you want to keep publishing again and again over the calls to your Lambda function I think this will reduce the chances that you need to open the connection again "
49513996,49510870,"stackoverflow.com",0,"2018-03-27 16:19:36+03","2024-05-17 05:09:12.97338+03","There is a long discussion about this behavior httpsgithub comserverlessserverlessissues3183 In my experience you need to create another project just for dynamodb tables later you will need to add autoscaling for every table and it will mess your project totally Create another project build stages DEVPROD so in DEV you can delete and add new GSI key without problem "
49535772,49532186,"stackoverflow.com",1,"2018-03-28 16:23:01+03","2024-05-17 05:09:14.060862+03","We do the same in our systems AWS solved it long time back It is the cloud service called CloudFront which lets you connect multiple origins including external origins that are outside of AWS cloud Created a simple architecture diagram to help you view the same Hope it helps "
40101789,40096470,"stackoverflow.com",32,"2016-10-18 10:11:28+03","2024-05-17 05:09:15.15324+03","Change the output setting to be name driven e g "
56046325,40096470,"stackoverflow.com",16,"2019-05-08 20:40:34+03","2024-05-17 05:09:15.15424+03","To expand upon basarats answer you can use the glob package from nodes standard library to build the entry config This builds files with the same name as their source replacing ts and tsx with js "
59422506,40096470,"stackoverflow.com",3,"2019-12-20 11:07:27+02","2024-05-17 05:09:15.156235+03","OPs answer copied out of the question Ended up finding a solution that fit my needs although again in that webpacky way requires some additional configuration Still would like to make it a little more dynamic but will perfect this at a later point in time The resolution I was looking for was the ability to chunk common modules but I stated it as filename given entrypoints provided in webpack I did not mind some files being combined where it made sense but wanted overall files to be at a componentlevel given the project was not a SPA single page application The additional code ended up being I also had to parameterize the entry points see below for example by variable name so that I could assign react reactdom and redux modules to common js file "
49603905,49532205,"stackoverflow.com",1,"2018-04-02 03:27:45+03","2024-05-17 05:09:16.003411+03","I believe this is a recently introduced bug in VSCode Evidence I have been using serverlesswebpack and serverlessoffline with Typescript lambda functions for several months Debugging with a setup very much like yours has never been a problem until recently The symptoms you describe are easy to recreate with a fresh project created by plus the two plugins using the latest VSCode 1 21 1 Workaround I am not using React Native but the workaround described in this issue works for me After the debugger has started and all breakpoints are grayed out add a new breakpoint or remove and readd an existing breakpoint Either action seems to wake up the debugger and the other breakpoints all bind Recently Introduced I rolled back my VSCode to 1 18 1 and the problem disappeared I then stepped through a series of upgrades and verified that 1 19 3 and 1 20 1 also seem to work fine Version 1 21 1 seems to be the only one that suffers from this bug So if the workaround does not work for you or you would rather not use it rolling back to VSCode 1 20 1 or earlier may solve your problem Notes The launch json configuration I use for this setup is pretty minimal and usually looks like this This configuration has always worked for me I have never had to specify things like outFiles or sourceMaps to get debugging working with typescript serverlessoffline and serverlesswebpack "
49567959,49566890,"stackoverflow.com",3,"2018-03-30 06:23:22+03","2024-05-17 05:09:17.483394+03","It is reasonable to import the code you need from another module rather than rewriting it This has the side benefit of making it easier to maintain your application because you will not have duplicated logic all over the place The trick with serverless applications is finding the balance between code reuse and separation of concerns The specifics of how to do this are somewhat application dependent However if you are putting too much code into each function then it is likely that your application is too tightly coupled and could use decomposition into smaller functions that more tightly model their problem space If you find large swaths of shared code within your Lambda functions that might be a good indicator that they should be refactored into other functions If you are modeling really complex business domains then you may also want to consider calling other Lambda functions from within Lambda functions or investigating AWS Step Functions which provide a state machine on top of Lambda "
49591121,49571663,"stackoverflow.com",0,"2018-03-31 21:34:39+03","2024-05-17 05:09:17.55307+03","This happens because you are trying to call an asynchronous method from inside your Lambda When AWS calls your Lambda it simply invokes the handler method specified when you deployed the lambda When that method returns AWS puts the container that ran your code to sleep until another request is received In this case what happened was probably you called the cognito list users call which is asynchronous and your main method ran to completion before receiving a response on the callback AWS then put the container to sleep which killed the connection to cognito and so you never received the response Unless there is a synchronous variant of the cognito list users call then you will either have to figure out how to make the call block and wait for the response in your code or find another way to call cognito Since cognito is an AWS service I am pretty sure you are simply using the wrong SDK or API call variant to interact with cognito from inside the lambda "
49592618,49590451,"stackoverflow.com",0,"2018-04-01 00:32:31+03","2024-05-17 05:09:18.622136+03","After few hours of research I found why All examples that are shown in Serverless docs use specific class DocumentClient that simplifies development in javascript world by omitting DynamoDB datatypes Methods and parameters they accept are also different so that it is possible to sendretrieve data without specifying a datatype However in the end DynamoDB still requires types but all conversion is happening behind the scenes "
49607389,49607378,"stackoverflow.com",11,"2019-09-02 19:25:37+03","2024-05-17 05:09:19.503752+03","Well the problem was a bad indent source "
49643625,49643250,"stackoverflow.com",3,"2018-04-04 08:44:36+03","2024-05-17 05:09:20.526866+03","How much files does your API receive in a second The Payload size limit of an API Gateway is 10MB and cannot be increased its a hardlimit But if you say that it already doesnt work with files of 5MB this shouldnt be the issue Do you receive any kind of errors while uploading 5MB 10MB files A helpful link to discover API Gateway Limits To answer your second question how solve it An alternative would be to use PreSigned URLs to upload putObject files to S3 "
49644355,49644106,"stackoverflow.com",1,"2018-04-04 09:38:43+03","2024-05-17 05:09:21.593355+03","I created it You can find it here httpsgithub comawslabscognitoproxyrestservice BTW the question to ask yourself is why you need to push down the auth logic to the backend when Cognito has everything you need to work with it on the frontend"
49653040,49644106,"stackoverflow.com",0,"2018-04-04 17:01:05+03","2024-05-17 05:09:21.595356+03","Yes it is possible To achieve this basically you need to wrap Cognito API at your own Lambda functions and then configure http event for them at your serverless framework template "
51455435,49644106,"stackoverflow.com",1,"2018-07-21 14:06:00+03","2024-05-17 05:09:21.596356+03","You should consider that Cognito is Backend as a Service BaaS for authentication and it exposes APIs for using it So you can use AWS SDKs in order to access these APIs However you can write your own Lambda function serverless application using Serverless Framework Toolkit in order to access Cognito You can find more information about integrating Cognito in either Mobile or Webapplication using Javascript SDK here Also you can find more information about AWS SDk in other languages here "
49766746,49645157,"stackoverflow.com",1,"2018-04-11 07:54:22+03","2024-05-17 05:09:22.461304+03","Based on Peters comments I resolved the issue by using the full path of my bitbucket repo in my GOPATHsrc so the path should be bitbucket orgusernamerepo"
62929071,49677774,"stackoverflow.com",1,"2020-07-16 11:52:39+03","2024-05-17 05:09:24.917201+03","I faced similar situation except in my case using the code below works well"
49690270,49677774,"stackoverflow.com",0,"2018-04-06 13:03:35+03","2024-05-17 05:09:24.918202+03","Okay so after tedious debugging I found out that in the cacheable wrapper I was using this snippet httpsgithub comsequelizesequelizeissues2325issuecomment366060303 I do not really know still why exactly this error only showed up on Lambda and not locally but it stopped erroring when I only used the selectQuery method and only returned that instead of the whole Model addHook stuff and so on So basically changed this to this"
49678505,49678102,"stackoverflow.com",7,"2018-04-05 20:48:30+03","2024-05-17 05:09:25.566042+03","According to the doc You can access it with the FnGetAtt intrinsic function with the StreamArn parameter For example"
74714681,49678102,"stackoverflow.com",0,"2022-12-07 11:52:32+02","2024-05-17 05:09:25.568042+03","You may need a custom resource to do this Your custom resource can use API to get the stream ARN and return it "
49742135,49707752,"stackoverflow.com",4,"2018-06-22 11:41:05+03","2024-05-17 05:09:27.410003+03","It looks like the role you gave AppSync to run the lambda function does not have permission to invoke that particular lambda You will need to create or modify a role so it has the following permissions The IAM role should have a policy which enables anybody who assumes it to runinvoke your lambda function The role should also have a trust policy This trust policy will allow AppSync to assume the role on your behalf This is how AppSync invokes your lambda whenever a graphQL request comes in Once you have an IAM role with necessary permissions you will need to make sure it is associated with the lambda data source in AppSync You can select the role in the Data Sources section of the AppSync console or use the AppSync CLI to update the lambda data source and make it use your role For more information about creating a lambda function which plays nice with AppSync here is the documentation httpsdocs aws amazon comappsynclatestdevguidetutoriallambdaresolvers htmlconfiguredatasourceforawslambda"
50984496,49707752,"stackoverflow.com",2,"2018-06-22 12:08:14+03","2024-05-17 05:09:27.412579+03","I think they made a mistake in the httpsdocs aws amazon comappsynclatestdevguidetutoriallambdaresolvers htmlconfiguredatasourceforawslambda documentation The allowed action should be lambdaInvokeFunction and NOT lambdaInvoke This is working"
49970145,49722289,"stackoverflow.com",2,"2018-04-22 22:55:55+03","2024-05-17 05:09:28.400905+03","I think this is due to us using the old provider API We are fixing this in the next release httpsgithub comfernandomcserverlessfinchpull42 Disclosure I am the maintainer for this project "
50270558,49724939,"stackoverflow.com",0,"2018-05-10 13:46:09+03","2024-05-17 05:09:29.127362+03","You need to use python pip locally within your projects python venv to install your packages Make sure you have Docker installed then when you run serverless deploy it will use docker to package up your python libs to be correctly deployed in lambda See httpsstackoverflow coma500270311085343"
49740251,49733367,"stackoverflow.com",2,"2018-04-09 22:25:33+03","2024-05-17 05:09:30.777062+03","You can use the plugin httpsgithub comsbstjnserverlessdynamodbautoscaling So for the configuration you could use serverless variables like"
49756810,49745938,"stackoverflow.com",0,"2018-04-10 17:54:39+03","2024-05-17 05:09:31.593092+03","Please try configuring javaScript language level for your project seems that the parser does not recognize ES6 syntax in your code In Settings Preferences Languages Frameworks JavaScript make sure to set JavaScript language version to ECMAScript 6 ESLint and JSCS notifications have nothing to do with your issue WebStorm has autoenabled the corresponding integrations for you likely because the corresponding configuration filesdependencies were found in your project You can disable these integrations in Settings Languages Frameworks JavaScript Code Quality Tools ESLint and JSCS or set them up properly by providing correct paths to Node js interpreter packages etc "
49771575,49764571,"stackoverflow.com",6,"2018-08-18 10:38:57+03","2024-05-17 05:09:33.220209+03","logging process version in function shows me 6 11 0 As David has said Azure Functions runtime v1 1 locks your node version at v6 11 When changing WEBSITE_NODE_DEFAULT_VERSION to 8 9 4 you also need to change runtime version FUNCTIONS_EXTENSION_VERSION to beta in your application settings Note that runtime switch may cause breaking changes you can create a new function app instead if error occurs And the first method using package json does not work in my test some ES6 features like async arrow functions throws an error After successful update of node version async arrow functions also works fine in my test each time I run serverless deploy command it recreates service and resets WEBSITE_NODE_DEFAULT_VERSION value to default one serverless deploy is a command to deploy the whole service So it is normal to see the recreating back to default v6 5 happen as there is no parameter for node version in your yml file To avoid this use serverless deploy f functionname to deploy specific function instead of the whole app When I deploy a function with changes Azure Portal web ui still shows me not updated function old one while new version of that function is running there Same thing happened on my side I found script can be shown correctly in App Service Editor But in portal and kudu update fail to display I also checked log files in kudu it says Script for function functionname changed Reloading Have opened an issue on github about the last issue we met "
49775880,49764571,"stackoverflow.com",3,"2018-04-11 16:02:40+03","2024-05-17 05:09:33.22321+03","Just to formalize from my comment Functions runtime v1 is locked down to a specific Node js version 6 11 2 currently per this reference document To use a configurable Node js runtime you will need to upgrade your Functions runtime to v2 x"
52855357,49764571,"stackoverflow.com",2,"2018-10-17 15:54:13+03","2024-05-17 05:09:33.224613+03","You can set the WEBSITE_NODE_DEFAULT_VERSION By checking the nodejs version from Dprogram files x86 nodejs It will list up with all available versions Choose one and update WEBSITE_NODE_DEFAULT_VERSION This worked for me I have changed 8 11 to 8 11 1"
49766243,49764571,"stackoverflow.com",2,"2018-04-11 06:54:30+03","2024-05-17 05:09:33.225771+03","Go to your App directory you will see iisnode yml Make sure that node js version correctly specified there also There should be something like this nodeProcessCommandLine some_dir\nodejs\8 9 4\node exe"
49771366,49769287,"stackoverflow.com",0,"2018-04-11 12:27:11+03","2024-05-17 05:09:33.806107+03","The answer is CloudFront it will handle the single domain topic preventing CORS headaches see more in this Developer Guide in general you are looking to configure CloudFront with multiple origins with S3 being the default one Also you may want to put API GW in front of lambda to handle versioning migration between versions authentication and throttling "
49841353,49795148,"stackoverflow.com",3,"2018-04-15 14:23:07+03","2024-05-17 05:09:34.816846+03","Since your Cassandra instance is using private IP you will need to configure your AWS lambda Network to use a VPC It could be the VPC you are running Cassandra in or a VPC you create for the purpose of your lambdas and that you VPCpeer to your cassandra VPC A few things to note from the documentation Your plan to use API AWS lambda has at least 3 potential issues which you need to consider carefully Having say all that currently the biggest limitation in using AWS lambda is concurrent execution and cold start latency For data processing that is usually fine For userfacing usage the percentage of slow cold start might affect your user experience "
56341963,49810775,"stackoverflow.com",0,"2019-05-28 15:09:09+03","2024-05-17 05:09:35.841815+03","For Python use the following you can remove the ContentType header from the above At the end of your lambda type Also check your code where you make the API call In my case I used javascript If you are using javascript remove the datatype json unless you are returning json data in body "
74957432,49810775,"stackoverflow.com",0,"2022-12-30 03:32:52+02","2024-05-17 05:09:35.843816+03","If you use api gateway this one waits 29 1 sec before return error 5XX response even if your lambda has more timeout"
49867689,49837840,"stackoverflow.com",0,"2018-04-17 02:35:18+03","2024-05-17 05:09:39.376557+03","I figured out what the issue was Not only did I need to make sure that pg was getting included properly with webpack but I also needed to set up the VPC configuration in my serverless yml under the provider section This set up configuration for all of my functions allowing them to access my database "
49901258,49839924,"stackoverflow.com",10,"2018-04-18 16:40:11+03","2024-05-17 05:09:39.96565+03","I have seen some people using the sys module in their lambda functions code to add the subdirectory vendored in this case to their python path I am not a fan of that as a solution because it would mean needing to do that for every single lambda function and add the need for extra boiler plate code The solution I ended up using is to modify the PYTHONPATH runtime environment variable to include my subdirectories For example in my serverless yml I have By setting this as an environment variable at this level it will apply to every lambda function you are deploying in your serverless yml you could also specify it at a per lambda function level if for some reason you did not want it applied to all of them I was not sure how to self reference the existing value of PYTHONPATH to ensure I was not incorrectly overwriting it while in the process of adding my custom path vartaskvendored would love to know if anyone else has "
49856957,49856483,"stackoverflow.com",1,"2018-04-16 15:07:31+03","2024-05-17 05:09:40.989437+03","The authorative nameserver for example com needs to delegate authority for api example com to the AWS nameservers Which means NS records for api example com containing the authoritive AWS nameservers for api example com need to be in the record of example com Now you have a bunch of nameservers which are the authority for api example com Now you just need to officially delegate authority for that subzone The lookup works like this from a clients perspective"
52810371,49884618,"stackoverflow.com",1,"2018-10-15 08:46:07+03","2024-05-17 05:09:41.687662+03","I was facing same issue but solved with following CodeUri must have Bucket and key parameters and values "
50635809,49885172,"stackoverflow.com",25,"2018-06-01 06:45:00+03","2024-05-17 05:09:42.50829+03","Example on how to configure dynamodb stream in serverless yml"
49929639,49885172,"stackoverflow.com",5,"2018-04-20 18:25:30+03","2024-05-17 05:09:42.509763+03","this is proper way to get the trigger to be created on dynamodb"
62433915,49885172,"stackoverflow.com",2,"2020-06-17 19:52:17+03","2024-05-17 05:09:42.510766+03","We had the same problem And the answer was basically that you could not Or more precisely you could not without having to destroy the Dynamo DB table every time We built this plugin that allows you to connect it up httpswww npmjs compackageserverlessdynamostreamplugin Our team creates Dynamo DB tables via ansible or terraform depending on the project and then we use this plugin to wire it together We are maintaining this via our open source repo on github so if you have any issues or suggestions you can message me here or there Hope this helps as we are using it in our production code base now "
50213831,50212300,"stackoverflow.com",7,"2018-05-07 15:06:00+03","2024-05-17 05:09:43.516182+03","I would argue that using something else other than dynamoDB would be overkill DynamoDB is made for this exact purpose And it is virtually free for your use case And will only add around more or less 10ms to your run time Very negligible compared to the cold starts your Lambda will be getting If you really want to trim both your DynamoDB cost and your Lambda runtime you can cache the counter inside the Lambda container outside your handler Assuming there are no concurrent invocations triggered via scheduled events only I would do something like this for each invocation"
50214005,50212300,"stackoverflow.com",0,"2018-05-07 15:14:28+03","2024-05-17 05:09:43.518182+03","Thinking out of the box you could log messages to CloudWatch Logs You could then create a metric filter to get an actual value You can even trigger CloudWatch events with this "
50232208,50212300,"stackoverflow.com",0,"2018-05-08 14:12:11+03","2024-05-17 05:09:43.519182+03","Make your lambda handler callbackthrow an error if the API call fail Example for JS Using CloudWatch you can get metrics about the lambda invocation and error By using a math expression you can do invocationerror that will give you the number of successful call You can define the period and the time frame If you need programmatic acces use the AWS CLI or AWS SDK AWS CLI getmetricdata"
50276358,50241159,"stackoverflow.com",4,"2018-05-10 18:23:36+03","2024-05-17 05:09:45.403977+03","After taking some time off the computer I thought that the problem could be somewhere else which is what the case here I was using UserPoolId under environment variables and realized that this is where the problem lies I changed to the following way and the problem got solved I was not referencing user pool ID correctly it was referencing serverless yml local variable named UserPoolId which did not exist This was just my mistake and is now solved "
50245061,50241159,"stackoverflow.com",1,"2018-05-10 02:48:24+03","2024-05-17 05:09:45.405978+03","I found whilst writing my template it sometimes would do incorrect syntax highlighting and I would have to write it so that it could not check Ref UserPool FnSub UserPool By swapping it with Substitute it cannot check that the contents of the string are valid and then it will construct a string containing the Ref to the UserPool or the user pool id FnSub documentation"
50257546,50257269,"stackoverflow.com",1,"2018-05-09 18:57:22+03","2024-05-17 05:09:46.254977+03","I found a package dotenvwebpack also recommended by apokryfos Just require it in const Dotenv require dotenvwebpack and include it in the webpack config js Just include your env in the root with your webpack config js and you can declare your process env anywhere you need to with no other configuration "
50273450,50272459,"stackoverflow.com",6,"2018-05-10 19:18:10+03","2024-05-17 05:09:47.244856+03","As michaelsqlbot pointed out you can get this behavior by using the XAmzInvocationTypeEvent Header Getting this setup is a little screwy and the linked documentation is accurate but a little outdated in my opinion Or you could also achieve this by having your lambda that is wired up to the API Gateway we will call it lambda A invoke another lambda lambda B using the Event invocation type This way A does not care about the response of B and can return a successful response to the API Gateway within several hundred milliseconds assuming you are not doing too much else Then Lambda B can continue running for however long is necessary as long as it is under the 5 minute lambda limitation or your configured timeout "
50289713,50288961,"stackoverflow.com",2,"2018-05-11 12:56:10+03","2024-05-17 05:09:48.834761+03","If every other time you get an internal server error that means your code is syntactically sound but has some sort of logic error It is impossible to help without example code but some of the more common errors I have seen that only sometimes occur can be Unfortunately without specific examples the best we can offer is wild speculation and personal experience Have you tried looking at AWSs CloudWatch and what it says about your execution There should be some errors logged in there too "
50290801,50288961,"stackoverflow.com",1,"2018-05-11 13:58:19+03","2024-05-17 05:09:48.836374+03","I think there was an issue with the DB call not returning data in time for the callback therefore i was finding inconsistent results So basically what i did was create a Database class returning Promises like so So when i made my query there was a result ready for the callback I hope that helps anyone that may have run into the same issue "
73698502,50294527,"stackoverflow.com",0,"2022-09-13 09:40:39+03","2024-05-17 05:09:49.234848+03","You need to allow webpack config to resolve js files or you can add the js extension after the import file name "
50304599,50300743,"stackoverflow.com",0,"2018-05-12 12:10:46+03","2024-05-17 05:09:50.236468+03","The problem was with generateName handler I should not have used then on method generateName"
54113899,50396250,"stackoverflow.com",0,"2019-01-09 17:57:15+02","2024-05-17 05:09:53.500648+03","Lambda timeout This might not have been the problem in your case but it is a common problem that causes this result Your lambdas are not getting executed simultaneously but with a bit of a delay This is a clue that it is not just getting a duplicate execution I would guess that your lambda is first terminating with an error for example timing out the default lambda timeout is quite small and the lambda is being rerun after it fails I have had this problem with timeouts and it is quite confusing if you do not notice that the lambda has timed out "
50590177,50471802,"stackoverflow.com",9,"2019-06-08 17:31:51+03","2024-05-17 05:09:55.545376+03","So the only way I managed to sort this issue was Notice I added dockerSshSymlink to report the location of the ssh files on my local machine ssh In requirements txt I added my private dependency like this gitssh [email protected] my_compmy_repo giteggMyRepo All works "
50559260,50471802,"stackoverflow.com",0,"2018-05-28 07:45:56+03","2024-05-17 05:09:55.546377+03","Although not recommeneded Have you tried using sudo sls deploy awsprofile my_id stage dev region euwest1 This error can be also created by using the wrong password or ssh key "
50499538,50498620,"stackoverflow.com",34,"2018-05-24 04:15:09+03","2024-05-17 05:09:57.481921+03","Yes it is The vpc configuration in serverless yml just needs to reference existing subnets and security groups Something like this Take a look at httpsserverless comframeworkdocsprovidersawsguidefunctionsvpcconfiguration"
57855290,50498620,"stackoverflow.com",8,"2019-09-09 16:43:25+03","2024-05-17 05:09:57.483922+03","The following setup worked perfectly for me in Serverless version 1 51 0 I included staging variables since my environments use different subnets and security groups for logical isolation My network setup is an already existing VPC with subnets and security groups "
60788603,50498620,"stackoverflow.com",6,"2020-03-21 15:50:19+02","2024-05-17 05:09:57.484787+03","An extension to the answer provided by Nebulastic This is when you want to configure your VPC Lambdas to execute from more than one subnet for various Stages "
55626484,50503912,"stackoverflow.com",2,"2019-04-11 10:02:06+03","2024-05-17 05:09:58.257788+03","I know I am answering it very late just putting it here for reference for other people I did the following things If this does not work then there are some additional things that can be done like removing pyc files etc as mentioned here"
50509556,50503912,"stackoverflow.com",0,"2018-05-24 15:32:39+03","2024-05-17 05:09:58.259712+03","You can maybe use the ephemeral disk capacity tmp that have a limit of 512Mb but in your case memory will still be an issue The best choice can be to use an AWS batch if serverless does not manage it you can even keep a lambda to trigger your batch"
50699016,50679725,"stackoverflow.com",0,"2018-06-05 14:30:55+03","2024-05-17 05:10:13.654968+03","Instead of adding Lambda as subscriber from CloudWatch Events Console configure the cloudwatch event trigger from Lambda console Doing this will add LambdaInvoke permission for configured CloudWatch Event Alternatively add the invoke permission manually for CW events to make the invocation using the addpermission method of Lambda CLI example"
54448447,50503912,"stackoverflow.com",0,"2019-01-30 21:56:01+02","2024-05-17 05:09:58.260709+03","The best way to do it would be to use the Serverless Framework as outlined in this article It helps to zip them using a docker image which mimics Amazons linux environment Additionally it automatically uses S3 as the code repository for your Lambda which increases the size limit The article provided is an extremely helpful guide and is the same way that developers use tensorflow and other large libraries on AWS If you are still running into the 250MB size limit you can try to follow this article which uses the same pythonrequirementsplugin as the previous article but with the option slim true This will help you to optimally compress your packages by removing unnecessary files from them which allows you to decrease your package size before AND after unzipping "
50522635,50518925,"stackoverflow.com",11,"2018-05-25 09:14:59+03","2024-05-17 05:09:58.72059+03","You are on the right track If the DynamoDB StreamViewType is set to NEW_AND_OLD_IMAGES then when record eventName MODIFY record dynamodb NewImage will contain the updated version of the item and record dynamodb OldImage will contain what the item was before the update You could then inspect the 2 objects and look for changes in the fields you are interested in "
50529230,50529092,"stackoverflow.com",2,"2018-05-25 15:27:11+03","2024-05-17 05:09:59.435272+03","You need to specify java library path JVM property By modifying JVM command line options Or modify it directly in your code Also you can use JNA library JNA provides functionality to autounpack and load native librarians from the JAR archive resources added to the JVM class path It is includes selecting correct operating system and CPU architecture version binaries "
50529209,50529092,"stackoverflow.com",0,"2018-05-25 15:26:31+03","2024-05-17 05:09:59.438273+03","Not all native libraries are present in lambda environment you have to make a custom deployment package using either a docker or Ec2 here is how you will do that httpsdocs aws amazon comlambdalatestdgwiths3exampledeploymentpkg htmlwiths3exampledeploymentpkgjava"
50939318,50578577,"stackoverflow.com",0,"2018-06-20 05:24:43+03","2024-05-17 05:10:00.50273+03","You will have to use the ProjectionExpression To use reserved keywords in a ProjectionExpression or any other DDB Expression you use ExpressionAttributeNames and ExpressionAttributeValues if needed to map them You must use the hash symbol ExressionAttributeValues will use a colon instead of a hash In an update you would do UpdateExpression SET ro ro "
50613829,50590798,"stackoverflow.com",6,"2020-06-20 12:12:55+03","2024-05-17 05:10:01.542315+03","Updated Answer We have identified and found the issue relating to this We have published a new NPM module specifically with this fix and created a PR on the original plugin but he has not updated it in ages Fixed Version httpswww npmjs compackagehewmenserverlessplugintypescript Original Answer We are running into the exact same issue Multiple serverless projects multiple computers We have tried different node versions as well Completely wiping fetched repo and redoing it We have tried using elevated consoles No idea what is going on but we are getting that exact same error on all our stuff across 3 different machines Current suspicion is Windows did an update recently that hosed it but I figured we would see more info on it However we did find a workaround for this Delete your current build and serverless folders inside the root project area In command prompt run sls package and let it fail with the same error Manually copy over node_modules and package json from the root directory into the build folder delete the ones in there This will not cause the error saying the build directory already exists which you might have seen fighting this In command prompt run sls deploy or sls package if you only want to package Not ideal but it got us deploying again until we figure out the real issue The issue seems to be in the way the symbolic link is created for node_modules inside the build folder It is linking a directory but seems to be treating it as a file all of a sudden for whatever reason In windows if you use mklink without the D option on a directory it will give you the same result we see in the build folder If you do the mklink D you do not have this issue This is why our current suspicion is a Windows update Because everything was working perfectly fine then all projects across all our Windows PCs stopped We have rolled back our repos to stuff we have successfully deployed and no issues We have attempted to redeploy projects we have not touched in months same issue It is very annoying to say the least I will update this if we find out what is wrong "
50601806,50597791,"stackoverflow.com",17,"2018-11-30 10:58:32+02","2024-05-17 05:10:03.567695+03","The reason it is not working is that the keys in a Local Secondary Index must have the same partition key as the table So in your case your Local Secondary Indexes must have messageId as its HASH key and room and userId as RANGE keys on their respective indexes And since your table is already keyed by messageId userId then you do not need the userId Local Secondary Index This setup would technically work However if what you want to do is query by rooms and users then you probably want to go with a different table design What you are trying to do would end up requiring you to always query the table using the messageId as part of the query since it is the partition key So you would not be able to query by just room and userId What you probably want are Global Secondary Indexes In that case this would work Note that making your ProjectionType KEYS_ONLY means when you query roomIndex or userIndex what you would get back is just messageIds you would then have to requery the table with the messageIds to get other attributes You might want to use a different ProjectionType depending on what your usage pattern is "
50598702,50598080,"stackoverflow.com",0,"2018-05-30 13:36:41+03","2024-05-17 05:10:04.341666+03","I have used this approach and it works for me httpsstandardofnorms wordpress com20171203locallydebuggingawslambdaswritteninnodejs The bad thing is that I would like to use serverless and not lambdalocal package due to the greater community of serverless Lambdalocal works like charm though so I send a big hug to its creator from here Answers to the first question are still very welcome EDIT Ok I figured this out Results that Serverless as framework uses a serverless yml file when we need to add some configuration There I had to create the function I am going to run with the serverless command and then point it to the file where I have my handler This is my serverles yml right now Sure I have to research a little more on this file but I have solved my issue Hope this helps someone sometime "
50598392,50598291,"stackoverflow.com",2,"2018-05-30 10:08:26+03","2024-05-17 05:10:05.456578+03","I am suspecting your Amazon SES is still in sandbox mode As long as you are in sandbox mode you can only do the following things You can only send mail to verified email addresses and domains or to the Amazon SES mailbox simulator You can send a maximum of 200 messages per 24hour period You can send a maximum of 1 message per second To get yourself removed out of sandbox mode you need to open a SES Sending Limits Increase case to the AWS Support center "
50614455,50614381,"stackoverflow.com",0,"2018-05-31 02:04:19+03","2024-05-17 05:10:06.591419+03","At least you can try and run the image as root That way in your bash root session you should be able to install what you want Keep in mind that container will be stopped and eventually removed when your bash session ends "
50614432,50614381,"stackoverflow.com",0,"2018-05-31 02:01:40+03","2024-05-17 05:10:06.592419+03","Log into the server directoy with the root user It should be configured when the OS was installed If you are attempting to connect through a terminal try root server ip If this does not help please let us know how you log into the machine "
55934450,50679725,"stackoverflow.com",0,"2019-05-01 12:53:26+03","2024-05-17 05:10:13.656448+03","The CloudWatch Rule Event Pattern source is incorrect It should be source [aws ec2] httpsdocs aws amazon comAmazonCloudWatchlatesteventsCreateCloudWatchEventsCloud"
51165132,51026602,"stackoverflow.com",1,"2018-07-04 05:51:08+03","2024-05-17 05:10:22.195496+03","To add to bwinants answer cfstack name output name does not work if you want to reference a variable in another stack which is located in another region There is a plugin to achieve this called serverlessplugincloudformationcrossregionvariables You can use it like so"
72643163,54310730,"stackoverflow.com",5,"2022-06-16 12:00:25+03","2024-05-17 05:13:10.988272+03","You can use serverless parameters Parameters can be passed directly via CLI param flag following the pattern paramkeyvalue Parameters can then be used via the paramXXX variables httpswww serverless comframeworkdocsguidesparameters"
50628533,50619290,"stackoverflow.com",4,"2018-05-31 19:08:20+03","2024-05-17 05:10:07.896249+03","You can disregard any specific meaning of XCache Error from CloudFront because it is a standard header added by CloudFront to any request that it handles where the HTTP response code is 400 The CloudFront infrastructure handles transport of requests for some other services including API Gateway EdgeOptimized endpoints and S3 Transfer Acceleration and you can generate that same header by trying to make an accelerated connection to a bucket that does not have the transfer acceleration feature enabled It essentially means CloudFront handled this request and something did not work but it does not give you a hint as to what exactly because the error could have been internal or external to CloudFront and this header would be present in both cases To narrow this down I did some further testing As it turns out CloudFront has no problem with the ^ character in the query string parameter Confirmed with a CloudFront distribution and a custom origin this character in this position is not an issue But API Gateway chokes on it Confirmed this with a Regional API Endpoint which does not use CloudFront for transport API Gateway chokes on the ^ and returns almost nothing There is a documented case of something similar The plain text pipe character is not supported for any request URL query string and must be URLencoded httpsdocs aws amazon comapigatewaylatestdeveloperguideapigatewayknownissues html Putting an unescaped pipe character into the query string triggers the exact same behavior so this appears to be a limitation in API Gateway it is rejecting this character and calling it a Bad Request which means the client request is malformed It appears that API Gateway has the same issue with ^ In both cases the rejection occurs so early in the API Gateway infrastructure that not only does your code not see it it is so early that the request does not even make it to the CloudWatch logs for the API endpoint Based on that is possible that it may not even count toward your throttling limits because API Gateway may have stopped parsing the request before it even associated it to your API specifically speculation If you urlescape the ^ as 5E then API Gateway has no problem In fact it even correctly decodes it and shows the value in the logs So I would say your QA team has found an issue with API Gateway you will need to urlescape the ^ character in query strings But it is returning a valid HTTP error code just no response body "
50633951,50624912,"stackoverflow.com",0,"2018-06-01 02:01:07+03","2024-05-17 05:10:09.323249+03","I saw your question on the serverless forums and added an answer there will duplicate it here I think the issue is that executeapi API Gateway permissions are more complicated than that They have the form arnawsexecuteapiregionaccountidapiidstagenameHTTPVERBresourcepath and last time I tried something like this a wildcard right after the apiid portion was not enough So you probably want something like Do not quote me on this but I also think that if your resourcepath also has in it then you need to build out the permission so that it has the same number of as your resource So if your API has a GET and POST to xyz then the policy statement would have to look like arnawsexecuteapiregionaccountidapiidstagename I vaguely remember having to have permissions like to match the levels in all my resources I could be wrong though so YMMV See httpsdocs aws amazon comgenerallatestgrawsarnsandnamespaces htmlarnsyntaxapigateway httpsdocs aws amazon comapigatewaylatestdeveloperguideapigatewayiampolicyexamplesforapiexecution html"
76368462,50628841,"stackoverflow.com",2,"2023-05-30 23:37:56+03","2024-05-17 05:10:09.708871+03","AWS Lambda supports the use of Docker images as a deployment format for your functions This allows you to create more complex runtime environments that better suit your needs Heres an example of how you can configure your serverless yml file to work with Docker In this example yourdockerimagetag is the name of the Docker image you want to use for your Lambda function You should replace this with the name of your own Docker image your handler is the path to your functions handler This should be the name of the file where your handler is located and the name of the handler itself separated by a dot It is important to note that the Docker image needs to be uploaded to AWSs Elastic Container Registry ECR The Serverless Framework handles this process automatically but make sure you have the necessary permissions to work with ECR Also note that the runtime is set to provided This instructs AWS Lambda to use a custom runtime that you provide with your Docker image Another important detail is lambdaHashingVersion 20201221 This option is required for using container images in AWS Lambda and signifies that the new hashing scheme is being used "
59075177,50628841,"stackoverflow.com",0,"2019-11-27 19:03:52+02","2024-05-17 05:10:09.711872+03","there seems to be some confusion on your part I want to mainly address these 2 comments from your original question TLDR Serverless Framework will not use your Dockerfile and you cannot force it to These 2 technologies are like apples and oranges To resolve this your serverless yaml must simply be configured to find the path to your Functions Handler You are using a popular Docker image called dockerlambda This image is only for local testing The best use case I can think of is that it is available without an internet connection coding while camping on an airplane without WiFi etc To quote the projects README this images only purpose is use it for running your functions in the same strict Lambda environment knowing that they will exhibit the same behavior when deployed live You can also use it to compile native dependencies knowing that you are linking to the same library versions that exist on AWS Lambda and then deploy using the AWS CLI When you are ready to packagedeployetc to the AWS Cloud the dockerlambda is of zero use to you "
50691051,50661792,"stackoverflow.com",16,"2018-06-05 08:46:20+03","2024-05-17 05:10:10.226658+03","I found a better answer than was posted by jake lang EDIT I did not see his second comment in which he suggested the following As he noted his was incorrect since the ARN can change for valid reasons However the solution arose because the ARN for a Global Secondary Index is to append INDEXNAME to the ARN for the table This means the policy statement can be The FnJoin bit is from CloudFormation and is a join operation It takes an array of strings concatenating them using the first argument Hence it is a rather convoluted and overly complex method to calculate the ARNs required in this policy statement For documentation see httpsdocs aws amazon comAWSCloudFormationlatestUserGuideintrinsicfunctionreferencejoin html"
56181646,50661792,"stackoverflow.com",11,"2019-05-17 10:29:21+03","2024-05-17 05:10:10.228344+03","Instead of FnJoin you can use Sub MessagesDynamoDBTable Arn it is simpler Moreover if you want to access all indexes that is usually my case then index is all you need Example"
50661918,50661792,"stackoverflow.com",1,"2018-06-03 02:04:19+03","2024-05-17 05:10:10.229612+03","Heres how I have declared my working GSI permissions in serverless I am not sure but maybe the issue is you need to declare actions for each resource independently This hardcoding of the arn is probably not the best thing to do but it is worked fine for my development thus far "
50672648,50672380,"stackoverflow.com",1,"2018-06-04 05:10:28+03","2024-05-17 05:10:11.09429+03","IMO CloudWatch Logs are perfectly fine for most Serverless applications You can access them via serverless logs on the client or via the AWS Console The main thing you can do to make your life easier is to log at various levels via console log console info console warn and console error to allow you to separate different kinds of error messages out further down the line and to attach some metadata to each log line e g the ID of the thing you are operating on Winston is a library that can make this process easier for you "
69475513,50674084,"stackoverflow.com",13,"2021-10-07 07:33:19+03","2024-05-17 05:10:12.168143+03","This seems to be an issue only while uploading individual lambda function using serverless but if you do not give function parameter and deploy full stack then it works fine "
50742008,50674084,"stackoverflow.com",7,"2018-06-07 16:04:52+03","2024-05-17 05:10:12.170143+03","The package size should be lower than 50MB according to the docs httpsdocs aws amazon comlambdalatestdglimits html from this blog post The 20 MB addition presumably is there there to account for request overhead involved with the AWS API e g base64 encoding of the zip file data So far the 50 MB limit holds trueish But were not defeated yet "
76832547,50674084,"stackoverflow.com",0,"2023-08-04 05:45:13+03","2024-05-17 05:10:12.172144+03","If need to update only config should use this updateconfig or u Pushes ONLY Lambdalevel configuration changes e g handler timeout or memorySize"
62055435,50679725,"stackoverflow.com",0,"2020-05-28 04:24:39+03","2024-05-17 05:10:13.657446+03","specifically cloudtrail ones since I am deploying into useast1 but would like to get events from all regions CloudTrail has two main delivery mechanisms When you create a multiregion Trail events from all regions are sent to that one multiregion Trail configuration i e to that unique S3 bucket and CloudWatch Logs Group But event forwarding to CloudWatch Events because it is not part of the Trail configuration does not work in the same way Event forwarding is done in region Events happening in useast1 will be forwarded to CloudWatch Events in useast1 and events happening in uswest2 will be forwarded to CloudWatch Events in uswest2 To achieve what you ask the best approximation would be to create one Lambda function in useast1 and then one CloudWatch Events rule per region pointing to the same Lambda function crossregion in useast1 "
51139440,50688624,"stackoverflow.com",7,"2018-07-02 18:29:10+03","2024-05-17 05:10:14.195293+03","I faced the same problem here It seems authorizers dont support asyncawait yet One solution would be get your entire asyncawait function and call inside the handler Something like this"
64810860,50688624,"stackoverflow.com",3,"2020-11-12 21:54:49+02","2024-05-17 05:10:14.197293+03","For folks arriving here in 2020 now it works as OP described "
57144934,50895367,"stackoverflow.com",17,"2019-10-31 10:51:58+02","2024-05-17 05:10:15.591426+03","Use Serverless self reference interpolation which can include further interpolation Define a custom variable where necessary You can also use a default value if the variable does not exist Example Then when you deploy you can pass the stage prod or stage uat argument In this example no setting the stage will default to dev"
57920291,50895367,"stackoverflow.com",7,"2019-09-13 11:51:46+03","2024-05-17 05:10:15.593426+03","serverless yml Command line optstage dev takes the value passed from command line stage option In this case prod If no option is passed dev is taken as default More info here httpsserverless comframeworkdocsprovidersawsguidevariablesrecursivelyreferenceproperties"
50914573,50909510,"stackoverflow.com",0,"2018-06-18 20:20:11+03","2024-05-17 05:10:15.653411+03","I ended up using nodeprune So much easier and actually reduced size is smaller than when webpack is used "
60150600,50909510,"stackoverflow.com",0,"2020-02-10 14:36:36+02","2024-05-17 05:10:15.654412+03","You can use yarn to make use of its autoclean feature I wrote a yarnclean file which is a combination of the default file created by yarn in combination of the extensions deleted by nodeprune Then using serverlesswebpacks scripts config option you can run yarnclean to cleanup your node_modules as described here httpsgithub comserverlessheavenserverlesswebpackissues519issuecomment577727171 you can see my full setup here which includes a few more improvements you can do to get the optimal package size httpsmedium comfaunhowtooptimiseyourserverlesstypescriptwebpackeslintsetupforperformance86d052284505"
50936157,50931569,"stackoverflow.com",0,"2018-06-19 23:05:54+03","2024-05-17 05:10:16.569148+03","Have you tried running serverless config before serverless deploy and see if that works Under the issues section of Aliyun plugins repo you can find this thread httpsgithub comaliyunserverlessaliyunfunctioncomputeissues3"
50938155,50931730,"stackoverflow.com",1,"2018-06-20 02:18:13+03","2024-05-17 05:10:17.467179+03","You could use serverlessapicloudfront which automatically creates properly configured AWS CloudFront distribution that routes traffic to API Gateway To use it it is simple you have to install it using npm i savedev serverlessapicloudfront after that you have to add in your serverless yml file If you which to understand how it works and other possible configurations you may visit their Githubs page "
56157024,50931730,"stackoverflow.com",1,"2019-05-15 23:18:47+03","2024-05-17 05:10:17.46903+03","You can refer to any resources created by serverless using the same way you would refer them in cloudformation Serverless framework creates the cloudformation template to deploy Sls package cd serverless cat cloudformationtemplateupdatestack json You can get the name of the resource that sls creates for you"
51391219,50991250,"stackoverflow.com",2,"2020-06-20 12:12:55+03","2024-05-17 05:10:19.655161+03","If you want to tie into Serverless lifecycle events and do stuff one typical approach would be to write a plugin I found the learning curve with Serverless plugin development to be quite gentle and would recommend writing one to any Serverless user that has not done so However sometimes a plugin is overkill or undesirable for other reasons One very handy yet often overlooked feature of Serverless is that it can resolve variables from javascript files Note the exported function with this signature This serverless object gives you access to all sorts of stuff In your case you could either put your env itemsupdates data directly into a js function or have your function read up the file s For the sake of simplicity I will assume you are willing to stuff the data right into the function I will also illustrate using only a singe itemsupdates js file rather than separate files per stage itemsupdates js Then in your iamRoleStatements Note While the example above shows a default export I do typically use named exports so I can put more than one resource in a single js file "
51252125,51005379,"stackoverflow.com",0,"2018-07-09 21:42:28+03","2024-05-17 05:10:20.352504+03","Using API versions would be great except there are no crossAPI subscriptions Your mutations in one API will not trigger subscriptions in another I am hoping AppSync will come to support some sort of API versioning Not sure if serverless rollback works with AppSync they probably should However it is just using serverlesss S3 bucket cache it is not a CloudFormation feature Therefore you could pretty much use version control to roll back changes and redeploy just as well your use case may vary We do have dev and test stages in one AWS account and prod in another AWS account In test stage we can test that the API is working correctly before deploying it to production For our other deployments we also have beta stage before production but for AppSync this is not possible because of the crossAPI subscription issue We cannot have some data changing in production through beta API and not triggering a subscription in production API "
70642467,51014209,"stackoverflow.com",11,"2022-12-03 22:11:10+02","2024-05-17 05:10:21.372694+03","Took so much time on this Just figured out that when I typed the command below I mentioned the function name in the handle js file which is wrong I should call the handler name itself that exists in the serverless yml file instead For example This was wrong This is right"
51104477,51014209,"stackoverflow.com",10,"2018-11-30 15:59:40+02","2024-05-17 05:10:21.374246+03","Your template is invalid You cannot just put your function under an arbitrary node to tell the framework that it applies to some object of your app Your stories node should be a comment Try something like this"
51146571,51026602,"stackoverflow.com",2,"2018-07-03 07:13:20+03","2024-05-17 05:10:22.192494+03","If use them in other resources means another serverless service or another CloudFormation stack then use CloudFormation Outputs to export the values you are interested in Then use CloudFormation ImportValue function to reference that value in another stack See httpsdocs aws amazon comAWSCloudFormationlatestUserGuideoutputssectionstructure html and httpsdocs aws amazon comAWSCloudFormationlatestUserGuideintrinsicfunctionreferenceimportvalue html Within Serverless Framework you can access a CloudFormation Output value using httpsserverless comframeworkdocsprovidersawsguidevariablesreferencecloudformationoutputs If you want to use the autogenerated value within the same stack then just use CloudFormation GetAtt function See httpsdocs aws amazon comAWSCloudFormationlatestUserGuideintrinsicfunctionreferencegetatt html For example I have a CloudFormation stack that outputs the URL for an ElasticSearch cluster Assuming that the CloudFormation stack name is mystack then in my Serverless service I can reference the SearchUrl by"
51086131,51059692,"stackoverflow.com",0,"2018-06-29 12:06:15+03","2024-05-17 05:10:23.029313+03","There are a number of issues stopping this working runtime nodejs10 3 0 IBM Cloud Functions does not support this runtime Use nodejs8 http ANY IBM Cloud Functions API Gateway does not support the ANY method Replace with valid HTTP verb httpsgithub comdougmoscropserverlesshttp expects to run on AWS Lambda and uses AWS specific runtime and event properties This will not work on IBM Cloud Functions There is a different project to get Express js apps running on IBM Cloud Functions See this repo for details httpsgithub comIBMexpressjsopenwhisk"
51156693,51088606,"stackoverflow.com",1,"2018-07-03 17:07:49+03","2024-05-17 05:10:23.964203+03","To answer my own question the solution I found was to specify an updated version of the fsevents library After adding fsevents ^1 2 4 to the devDependencies section in package json the project was able to build "
51098881,51089982,"stackoverflow.com",1,"2018-06-29 12:44:55+03","2024-05-17 05:10:25.062059+03","It is possible to do using AWS Step Functions You can configure a state machine in which depending on the return value of your first lambda a wait state will occur and afterward another lambda will be invoked "
54635818,51094284,"stackoverflow.com",0,"2019-02-11 19:18:26+02","2024-05-17 05:10:25.715053+03","I do not know for sure if you can use pip for a AWS serverless deployment I have some apps serverless and I have to put in root folder my dependencies You have to run So when you have this in your root you do not need to run pip Also you can find other steps in AWS Docs AWS Deployment Lambdas Obviously this is a Lambda documentation but in short words serverless creates lambda functions with a API Gateway Endpoints through a CloudFormation settings Hope it helps "
56247492,51094284,"stackoverflow.com",0,"2019-05-22 02:26:13+03","2024-05-17 05:10:25.717053+03","I suggest a few things If none of that works enable the SLS_DEBUG In Powershell you do that with envSLS_DEBUG And run your deployment again to see a more descriptive error "
51166542,51106025,"stackoverflow.com",0,"2018-07-04 08:47:00+03","2024-05-17 05:10:27.315914+03","This is because pillow is not a standard python library found in AWS Lambda environment by default To include and use it in your code you need to make a custom deployment package with all your dependicies and your code included and then deploy it And this is what you did to make it run "
51119595,51118756,"stackoverflow.com",2,"2018-07-01 03:43:09+03","2024-05-17 05:10:27.886985+03","Its better to use individual serverless yml files per each service To use the shared code You can convert the code into a library and use it as a dependency and installed via a package manager for each individual service similar to a library This is useful since updating a version of common code will not affect the other services Keep the shared code in a different repository and use git submodule for individual service For more information refer the article Can we share code between microservices which I have originally written considering serverless "
51165567,51120613,"stackoverflow.com",0,"2018-07-04 07:02:41+03","2024-05-17 05:10:28.985741+03","Figured it out I had imported it at the top of my test file like so import it from jest Removing it fixed it Time to go read about modules "
51123754,51123626,"stackoverflow.com",0,"2018-07-01 16:33:22+03","2024-05-17 05:10:29.498021+03","There is known an issue open httpsgithub comapacheincubatoropenwhiskruntimenodejsissues57 To find the root of the src from the zip use nodejs variable __dirname Like let filePath __dirname workflow json"
51270844,51130235,"stackoverflow.com",1,"2018-07-11 00:18:42+03","2024-05-17 05:10:30.520842+03","Here is my thoughts on this broad question my answer is AWS orientated but all cloud vendors offer similar services You can absolutely save big money going from Monolith to Serverless in most scenarios Moving to serverless does mean rearchitecting your application to be event driven This can be quite a challenging paradigm shift but one that you will not look back from You may choose to start with a complete rewrite or use the strangler pattern to move more gradually I suggest the following AWS Services to most serverless systems Depending on your specific usecase you may also make use SNS SES Kinesis and more The Serverless Framework is great for coordinating all of these services in an InfrastructureasCode style approach Greatly simplifying the deployment of all the dependencies They also have many examples of common architecture examples to start from Serverless functions scale really smoothly however be mindful of stressing services that do not such as Relational Databases or thirdparty APIs Scenarios where you will not save money are if you have consistent load 247 high utilised EC2 or you run long processing batch jobs scaling EC2 on SQS is more suitable "
51156877,51154324,"stackoverflow.com",0,"2018-07-03 17:16:42+03","2024-05-17 05:10:31.636215+03","You can remove your existing CloudFormation stack manually if sls remove did not work And then redeploy your stack from the scratch Of course make sure you deleted serverless directory before new deployment "
51201061,51154324,"stackoverflow.com",0,"2018-07-10 04:18:43+03","2024-05-17 05:10:31.638216+03","I believe the issue is that stageParams does not do what you think it does It does not attach the lambda to the Cloudwatch trigger only in the prod stage The Serverless docs httpsserverless comframeworkdocsprovidersawseventsschedule has a confusing example that lists stageParams as an input value to the trigger All that means is that Cloudwatch will invoke the lambda with the value of input as the event data There is not a way to selectively not deploy resources listed in serverless yml depending on the stage What you could do is set enabled to false when stage is not prod by using some custom configuration parameters This would deploy the trigger to your staging environment but it would not be invoked The CloudFormation error also suggests there is a naming conflict Serverless should be generating unique lambda names based on the stage so if I had to guess the schedule name fulldataimport is not unique I would try renaming it to something like Depending on how you reference your stage parameter You could try something like See httpsserverless comframeworkdocsprovidersawsguidevariables for ways you could set value of importEnabled"
52486056,51170451,"stackoverflow.com",4,"2018-09-24 22:25:30+03","2024-05-17 05:10:32.175909+03","I am the author of serverlessdotenvplugin There was a logistical problem when trying to dynamically load env files from the provider or other options I have since updated the plugin however so that you can dynamically load env files based what environment is set For example if you run NODE_ENVproduction sls deploy it will look for a file called env production If it is not found it will fallback to env See the README for more detail httpsgithub cominfrontlabsserverlessdotenvplugin"
52426380,51170451,"stackoverflow.com",3,"2020-11-30 14:49:45+02","2024-05-17 05:10:32.17791+03","I got your use case to work by just using the normal dotenv plugin In my serverless yaml I specify environment variables to be loaded from a file based on the stage parameter dev is default Then one file per stage that loads the environment variables from the right env file config dev js config production js Instead of exporting every environment variables in each of the above config files I created a helper file for this environmentVariables js Last but not least the env files containing the actual variables I named the files dev env and production env It works like a charm the only downside being that you have to edit several different files whenever you want to add a new environment variable "
75803291,51722526,"stackoverflow.com",0,"2023-03-21 17:35:52+02","2024-05-17 05:11:00.651708+03","For anyone coming here like myself who is using Zappa and seeing this error the solution is similar to sls prune but instead use num_retained_versions 10 or whatever retained value you care for in your zappa_settings json As has been mentioned you have a certain quota of versions for your region and it is unclear when you will hit it but when you do it is a pain to work out the reason "
51184125,51181290,"stackoverflow.com",2,"2020-06-20 12:12:55+03","2024-05-17 05:10:33.291201+03","The issue is that you are using RequestResponse as InvocationType but your sendHealthData AWS Lambda does not return a valid JSON just a string A small quote out of the documentation says Payload Buffer Typed Array Blob String It is the JSON representation of the object returned by the Lambda function This is present only if the invocation type is RequestResponse So as soon as you change the return value of your sendHealthData AWS Lambda to the following it should work as you expect"
51900323,51181359,"stackoverflow.com",4,"2018-08-17 21:34:42+03","2024-05-17 05:10:34.067482+03","Cloudformation just started to support FilterPolicy yesterday I have been struggling for a while too Syntax JSON YAML Ref httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawsresourcesnssubscription htmlcfnsnssubscriptionfilterpolicy httpsaws amazon comblogscomputemanagingamazonsnssubscriptionattributeswithawscloudformation"
51353620,51181359,"stackoverflow.com",3,"2018-07-16 04:47:22+03","2024-05-17 05:10:34.068482+03","I fixed it like this serverless yml configureSubscriptions js Top level is the different topic names next level is the lambda names and the third level is the filter policies for the related subscriptions lambdaTopicFilters json"
52986525,51181359,"stackoverflow.com",2,"2020-01-17 06:21:06+02","2024-05-17 05:10:34.070482+03","If you are using serverless it is now supporting sns filter natively httpsserverless comframeworkdocsprovidersawseventssnssettingafilterpolicy"
51234491,51188498,"stackoverflow.com",2,"2018-07-08 20:35:04+03","2024-05-17 05:10:34.437254+03","Ok I figured out the problem When you use the put function you cannot get the new item that you just insert in dynamoDB You have to put ReturnValues ALL_OLD in your params object first argument of the put function like that no error is thrown and you supposed to return the values you want since you just entered them in the db More details here httpsgithub comawsawssdkjsissues803"
51203291,51188668,"stackoverflow.com",10,"2018-07-06 08:00:29+03","2024-05-17 05:10:35.695423+03","I think the OP is asking about how to specify the Serverless S3 deployment bucket not how to reference a random bucket in a Lambda function You can set the deployment bucket in serverless yml like this"
51188795,51188668,"stackoverflow.com",0,"2018-07-05 13:17:25+03","2024-05-17 05:10:35.697424+03","You can use parameters within the serverless application cloud formation script I am guessing you are using this in a serverless function Lambda so you can pass this using the environment variables Then within the code just pull out the Arn using S3Arn environment variable "
51256064,51246325,"stackoverflow.com",0,"2018-07-10 04:25:58+03","2024-05-17 05:10:36.283053+03","As long as you still have access to the serverless yml and any source code cause you checked it into source control or backed it up somewhere and you still have credentials for the AWS account the serverless app is deployed to sure"
78413172,51246325,"stackoverflow.com",0,"2024-05-01 13:13:25+03","2024-05-17 05:10:36.285054+03","In case anyone comes across this like I did in a situation where you do not have the original serverless yml file heres how to get the CLI to connect to your projects This does not give you back all of the config you had in your serverless yml file but it is enough to connect you back to the app if you need to remove services and such you have not touched for a while In my case it appears Serverless reduced the allowed number of services on free accounts and I needed to go and remove the services without having full access to my dashboard These were old test projects I had forgotten about and no longer had code for "
51316925,51247080,"stackoverflow.com",4,"2018-07-13 05:35:49+03","2024-05-17 05:10:37.402219+03","For those who need this I ended up doing the following Will set the variable URL to your serverless app endpoint "
51249251,51247080,"stackoverflow.com",1,"2018-07-09 18:37:48+03","2024-05-17 05:10:37.40322+03","You have a few options I recently made a plugin called serverlessbuildclient that might help In my projects my client is its own Serverless Framework project and in the environment variable section I crossreference the endpoint from another stack This plugin will build your client with the environment variables in the serverless yml file set The meat of the plugin is this Before I made this plugin I used another plugin called serverlessstackoutput which writes all of the serverless yml outputs to a json file One of the outputs is ServiceEndpoint and I wrote a custom script to grab that value from the json file and set the environment variable before building"
76964894,51247080,"stackoverflow.com",0,"2023-08-24 00:08:38+03","2024-05-17 05:10:37.406221+03","In my case I did not just want the Service Endpoint but also the complete URLS including paths to each API Gatewayintegrated Lambda function that deployed to my stack I have written up the full solution here with explanations but in summary"
51254769,51249449,"stackoverflow.com",12,"2018-07-10 01:18:15+03","2024-05-17 05:10:38.487107+03","DynamoDB is a NoSQL Database This gives it partitioning incredible performance scalability reliability etc The tradeoff is that it is not a relational database Therefore DynamoDB will not perform a table join You will need to read the tables separately and join desired data within your application For example retrieve the UserId from your Post table then retrieve records from the User table for the given UserId "
51309163,51249449,"stackoverflow.com",0,"2018-07-12 18:12:28+03","2024-05-17 05:10:38.489107+03","I found solution User eltonio450 on gitHub showed how to do this "
59011792,51249449,"stackoverflow.com",0,"2019-11-24 19:49:53+02","2024-05-17 05:10:38.490108+03","Technically you could use Amazon EMR Hadoop to create an EXTERNAL Hive table on top of a DynamoDB table then query the Hive table using HiveQL However the Hive DynamoDB approach is not recommended since DynamoDB is designed for lowlatency queries Hive is batchoriented Additionally Hive might issue a scan intrinsically inefficient request to the DynamoDB table s before applying any query predicate s "
51268125,51262660,"stackoverflow.com",3,"2018-07-10 17:41:53+03","2024-05-17 05:10:39.60415+03","The serverlessdynamodblocal docs say that the custom block should be structured like this You have dynamoDb instead of dynamodb"
57713217,51262660,"stackoverflow.com",3,"2019-08-29 18:16:33+03","2024-05-17 05:10:39.606145+03","If anyone else is having issues with this I spent hours trying to track down this issue and it was because I inadvertently had the wrong case for the [r]esources section in serverless yml"
55364786,51271780,"stackoverflow.com",1,"2019-03-26 21:20:43+02","2024-05-17 05:10:40.728225+03","The scrypt binaries used by web3 have to be compiled on Lambdas execution environment specified in the docs for the function to work Detailed instructions can be found in this blog post towards the end You can use the below Dockerfile to automate the process without creating an instance Build and run the Dockerfile Hope it helps"
51293260,51271780,"stackoverflow.com",0,"2018-07-11 22:50:04+03","2024-05-17 05:10:40.730226+03","I do not want to answer with a link but you can find some information about there about building node files Otherwise known as the nodeaddonapi It allows you to compile native extensions files that end in node Yes you need to do this for your target platform and node version However you will not need to spin up your Docker image on each deploybuild You can just copy along your node file for the ride You could even just run Docker locally and do this too It should vastly simplify your process I use this in AWS Lambda with Node js 8 10 for another proprietary driver module and it works just fine So I can confirm these native modules do work in AWS Lambda In fact the company that compiled it did so against linux and not some specific AWS Lambda variant of Linux from what I gather So it seems as if it is a more forgiving approach when you have the need to compile things "
63312798,51925672,"stackoverflow.com",3,"2020-08-08 10:20:08+03","2024-05-17 05:11:10.371655+03","Make sure you have installed jre to run dynamodb jar Otherwise this error will be thrown "
51494442,51273658,"stackoverflow.com",2,"2018-07-24 11:54:17+03","2024-05-17 05:10:41.437251+03","Amazon Lambda does not allow the setting of XForwared headers It is already a part of the blacklisted headers If you were to set it as a part of your Lambda function the default behaviour of CloudFront is that the request fails CloudFront validation CloudFront returns HTTP status code 502 Bad Gateway to the viewer See the following link for more on list of blacklisted headers httpsdocs aws amazon comAmazonCloudFrontlatestDeveloperGuidelambdarequirementslimits htmllambdacloudfrontstarheaders If you want CloudFront to add any of the CloudFront headers you must configure CloudFront to cache based on these headers For information about configuring CloudFront to cache based on specified headers see this link for more httpsdocs aws amazon comAmazonCloudFrontlatestDeveloperGuidedistributionwebvaluesspecify htmlDownloadDistValuesForwardHeaders Please note that for viewer events CloudFrontViewerCountry is blacklisted Blacklisted headers are not exposed and cannot be added by Lambda functions If your Lambda function adds a blacklisted header the request fails CloudFront validation and CloudFront returns HTTP status code 502 Bad Gateway to the viewer Hope this helps "
51490761,51273658,"stackoverflow.com",0,"2018-07-24 07:59:57+03","2024-05-17 05:10:41.441253+03","Are you using leboncoin fr s API If so it seems they use datadome to enable botprotection which would explain where this header is set and why the API blocks your request then "
51276698,51275679,"stackoverflow.com",1,"2018-07-13 22:39:52+03","2024-05-17 05:10:42.305509+03","My solution was to create a resource httpsserverless comframeworkdocsprovidersawseventss3 In response to my negative votes I thought I would give further explanation With serverless you can use pure Cloudformation I created a custom resource to add the required permissions to my S3 bucket I hope this helps anyone encountering the same issue I was having As for adding the trigger that can only be done through Cloudformation on a bucket that does NOT already exist The workaround in my case for that was to simply use AWS CLI in my deployment to add the trigger on an existing S3 bucket There is a serverless plugin out there called serverlesspluginexistings3 that was designed to allow adding triggers to existing S3 buckets but I have discovered bugs with this plugin That is why I will stick with using the CLI for adding the trigger httpsgithub commattfilionserverlessexternals3event"
52754802,51275679,"stackoverflow.com",0,"2018-10-11 10:41:00+03","2024-05-17 05:10:42.30751+03","I finally managed to get this to do the following Here is the serverless yml extract that did it The key is that by splitting functionsprocesseventss3 into multiple fields bucket and event then it will not try to create the S3 bucket When it is all listed on one line without an event then it tries to create the bucket and you end up with a naming conflict as it tries to create two buckets with the same name one from functions and one from resources "
51697619,51484745,"stackoverflow.com",0,"2018-08-05 22:32:57+03","2024-05-17 05:10:44.315492+03","You need to add the correct identation"
51537028,51528696,"stackoverflow.com",0,"2018-07-26 13:47:55+03","2024-05-17 05:10:45.984202+03","What is your AccessControlAllowOrigin value in your request If you do not have it Add the following to the header of your request I am suspicious that because your request goes through multiple access points the token is not being passed by Also add domainwhateverdomain com when you attach the cookie for the authorized user "
56980280,51528696,"stackoverflow.com",0,"2019-07-11 03:46:10+03","2024-05-17 05:10:45.985883+03","I had the same problem The difference in my environment is that I have an extra cloudfront distribution fronting the API gateway for TLS version enforcement purposes I received the same errors Turns out the API gateway had a custom WAF which did not have any rules and was denying everything The fact that I have multiple cloudfront distributions and that the WAF is hidden under a regional filter made detection of this quite hard "
51530043,51529804,"stackoverflow.com",0,"2018-07-26 05:13:44+03","2024-05-17 05:10:46.510898+03","Are you including the correct file Your handler js code seems to have only one function hello which needs to be referenced as handler hello If this was just a dummy file for asking the question here try referencing the function in yaml as handler functionname"
58009051,51533060,"stackoverflow.com",1,"2019-09-19 13:32:28+03","2024-05-17 05:10:47.066112+03","vpcConfig yml file format should be like"
51537857,51533060,"stackoverflow.com",0,"2018-07-26 14:34:40+03","2024-05-17 05:10:47.067113+03","your lambda should have the subnetxxxxxxxx"
51541459,51537795,"stackoverflow.com",39,"2019-01-16 11:02:15+02","2024-05-17 05:10:47.718852+03","Your IAM role does not cover the indexes Try to add them in the roles ressources For reference the FnJoin will append index to DialerDynamoDbTable s ARN It worked locally because Serverless uses the admin IAM user you configured it with "
63697400,51537795,"stackoverflow.com",7,"2021-01-21 10:08:19+02","2024-05-17 05:10:47.720853+03",""
54084442,51537795,"stackoverflow.com",1,"2019-02-05 10:33:16+02","2024-05-17 05:10:47.721342+03","for those in search of cloud formation"
78347115,51537795,"stackoverflow.com",0,"2024-04-18 14:36:50+03","2024-05-17 05:10:47.72234+03","Need to add that Index in IAMRoles arnawsdynamodb table selfcustom myTable index"
57812969,51541513,"stackoverflow.com",6,"2019-09-06 00:04:37+03","2024-05-17 05:10:48.822443+03","serverlessplugintypescript plugin compiles ts only starting from root files taken from severless functions handlers in your case srcgraphql handler and root file srcgraphql ts All files not referenced in the dependencies chain are ignored serverless package include options just additionally copies speficied locations without ts compilation So if you miss some file in build directory make sure you import it somewhere in handler I faced to similar issues when some lib dynamically loaded files from configured globs like some_dir ts as I remember typeorm entities or migrations Solution was to make some_dirindex ts and export all files in this dir Then just import as all_some_dir from some_dir in your handler "
51590861,51552861,"stackoverflow.com",0,"2018-07-30 12:28:31+03","2024-05-17 05:10:49.838942+03","OK so I got around this issue by using the plugin serverlessplugincreatedeploymentbucket It works well and now I can take over old serverless 0 5 stacks just need to be able to update lambdas now "
51588375,51579814,"stackoverflow.com",2,"2022-08-03 11:13:10+03","2024-05-17 05:10:50.640016+03","Looks like this is mainly a problem with chrome and the way chrome uses preconnections httpshackernoon comchromepreconnectbreakssinglythreadedservers95944be16400 The way to solve this is to set the WSGI server to allow threading I have raised an issue with the team as its currently not possible to do this if you are using serverlesswsgi As a work around you can turn off the preconnection feature in chrome Go to Settings Advanced Privacy and security and turn off Use a prediction service to load pages more quickly This is now in Settings Cookies and other site data Preload pages for faster browsing and searching in more recent versions of Chrome "
51583500,51583083,"stackoverflow.com",0,"2018-07-29 21:51:11+03","2024-05-17 05:10:51.191035+03","There is a drawer on the left side of the Lamba page open it up and click functions "
51608807,51597303,"stackoverflow.com",2,"2018-07-31 11:28:29+03","2024-05-17 05:10:52.880028+03","In MQTT using AWS I do not think there is a provision to get the IP of the user The message broker does not support persistent sessions connections made with the cleanSession flag set to false The AWS IoT message broker assumes all sessions are clean sessions and messages are not stored across sessions If an MQTT client attempts to connect to the AWS IoT message broker with the cleanSession set to false the client will be disconnected You can get more details on this AWS Docs Also if they are providing the IP the end user might get exposed "
51718071,51667179,"stackoverflow.com",2,"2018-08-07 05:41:01+03","2024-05-17 05:10:54.967263+03","It turned out that the key is on how you set the IAM Role for the bucket and all the objects under it Based on the AWS docs here it states that S3 waitFor is relying on the underlying S3 headObject Waits for the objectExists state by periodically calling the underlying S3 headObject operation every 5 seconds at most 20 times Meanwhile S3 headObject itself relies on HEAD Object API which has the following rule as stated on AWS Docs here You need the s3GetObject permission for this operation For more information go to Specifying Permissions in a Policy in the Amazon Simple Storage Service Developer Guide If the object you request does not exist the error Amazon S3 returns depends on whether you also have the s3ListBucket permission It means that I need to add s3ListBucket Action to the Bucket resource containing the objects to be able to get response 404 when the objects does not exist Therefore I have configured the cloudformation AWSIAMPolicy resource as below where I added s3Get and s3List action specifically on the Bucket itself i e S3FileStorageBucket Now I have been able to do S3 waitFor to poll for fileobject under the bucket with only a single API call and get the result only when it is ready or throw error when the resource is not ready after a specific timeout That way the client implementation will be much simpler As it does not have to implement poll by itself Hope that someone find it usefull Thank you "
53223427,51678005,"stackoverflow.com",31,"2018-11-09 13:10:42+02","2024-05-17 05:10:55.897144+03","have you tried serverless deploy awsprofile testUser_atWork force to force it to update the stack Otherwise try deleting the stack in cloudformation or with the serverless remove command"
70391335,51687740,"stackoverflow.com",1,"2021-12-17 11:56:24+02","2024-05-17 05:10:57.263054+03","just a change of one line would get you the respose do change in params"
51692057,51687740,"stackoverflow.com",0,"2018-08-05 09:49:47+03","2024-05-17 05:10:57.264055+03","Try to send the event in the payload"
51697429,51687740,"stackoverflow.com",0,"2018-08-05 22:06:02+03","2024-05-17 05:10:57.265054+03","I strongly suspect this problem is rooted in your handler signature for funcTwo NodeJS 6 10 does not support async await Node always complains about the token after the async token for whatever reason If you do not use a fat arrow function Node will complain Unexpected token function Options Note If you stick with the 6 10 runtime I think you will want to do something like context succeed 2 or callback null 2 rather than return 2 Simply using a return statement does seem to work on 8 10 "
51699022,51687740,"stackoverflow.com",0,"2018-08-06 02:33:00+03","2024-05-17 05:10:57.267055+03","I have found the issue We need JSON stringfy in the playload in lamdda invoke method No extra parameters are needed Only the JSON as per the documentation From the AWS documentation for playload we have the async in front of the functions and Gives me this output While the absence of async gives me I am going to share the handle js code just in case someone else needs it"
51733304,51706578,"stackoverflow.com",2,"2018-08-07 21:31:13+03","2024-05-17 05:10:57.785973+03","I looked into this a while back From what I remember I think you can deploy multiple lambda functions using the same CloudFormation template in one of two ways The issue with 1 is that it is a lot more of a manual process or a lot more scripting that has to be done The issue with 2 is that each Lambda function that is created will contain the files for all functions that are part of that package even though you are only accessing one Function handler Also file conflicts are possible if different versions of the same assembly are used in different projects There is also a limit on the package size that can be loaded for Lambda functions 50MB compressed 250MB uncompressed so that may also be a factor for some people Due to these added complexities potential issues we just decided to have a separate CloudFormation template stack for each Lambda function Lambda limits see AWS Lambda Deployment Limits"
51727522,51714795,"stackoverflow.com",0,"2018-08-07 16:03:08+03","2024-05-17 05:10:58.869929+03","This is quite specific to the library you are using and how it names the statemachine which is getting created based upon whether the name field is provided under the hellostepfunc1 or not Have a look at the testcases here and here to understand better Inshort a yaml like has name of statemachine like hellostepfunc1StepFunctionsStateMachine as no name was specified Whereas for a yaml like the name of statemachine is alpha as you had name was specified "
58231961,51717381,"stackoverflow.com",3,"2019-10-04 10:42:20+03","2024-05-17 05:10:59.813759+03","For me the solution was and then try deploying again "
51718668,51717381,"stackoverflow.com",0,"2018-08-07 07:06:33+03","2024-05-17 05:10:59.81576+03","So the solution to this problem was to ensure that all previous traces of the CloudFront stack was removed In my case I had manually taken out a few functions from Lambda and the 401 errors I was getting were likely occuring in the removal attempts rather than my assumption that it was related to adding these functions Bear in mind you may find yourself like I did where the first attempt to delete fails In this case try again and make sure to check off any checkboxes exposed by UI that indicate what had caused the issues the prior attempt Once I would done that I was able to deploy as per normal from the serverless framework "
54582098,51722526,"stackoverflow.com",50,"2019-02-07 22:52:56+02","2024-05-17 05:11:00.639722+03","Looking specifically for this problem related to serverless I found httpsgithub comserverlessserverlessissues400 It is a known problem of the serverless framework However some enterprising individual created a solution to the problem with a plugin that is able to prune old versions httpsgithub comclaygregoryserverlesspruneplugin This allows you to clean up the old versions and code storage without deleting the entire stack For example you can delete all but the last 10 versions using There are further options for restricting by stage or region Even better it is possible to integrate the plugin into the deploy to automatically remove all but x versions I used this plugin for my current serverless project and it delivered on the promise "
51736941,51722526,"stackoverflow.com",19,"2018-08-08 03:00:38+03","2024-05-17 05:11:00.641723+03","From PublishVersion AWS Lambda CodeStorageExceededException means You have exceeded your maximum total code size per account From AWS Lambda Limits AWS Lambda "
58833045,51722526,"stackoverflow.com",9,"2019-11-13 10:30:36+02","2024-05-17 05:11:00.643354+03","As already mentioned above Total size of all the deployment packages that can be uploaded per region 75 GB So if we set By default the framework creates function versions for every deploy This behavior is optional and can be turned off in cases where you do not invoke past versions by their qualifier If you would like to do this you can invoke your functions as arnawslambda functionmyFunc3 to invoke version 3 for example To turn off this feature set the providerlevel option versionFunctions provider versionFunctions false"
51743038,51722526,"stackoverflow.com",4,"2018-08-08 12:23:20+03","2024-05-17 05:11:00.645352+03","I am Solve that issues to delete cloud formation stack using sls remove and deploy serverless project using sls deploy command that is work for me "
60348279,51722526,"stackoverflow.com",3,"2020-02-22 03:45:45+02","2024-05-17 05:11:00.646352+03","Requesting an AWS Lambda function and layer storage quota increase and calling aws cloudformation continueupdaterollback after the request is approved should remedy stacks stuck in UPDATE_ROLLBACK_FAILED due to CodeStorageExceededException"
52541438,52538062,"stackoverflow.com",2,"2018-09-27 19:29:23+03","2024-05-17 05:11:45.335509+03","If you are referring to the permissions that you give to the Lambda Function to have at execution time after it has been deployed by the Serverless Framework then you add role permissions in the serverless yaml file within the provider section Here is an example of permissions for the Lambda to talk to S3 Execute other Lambdas and Send Emails with SES"
51745313,51745107,"stackoverflow.com",32,"2018-08-08 14:15:27+03","2024-05-17 05:11:01.738883+03","Lambda executes functions based on certain triggers The use case for Lambda is quite broad and there is heavy integration with many AWS Services You can even use it to simply execute the code via AWSs API and receive the code into your scripts separate from AWS Common use cases include Lambdas being simply executed and the output received plugged into API Gateway to serve user requests modifying objects as they are placed into S3 buckets etc Lambda is a service that allows you to execute Lambda functions that modify the behaviour of CloudFront specifically Lambda simply runs during the request cycle and makes logical decisions that affect the delivery of the CloudFront content httpsaws amazon comlambdafeatures httpsdocs aws amazon comlambdalatestdglambdaedge html"
54557741,51745107,"stackoverflow.com",4,"2019-02-06 18:01:03+02","2024-05-17 05:11:01.741099+03",""
51745294,51745107,"stackoverflow.com",3,"2018-08-08 14:14:16+03","2024-05-17 05:11:01.742097+03",""
70685920,51745107,"stackoverflow.com",2,"2022-01-12 19:32:00+02","2024-05-17 05:11:01.743097+03","Lambda is a serverless AWS compute service that allows user to run code as function trigger In file processing optimization lot of use cases On the other hand Lamda is extension of AWS lambda is a feature of cloudfront that allows user run code closer to the application so improves performance and reduce latency Here is the official documentation describe nicely about Lambda httpsdocs aws amazon comlambdalatestdglambdaedge html"
51753600,51752703,"stackoverflow.com",9,"2018-08-08 21:49:16+03","2024-05-17 05:11:03.706676+03","The Resource Reference Name seems to matter I have always had to use the name of the bucket in the resource name For example a bucket with www example com needs a reference name of S3BucketWwwexamplecom However I also notice that the BucketName element is missing from your example This is from working example for a static website with a Bucket Policy"
51754350,51752703,"stackoverflow.com",1,"2018-08-08 22:32:23+03","2024-05-17 05:11:03.708677+03","Since you are using a lambda to upload you should create an IAM Role for your Lambda and an IAM Policy with only the permissions required for operation You might accomplish this by using the following excerpt in your cloud formation Then in your serverless yml just refer to the task role created using something like this to reference the execution role We have a setup like this working in several projects I hope it works for you "
51950426,51762438,"stackoverflow.com",3,"2018-08-21 16:59:33+03","2024-05-17 05:11:05.024162+03","Unfortunately there is not a way to do this with wildcards This is a limitation of AWS not Serverless You could write a script that loaded a list of all your CloudWatch logGroups and then applied those events to your reportFatalError function on deployment See here httpsserverless comframeworkdocsprovidersawsguidevariablesreferencevariablesinjavascriptfiles"
72733295,51762438,"stackoverflow.com",1,"2022-06-23 19:17:55+03","2024-05-17 05:11:05.026163+03","I also faced that issue recently and after researching the topic I ultimately decided to go with the implementation leveraging JavaScript based on what Jeremy suggested Frankly it seems there is no alternative apart from having a long long serverless yml configuration file I will be more than happy to learn about other options if there are any So as a followup I decided to share a minimal working example in case someone else needs to provide the same capability in the project and wonders how to actually achieve that _serverlesscloudwatch_log_events js The _serverless directory is meant to store all Serverless Frameworkrelated configuration files handler py serverless yml Generated using the sls print command The following tests involve Localstack therefore it needs to be added as a dependency in the plugins Updated config serverless yml dockercompose yml It is a minimal Localstack config file needed for the demo Just run it with dockercompose up Be aware that following this solution means that defining other events using the YAML syntax will not work The reason is that YAML does not fully support merging arrays at the moment related threads 1 2 For instance the following code would produce the config as follows Notice the nested list In order for the config to be valid there would need to be an option to extend the cloudwatch_log_events array which is not possible today However to handle different categories of CloudWatch log events e g per service you can go ahead with granularity on the JavaScript level Following the above example _serverlessservicesfoo js _serverlessservicesbar js _serverlesscloudwatch_log_events js The rest of the files remain untouched "
51800021,51799267,"stackoverflow.com",2,"2018-08-11 16:11:18+03","2024-05-17 05:11:05.567808+03","After some discussion on chat we discovered that statusCode on the response body is missing"
51918634,51848242,"stackoverflow.com",1,"2018-11-08 18:28:49+02","2024-05-17 05:11:07.358049+03","I know there are some interesting bespoke solutions that are being developed for live serverless debugging if you are prepared to leave Eclipse Rookout is one example"
51851138,51848242,"stackoverflow.com",0,"2018-08-15 02:47:42+03","2024-05-17 05:11:07.359119+03","You will want to take a look at AWS Sam Basically you create a yaml file that runs your application attach to the process and use the Sam CLI to send events at your code From this github you can see that a very simple yaml file is"
63192120,51848242,"stackoverflow.com",0,"2020-07-31 15:47:38+03","2024-05-17 05:11:07.360116+03","I would assume you plan to debug it locally You need awssamcli tool where a lambda could be run locally using sam local command sam local startapi d 5858 and port 5858 will be used for debugging purposes httpsdocs aws amazon comserverlessapplicationmodellatestdeveloperguideserverlesssamcliusingdebugging html Here an example how to do this with eclipse httpsdocs aws amazon comtoolkitforeclipsev1userguidetkesamlocal htmldebuglamfunctionlocally"
74140170,51893940,"stackoverflow.com",0,"2022-10-20 15:50:03+03","2024-05-17 05:11:08.678149+03","I know it is probably late but I had a similar issue Check in the console if the SQS trigger is enabled in some cases it can get automatically disabled that is what happened to me "
68261470,51902484,"stackoverflow.com",1,"2021-07-05 22:51:50+03","2024-05-17 05:11:09.049769+03","First of all the outputs section of the template is for displaying the information after the template has been executed You cannot get a reference to anything in that section while resources are being created The documentation for the Api type of the Events property in a serverless function is at httpsdocs aws amazon comserverlessapplicationmodellatestdeveloperguidesampropertyfunctionapi html There is a property called RequestModel which could be added below your Path and Method properties to reference your ModelResource model So your events section might look like and take out the RestApiId property I have not tested this so I do not know if it will work but give it a shot And since I am answering this 2 years after you asked the question you have probably already figured it out "
51903416,51902951,"stackoverflow.com",4,"2018-08-18 01:08:42+03","2024-05-17 05:11:09.730793+03","I can see two mistakes 1 your getParams are wrong You make get request on PK but you provide GSI key in the params section It should be like This is the reason of the error Your hash key is not on attribute longUrl 2 Anyway you cannot make get request on GSI Its not have GSIs are designed GSI does not force uniqueness so there can be multiple items in the same GSI Hash key therefore you can only query instead of get What you are trying to do is something like"
51927828,51925672,"stackoverflow.com",36,"2018-08-20 12:30:42+03","2024-05-17 05:11:10.369489+03","Apparently there is a bug with version [email protected] issue on github I have downgraded to version 0 2 30 by using"
54411784,51925672,"stackoverflow.com",5,"2019-01-29 01:29:32+02","2024-05-17 05:11:10.370489+03","Following the advice on serverlessdynamodblocalissues195 I just ran sls dynamodb install localPath bin and Dynamodb installed correctly Running serverless offline start then worked without error "
76660212,51925672,"stackoverflow.com",0,"2023-07-11 11:53:43+03","2024-05-17 05:11:10.372606+03","In my opinion it is better to go with serverlessdynamodb instead of serverlessdynamodblocal Actually that is a fork from dynamodblocal only Here is the documentation for migrating it httpsgithub comraisenationalserverlessdynamodbmigratingfromserverlessdynamodblocal You need java installed in your system in both of the case Use this command if you are a ubuntu user for installing java sudo aptget install openjdk8jdk Need to setup java home env variable as well "
51935970,51935386,"stackoverflow.com",4,"2018-08-20 20:42:15+03","2024-05-17 05:11:11.122577+03","When you configure a Lambda function to run in the VPC it uses an ENI that is created with and IP address in one of the subnets you select Based on the formula of expected ENIs needed it seems that ENIs can be shared between lambdas There are only two reasons that I know of for running your lambda in a VPC Lambda functions cannot received inbound connections in any case Disadvantages of putting your lambda in a VPC are"
51939991,51939336,"stackoverflow.com",0,"2018-10-24 06:50:06+03","2024-05-17 05:11:12.102709+03","Check the region that this function is in and then check the region from which you are calling the function Instead of supplying the function name insert the functions ARN as the function name instead of just"
51939509,51939395,"stackoverflow.com",3,"2018-08-21 08:27:52+03","2024-05-17 05:11:12.799777+03","If it works for you to generate a single output file and use a module loader you can use outFile and prepend If you want multiple output files maybe it is worth filing a suggestion to ask for an option to bundle dependencies in that case you would be the second person who has asked about this on Stack Overflow today Edit Suggestion is here Edit 2 After extensive discussion the conclusion was to enable the nohoist option on the final Yarn workspace which gives us symlinks from node_modules to the other workspaces After we call tsc b in the final workspace the Serverless packaging tool follows the symlinks and produces a zip file with the correct structure No bundling is needed at the TypeScript level Caveat Yarn seems to install devDependencies of dependencies in the final workspace which seems wrong to me and confuses Serverless into unnecessarily including those modules in the bundle "
52134523,52134100,"stackoverflow.com",29,"2018-09-02 09:24:36+03","2024-05-17 05:11:13.36032+03","Since mentioned Lambda does not have access to the public internet to access AWS APIs please setup a VPC endpoint As per the description VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services For AWS Systems Manager follow this procedure Setting Up VPC Endpoints for Systems Manager"
71653843,52134100,"stackoverflow.com",4,"2022-04-18 22:14:29+03","2024-05-17 05:11:13.362321+03","This will happen if your Lambda is in a VPC You need to do two things CDK Example"
55130348,52146473,"stackoverflow.com",8,"2019-03-12 22:44:04+02","2024-05-17 05:11:14.455115+03","The error message does not help too much but I found out that this message is often caused by a lost npm package If you test the lambda in the AWS console you can see the specific details "
52146996,52146473,"stackoverflow.com",3,"2018-09-03 12:42:01+03","2024-05-17 05:11:14.456339+03","Ok I solved it by removing my package json then adding it again and installing NOT as dev dependencies my packages and it worked "
56147920,52146473,"stackoverflow.com",2,"2019-05-15 14:08:55+03","2024-05-17 05:11:14.457336+03","You will also get this error when you require a module or file using a wrong path In other words requiring a modulefile that does not exist It could be a custom module or npm Please double check all your module import paths and see that they are accurate "
57902544,52146473,"stackoverflow.com",2,"2019-09-12 11:18:47+03","2024-05-17 05:11:14.458533+03","In my case it turned out that I used both python and nodejs for my lambdas but had not set the nodejs lambda functions runtime environment as nodejs Previously my serverless yml looked similar to this Simply give the runtime environment for the nodejs lambda in order to solve this"
52152938,52152414,"stackoverflow.com",0,"2018-09-03 18:49:30+03","2024-05-17 05:11:15.514301+03","We have accomplished similar need as follows AWS Lambda Functions Code in Java 8 with Maven as dependency and build tool All the functions are defined in single deployment package and at project level all dependencies are controlled through single Maven This way build generates one deployment bundle and same bundle can be used for all Lambda functions for confirmation with lambda handler set differently for each function This has greatly reduced the deployment process "
52191091,52156214,"stackoverflow.com",1,"2018-09-05 21:05:27+03","2024-05-17 05:11:16.467735+03","You should be able to resolve this error by performing the following step locally before deploying This should install the version of the grpc binary that is needed on the system you are deploying to "
52167210,52157979,"stackoverflow.com",1,"2018-09-05 11:08:40+03","2024-05-17 05:11:17.844191+03","EDIT 05092018 I have found this blog post which describes my approach pretty well Ensure Secure Communication with AWS IoT Core Using the Certificate Vending Machine Reference Application You could take a look at JustinTime Provisioning or build your own solution based on Programmatic Provisioning I have dealt with this topic many times and had to realize that it depends a lot on the use case which makes more sense Also security is an aspect to keep an eye on You do not want to have a public API responsible for JIT device registration accessible by the whole Internet A simple Programmatic Provisioningbased scenario could look like this You build a thing maybe a sensor which should be abled to connect to AWS IoT and have an inhouse provisioning process Simple provisioning process The registration code running on the server could look something like this JS AWS JS SDK registerthingtemplatebody json Make sure you got all the NewDevice Thing types groups and policies in place Also keep in mind ThingName SerialNumber important for unregisterThing "
52310520,52169226,"stackoverflow.com",1,"2018-09-13 12:18:37+03","2024-05-17 05:11:18.436444+03","works for me "
52223423,52219478,"stackoverflow.com",3,"2018-09-28 07:32:17+03","2024-05-17 05:11:19.518702+03","I did some digging and have finally managed to solve this See httpsgithub comspulecmotoissues1793 This issue was due to some incompatibilities between boto and moto Turns around that everything works fine when we downgrade botocore to 1 10 84"
52313530,52265824,"stackoverflow.com",1,"2018-09-13 15:13:14+03","2024-05-17 05:11:23.198415+03","I found two solutions for this so I will just leave them here Regarding the error to launch the container I was able to correct that by changing the dockercompose file to do instead of After that and being able to launch the container I realized there was an update on the packages Reverting them fixed the problem I did not notice those packages updates they were not done on purpose so I guess npm install or something like this updated the packages These were the offending package versions These were the correct and old ones Hope this helps someone facing the same problem "
52297944,52290976,"stackoverflow.com",3,"2020-06-20 12:12:55+03","2024-05-17 05:11:24.061924+03","To create a new service use the following command Your docs are for Serverless 0 5 You will find the correct docs at httpsserverless comframeworkdocs You can also have a list of available commands with And a list of available templates with If your project has a lot of resources you will need to split it in multiple services or use nested stacks You will find some solutions here "
52338185,52333418,"stackoverflow.com",3,"2018-09-14 22:28:01+03","2024-05-17 05:11:26.012493+03","You are confusing IAM Roles with VPC Security Groups The error that you are receiving means that the security group rule does not exist for the specified security group This has nothing to do with IAM Roles If your goal is to add remove permissions from IAM Roles then you will need to rewrite your code to deal with IAM Policies "
52231423,52228361,"stackoverflow.com",21,"2018-09-08 07:00:08+03","2024-05-17 05:11:20.296718+03","You can check this article comparing SAM and Serverless The key differences listed on that page are as follows The Serverless Framework is a framework that makes it easy to write eventdriven functions for a myriad of providers including AWS Google Cloud Kubeless and more For each provider a series of events can be configured to invoke the function The framework is open source and receives updates regularly The AWS Serverless Application Model SAM is an abstraction layer in front of CloudFormation that makes it easy to write serverless applications in AWS There is support for three different resource types Lambda DynamoDB and API Gateway Using SAM Local Lambda and API Gateway can be run locally through the use of Docker containers Both frameworks have in common that they generate CloudFormation In other words they both abstract CloudFormation so that you need to write less code to build serverless applications in the case of SAM and to deploy Lambda functions for both SAM and Serverless The biggest difference is that Serverless is written to deploy FaaS Function as a Service functions to different providers SAM on the other hand is an abstraction layer specifically for AWS using not only FaaS but also DynamoDB for storage and API Gateway for creating a serverless HTTP endpoint Another difference is that SAM Local allows you to run Lambda functions locally and to spin up an API Gateway locally This makes it easier to develop and test Lambda functions without deploying them to AWS With the Serverless framework you can also invoke Lambda functions from the command line but only if they are deployed to AWS and available through API Gateway "
54156063,52228361,"stackoverflow.com",12,"2019-03-23 16:53:03+02","2024-05-17 05:11:20.300719+03","Biggest differences between SAM and SF 1 SAM is AWS only SF supports multiple backends so it supports deployment for multihybrid cloud application SF also support kubernetes backend 2 For AWS both SAM and SF templates compile to Cloudformation CF SAM has the ability to use Transform which is essentially macros for CF 3 SAM is written in Python SF is written in Javascript 4 SF has plugins which allow you to run any codes including nonJavascript which effectively means it is possible to go beyond limitation of Cloudformation CF since there are always something newish that is not supported in CF yet Plugin system is also extremely flexible and can be very useful 5 SF variables system is more flexible which allow you to do dynamic include based on existence of other parameters such as stage SAM variables are much closer to CF "
76567834,52228361,"stackoverflow.com",0,"2023-06-27 22:02:46+03","2024-05-17 05:11:20.302043+03","reading several answers while build several projects on serverless have to notice "
74012706,52250522,"stackoverflow.com",0,"2022-10-10 12:25:04+03","2024-05-17 05:11:21.343489+03","You would need to set your NODE_ENVproduction or set playgroundAlwaysfalse In development Apollo Server enables GraphQL Playground on the same URL as the GraphQL server itself e g httplocalhost4000graphql and automatically serves the GUI to web browsers When NODE_ENV is set to production GraphQL Playground as well as introspection is disabled as a production bestpractice "
52777428,52251075,"stackoverflow.com",110,"2024-02-01 22:59:51+02","2024-05-17 05:11:22.140412+03","Data string As being said you can use the data option to pass a string data as an event to your function Data file What you can also do is to pass a path to a json file with data as the event and within the event file define the data you want You could somehow return the event from a call and save it in a JSON file or grab one from Amazon They provide some examples httpsdocs aws amazon comlambdalatestdgeventsources html Keep in mind that if you pass both path and data the data included in the path file will overwrite the data you passed with the data flag Documentation httpsserverless comframeworkdocsprovidersawsclireferenceinvokeinvokelocal"
56056686,52251075,"stackoverflow.com",19,"2019-05-09 13:11:47+03","2024-05-17 05:11:22.142262+03","I have tried the answers with the attribute data but neither works In fact the problem is that the data will pass a string value to the framework So if you write in your javascript file console log typeof event you will get String instead of Object Which means serverless framework does not transform the input to a JSON object correctly That is why you got xx of undefined error My solution is to use the p or path attribute In your example follow these steps"
52251139,52251075,"stackoverflow.com",17,"2018-09-10 07:34:59+03","2024-05-17 05:11:22.143853+03","Use data and pass is any format of data you want to send it to the local lambda String Data Example serverless invoke function functionName stage dev region useast1 data hello world JSON Data Example serverless invoke function functionName stage dev region useast1 data property1 value JSON Data from file serverless invoke function functionName stage dev region useast1 path libdata json Complete documentation can be accessed from here Hope it helps "
66691853,52251075,"stackoverflow.com",10,"2021-03-18 15:14:52+02","2024-05-17 05:11:22.145357+03","QueryStringParameters and pathParameters are two different input types For the former the accepted response works to me For the later is pretty much the same just replace queryStringParameters by pathParameters"
52774128,52251075,"stackoverflow.com",9,"2018-10-12 10:08:25+03","2024-05-17 05:11:22.14636+03","For future reference Your case would have been solved like this Just figured it out thanks to Kannaiyans JSON Example "
58339250,52251075,"stackoverflow.com",2,"2019-10-11 13:23:57+03","2024-05-17 05:11:22.14736+03","There is also an alternative to all the other options here I wrote a blog article about this and will link it after going through some of the detail There are two basic aspects you need to handle For the first one I just added Mocha to the project and used the unit testing framework as a way to be able to click a button in my IDE and have my code be executed with test data I can also do local debugging step through code etc without issue The added advantage is if you just set this up like I have purely to execute your code you still have the beginnings of a unit test suite you can flesh out later if you wish For the second one its even easier There is an npm module called awssdkmock and it allows you to register a listener for a specific AWS service and method such as DynamoDB query or S3 putItem and respond to that request with whatever you wish even errors if you wish to test how your code handles the unthinkable an AWS service going down With the setup of these two elements I can locally test any handler I develop Ultimately you will always need to do some integration testing in the cloud as there is just no local substitute no matter how elaborate or awesome it seems for the actual cloud services you will be using but this can get you quite far httpsserverless comblogserverlesslocaldevelopment"
70572516,52251075,"stackoverflow.com",1,"2022-01-04 01:27:04+02","2024-05-17 05:11:22.149355+03","By the way you can create another function and put into it all test cases for the one you wanted to invoke then"
58335432,52251075,"stackoverflow.com",0,"2019-10-11 09:30:11+03","2024-05-17 05:11:22.151355+03","One challenge with local invoke that you seem to have run into is debugging the lambda runtime vs the dynamodb resource Ideally you want to invoke both the lambda runtime and the live dynamodb table Today serverless local invoke and the AWS SAM CLI both invoke the lambda runtime but not the live cloudstack As long as you have your Lambdas ARN you can invoke both the lambda runtime as well as the cloudstack with the Stackery CLI it is free Here is an example debugging a serverless framework lambda "
77772917,52251075,"stackoverflow.com",0,"2024-01-07 13:44:47+02","2024-05-17 05:11:22.152356+03","I have been looking for a direct way to do that from the command line instead of the file passing method that worked for me For your case data json [I see it is 2 years ago but I ran to similar state]"
52370299,52355285,"stackoverflow.com",0,"2018-09-18 13:52:21+03","2024-05-17 05:11:26.90559+03","After seeing your serverless yml file I believe that the issue may well be in the webpack config js file It may be worth trying changing your custom section and putting quotes around webpack config js Like I think that the issue is in your App js class here You have a invalid JSON structure where the object event is being passed it is not being given a keyname You probably want something like Also your constant THE_MESSAGE is not inside your App class You should move it inside the App classes like this"
52359492,52359263,"stackoverflow.com",1,"2018-09-17 07:01:58+03","2024-05-17 05:11:27.939391+03","I found a way around this by having my JSON in a file rather than in the command itself this does not solve the issue I am experiencing in the question but it is a way to invoke the function with Json I added a eventsstartAndEnd json file that contains my json data And referenced that file in the invoke command serverless invoke f orders path eventsstartAndEnd json"
71705467,52359263,"stackoverflow.com",0,"2022-04-01 13:27:07+03","2024-05-17 05:11:27.941392+03","Incase you hit this issue when running the command via npm I also had a similar error when invoking it with By changing the double quotes to single quotes on the data it then suddenly started working"
52378804,52378254,"stackoverflow.com",21,"2018-09-18 07:09:59+03","2024-05-17 05:11:28.984281+03","You can remove an environment variable by"
52421925,52405613,"stackoverflow.com",0,"2018-09-20 12:34:07+03","2024-05-17 05:11:31.050611+03","Fixed I commented out the sections of serverless yml related to the identity pool deployed destroyed then uncommented that section redeployed and restored from backup It seems to be a bit of a hack but it worked I also feel like there should be a way to edit identity pool roles through cloudformation "
52424475,52406234,"stackoverflow.com",8,"2018-09-21 16:26:21+03","2024-05-17 05:11:32.168564+03","The error here gives you the solution API Gateways callback expects a string and not a javascript object You have to stringify it before passing it to the callback EDIT Then on the client side parse the JSON string to get it back to an object this example assume that your client is Javascript too "
52431687,52414232,"stackoverflow.com",2,"2019-09-03 05:58:17+03","2024-05-17 05:11:33.2648+03","finally I was able to fix the issue I was missing node path"
52424891,52416184,"stackoverflow.com",6,"2018-09-20 15:18:59+03","2024-05-17 05:11:34.259653+03","DynamoDB requires that you declare ONLY the attributes that compose your key schema See AWS docs If id is the only attribute that is used to compose your key schema your resource should look like this DynamoDB does not care about the other attributes Upon inserting your data DynamoDB will detect the new attributes without having to declare them in a schema That is the whole point of a nonrelational database Additionally if you wanted to have a date as sort key in your key schema you could have something like this A key schema always have at least a partition HASH key and can optionally have a sort RANGE key Check this to learn more about DynamoDBs key schema "
52416856,52416775,"stackoverflow.com",5,"2018-09-20 05:51:45+03","2024-05-17 05:11:35.193156+03","You should be able to rely on AWS_SAM_LOCALtrue per this commit "
52435110,52435006,"stackoverflow.com",4,"2018-09-21 03:44:21+03","2024-05-17 05:11:36.124035+03","iamRoleStatements is an array of objects"
52462317,52436836,"stackoverflow.com",0,"2018-09-23 04:53:27+03","2024-05-17 05:11:36.898521+03","I cannot add them in the resize handler block I believe you can Declaring the bucket in the resources section does not remove your ability to specify rules in your handlers S3 event Full example used to test"
52453889,52445951,"stackoverflow.com",6,"2018-09-22 08:50:35+03","2024-05-17 05:11:37.640994+03","You could cache at the API Gateway layer and invalidate the cache by sending a CacheControl maxage0 header to API Gateway for example from the Lambda that is altering DynamoDB records during PUT requests You would need to grant specific IAM permissions for that to work Keep in mind that you can only invalidate 1000 requests per month for free after that you will be charged 0 005 per path invalidated CloudFront has similar caching and invalidation options but you can probably get all the same caching options from just API Gateway directly Another option would be to cache at the DynamoDB layer using DynamoDB Accelerator It provides significant retrieval improvements to DynamoDB requests and handles invalidation for you Maintainabilitywise it is hard to get a better option The downside is you will not bring the latency down as much as if you use CloudFront or API Gateway caching Lastly you can also look into ElastiCache which you would access from your Lambda function But given the overhead of writingreadinginvalidating the cache yourself the other options are probably more maintainable in the long term You may find the AWS Caching Overview helpful in coming up with more ways to cache depending on your needs "
52472083,52450015,"stackoverflow.com",1,"2018-09-24 05:44:50+03","2024-05-17 05:11:38.575416+03","Assuming you are setting those variables as environment variables and accessing them using process env the values should always be strings From node docs Assigning a property on process env will implicitly convert the value to a string If you need that value to be an integer perhaps use parseInt Running parseInt on an integer works fine so should work locally as well "
52524250,52479994,"stackoverflow.com",10,"2018-09-26 21:44:34+03","2024-05-17 05:11:39.625083+03","You have two options Stop using as the type selector This is treating everything as binary and therefore base64 encoding everything Unfortunately you cannot express an exception to the rule only things that follow the rule You could add a comprehensive list of the types that must be treated as binary but that sounds fragile to me Just accept the base64 JSON and debase64 it on the other side This seems easiest You are using node it looks like and there are ample tutorials about this Sure it adds some steps and a bit of bloat but let us be honest you are using API Gateway and Lambda which are nice tools but so clearly performance does not have to be tuned to the millisecond here "
52522920,52522096,"stackoverflow.com",6,"2021-10-06 22:02:51+03","2024-05-17 05:11:40.730703+03","In iamRoleStatements specification there are and error it should be Or This error is related to Because this is not a reference to a parameter or a variable Enjoy "
52885714,52525042,"stackoverflow.com",29,"2018-10-19 06:58:34+03","2024-05-17 05:11:41.821859+03","Hooks can be added to the lifecycle events of the Serverless framework I used serverlesspluginscripts plugin httpswww npmjs compackageserverlesspluginscripts to invoke custom jobs after deployment and removal of stack Here is an example Now after successful deployment via serverless deploy sls invoke f functionName is triggered Similarly on removal using serverless remove npm run scriptName sls invoke f anotherFunctionName executes Complete list of Serverless frameworks Lifecycle events commands is available here httpsgist github comHyperBrain50d38027a8f57778d5b0f135d80ea406"
52540354,52525042,"stackoverflow.com",6,"2018-09-27 18:23:59+03","2024-05-17 05:11:41.823859+03","Not sure if this entirely fits your needs but I have had success with configuring a Lambda function with a CloudWatch event that will trigger on CloudFormation API calls You will need CloudTrail enabled to do this You could probably limit the functions execution to specific stacks probably using the resources attribute in the CloudTrail event "
52546823,52528384,"stackoverflow.com",2,"2018-09-28 03:43:35+03","2024-05-17 05:11:42.781284+03","Unfortunately there are not any official serverless tools to deploy multiple services like this I ended up writing a small bash script to do it You could use something like"
52552360,52535433,"stackoverflow.com",0,"2018-09-28 12:32:16+03","2024-05-17 05:11:43.587234+03","I have now successfully resolved this issue I do not know why this was the case but the problem seemed to be with deploying to node8 10 using an older version of Serverless 1 27 2 Upgrading to the latest version of Serverless 1 32 0 fixed it immediately "
52826094,52817039,"stackoverflow.com",0,"2018-10-16 02:40:14+03","2024-05-17 05:11:47.488909+03","Regardless your decision in the end it will all be an architectural decision which should be made considering processes team experience culture architectural knowledge etc To give you a bit of insight you can structure your project in a way you have your functions in a folder and any other module in a diff folder What you need to be careful is that you define the package include andor package exclude in your config file Moreover if by structure you mean all the CloudFormation resources you have a section for it in your serverless yml config files If it starts getting too big you could as well use an extra config file and import it inside your serverless yml Ex The benefits of having everything in one repo I hope those reasons are enough for you to make your decision "
66534634,52826389,"stackoverflow.com",20,"2021-03-08 19:44:01+02","2024-05-17 05:11:48.336568+03","Might be a bit late to answer but I recently had some experience that might be useful to share about this topic I had used Serverless Framework to develop applications for a customer The customer already has a middleware team with their own way to manage the infrastructurerelated AWS resources I had a similar question what are the AWS resources should I put in my serverless project For the first attempt I used Serverless Framework to create VPC and networking stuff along with the lambda functions The problem with this solution is that my project has a lot of conflicts with the way the middleware team works They have a central document to keep track of subnets and IP addresses They have a single Internet NAT and VPN gateway for the whole organization to which other VPCs must connect via a transit gateway They use a third party software to manage firewall rules And etc I had to rework many times to modify my code to be aligned with their networking prerequisites For the second attempt I put only Lambda functions and a few stuff in my serverless project and have other stuff such DynamoDB tables SNS SQS S3 IAM role Security Group VPC etc preprovisioned by the existing IT request process of the middleware team and put the reference IDs in my configuration A new problem occurred with this solution as the application grows new AWS resources such as DynamoDB tables are required all the time and the provisioning plan should be aligned with the development cycle The IT request process usually is out of sync with the development cycle The project was delayed due to the pending provisioning of the required AWS resources Yet to mentioned that the resource specification usually changes as we discover more during development Issue another IT request to change the specification makes the process incredibly terrible From this experience I have learned the importance of separating the applicationconcerned resources from infrastructureconcerned resources Mixing one into another causes the problems as mentioned above What works for me is to not put networking concerns into a serverless project Instead have such resources preprovisioned by other means such as using Terraform in a separated project or by manual provisioning and put their reference IDs in the configuration of your serverless project And for what resources should be in a serverless project the minimal list for me in general are Lambda functions DynamoDB tables SNS topics SQS queues and CloudWatch Events EventBridge I start with these dummies in general and then start analyzing the requirements to see if my application will need some other kind of resources during its growth Add such resources to the applicationconcerned list and discuss with the related party such as middleware architect and security team to determine whether all the resources in the list can be controlled in a Serverless Framework project "
52830648,52826389,"stackoverflow.com",17,"2018-10-16 11:07:51+03","2024-05-17 05:11:48.340569+03","First you could read terraformserverless comparison in httpsserverless comlearncomparisons page And actually you can pick either one of them or use both of them together because they are technically not mutually exclusive one focus on one thing the other focus on solving the other thing in the similar problem space but not the same problem etc Choosing which one depends on a lot of factors really Basic thinking may goes like this When you want to focus on serverless application related resources you might think of using serverless framework serverless yml But if you want to focus on defining Fullfledged infrastructure or more traditional style cloud infrastructure i e defining networking servers storing load balancer etc by yourself you might think of using Terraform The best way is to experiment just pick either one at a time to experiment Then you will see what fits to your specific task and what makes your self easier "
52859859,52826389,"stackoverflow.com",7,"2018-10-17 19:46:56+03","2024-05-17 05:11:48.342924+03","I think it is a difficult question to answer as it depends on many factors like your companys internal structure As a rule of thumb I would say that every resource that is only used by your serverless service should be defined in your serverless yml file and the shared resources should be defined using a terraform or another technology project I used this approximation in the past and it works fine Yan Cui has a nice article talking about sharing code and infrastructure httpshackernoon comawslambdahowbesttomanagesharedcodeandsharedinfrastructure827bed395eb7 "
52887216,52887034,"stackoverflow.com",1,"2018-10-19 09:55:41+03","2024-05-17 05:11:49.434104+03","There is a chance that pymysql is there in your system packages So when you built the virtualenvironment it used the system package Create a clean virtualenv using Or else you can use the current one with"
53046588,52887034,"stackoverflow.com",1,"2018-10-29 15:31:42+02","2024-05-17 05:11:49.435564+03","Use the plugin serverlesspythonrequirements with docker This will package all your python virtual env dependencies into your serverless package See this answer for more details"
66246012,52890795,"stackoverflow.com",4,"2021-02-17 18:27:59+02","2024-05-17 05:11:50.526131+03","The excludeDevDependencies setting is an option you can provide in the package config in serverless yml file By default it is already defined as true so setting it explicitly as true will not have an impact other than expliciting that such config is there There would only be an impact if you setted it as false When using this feature for example when running serverless package or serverless deploy the CLI will follow these steps For example this makes sense for Serverless AWS Lambda environment where you have to upload the exact sources that you need as a whole package Now since you stated that in the case of GCP the packages are handledinstalled by the provider then you should not need to upload any node_modules paths so you should be safe by setting the entire folder in the serverless yml for exclusion by using the exclude setting Extra note There were some bugs to this excludeDevDependencies feature in the past that were causing it to not work correctly in some cases but many if not all of them have been resolved in the latest 2 x releases You can find those in their changelog look for the Packaging feature If this is happening to you consider upgrading to latest version which is 2 25 0 as of today "
57298865,52890795,"stackoverflow.com",2,"2019-08-01 00:09:27+03","2024-05-17 05:11:50.528981+03","I had the same problem and if I used include of the node_modules then also the dev dependencies were included After some time I found that you just need to add node_modules and any folder of your interest by not excluding them by using To include is excludeDevDependencies true is optional because it is set by default I think this is a bug of Serverless it should work straightforward "
53046552,52890795,"stackoverflow.com",0,"2018-10-29 15:29:07+02","2024-05-17 05:11:50.530984+03","This is expected behavior no You dont put node_modules into exclude Any dependencies you have in package json should be loaded into your serverless package Any devdependencies in package json will be excluded if you have excludeDevDependencies true GCP will install the node_modules based on packagelock json I dont know what GCP is but how do you expect it will install package json on lmabda when there is no local file storage Any node_modules you need for lambda to run MUST be included in your serverless package "
52955905,52900200,"stackoverflow.com",2,"2018-10-23 21:43:42+03","2024-05-17 05:11:51.607786+03","Turns out this was due to SES not being available in the region I was using It sure would have been nice with a error message such as type not supported in region but instead the generic unrecognized type was raised "
52909646,52909194,"stackoverflow.com",0,"2018-10-20 23:03:01+03","2024-05-17 05:11:52.336309+03","So I found the answer here There are 2 parts to this"
52909605,52909604,"stackoverflow.com",9,"2019-06-14 07:10:41+03","2024-05-17 05:11:52.956652+03","The whole ARN for an API is of the form arnawsexecuteapiregionaccountidapiidstageMETHOD_HTTP_VERBResourcepath Using Ref ApiGatewayRestApi link within your serverless yml gives you the apiId You can do something like the below see the Resource section to convert this to a whole Arn to reference an API"
53058786,52914527,"stackoverflow.com",1,"2018-10-30 09:02:30+02","2024-05-17 05:11:54.566305+03","That is a problem with the serverless framework AWS SAM framework too sadly and not with your code The Node JS and Go local invokes suffer from the same issue unfortunately The problem stems from not execution of the code but the fact that on every invoke the execution environment needs to be reconfigured and the time needed for that depends on the machine One alternative to invoke is as I did write a API HTTP Server wrapper upon your lambda handlers On the local environment you can start the server using the wrapper and continue testing Edit This is basically what you need to do to create a wrapper"
52921598,52921582,"stackoverflow.com",1,"2018-12-06 06:34:44+02","2024-05-17 05:11:55.028434+03","After take a look about the error stack The reason of this issue is I did not set the environment variables for google credentials After that it works "
52939516,52936651,"stackoverflow.com",3,"2018-10-23 03:45:12+03","2024-05-17 05:11:56.142096+03","I believe there are two potential solutions to that problem The first and the simplest option is to take advantage of lambda hot state it is the concept when Lambda reuses the execution context for subsequent invocations As per AWS suggestion Any declarations in your Lambda function code outside the handler code see Programming Model remains initialized providing additional optimization when the function is invoked again For example if your Lambda function establishes a database connection instead of reestablishing the connection the original connection is used in subsequent invocations We suggest adding logic in your code to check if a connection exists before creating one Basically while the lambda function is the hot stage it mightshould reuse opened connection s The limitations of the following The second option would be to use a connection pool connection pool is an array of established database connections so that the connections can be reused when future requests to the database are required While the second option provides a more consistent solution it requires much more infrastructure While it might be overwhelming to provision that much additional infrastructure for connection pooler it still might be a valid option depending on the scale of the project your existing infrastructure may be you already using containers and cost benefits"
53054074,52936651,"stackoverflow.com",0,"2018-10-29 23:26:33+02","2024-05-17 05:11:56.144715+03","Best practices by AWS recommends to take advantage of hot start You can read more about it here httpsdocs aws amazon comamazondynamodblatestdeveloperguideStreams Lambda BestPracticesWithDynamoDB html"
52959360,52959008,"stackoverflow.com",1,"2018-10-24 03:05:02+03","2024-05-17 05:11:57.833208+03","Try closing the connection to the database once you are done using it I previously had a similar error because the open connection was keeping the Lambda alive until it got timeout"
52981167,52979424,"stackoverflow.com",0,"2018-10-25 07:01:41+03","2024-05-17 05:11:58.949913+03","I found a github issue that solves my problem httpsgithub commaciejtrederngtoolkitissues448 use nvm install v10 6 0"
52981020,52979583,"stackoverflow.com",4,"2018-10-25 06:49:24+03","2024-05-17 05:11:59.585111+03","It looks like you are already referencing Ref in your template This is referenced here as to what the value will be when you use Ref on a Lambda function Ref Documentation I usually reference them via"
53049011,53017599,"stackoverflow.com",0,"2018-10-29 17:39:46+02","2024-05-17 05:12:00.92841+03","It was the warmer Node lambda going haywire even though all the logs pointed at the worker Python lambdas After setting context callbackWaitsForEmptyEventLoop false the problem disappeared "
62824861,53057390,"stackoverflow.com",2,"2020-07-10 01:48:09+03","2024-05-17 05:12:01.626816+03","I had a similar issue and concluded that the size of the key did not matter too much as all my options were going to be small and lightweight with only minor tradeoffs I decided that a programmer friendly way i e me would be to use the sub that is the number created by cognito for each unique user That way all the collision issues should they arise would also be taken care of by cognito I could then encode or not encode So howseover a user logs in they will end up with the sub then I match that with the records in the hash key of dynamodb and that immediately grants them finegrained access to only their data Three years later I have found that to be a very reliable method "
53109711,53104847,"stackoverflow.com",3,"2018-11-01 23:37:22+02","2024-05-17 05:12:02.151529+03","AWS announced support for AWS Serverless Application Model Supports Amazon API Gateway Authorizers last week it could be done previously as well but then one had to use inline Swagger in the SAM template There are a few GitHub examples linked from the page the above and I guess that the Lambda Request Authorizer is closest to your problem The code below is copied from the template yaml Please also see the API Auth Object part of the AWS SAM specification "
53130287,53126841,"stackoverflow.com",0,"2018-11-03 12:08:59+02","2024-05-17 05:12:03.313979+03","You already can use path parameters e g The parameter parameterOne will be mapped into the Lambda event "
53265090,53126841,"stackoverflow.com",0,"2018-11-12 17:16:48+02","2024-05-17 05:12:03.31498+03","I might be interpreting this wrong but at least in the case of the aws you are able to add several resources into a lambda You will be responsible to handle the correct behavior which you can do by parsing the event which will have filled the path parameters in case they exist as mentioned on the blog post You can inspect the incoming HTTP requests path and method by parsing the event body in your code and then perform the correct operation in response Its like having a small router in the beginning of your Lambda code you can specific a more complete event with"
53264912,53140947,"stackoverflow.com",0,"2018-11-12 17:04:47+02","2024-05-17 05:12:04.291536+03","If you are using serverless with aws API Gateway you probably can use get resources method from the API Gatway methods from the aws sdk example in JS sdk httpsdocs aws amazon comAWSJavaScriptSDKlatestAWSAPIGateway htmlgetResourcesproperty"
53143510,53141934,"stackoverflow.com",12,"2018-11-04 19:31:16+02","2024-05-17 05:12:05.651399+03","Unfortunately the test events are a feature of the AWS console alone and are not made available on the AWS API docs As you have noticed the Serverless Framework includes invocation commands you have linked to Invoke Local but Invoke also exists which invokes your function on the cloud just like the AWS console As Serverless Invoke command can take a JSON file as an event a work around I might suggest is to create a folder like testspayloads of JSONs events as part of your code That way you can then use serverless invoke f functionName p testspayloadspayloadName json to emulate the experience the AWS Console gives you "
53164983,53161533,"stackoverflow.com",0,"2018-11-06 04:26:52+02","2024-05-17 05:12:06.192359+03","The default request templates you can use out of the box are referring to a lambda integration not a default integration where you leave the parameter blank If no integration is defined then it is the default integration So under http add integration lambda However that being said you should still have access to the headers when you do not specify the integration Lambda Integration httpsserverless comframeworkdocsprovidersawseventsapigatewayexamplelambdaeventbeforecustomization Default Integration httpsserverless comframeworkdocsprovidersawseventsapigatewayexamplelambdaproxyeventdefault"
53618147,53616268,"stackoverflow.com",1,"2018-12-04 19:07:23+02","2024-05-17 05:12:28.254759+03","The environment variables are not available when you exec because they are not part of the same execution You are branching the process off not attaching What I would do is create a file profile which contains the environment variables you need Copy it to the root or whatever user you are executing things with home directory as profile and you should be good to go "
53182184,53165530,"stackoverflow.com",0,"2020-06-20 12:12:55+03","2024-05-17 05:12:07.338448+03","Here are few considerations based on your question Here is what should be a good practice and solution based on your problem Considering you have a production environment you want to isolate from a given group in your company you should create VPCs and configure their resources access accordingly Then you create users to have diff access When your developer try to execute the code accessing a resource dynamoDB for example in a VPC they do not have access they will be blocked AWS configure to define which user will execute the SLS command Your development team will still have access to your configuration file Note In this case the persongroup with access to the production VPC will have to do the deploy If the answer does not suffice could you please reinforce which type of resource s are sensitive across your Serverless project I am taking for granted it is the DB as it is the most common scenario "
57129785,53198356,"stackoverflow.com",3,"2019-07-21 04:42:45+03","2024-05-17 05:12:09.416032+03","To make the serverless framework display any stack outputs you need to use the v or verbose flag or See docs here "
53213005,53212285,"stackoverflow.com",1,"2018-11-08 19:19:41+02","2024-05-17 05:12:10.423995+03","You are setting DUB_CROSS_ACCOUNT_IAM_ROLE to be an array Try or using the short syntax"
53239602,53221483,"stackoverflow.com",0,"2020-06-20 12:12:55+03","2024-05-17 05:12:10.985465+03","I do not think the create function works as you expect it to i e it wont pull in a repo from a URL From the docs Creating a new service using a local template serverless create templatepath pathtomytemplatefolder path pathtomyservice name mynewservice First clone your repo onto your local machine then run the create command with your local path"
53249594,53221483,"stackoverflow.com",0,"2018-11-11 16:14:40+02","2024-05-17 05:12:10.986466+03","I should have used git source path URL not just git repository URL "
53864678,53232729,"stackoverflow.com",10,"2018-12-20 10:11:23+02","2024-05-17 05:12:12.119389+03","The solution we ended up using is a serverless plugin called serverlessexportenv After adding this plugin you can run serverless exportenv to export all the resolved environment variables to an env file This resolves ssm parameters correctly and made integration testing much simpler for us BTW to get the environment variables set from the env file use the the dotenv npm package Credit to grishezz for finding the solution"
54339619,53232729,"stackoverflow.com",2,"2019-01-24 06:58:11+02","2024-05-17 05:12:12.121383+03","You can run node with require option to inject env file to a serverless command Then you can get environment variables in the handler process env XXX "
53233917,53232729,"stackoverflow.com",0,"2018-11-10 00:12:13+02","2024-05-17 05:12:12.122464+03","Are you looking to do mocked unit tests or something more like integration tests In the first case you do not need real values for the environment variables Mock your database or whatever requires environment variables set This is actually the preferable way because the tests will run super quickly with proper mocks If you are actually looking to go with endtoendintegration kind of approach then you would do something like sls invoke but from jest using javascript So like regular network calls to your deployed api Also I would recommend not to store keys in serverless yml Try the secret envMY_SECRET syntax instead httpsserverless comframeworkdocsprovidersawsguidevariablesreferencingenvironmentvariables and use environment variables instead If you have a cicd build server you can store your secrets there "
71651993,53232729,"stackoverflow.com",0,"2022-03-28 21:24:09+03","2024-05-17 05:12:12.124269+03","After searching I did my custom solution The SLS environment variables in my case at the file secrets [stage] json Serverless yml has"
53266988,53266709,"stackoverflow.com",1,"2018-11-12 19:12:02+02","2024-05-17 05:12:13.07549+03","First and foremost you will want to make sure serverless is right for the project Serverless architecture is fantastic but bear in mind it is not always the right solution To answer your question I have experience with both AWS Lambda on its own and with Serverless the specific framework you linked Serverless the framework handles a lot of the complexity for you and is definitely worth looking into if you are building data pipelines which it appears you are That being said I would recommend getting familiar with the serverless architecture first and then using a framework like Serverless after you know what you are doing Learning curve is not too bad "
53277210,53274624,"stackoverflow.com",2,"2018-11-13 10:58:22+02","2024-05-17 05:12:14.003453+03","You will need to configure Serverless to package individually To do this add the following to your serverless yaml "
61965352,53274624,"stackoverflow.com",0,"2020-05-23 02:22:42+03","2024-05-17 05:12:14.005454+03","Besides including into serverless yml as proposed by thomasmichaelwallace the Try changing the path of your handler function into hello_catfunctions yml from handler service handler to"
58439956,53277683,"stackoverflow.com",2,"2019-10-17 23:01:47+03","2024-05-17 05:12:15.041375+03","The serverlessplugintesthelper plugin can help here It will generate a YAML file containing all of the outputs of your stack This includes a couple of standard ones the S3 bucket that was used ServerlessDeploymentBucketName and the base service endpoint ServiceEndpoint If you are using Node and have your tests in the same directory as the stack being tested then there is also a module to read this file Otherwise it is just standard YAML and you can use whatever tools are convenient "
53286887,53277683,"stackoverflow.com",0,"2018-11-13 19:54:03+02","2024-05-17 05:12:15.042309+03","Consider adding an APIGateway custom domain for your API You can then use a known DNS name for your acceptance tests You will need to add an ApiGateway base path mapping apigateway domain name and a route53 recordset to the resources section of your serverless yml "
53300489,53282581,"stackoverflow.com",0,"2018-11-14 14:40:45+02","2024-05-17 05:12:15.832392+03","This was identified as a bug in serverless plugin according to this issue httpsgithub comserverlessserverlessissues5480 The solution will come from this PR httpsgithub comserverlessserverlesspull5481 serverless plugin simple does not support proxy up to this version "
55490217,53302266,"stackoverflow.com",5,"2022-02-22 13:21:15+02","2024-05-17 05:12:17.786968+03","EDIT Apparently my answer is now outdated For newer versions of serverless see the other answers I do not know which answer is bestmost uptodate but if someone lets me know I will change which answer is accepted to that one I eventually got it to work so heres how I set up my autherizers serverless yml Things to note Even though the authorizer function is called authorizer you need to capitalize the first letter and append LambdaFunction to its name when using it with GetAtt so authorizer becomes AuthorizerLambdaFunction for some reason I also had to add the lambda permission resource The API gateway resource also needs two outputs its API ID and its API root resource ID Heres how my API gateways serverless yml is set up Now you just need to specify to your other services that they should use this API gateway the imported values are the outputs of the API gateway After that the authorizer can be added to individual functions in this service like so"
53629167,53616268,"stackoverflow.com",0,"2018-12-05 11:36:38+02","2024-05-17 05:12:28.25676+03","Based on the answer of George Appleton I have added the export command I am already using to the bashrc with the following command for the dockercompose yml If I now exec bash all environment variables are available "
53643001,53621187,"stackoverflow.com",2,"2018-12-06 02:47:46+02","2024-05-17 05:12:29.816462+03","Instead of using aliases like this I would suggest taking advantage of the stages in Serverless Then as far as naming your step function it would be something like Then when you deploy it would be something like sls deploy stage dev or sls deploy stage prod This will utilize two different stacks and you will not run into stuff getting deleted because it thinks you renamed it "
54401118,53302266,"stackoverflow.com",2,"2019-01-28 13:34:47+02","2024-05-17 05:12:17.788969+03","I had the same issue that you describe Or at least I think so And I managed to get it solved by following the documentation on links you provided The serverless documentation states for the authorizer format to be Per my understanding my solution provide below follows the hardcoded authorizer ID approach In the service that has the shared authorizer it is declared in the serverless yml in normal fashion i e Then in the service that wishes to use this shared authorizer the function in servlerless yml is declared as It was crucial to add the name property It would not work without it at least at the moment For details see Unfortunately I cannot say whether this approach has some limitations compared to your suggestion of defining authorizer as a resource In fact that might make it easier to reuse the same authorizer in multiple functions within same service "
55886879,53302266,"stackoverflow.com",1,"2019-04-28 06:54:34+03","2024-05-17 05:12:17.79104+03","Serverless 1 35 1 For people stumbling across this thread here is the new way Wherever you create the user pool you can go ahead and add ApiGatewayAuthorizer Then when you define your functions"
58014045,53302266,"stackoverflow.com",0,"2019-09-19 18:12:46+03","2024-05-17 05:12:17.792274+03","This is how I did my set up since the answer posted above did not worked for me May be it could be helpful for someone I hope you know how to add an API gateway and import it here like since it is already specified in the accepted answer And In my case I passed value in the event header as Authorization type to get it in the authorizer lambda function and my type is REQUEST"
60235433,53302266,"stackoverflow.com",0,"2020-02-15 04:03:35+02","2024-05-17 05:12:17.793482+03","Changing to a shared custom API Gateway Lambda Authorizer was straightforward once it was working as part of the service At that point it was just add an arn to a deployed lambda authorizer and remove the authorizer definition from the service to a separate deployable service Then the other service just has some custom authorizers shared by multiple deployed microservices Side note If you would want a token type authorizer which does forward the authorization bearer xyzsddfsf as a simple event"
56590377,53488312,"stackoverflow.com",9,"2019-06-14 04:37:42+03","2024-05-17 05:12:19.323642+03","You can create a Cognito user pool and add groups to it in a single deployment All you need to do is to give the reference of the user pool in the group section The following code in serverless yml file creates a Cognito user pool and adds a group to it "
69673752,53515869,"stackoverflow.com",0,"2021-10-22 11:30:50+03","2024-05-17 05:12:19.741574+03","The serverless documentation says that you add the region in brackets after ssm in the variable name httpswww serverless comframeworkdocsprovidersawsguidevariables e g ssm euwest1 However that did not work for me Adding it with a dot between the ssm and the region e g ssm euwest1 did work and you can see it referenced in the pull request that created it httpsgithub comserverlessserverlesspull7625 This was in serverless 1 77 x so I am providing both answers in case the bracket notation is a serverless 2 thing "
53523530,53523232,"stackoverflow.com",1,"2018-11-28 18:02:01+02","2024-05-17 05:12:20.752621+03","I can solve it with the plugin With the plugin It cloud be solved by replacing AWSAccountId with AWSAccountId "
53644816,53523232,"stackoverflow.com",0,"2018-12-06 06:46:06+02","2024-05-17 05:12:20.754622+03","You can accomplish support for the native AWS syntax with a single config line in serverless yml to define the variableSyntax Details can be found here httpsgithub comserverlessserverlesspull3694 "
53554683,53539193,"stackoverflow.com",1,"2018-11-30 11:28:14+02","2024-05-17 05:12:21.902866+03","As with a lot of the Serverless Framework there is a plugin for those times that CloudFormation has not yet offered an option httpsgithub comrschickserverlesspluginlambdaaccountaccess The custom authorizers serverless yml should then include"
55944632,53542085,"stackoverflow.com",2,"2020-06-20 12:12:55+03","2024-05-17 05:12:22.911097+03","I am a little confused on your Join statement but you can include single quotes by enclosing them in double quotes no need to escape You also need to wrap your commas in double quotes From the Serverless documentation Passing the CacheControl header as maxage120 means API Gateway will receive the value as maxage120 enclosed with single quotes In yml strings containing [ ] ` must be quoted So to produce xyz executeapi eucentral1 amazonaws com something you can use the following Join statement Disclaimer I am not exactly sure what you want your output string to be but this demonstrates the general concept For those not writing this in JSON you can use the intrinsic FnJoin function in your serverless yml file for a more comprehensive structure Hope this helps Reference httpsserverless comframeworkdocsprovidersawseventsapigatewaycustomrequesttemplates"
58509063,53546181,"stackoverflow.com",16,"2019-10-22 19:57:09+03","2024-05-17 05:12:23.938719+03","As you mention that happened because you are running locally using serverlessoffline plugin and the serverlessoffline plugin does not provide a valid XRAY context One possible way to pass this error and still be able to call your function locally is setting AWS_XRAY_CONTEXT_MISSING environment variable to LOG_ERROR instead of RUNTIME_ERROR default Something like I did not test this using serverless framework but it worked when the same error occurred calling an amplify function locally "
66910370,53546181,"stackoverflow.com",6,"2021-04-01 22:19:50+03","2024-05-17 05:12:23.939719+03","I encountered this error also To fix it I disabled XRay when running locally XRay is not needed when running locally because I can just set up debug log statements at that time This is what the code would look like If you do not like this approach you can set up a contextStrategy to not error out when the context is missing Link here"
71356267,53546181,"stackoverflow.com",0,"2022-03-04 21:18:22+02","2024-05-17 05:12:23.94172+03","If you do not want the error clogging up your output you can add a helper that ignores only that error "
53549670,53549447,"stackoverflow.com",1,"2018-11-30 02:43:40+02","2024-05-17 05:12:24.642301+03","Those are variable definitions They can be defined somewhere else Example Then I define my variables in my env yml as such that resource is the resource arn for an example DYANMODB_TABLE You would preform something like this But in this case your block would relate to dynamodb "
68763419,53571905,"stackoverflow.com",8,"2022-08-14 21:59:48+03","2024-05-17 05:12:26.596066+03","I had the same issue eventually I discovered that my SQS queue name was not the same in all 3 places The following 3 places that the SQS name should match are shown below"
62800534,53571905,"stackoverflow.com",1,"2020-07-08 20:39:07+03","2024-05-17 05:12:26.597066+03","Ended up here with the same error message My issue ended up being that I got the resource and Resource keys in serverless yml backwards Correct "
53573288,53571905,"stackoverflow.com",0,"2018-12-01 19:25:33+02","2024-05-17 05:12:26.598066+03","I missed copying a key part of my config here the actual reference to my Resources file The issue was that I had copied this from a guide and had accientally used selfprovider stage rather than selfcustom stage When I changed this it could then deploy "
71743977,53571905,"stackoverflow.com",0,"2022-04-05 00:34:15+03","2024-05-17 05:12:26.599066+03","In general when YAML is not working I start by checking the indentation I hit this issue in my case one of my resources was indented too much therefore putting the resource in the wrong nodeobject The resources should be two indents in as they are in node resources subnode Resources For more info on this see yaml docs"
53605751,53594391,"stackoverflow.com",1,"2018-12-04 06:34:41+02","2024-05-17 05:12:27.339335+03","I dont know if this is the best or right way to fix this But this worked I changed default cors value into custom values to this and it worked "
53596773,53594391,"stackoverflow.com",1,"2018-12-03 17:25:02+02","2024-05-17 05:12:27.340527+03","AccessControlAllowHeaders does not accept wildcards If you can not set header values exactly just remove AccessControlAllowHeaders setting "
53764327,53621187,"stackoverflow.com",1,"2018-12-13 16:41:34+02","2024-05-17 05:12:29.818462+03","This is how I ended up implementing this not necessarily the best solution for everyone I split my serverless yml file into two files I kept my lambda function definitions in one file and then moved the step function definitions into a separate file Of course the serverless framework does not let you rename the serverless yml file which means the two files I just mentioned cannot coexist in the same folder at the same time My lambda function yaml file would look like this And I will deploy it this way The other yaml file for step functions would look like this And I will deploy it like this I am not saying that this is flawless but it works In the example given above the func1 is supposed to instantiate an instance of the step function MyStepFunction that is why it needs the step functions ARN "
53642937,53626962,"stackoverflow.com",6,"2018-12-06 02:40:04+02","2024-05-17 05:12:30.277456+03","I did a bit of digging on this and ran into a couple realizations By default it seems that no origin header passed in the API Gateway events I created a fresh serverless project and just echoed back exactly what the API gateway event was So this is coming from some other source I figured it may be a custom domain and tested that No dice My only other guess is that you have this behind some other layer CloudFront that is forwarding these headers for you If that ends up being the case I would suggest you look and see if you can make it forward these headers for the POST request as it is for the GET request My only other final thought if none of the above is true is that there is some magic going on in some of the express middleware I doubt this is the case For reference this was my full serverless yml and handler js I tested with as well as a full unaltered event object I got in the endpoint And the nodejs code Finally the response object"
55165248,53645757,"stackoverflow.com",1,"2019-03-14 16:37:04+02","2024-05-17 05:12:31.086138+03","You can use the promise of many ways Personally separate the promise in another function I made a example with request module If you use asyncwait you need add trycatch to handler errors "
56100538,53645757,"stackoverflow.com",1,"2019-05-12 18:07:32+03","2024-05-17 05:12:31.088139+03","I am coding a serverlesskubeless api now for the mysql world database I had to solve this problem yesterday I arrived at the following solution It is not feature complete But you did not ask for that So here is a working GET endpoint which accepts various query parameters to customise the query and database js"
57301620,53645757,"stackoverflow.com",1,"2019-08-02 20:44:38+03","2024-05-17 05:12:31.089139+03","I commonly do stuff with Promises in my serverless projects It gives me the ability to make promised calls to my services easily"
66596988,53645757,"stackoverflow.com",1,"2021-03-12 11:05:24+02","2024-05-17 05:12:31.090431+03","Await and async are not bad practices if used correctly If you do not have promises depending on each other you can call them in parallel by adding all promises without await in an array and use const responses await Promise all promisesArray to wait for all responses to be successful For more information refer to this answer which explains very well Call asyncawait functions in parallel"
53792433,53663879,"stackoverflow.com",2,"2018-12-15 17:23:41+02","2024-05-17 05:12:32.001956+03","I understand that you are able to implement the Implicit Flow without using the Hosted UI but you want to know how to implement Authorization Grant Flow You can use any HTTP client within your Web application to send HTTP requests to Cognito Auth Endpoints to go through the Code grant flow These are REST API endpoints and also no SDK is required to perform the operation Please see the below steps to understand the flow of the process using the API calls 1 Make a GET request to AUTHORIZATION endpoint to receive the XSRF tokens [1] You will need to pass the required parameters while making this request The required parameters are response_type code or token client_id and redirect_uri As per your usecase since you are using Authorization Grant Flow you need the value of response_type to be set to code Once you make this request you will receive an XSRF token in the response as a Cookie This XSRF token will be needed in the next step 2 Make a POST request to LOGIN endpoint to receive tokens [2] You need to pass the same required parameters mentioned while making AUTHORIZATION request Along with the required parameters you can pass POST body parameters CSRF token username and password Once we make this request you will be able to receive the Tokens in the response It also provides a Cookie in the response which you can use later to make a request for refresh tokens 3 Make a POST request to the TOKEN endpoint to receive refresh tokens[3] We need to pass the required parameters while making the request The required request parameters are grant_type and client_id Once you make a successful request you will receive a new set of Tokens in the response [1] Authorization Endpoint httpdocs aws amazon comcognitolatestdeveloperguideauthorizationendpoint html [2] Login Endpoint httpdocs aws amazon comcognitolatestdeveloperguideloginendpoint html [3] Token Endpoint httpdocs aws amazon comcognitolatestdeveloperguidetokenendpoint html"
55245240,53666687,"stackoverflow.com",4,"2019-03-20 12:44:56+02","2024-05-17 05:12:33.27432+03","In fact the first slow request is not only caused by the lambda cold start With Net Core in lambda you have 2 cold start the cold start of the lambda itself and the cold start of net core itself In order to avoid these 2 cold start you have to refer to this github issue to know more on the first slow request in Net Core still hope this problem will be fixed or better managed in next dotnet core releases but right now you do not have better option "
53669176,53666687,"stackoverflow.com",1,"2018-12-07 14:03:33+02","2024-05-17 05:12:33.276321+03","Cold start first lambda invocation is not the specific problem of Net Core You can find a timing comparison for different languages in this article "
53690670,53689712,"stackoverflow.com",1,"2018-12-09 10:39:57+02","2024-05-17 05:12:34.174757+03","Yes that should be possible I recommend checking out this Dark Vision app using IBM Cloud Functions You can upload videos which then are split into frames the frames processed with Visual Recognition The source code for Dark Vision is available on GitHub In addition you should go over the documented IBM Cloud Functions system limits to see if they match your requirements "
53735322,53735289,"stackoverflow.com",0,"2018-12-12 04:42:41+02","2024-05-17 05:12:36.214719+03","You missed the other one "
53765066,53764902,"stackoverflow.com",23,"2018-12-13 17:24:09+02","2024-05-17 05:12:37.010273+03","You have to install your module with save flag before uploading your zip to Amazon "
53771606,53764902,"stackoverflow.com",14,"2018-12-14 01:35:16+02","2024-05-17 05:12:37.011512+03","npm i save puppeteer results in a too big package Max 50MB for Lambdas So instead puppeteer was installed with npm i savedev puppeteer ignorescripts Ignore scripts to prevent Chromium from being installed The serverlesswebpack plugin had to be told to ignore puppeteer in its packaging Otherwise puppeteer would bloat the package The puppeteer module was put in a Layer in the folder structure mentioned in the question and require puppeteer now works "
62768480,53764902,"stackoverflow.com",1,"2020-07-07 08:08:17+03","2024-05-17 05:12:37.012509+03","If you are using stencil js it gives a very similar error Try updating the stencil core version to at least stencilcore ^1 15 0 In your shell prompt try"
65983542,53764902,"stackoverflow.com",1,"2021-01-31 21:50:03+02","2024-05-17 05:12:37.01451+03","Try running your script by forcing the environment variable NODE_PATH Such as For a specific reason I had to build from source a version of node without affecting the currently installation and this workaround worked for me I have got to this solution based on the following question here "
74653873,53764902,"stackoverflow.com",0,"2022-12-05 18:01:11+02","2024-05-17 05:12:37.015232+03","You have to install your module with save flag before uploading your zip"
54074466,53797848,"stackoverflow.com",0,"2019-01-07 14:28:28+02","2024-05-17 05:12:38.156181+03","It is possible in serverless If I were you I will use AWS Lambda to verify the id_token which is sent to the user In this scenario you should first transfer the key to AWS Lambda function using Api Gateway or other methods Then follow this guide to verify the token The code can be found in httpsgithub comawslabsawssupporttoolstreemasterCognitodecodeverifyjwt After verifying it you can add your code here"
53839150,53817238,"stackoverflow.com",0,"2018-12-18 20:33:39+02","2024-05-17 05:12:39.625309+03","There is a docker container that is extremely close to aws lambda You can deploy your serverless onto the container and trial and error with what you want done You can also create a lambda layer which serverless supports this way "
54074078,53818812,"stackoverflow.com",0,"2019-01-07 14:03:16+02","2024-05-17 05:12:40.101449+03","In this case it might because of the Api Gateway configurations Does your api public Because I have met such problems when the api is not public "
53885168,53857201,"stackoverflow.com",0,"2018-12-21 14:55:58+02","2024-05-17 05:12:41.109884+03","After long google and test I came to a solution for this leave it here for reference The firebase database connection is not released after saving data so that AWS Lambda cannot return the callback The simple solution is close database connection after task"
53871436,53857591,"stackoverflow.com",0,"2018-12-20 17:19:42+02","2024-05-17 05:12:41.931251+03","I used the serverlesswebpack npm package to solve the problem This ended up being my webpack config js file"
53862402,53859666,"stackoverflow.com",0,"2018-12-20 06:27:54+02","2024-05-17 05:12:42.991277+03","You should add an HTTP request header as Contenttype Now go to Integration requests and change the mapping template as follows Hope it helps and do not forgot to deploy the API before testing "
53904770,53862698,"stackoverflow.com",0,"2018-12-23 17:22:56+02","2024-05-17 05:12:44.0167+03","serverlesswsgi provides a flask wrapper for you to use so it actually handles the event variable in the background and maps it to a Flask request Import the flask request object and use that in your code instead Read the Flask docs around the request but it will have all the headers etc you would expect in the lambda event Also you are using agent UserAgent PostmanRuntime7 4 0 to send the request that gives the second set of headers You literally as a developer set those headers so I am confused why you would expect them to match unless you set them to match What header are you specifically looking to check"
53874653,53864505,"stackoverflow.com",0,"2018-12-20 21:10:32+02","2024-05-17 05:12:45.596405+03","Your IAM role definitions should be configured under provider and not as a root property in your serverless yml If you want to be more granular you can also put that in the function level "
55506328,53875503,"stackoverflow.com",4,"2019-04-04 03:53:57+03","2024-05-17 05:12:46.122746+03","For triggering a function upon a Firestore document change you should write something like this in your serverless yml file I have written an article on how to use Serverless Framework with Firebase triggers httpsmedium componceagtechusingfirebasetriggersinserverlessframeworkad99594b86fa Hope it helps "
53879042,53878014,"stackoverflow.com",15,"2018-12-21 06:09:16+02","2024-05-17 05:12:46.961989+03","Try"
65520136,53878014,"stackoverflow.com",4,"2022-09-13 15:27:30+03","2024-05-17 05:12:46.962989+03","UPDATE In response to the first comment I would like to give some context to this answer the author asked how to export the ARN without the version number The actual problem lies in the Key that was used to export the function ARN ServiceLambdaFunctionQualifiedArn The Serverless Framework by default exports each Lambda function with their Qualified Function ARN i e ARN plus version suffix as CloudFormation Stack Output That means the Lambda function hello will be exported as HelloLambdaFunctionQualifiedArn even if you do not define any outputs in the resources section Just check the CloudFormation Stack output of a deployed Serverless service That means Serverless will overwrite your output declaration ServiceLambdaFunctionQualifiedArn with their own declaration during deployment By the way that is also the reason that it was working even though no actual Value was specified In order to export the ARN without version number you must manually declare an additional output with a different Key for example HelloLambdaFunctionArn or ServiceLambdaFunctionArn remove Qualified from the string and reference the ARN as Value This has worked for me Export in first serverless stack with a unique export name stackstageTransformDataLambdaArn And import in the second stack with the same stage like this"
62856611,53878014,"stackoverflow.com",2,"2020-07-12 05:55:13+03","2024-05-17 05:12:46.96544+03","Update 072020 This question is still valid and the accepted answer did not work for me with serverless 1 74 1 My use case was to export a custom authorizer lambda as a separate service to other services Here is what worked for me You do not need a reference to value the arn should be exported with just the name Then to use the Export in another service The name added to this ended up being required for the custom authorizer use case "
59845163,53878014,"stackoverflow.com",1,"2020-01-21 18:49:51+02","2024-05-17 05:12:46.967438+03","In short E g "
61104882,53878014,"stackoverflow.com",0,"2020-04-08 18:56:01+03","2024-05-17 05:12:46.96844+03","nah just add versionFunctions false inside your serverless yml inside the provider like this"
53885196,53883835,"stackoverflow.com",2,"2018-12-21 14:57:57+02","2024-05-17 05:12:47.857247+03","Ok i GOT IT I had to add another permission to allow the Lambda function to be triggered by the SNS Topic And now in the console i can the SNS as trigger for my Lambda function "
53892360,53885255,"stackoverflow.com",1,"2018-12-22 03:19:12+02","2024-05-17 05:12:49.007954+03","I believe your issue is that you are using Cloudformation intrinsic functions within the Serverless Framework function section Specifically And You do not want to use intrinsic functions here but rather an ARN using serverless variables Check out role names in the serverlessiamrolesperfunction docs"
54090419,54089897,"stackoverflow.com",2,"2019-01-14 10:52:42+02","2024-05-17 05:12:50.386805+03","EDIT My previous code ignored the fact that your Insider query is asynchronous This new code handles that and matches your edit Initial partly incorrect answer for reference You are pushing the result of your mapping into the object that you are currently mapping and callback is called more than once here That is a pretty good amount of unexpected behavior material Try the following"
54298260,54285038,"stackoverflow.com",0,"2019-01-21 23:53:15+02","2024-05-17 05:13:07.395271+03","Ref expects to reference something right now you are not passing it anything to reference So assuming you want the ARN of DialogflowFunction and that function config looks something like this in your functions file Then your ref would look something like this Ref takes the logical id of the resource for which you want to reference in this case that is DialogflowFunction and will return the ARN of that resource "
54295533,54290649,"stackoverflow.com",16,"2019-01-21 20:02:55+02","2024-05-17 05:13:07.873854+03","If you need to download the object to the disk you can use tempfile and download_fileobj to save it Note that there is a 512 MB limit on the size of temporary files in Lambda I would argue an even better way is to process it all in memory Instead of tempfile you can use io in a very similar fashion This way the data does not need to be written to a disk which is a faster and b you can process bigger files basically until you reach Lambdas memory limit of 3008 MB or memory "
55897608,54290649,"stackoverflow.com",1,"2019-04-29 08:46:05+03","2024-05-17 05:13:07.875849+03","In one of my project I converted webp files to jpg I can refer to the following github link to get some understanding httpsgithub comadjr2webptojpgblobmastercodes py You can directly access the file you download in lambda function I am not sure whether you can create a new folder or not even I am pretty new to all this stuff but surely you can manipulate the file and upload back to the same or different s3 bucket Hope it helps Cheers "
53907610,53901880,"stackoverflow.com",7,"2018-12-24 00:25:40+02","2024-05-17 05:12:50.391806+03","At present there is no way to run the real Cloudflare Workers runtime locally The Workers team knows that developers need this but it will take some work to separate the core Workers runtime from the rest of Cloudflares software stack which is otherwise too complex to run locally In the meantime there are a couple options you can try instead Cloudworker is an emulator for Cloudflare Workers that runs locally on top of node js It was built by engineers at Dollar Shave Club a company that uses Workers not by Cloudflare Since it is an entire independent implementation of the Workers environment there are likely to be small differences between how it behaves vs the real thing However it is good enough to get some work done The preview seen on cloudflareworkers com can be accessed via API With some curl commands you can upload your code to cloudflareworkers com and run tests on it This is not really local but if you are always connected to the internet anyway it is almost the same You do not need any special credentials to use this API so you can write some scripts that use it to run unit tests etc Upload a script called worker js by POST ing it to httpscloudflareworkers comscript Now SCRIPT_ID will be a 32digit hex number identifying your script Note that the ID is based on a hash so if you upload the exact same script twice you get the same ID Next generate a random session ID 32 hex digits It is important that this session ID be cryptographically random because anyone with the ID will be able to connect devtools to your preview and debug it Let us also define two pieces of configuration These specify that when your worker runs the preview should act like it is running on httpsexample com The URL and Host header of incoming requests will be rewritten to this protocol and hostname Set HTTPS1 if the URLs should be HTTPS or HTTPS0 if not Now you can send a request to your worker like The 32 zeros can be any hex digits When using the preview in the browser these are randomlygenerated to prevent cookies and cached content from interfering across sessions When using curl though this does not matter so allzero is fine You can change this curl line to include a path in the URL use a different method like X POST add headers etc As long as the hostname and cookie are as shown it will go to your preview worker Finally you can connect the devtools console for debugging in Chrome currently only works in Chrome unfortunately Note that the above API is not officially documented at present and could change in the future but changes should be relatively easy to figure out by opening cloudflareworkers com in a browser and looking at the requests it makes "
56895838,53901880,"stackoverflow.com",2,"2019-07-05 05:28:11+03","2024-05-17 05:12:50.395807+03","You may also be able to test locally by loading the Cloudflare worker as a service worker Note"
60790424,53901880,"stackoverflow.com",0,"2020-03-21 18:36:53+02","2024-05-17 05:12:50.396807+03","Dollar Share Club created Cloudworker It is not actively maintained but it is a way to run Cloudflare Workers locally You can read about it on the Cloudflare blog in guest post by the original maintainer of Cloudworker "
54240145,54094363,"stackoverflow.com",2,"2019-01-17 18:21:17+02","2024-05-17 05:12:51.338056+03","The seeding functionality is only supported in serverless offline by dynamo local plugin and its not supposed to work online github com99xtserverlessdynamodblocal You can however use cloudformation init function which will execute a lambda function during the stack creation you can use it to seed your dynamo db tables "
64550714,54094363,"stackoverflow.com",1,"2020-10-27 10:37:13+02","2024-05-17 05:12:51.340057+03","varnit is correct You can only seed offline with serverlessdynamodblocal For seeding data in your DynamoDB that is deployed to AWS by serverless there are a few libraries that can help as well if you do not want to do it yourself as varnit suggests httpswww npmjs compackageserverlessdynamodbseed httpswww npmjs compackageawscdkdynamodbseeder"
54096604,54096502,"stackoverflow.com",0,"2019-01-08 19:11:21+02","2024-05-17 05:12:52.252727+03","You can probably just return the value you get after await "
54096624,54096502,"stackoverflow.com",0,"2019-01-08 19:13:06+02","2024-05-17 05:12:52.253727+03","There is no need of callback function on mongoose connect if you are using await Callback function is use for Promise not for await use trycatch in asyncawait "
54138593,54129180,"stackoverflow.com",0,"2019-01-11 01:52:28+02","2024-05-17 05:12:54.327613+03","You cannot integrate an API Gateway method with multiple lambdas You would have to detect the lambda failure and redeploy the entire API Gateway with all its methods pointing to the lambdas in the other region This would take time to do so you would have a period of downtime before the lambda integration would be switched over "
54184969,54146467,"stackoverflow.com",1,"2019-01-14 18:01:09+02","2024-05-17 05:12:55.226458+03","If I am understanding your question correctly you have a structure something like this You can write a script which parses all these files runs any validations you may want on the items within and returns a file for serverless yml to use so that your serverless yml might look like this All this scripts or scripts need to do is loop over a given directory load the yml concat each files yml to a temp file then resolve with that temp file "
54171707,54162354,"stackoverflow.com",4,"2019-01-13 20:05:32+02","2024-05-17 05:12:56.296466+03","Try this and ensure that in serverless yml you have set runtime nodejs8 10 under provider Lambda response body must be a string as defined here "
54166787,54165697,"stackoverflow.com",1,"2019-01-13 09:05:44+02","2024-05-17 05:12:57.320698+03","Unfortunately this is not possible with AppSync at this time I have noted this as a feature request for the service though and will bring it back to the team for prioritization in a future release Thanks for the feedback "
54186753,54185709,"stackoverflow.com",0,"2019-01-14 19:58:42+02","2024-05-17 05:12:58.552075+03","You should always put async code inside try catch block Also forEach will not work with promise you will need to use for loop Try this Shared function"
54209378,54207835,"stackoverflow.com",1,"2019-01-16 03:53:36+02","2024-05-17 05:12:59.156613+03","It sounds like your Lambda function cannot connect to DynamoDB That typically is caused by one thing you are running the Lambda function in a VPC and the Lambda function has no viable route to DynamoDB either over the public internet or to DynamoDB via a private VPC Endpoint Presumably you are running your Lambda function inside a VPC so that it can access your private RDS database which would not otherwise be reachable over the public internet You have a couple of choices Note that both are over TLS so are secure The decision is whether or not to allow the Lambda function to have unfettered outbound internet access 1 or to constrain it to DynamoDB 2 "
61538179,54219776,"stackoverflow.com",1,"2020-05-01 10:00:36+03","2024-05-17 05:13:01.270597+03","Looking at your logs it seems that Flasks WSGI is expecting a headers node within the payload you are receiving which would be there should your event be an HTTP event That is not the case when you wire two AWS services directly The event object will be different for each service Since you are not handling HTTP requests I would suggest you to remove Flask from this lambda Your code then would look like this Your SQS queue ARN is displayed on your queue details on AWS Console Here is a complimentary read SLS and SQS Queues"
54220629,54220447,"stackoverflow.com",1,"2019-01-16 17:52:22+02","2024-05-17 05:13:02.242414+03","You have to use the SES sendRawEmail API in order to send email with attachments There are a few examples in the SES Developer guide how this can be implemented in various programming languages currently Java PHP Python and Ruby Please also read the Sending Raw Email chapter for generic information "
63295834,54229782,"stackoverflow.com",6,"2020-08-07 09:27:45+03","2024-05-17 05:13:03.336752+03","Run This Then do your sls deploy The other option you can try is Then do your sls deploy"
54259197,54238775,"stackoverflow.com",2,"2019-01-18 19:54:56+02","2024-05-17 05:13:04.251799+03","The problem is that your code has two newlines for one of your section part headers Change this line of code to this"
54272869,54272682,"stackoverflow.com",2,"2019-01-20 03:40:56+02","2024-05-17 05:13:04.917292+03","allowCredentials true will not work with AccessControlAllowOrigin You have to specify the origin explicitly AccessControlAllowOrigin https840b1a6d ngrok io also withCredentials in AccessControlAllowCredentials withCredentials should be true"
54286417,54285015,"stackoverflow.com",0,"2019-01-21 11:02:35+02","2024-05-17 05:13:05.749055+03","You can try something like below hope it helps "
54295907,54285038,"stackoverflow.com",2,"2019-01-21 20:31:01+02","2024-05-17 05:13:07.39278+03","Ref is a Cloudformation intrinsic function It needs to reference a resource The whole outputs section is also optional use it only if you need to reference the resources from one stack in another "
54323504,54320728,"stackoverflow.com",17,"2019-01-23 18:42:13+02","2024-05-17 05:13:13.835117+03","A policy document is nothing but a set of permissions to allowdeny access to AWS resources This policy can be attached to usersrolesgroups If this policy is attached to rolesgroups the users to which the roles are attached or the list of the users in the group will have the permissions defined in the policy For example having EC2 or VPC access etc An AssumeRolePolicy is provided in a role to help enabling trust relationship for other AWS servicesAWS accounts to consume this role and gain permissions For example Lambda will need an IAM role to be attached to define all the permissions it requires for its execution A normal IAM role cannot be attached to lambda since no trust relationship is defined i e the role does not allow it to be consumed by Lambda Once a trust relation is added for lambda the role can be attached to lambda thereby gaining permissions defined The same will also apply for cross account access if an account id is used as principal instead of lambda service where using a role in account A one can gain access for permissions defined in role B meaning you can access account B with access of account A if trust is established In trust relationship the assume role is using Security Token Service STS where temporary credentials are provided for accessing the AWS resources Hope this helps "
54330892,54328886,"stackoverflow.com",3,"2019-01-23 17:46:13+02","2024-05-17 05:13:14.719083+03","You are waiting for the response of an async call and it is likely that you are not getting one Check the SES API logs in CloudTrail to make sure that the request is actually being made It sounds like your lamdba function cannot access SES which would happen if you are running it in a VPC You would need to add a NAT Gateway to the VPC Consider moving your lambda outside of your VPC Here is a guide to help determine the tradeoffs "
54351175,54350957,"stackoverflow.com",5,"2019-01-24 18:24:07+02","2024-05-17 05:13:15.215309+03","Luckily you do not have to bundle boto3 when deploying your Lambda application It is already present in the Lambda python execution environment Feel free to remove boto3 from your requirements txt Note that AWS recommends you bundle your own as the one in Lambda might not be fully uptodate but that matters only if you want to use the very latest feature or services of AWS I have been using the bundled one for over 2 years and have not had an issue once "
54358136,54357368,"stackoverflow.com",5,"2019-01-25 04:44:42+02","2024-05-17 05:13:16.756951+03","Do not pass the variable along with the deploy command Instead set it from the terminal first In your terminal run URLhttpspostmanecho compost and then run sls deploy Alternatively you can use a plugin Heres a plguin to that httpsgithub comcolynbserverlessdotenvplugin then add the plugin to your config file create your usual dotenv file env and then access as usual And if you really need to run it from the console with different urls flags though I would recommend using just one env file without the command flags do this Put your different urls in env url1 env url2 env url3 and then sls deploy env url1"
54365992,54357368,"stackoverflow.com",2,"2019-01-25 15:10:12+02","2024-05-17 05:13:16.758951+03","The code that you posted works You just have to pass URL as an environment variable and NOT as an argument This should work passing URL as an environment variable This will not work you are passing URL as an argument to sls deploy "
54371374,54370736,"stackoverflow.com",2,"2019-01-25 21:01:12+02","2024-05-17 05:13:17.192755+03","I would do something like this to help you out with the syntax Then in your env yml you can do"
69640487,54370736,"stackoverflow.com",2,"2023-07-02 16:11:42+03","2024-05-17 05:13:17.193755+03","Enviroment variables 1 Add useDotenvtrue to your yml file 2 Add your variables like this envVARIABLE_NAME 3 Create a file called env dev and write the variables You can add env prod but you have to change the stage inside your yml file Example env dev"
59273985,54370736,"stackoverflow.com",0,"2019-12-10 21:18:44+02","2024-05-17 05:13:17.195756+03","I ended up solving it I had set up my Dynamo DB in AWS uswest region reinitialized in USEast2 and reset the region under provider within the yml file "
55273860,54378453,"stackoverflow.com",0,"2019-03-21 06:34:51+02","2024-05-17 05:13:18.327624+03","I have found the solution Heres what I was doing wrong I was using serverless framework and I am using serverlesspythonrequirements and docker deployment for the application deployment My config was as below The problem was the slim true it was excluding so files I removed it and it worked fine "
54498818,54384219,"stackoverflow.com",0,"2019-02-03 02:18:47+02","2024-05-17 05:13:19.383028+03","I ended up solving this issue I will post the code below but essentially this was a combination of random configuration errors and Cors properties on my bucket Because S3 is global it was not being created in the same default region as all of my other resources SES needed to access to write to the bucket from another origin but was not able to because of Cors properties not being set I ended up separating my role and policy and changing some minor formatting changes as well Here are what my resources and functions now look like Now we have a working receipt rule on our subdomain An S3 bucket that SES can upload emails to when it receives for any email address for a subdomain A lambda function that is triggered by uploading an object to that bucket "
54594841,54594610,"stackoverflow.com",6,"2019-02-08 17:07:44+02","2024-05-17 05:13:21.395584+03","I encountered a similar issue some time ago in some Jenkins build tasks You might be able to solve it using the NODE_OPTION environment variable From your error stacktrace it looks like new node processes are beeing spawned If you can set this environment variable it will be used by node to pass arguments to any new processes and so to the spawned processes that run out of memory here httpsnodejs orgdistlatestv8 xdocsapicli htmlcli_node_options_options"
69687963,54594610,"stackoverflow.com",1,"2021-10-23 15:03:20+03","2024-05-17 05:13:21.396583+03","Defining a fixed amouth of memory could be tricky since you will not use the full memory available on the runner machine I wrote a code to get the total memory on the runner machine deduct 400mb to other server resources and assign it as node memory limit You can adapt to your case and use the maximum memory to your script but respect the limit as well If you use docker it can be added to an entrypoint sh file that will run on build time but on each docker start as well "
54597252,54594610,"stackoverflow.com",0,"2019-02-08 19:10:12+02","2024-05-17 05:13:21.398584+03","Increase it from your start command e g"
54652212,54594610,"stackoverflow.com",0,"2019-02-12 16:23:43+02","2024-05-17 05:13:21.399585+03","There ended up being two workarounds for this issue The ENOMEM error was appearing first because I needed more memory for the Node process and second because the memory I was allotting to the Node process exceeded that which was available in the Bitbucket environment default 4gb Option 1 Using any of the other answers options AND ensuring the Bitbucket pipeline environment has enough memory for me that meant cranking the size to 2x see this link Option 2 Adding the transpileOnly flag to your webpack config js file using tsloader as your transpiler This skips typechecking and saves memory at packagedeploy time WARNING You still should typecheck your project on build For me this meant running tsc before packagingdeploying "
54750842,54614473,"stackoverflow.com",0,"2019-02-18 17:45:36+02","2024-05-17 05:13:22.372702+03","I figured it out by myself 1 It depends I am now using a combination of both for the following reason For a Facebook Login we are using a Cognito Identity Pool as the Cognito UserPool does not support a Facebook Login outside of the hosted UI For Email signup and login we use a UserPool inside the Cognito Identity Pool 2 The code snippet i posted was correct 3 The code snippet was also correct My mistake was that i did not correctly display and inspect the reply of my API and therefore thought the request was not authorized but it was "
70419881,54614852,"stackoverflow.com",0,"2021-12-20 11:37:38+02","2024-05-17 05:13:23.414203+03","This requires adding an alternate domain name to CloudFront which you can do so via the CloudFront console It let us CloudFront know that it is fronting this secondary domain in addition the one it already recognises The question did mention but it is worth reiterating that the certificate for your distribution must cover both domain names since this release "
54616794,54616601,"stackoverflow.com",3,"2019-02-10 16:39:09+02","2024-05-17 05:13:24.254526+03","I do not understand your purpose The whole purpose of Serverless is to not manage servers Why would you do that then Rather create a separate test environment on AWS itself to test the Serverless API gateways Lambda and DynamoDB are all AWS specific services You are making your task more complex than easier by thinking of creating a clone of DynamoDB Lambda API gateway inside docker You really have to work a lot on creating similar integration as of API Gateway Lambda integration even if you use MongoDB or Cassandra in place of DynamoDB Even if you just take such scalability in picture If you rather want to learn creating such web services in docker then you may use something like mongo db docker image Directly from docker hub in place of DynamoDB Update You can use httpshub docker comramazondynamodblocal for DynamoDB though First image push was six months back For Lambda you can try httpsgithub comlambcidockerlambda and see if it works "
54621345,54616601,"stackoverflow.com",0,"2019-02-10 23:39:28+02","2024-05-17 05:13:24.256813+03","Use Sam local for the lambda piece and DynamoDB local for the DynamoDB piece Also once in AWS you might be able to get away without api gateway by hook up ELBs to Lambda functions "
66271993,54630124,"stackoverflow.com",1,"2021-02-19 06:56:41+02","2024-05-17 05:13:25.127798+03","There are probably better ways but we have decided to place the certificates in a secure bucket with strict access and object locking controls Within the TypeScript NodeJS lambda we can now just initiate the S3 client and perform a request like this The S3 service is just a wrapper around the awsnode library "
54634065,54631337,"stackoverflow.com",3,"2019-03-18 22:22:35+02","2024-05-17 05:13:26.137974+03","The serverless framework appears to include native AWS SSM integrations However as you noted there is no similar functionality on GCP so you will need to roll some of this on your own You may be interested in some of the strategies outlined in Secrets in Serverless It is always important to ask do I actually need these secrets Could you leverage Cloud Provider IAM or even crosscloud OIDC instead of injecting secrets into my application Where possible try to leverage the IAM solution provided by the various clouds Obviously there are still quite a few cases where a secret is required Before the function is launched you encrypt the plaintext secrets locally into ciphertext encrypted strings Heres an example with gcloud but you can also use the API or other tools like HashiCorp Vault This will output a base64encoded encrypted string which you then store in your config js On startup configure your application to I am not sure what language s you are using but heres a nodejs sample You can find a lot more samples on GitHub at sethvargosecretsinserverless Since you are on GCP another option is to use Google Cloud Storage GCS directly to store the secrets This would remove your coupling from the serverless framework Make a bucket Make the bucket private Write some secrets into the bucket Even though they are being committed as plaintext they are encrypted at rest and access is tightly controlled via IAM Then create a service account which has permission to read from the bucket and assign that service account to your functions Finally read from the bucket at function start Python example this time "
54638920,54635500,"stackoverflow.com",0,"2019-02-11 22:49:13+02","2024-05-17 05:13:27.033274+03","It should not be necessary to convert the binary buffer to string before uploading Drop the toString base64 everything else ought to work "
55768486,54640494,"stackoverflow.com",1,"2019-04-20 01:28:10+03","2024-05-17 05:13:27.909919+03","After a long investigation together with AWS support we found out that SheltersVPCS3Endpoint was deleted before dbMigration was deleted and therefore the Lambda fn could not get any contact with the S3 bucket which triggered a timeout Since it is not possible to add any DependsOn to functions in Serverless I had to migrate from Serverless to Cloudformation When I added the following it seems to be solved "
54786738,54640494,"stackoverflow.com",0,"2019-02-20 14:47:12+02","2024-05-17 05:13:27.91192+03","You need to give your lambda function the correct permission if you add to your serverless config and then construct your iamRoleStatements json to give your function permission for S3 buckets placed in the same directory as your serverless config this does give all your resources the right to do every thing they want to your s3 bucket If you want to learn more about working with IAM i serverless functions have a look at httpsserverless comblogabcsofiampermissions"
54658372,54657766,"stackoverflow.com",1,"2019-02-12 22:39:00+02","2024-05-17 05:13:28.577114+03","Taken from this example in my blog Calling shed put_rule will allow you to change the event schedule "
54680996,54674817,"stackoverflow.com",3,"2019-02-14 01:24:22+02","2024-05-17 05:13:29.617823+03","This is not possible You have asked serverless to create a CloudFormation template that creates some lambdas When AWS executes the template it executes it in the cloud away from your computers local files Thats why your code is packaged uploaded to S3 and made available for CloudFormation use CloudFormation does allow for code to be inline in the template but serverless does not support this And there is no way to ask CloudFormation to create a lambda without code attached for manual upload at a later date Frankly the cost to have the additional bucket and a few small files is minimal if any If the concern is the additional deployment bucket you can specify a deployment bucket name for multiple serverless deployments "
54681124,54676885,"stackoverflow.com",12,"2019-02-14 01:39:45+02","2024-05-17 05:13:30.335063+03","The CloudFormation stack itself can have tags which is what stackTags is used for If you want to tag your Lambda functions you would use tags tags on a provider level are applied to all functions within the stack "
54854717,54699293,"stackoverflow.com",2,"2019-02-24 19:45:49+02","2024-05-17 05:13:31.284918+03","After taking a look around in deep I came with the following conclusion Using this pattern ONE_CLIENT_SECRET ssmonekey_onetrue means that the sls framework is going to download the values on compilation time and embed into the project this is where the problem comes you can see this after uploading the project your variables are going to be set on plain text on the lambda console My solution was to use a middy middleware to load ssm values when executing the lambda This means you need to code your project in a way that does not trigger any code until the variables are available and find a good strategy to catch the variables cold start otherwise it will add more time to the execution The limit of 4Kb cannot be changed and after read about this it seems obvious So short story find a strategy of middleware and embed values that work best for you if you find this problem "
54742667,54711268,"stackoverflow.com",8,"2019-02-18 09:57:13+02","2024-05-17 05:13:32.398288+03","You definitely can create an origin access identity and the CloudFront distribution in the same serverless yml I have modified your scenario and changed the OriginAccessIdentity to use FnJoin The serverless examples repo has a great example of this too httpsgithub comserverlessexamplesblobmasterawsnodesinglepageappviacloudfrontserverless yml"
54712190,54711268,"stackoverflow.com",2,"2019-02-16 04:49:37+02","2024-05-17 05:13:32.400289+03","Yes you can create both in the same CloudFormation template The cloudfrontoriginaccessidentity is a separate resource so needs to be moved out from underneath myDistribution "
54781796,54765370,"stackoverflow.com",3,"2019-02-20 10:25:29+02","2024-05-17 05:13:34.499248+03","For now serverless framework does not support Heroku Postgres Database to be configured If you want to connect to Heroku Postgres Database from lambda the solution is described here httpsmattwelke com20190106freetiermanagedsqlwithawslambdaandherokupostgres html The example is here Also you can try to use Amazon Aurora Serverless with PostgreSQL right now it is in a preview And you will be able to set up Amazon Aurora Serverless configuration in serverless yml in the resources section"
54823889,54765370,"stackoverflow.com",0,"2019-02-22 11:27:52+02","2024-05-17 05:13:34.501609+03","Here I am posting the answer of my question I am using pg and activerecord gem by using this ActiveRecordBase establish_connection your_heroku_database_url I am able to connect my heroku database "
54806911,54805730,"stackoverflow.com",6,"2019-02-21 14:18:39+02","2024-05-17 05:13:36.616855+03","I have wrestled with this problem for hours if not days before and it turned out not only I had to enable cors on the serverless yml file but also add the response headers as attributes in the object you return from your Lambda Something like this should do it This article saved my life back then and I hope it saves yours "
54806786,54805730,"stackoverflow.com",1,"2019-02-21 14:12:11+02","2024-05-17 05:13:36.61885+03","The CORS error is thrown when you are making requests to servers in other domains Depending on the server you are using to host the angular code there are many ways you can solve this You can try this google chrome extension to see if you can effectivelly fix the problem by ignoring the CORS errors One of the most commons ways to solve this problem is to configure a proxy server but you can also solve it by whitelisting your test domain manipulating the header AccessControlAllowOrigin in your requests "
54814015,54805730,"stackoverflow.com",1,"2019-02-22 17:34:22+02","2024-05-17 05:13:36.620905+03","This line in serverless yml arn file config selfprovider stage json userpoolarn Should have been arn file config optstage json userpoolarn "
54912157,54807051,"stackoverflow.com",2,"2019-02-27 20:24:43+02","2024-05-17 05:13:37.330802+03","One solution until serverless removes the autocomplete tabtab dependency is to run your docker command to only rebuild the modules that need it instead of the default which rebuilds them all So in your case if you just need nodesnowball rebuilt you could use this command"
55098941,54823750,"stackoverflow.com",2,"2019-03-11 11:39:57+02","2024-05-17 05:13:38.274609+03","A pattern Im seeing quite a lot is to build your function and its dependencies in a Docker container based on Amazon Linux e g that OS that your function will be running on in Lambda check out this blog post for a walkthrough and some examples Another example project is here "
58892984,54840576,"stackoverflow.com",6,"2019-11-16 18:47:30+02","2024-05-17 05:13:39.271335+03","OK I just went through this and thought I would share the solution The problem is related to mismatch between Serverless and AWS So we are going to get them both on the same page FIRST Serverless config This configures Serverless to server the associated Mime types as Base64 SECOND AWS Config In AWS API Select the Gateway and then Settings Scroll down and add the following binary types That is it It should work now "
54844893,54840576,"stackoverflow.com",2,"2019-02-23 21:16:17+02","2024-05-17 05:13:39.27333+03","Dealing with binaries in API Gateway is always a hassle I have managed to make it work though All you need to do is tell API Gateway that your response is encoded in base64 Heres a working solution The real problem I see here however is that Express is managing the routes for you therefore I do not think you can intercept API GWs response to add the field isBase64Encoded so I am afraid you will have to let this API be managed by API Gateway instead of Express in order to make it work properly Also Jimp offers a getBufferAsync method which returns a promise so you can just await on it to make the code slightly simpler Hope it helps EDIT I was still trying to make it work with Express so I found this httpsgithub comawslabsawsserverlessexpressissues99issuecomment332169739 I must admit I did not test but it may work if you really need to have Express handling the routes for you "
54895673,54878346,"stackoverflow.com",0,"2019-02-27 01:14:32+02","2024-05-17 05:13:39.78536+03","This is correct Default lambda integration is via lambda proxy which bypasses API Gateways requestresponse transformers You basically get the raw payload to deal with If you want API Gateway to do some of the work you need to switch integration to lambda and configure API Gateway to accept applicationjson as a request type httpsserverless comframeworkdocsprovidersawseventsapigatewaylambdaintegration"
55877044,54882872,"stackoverflow.com",1,"2019-04-27 06:38:40+03","2024-05-17 05:13:41.376755+03","You can conditionally select values in serverless yml by storing the conditional functions in a custom variable like When you deploy serverless you should pass in the command line option stagemyStageName so that when you pass in stagedev or stageprod the last line in the function section will be blank and nothing will deployed If you pass in stagetesting the last line in the functions sections will be filled with the file set in your custom variable section and then your test code will be deployed "
54891360,54882872,"stackoverflow.com",0,"2019-02-26 19:48:04+02","2024-05-17 05:13:41.378756+03","After going through all the serverless plugin list I found above requirement could be achieved through serverlesspluginselect Using this plugin we can select to deploy only a few functions from serverless yml depending on stage or region value In my case using region value Following is modified serverless yml plugins section added and regions key added in each function With the above config I use the following bash script to deploy for all region "
54929370,54894526,"stackoverflow.com",2,"2019-02-28 17:42:28+02","2024-05-17 05:13:41.906232+03","To resolve the issue I ended up giving up on creating a custom workspace and just making my GOPATH point to one consistent folder GOROOT was pointed to usrlocaloptgolibexec seems to be where Brew installs Go Small rant Gos setup experience is terrible I get the fact that it is meant to have opinions but something as simple as workspace placement and setting up paths this should be taken care of by the installation process and make clearer to the developer installing the build tools Booo "
54894748,54894526,"stackoverflow.com",0,"2019-02-26 23:53:56+02","2024-05-17 05:13:41.908233+03","This documentation can help you to set up Go specific development environment In short you need to set two variables GOPATH GOROOT Here is what your profile should look like "
66129271,54894526,"stackoverflow.com",0,"2021-02-10 02:31:35+02","2024-05-17 05:13:41.909241+03","You can make a symlink within GOPATHsrcgithub com and point it to your existing project directory Create the symlink Verify it worked Now you can go about your business in the exampleproject with things like"
54925253,54900150,"stackoverflow.com",0,"2019-02-28 14:00:33+02","2024-05-17 05:13:42.919647+03","You can set role and iamRoleStatements on the provider level to apply custom roles and role statements to all functions within your service e g Provider settings will be applied to both hello and chatMessage functions I advise you stick to setting iamRoleStatements at the provider level and only use role if you have to use a preexisting role outside your serverless deployment Check the IAM documentation for more details "
55942757,55353752,"stackoverflow.com",0,"2019-05-02 00:11:28+03","2024-05-17 05:14:10.304049+03","It looks like you are using AliCloud Function Compute and are trying to get the value of the body from the http request AliClouds functions send the body in as a buffer and their example code that you are referencing is a bit confusing You can extract the body from the req by doing something like this "
55353999,55353752,"stackoverflow.com",1,"2019-03-26 11:44:33+02","2024-05-17 05:14:10.306044+03","Declare Variable outside the function and then initialize it inside the function"
54913882,54912219,"stackoverflow.com",0,"2019-02-28 07:52:08+02","2024-05-17 05:13:44.21435+03","When using Serverless you can use environment variables in your yml file like this If you want to bind it to SNS your yml file should look something like Now if you mean you would like to accesss an Environment Variable from within your Lambda function it will depend on what language you are coding on but since you are using Python it would be something like EDIT After the OPs comment I think I fully understand the problem ARNs are predictable values They are pretty much a concatenation of your regionuseridtopicname like this If you provide an environment variable to your Serverless yml such as You could then programmatically create a Subscription to that topic based on the predictable subscriptions ARN the environment variable you defined Another option would be to define the Outputs section on your yml file and use the CloudFormation Event to trigger a Lambda which will programatically create a subscription based on the outputted ARN"
62557955,54912219,"stackoverflow.com",0,"2020-06-24 17:56:37+03","2024-05-17 05:13:44.217351+03","I create SNS topic and subscription in the resource section and then add a policy in the lambda role which can be assigned to lambda function for example and then your lambda roles resource look like below personally I separate the SNS and lambda roles resources into different yaml and lastly you can use this lambda role in your lambda function like below So what this functionality does is your lambda can fire an message to your SNS topic and SNS will forward the message to your email "
76101886,54912219,"stackoverflow.com",0,"2023-04-25 16:47:00+03","2024-05-17 05:13:44.218824+03","try this"
54931782,54921201,"stackoverflow.com",0,"2019-02-28 20:11:11+02","2024-05-17 05:13:44.573692+03","My experience tells me not to have a table Users when we use Cognito All data we can hold in Cognito attributes But maybe it required for you However you can use Lambda function as AppSync data source And get data from Cognito in Lambda function "
68342216,54921201,"stackoverflow.com",0,"2021-07-12 08:18:15+03","2024-05-17 05:13:44.575693+03","What has worked for me in the past is to delegate User management to Cognito So Cognito will handle authentication and authorization I set up a postconfirmation lambda that creates a record in the users table in the database The users table has a column for cognito_sub that can be used to reference it "
54943631,54930200,"stackoverflow.com",0,"2020-06-20 12:12:55+03","2024-05-17 05:13:45.489373+03","To define project level secrets you can go to Your Project Settings CICD Variables The secrets will be available via environment variables You can read more about variables here Regarding your issue with SUBNET_IDS list You can either define each one as a different variable or you can concat them with a character or for example and split them in your script "
54937330,54936032,"stackoverflow.com",3,"2019-03-01 05:00:15+02","2024-05-17 05:13:46.081988+03","I found the way "
54946159,54945218,"stackoverflow.com",0,"2019-03-01 16:07:05+02","2024-05-17 05:13:47.692176+03","I have created a very minimal working example using asyncawait Rather using the old style callback approach just invoke soap createClientAsync url Heres the code And here are the logs partially EDIT Functions output via AWSs Lambda Console EDIT 2 The REAL problem Check the link above around asyncawait The async keyword will execute asynchronously and will return a Promise Since it is an asynchronous function your Lambda is being terminated before it could actually execute your callback Using asyncawait will make your code simpler and will have you stop beating your brains out for such a small thing like this "
54953795,54946323,"stackoverflow.com",5,"2019-03-02 01:43:22+02","2024-05-17 05:13:48.113183+03","Out of the box serverless does not parse env that part belongs to you I see three options for you Use the serverlessdotenvplugin Write a script that exports env vars to your local environment before you run serverless Run serverless in dockercompose which is env aware I use this in combination with Makefile even in a CICD context "
66638251,54946323,"stackoverflow.com",5,"2021-03-15 14:43:20+02","2024-05-17 05:13:48.115183+03","Serverless now supports env files without the need for a plugin Add useDotenv true to your serverless yml file The variable should be at the root level same as service Add a env file at the root of your project and serverless will load the variables Example"
74105580,54946323,"stackoverflow.com",0,"2022-10-18 07:04:01+03","2024-05-17 05:13:48.117184+03","use this plugin to write env with serverless yaml serverlessexportenv so you just need to overwrite your region inside serverless yaml and your env will be generated based on whay you have written in serverless yaml "
54965511,54947779,"stackoverflow.com",1,"2019-03-04 00:56:41+02","2024-05-17 05:13:49.095515+03","I ended up determining that the type of integration you use matters There is a couple including lambda awsproxy and lambdaproxy By default it is lambdaproxy which means that all requests are handed to the function directly without any integration mapping or templates You need lambda to do that The correct answer and what seems to be standard acceptable practice is to always send errors as nil and set the status code and response as the response message If you send and error back Lambda will always return an internal error with 502 This also means that logging and error tracking are completely up to you In some cases this is the best approach if you go the lambda integration route you will have to define headers accepted body and use the integration mapping process properly You have to make a decision which way you want to go with Lambda For full clarity you are ALWAYS sending an events APIGatewayProxyResponse back and nil as a second parameter from your handler So a successful response would be And an error response would simple be the same response structure but with the StatusCode and Body values changed to your response needs Lesson learned p"
55133720,55129644,"stackoverflow.com",3,"2019-03-13 05:04:58+02","2024-05-17 05:13:51.130919+03","By default serverlessoffline does not parse your resources for endpoints enable it via custom config Ends up serving Documentation"
58225726,55130796,"stackoverflow.com",0,"2019-10-03 22:29:51+03","2024-05-17 05:13:52.55079+03","You will need to build your typescript project before serverlessoffline can run your code You can either create a tsconfig json file and build the project before running serverless offline start or you can use the serverlessplugintypescript to do this for you Just ensure the plugin is declared above the serverlessoffline plugin in your serverless yml file "
60764739,55130796,"stackoverflow.com",0,"2020-03-19 22:40:56+02","2024-05-17 05:13:52.552791+03","I had the same issue make sure you are not running something else on that PORT"
55153068,55145405,"stackoverflow.com",2,"2019-03-14 02:18:54+02","2024-05-17 05:13:52.972173+03","There is always a way but the simple enduser answer is no The serverlessframework has a naming strategy file per provider and for AWS its hardcoded to cloudformationtemplate[create update]stack json When the file writer does its job it looks at the extension and runs the JSON writer However as per the AWS naming file in their repo they have made it available to be modified by writing a custom plugin As long as your plugin changed the naming strategy to anything that ends in yml the file writing service will switch to a YAML writing strategy "
58864251,55151331,"stackoverflow.com",2,"2019-11-14 21:16:34+02","2024-05-17 05:13:55.086988+03","You are missing the mongodb layer for AWS lambda Open a terminal and type With this it should work without problems Regards"
71430985,55156501,"stackoverflow.com",0,"2022-03-10 23:26:28+02","2024-05-17 05:13:56.720861+03","You need to remove the commas in your YAML i e should be Once you do this the YAML will be valid and it should work "
55372844,55357793,"stackoverflow.com",1,"2019-03-27 10:37:37+02","2024-05-17 05:14:11.441392+03","The way you have exported and then imported should work I could be wrong but your FnImport looks odd compared to how I do it Try the shortened syntax instead"
55161998,55161871,"stackoverflow.com",5,"2019-03-14 14:16:13+02","2024-05-17 05:13:58.672081+03","You have set your Lambdas reservedConcurrency to 0 This will prevent your Lambda from ever being invoked Setting it to 0 is usually useful when your functions are getting invoked but you are not sure why and you want to stop it right away If you want to have it invoked change reservedConcurrency to a positive integer by default it can be a positive integer 1000 but you can increase this limit by contacting AWS or simply remove the reservedConcurrency attribute from your yml file as it will use the default values Why would one ever use reservedConcurrency anyways Well let us say your Lambda functions are triggered by requests from API Gateway Let us say you get 400 peak hours requestssecond and upon every request two other Lambda functions are triggered one to generate a thumbnail for a given image and one to insert some metadata in DynamoDB You would have in theory 1200 Lambda functions running at the same time given all of your Lambda functions finish their execution in less than a second This would lead to throttling as the default concurrent execution for Lambda functions is 1000 But is the thumbnail generation as important as the requests coming from API Gateway Very likely not as it is naturally an eventually consistent task so you could set reservedConcurrency on the thumbnail Lambda to only 200 so you would not use up your concurrency meaning other functions would be able to spin up to do something more useful at a given point in time in our example receiving HTTP requests is more important than generating thumbnails The other 800 left concurrency could then be split between the function triggered from API Gateway and the one that inserts data into DynamoDB thus preventing throttling for the important stuff and keeping the notsoimportantstuff eventually consistent "
55460141,55168878,"stackoverflow.com",2,"2019-04-01 19:58:55+03","2024-05-17 05:13:59.578326+03","Instead of encoding the string I had to decode it "
55195506,55168878,"stackoverflow.com",1,"2019-03-21 10:15:56+02","2024-05-17 05:13:59.579822+03","It looks like the ciphertext_blob argument in AwsKMSClientdecrypt expects a binary string that includes the encrypted Ciphertext that you want to decrypt In your example you are passing in an unencrypted Base64 encoded string into decrypt Instead you need to to pass in an encrypted binary string To get an encrypted string we can call AwsKMSClientencrypt with your keyId also know as your ARN and the string you want to encrypt in plaintext In the response from that call we get back a ciphertext_blob which is the encrypted binary string that we need to use in order to decode Sometimes you might see that binary data unpacked which you can demonstrate doing ciphertext_blob unpack H If you have unpacked data and want to decrypt it you will need to pack it encrypted_upacked_blob pack H Heres a full example of a round trip encoding and decoding of a plaintext string This example combines two examples provided by AWS Encrypting Data in AWS KMS using Ruby SDK and Decrypting a Data Blob in AWS KMS For a better understand of what [blob] pack H is doing check out this StackOverflow post and Rubys Arraypack documentation "
55195213,55194185,"stackoverflow.com",1,"2019-03-16 11:17:38+02","2024-05-17 05:14:01.724922+03","No What you have posted is how you would customise it with AWS but the Google provider functionality is actually provided via a plugin and its process is not consistent with the core repository The deployment bucket variable that gets set upon deployment to Google is actually deploymentBucketName not deploymentBucket name like AWS and it only reuses buckets from its own naming convention You can dig into the code here "
55224952,55223865,"stackoverflow.com",1,"2019-03-18 17:37:09+02","2024-05-17 05:14:02.706916+03","Probably not the answer you are looking for but the guys at Serverless com do not recommend using API Gateways builtin stages as it can get very messy really quick I have tried to find where they stated it once but I could not find it now but it was surely inside their forums Since I have gone likely the same path as you I ended up giving up about using API Gateways builtin environments and used Serverlesss environments instead This means that you will create N Api Gateways and N functions where N represents one stage On my case it is always dev test and prod but you may have more I cannot recommend you enough to read this Chapter Stages in Serverless Framework to get more acquainted with Stages and see if you want to go down the same road as I did Long story short using API Gateways builtin stages is harder to manage because one can easily screw up IAM roles forget to change the path in the URI dev test prod Using Serverless Frameworks stages is much easier because you can deploy different environments in different accounts easier to manage IAM roles and it is very unlikely one will mess up with the URLs themselves since you would have a different URL for every stage You can also set stagespecific endpoints if you want to metrics logs This list goes on and on but you get the idea over the flexibility you gain by deploying every stage independently "
55303150,55299255,"stackoverflow.com",4,"2019-03-22 17:37:02+02","2024-05-17 05:14:03.781294+03","So it turns out that they should implement the RequestStreamHandler interface for example This now successfully runs and throws no exceptions"
55307571,55307194,"stackoverflow.com",2,"2019-03-22 22:49:17+02","2024-05-17 05:14:05.373429+03","Typically when working with files the approach is to use S3 as the storage and there are a few reasons for it but one of the most important is the fact that Lambda has an event size limit of 6mb so you cannot easily POST a huge file directly to it If your zipped excel files is always going to be less than that then you are safe on that regard If not then you should look into a different flow maybe something using AWS step functions with Lambda and S3 Concerning your issue with unzipping the file I have personally used and can recommend admzip which would look something like this"
55320559,55320558,"stackoverflow.com",1,"2019-03-24 05:46:57+02","2024-05-17 05:14:06.811228+03","The s3PutObject permission alone allows you to add an item to the S3 bucket but if you configure any ACL attributes you will need the additional permission s3PutObjectAcl It should be like this"
55360798,55349017,"stackoverflow.com",0,"2019-03-26 17:26:25+02","2024-05-17 05:14:08.197623+03","The issue was with my return statement in my get request I assumed that once features were deleted the record would be deleted I was trying to return features on an object that had no features therefore it was erroring out and not returning anything "
53453054,46010926,"stackoverflow.com",24,"2018-11-23 23:21:05+02","2024-05-17 05:14:09.32211+03","If using yarn workspaces with webpacknodeexternals a better solution than setting modulesFromFile true is to use the following externals setting in your webpack config Essentially using two instances of nodeExternals 1 for the package node_modules and one for the root node_modules "
47610361,46010926,"stackoverflow.com",12,"2017-12-02 19:06:45+02","2024-05-17 05:14:09.32411+03","Thanks to blackxored I was able to fix it on my project In your webpack config file do the following Then add"
46747508,46010926,"stackoverflow.com",5,"2017-10-14 20:42:57+03","2024-05-17 05:14:09.326111+03","Yarn workspaces hoist compatible modules to the root node_modules directory leaving any incompatible different semver etc modules with the dependent workspaces node_modules directory If a package is requested without using a relative path it is either native from node_modules or possibly a symlinked package from one of your workspaces You probably want all of those packages to be external how to configure webpack externals to exclude all node_modules directories through the whole project not just in the root I would try using a function with webpacks external option You are passed the context of the require the name of the module requested and a callback to indicate whether this particular import require should be considered external "
55400757,55400408,"stackoverflow.com",1,"2019-03-28 16:58:23+02","2024-05-17 05:14:15.89967+03","You can setup Global Secondary Indexes one for querying base on userId and another base on projectId This will allows you to efficiently query for users base on projectId or vice versa httpsdocs aws amazon comamazondynamodblatestdeveloperguideGSI html"
55422113,55422052,"stackoverflow.com",1,"2019-03-29 18:52:36+02","2024-05-17 05:14:16.230109+03","As long as your second Lambda function is not attached to the API gateway then it will not be limited by anything So you can simply not include the events property in the second Lambda definition it is an optional parameter "
55435555,55434944,"stackoverflow.com",1,"2019-03-30 22:51:46+02","2024-05-17 05:14:17.80184+03","When deploying the service you used a nondefault AWS profile by passing the argument awsprofile numpyserverlessagent It means that it is deployed to the account specified by this profile When trying to inovke the lambda you did not pass this argument thus using the default profile which is probably specifying a different AWS account "
55530230,55454539,"stackoverflow.com",13,"2019-04-05 10:24:22+03","2024-05-17 05:14:18.736195+03","Most of your points are valid and it can indeed be called an antipattern to run Express inside your Lambda functions behind an API Gateway Should be noted that the initialization time is not that much of a concern While the execution time of a single invocation is capped at 15 minutes a single Lambda instance will serve multiple requests after it has been fired up A frequently invoked single Lambda instance has usually a lifetime or 6 to 9 hours and is disposed of at about 30 minutes of inactivity note that AWS does not publicly disclose these parameters and these figures should only be used as a ballpark Whoever is the unlucky one to get the cold start and eat the initialization delay could get an additional delay in the thousands of milliseconds however The singular main advantage of this approach is as you said providing a migration path for existing Node developers with existing Express knowledge and applications You should generally not consider this approach when developing an application from scratch and implement idiomatic serverless patterns instead e g utilizing API Gateway routing Just to reiterate the main downsides of this approach P S The main contender these days probably would not be a dedicated EC2 instance and rather Fargate containers running Express in Node js This pattern has many of the same benefits as Serverless while keeping existing development patterns and tools largely intact "
55483446,55482627,"stackoverflow.com",1,"2019-04-02 23:56:34+03","2024-05-17 05:14:19.793411+03","You need to update Lambda permission to allow invoking by Cognito user pool Option A update permission in JSON format Option B in console"
58568214,55496081,"stackoverflow.com",23,"2019-10-26 09:23:10+03","2024-05-17 05:14:20.478877+03","You might have solved it as you have asked your question long back but this might help if you did not I too faced the same issue and after some research through AWS documentation I got to know how to use the required attributes Below points to be considered regarding your question Please find the below snippet in response to your question References httpsdocs aws amazon comAWSCloudFormationlatestUserGuidequickrefcloudfront html"
77436317,55496081,"stackoverflow.com",1,"2023-11-07 09:45:56+02","2024-05-17 05:14:20.480872+03","In case it helps OAIs are now deprecated in favor of Origin Access Controls OAC If you use an OAC you still have to specify S3OriginConfig but you put an empty string for the OAI value The following is a valid DistributionConfig for a nonwebsite s3 bucket And this is how you would create the OAC"
55499976,55499975,"stackoverflow.com",5,"2019-04-03 19:26:00+03","2024-05-17 05:14:21.428669+03","It is a little hidden in a couple docs but this can be done by defining PointInTimeRecoverySpecification in the resources section of you serverless yml file e g "
55500363,55500247,"stackoverflow.com",2,"2019-04-03 19:51:00+03","2024-05-17 05:14:22.533941+03","You can create a request template that will map your request contents using the parameters as noted in the documentation Your function will look something like this if you want to take in the page parameter for example"
65028115,55500247,"stackoverflow.com",1,"2020-11-26 21:20:44+02","2024-05-17 05:14:22.534942+03","I have created the request template this way for passing multiple parameters in URL Here is the full code"
55657123,55654880,"stackoverflow.com",2,"2019-04-12 20:45:49+03","2024-05-17 05:14:23.85957+03","I have not used custom authorizers before but I put together a small hello world project to try this out and this is what I found The protected function and the authorizer function Note that I am returning the hash and not a string with to_json I got an error back from the authorizer when using to_json Also note that I am using event[methodArn] to get the protected lambda ARN using context invoked_function_arn also caused me an error Besides that not including an Authorization header in the request will return an Unauthorized error Lastly about the principalId The principalId is a required property on your authorizer response It represents the principal identifier for the caller This may vary from applicationtoapplication but it could be a username an email address or a unique ID Source httpswww alexdebrie compostslambdacustomauthorizers"
55668941,55667376,"stackoverflow.com",1,"2019-04-13 22:05:39+03","2024-05-17 05:14:24.433975+03","Maybe you are looking for a private endpoint in API Gateway httpsaws amazon comblogscomputeintroducingamazonapigatewayprivateendpoints"
55669092,55667376,"stackoverflow.com",0,"2019-04-13 22:27:21+03","2024-05-17 05:14:24.435367+03","Remove the serverless event bindingsconfiguration for the Lambda function The function will deploy without setting up a dependency on another AWS service "
55669275,55667603,"stackoverflow.com",2,"2019-04-13 22:50:39+03","2024-05-17 05:14:25.324181+03","For those looking for a basic example see the answer by ionut All of the configuration for the AWSCognitoUserPool can be found here As for the AWSCognitoUserPoolClient it can be found here "
62871943,55667603,"stackoverflow.com",1,"2020-07-13 11:53:57+03","2024-05-17 05:14:25.325826+03","It is very simple and straight forward All you need to do is create a few resources and then export them in from your template file What I do usually is first create a resource file for eg Cognitouserpool yml and the add the necessary resource and export declaration there After that I shall be calling the resource from my serverless yml file file cognitouserpool yml Inside your user pool resource declaration you would need to add definitions for In this blog post I have explained the steps in detail and also have added a YouTube video for explaining each steps Blog Link httpswww codegigs apphowtocognitouserpoolusingserverless Video Link httpsyoutu bebv_imx8gfLU"
55668903,55667603,"stackoverflow.com",0,"2019-04-13 21:58:35+03","2024-05-17 05:14:25.327824+03","a simple search returned this httpsserverlessstack comchaptersconfigurecognitouserpoolinserverless html"
55702439,55692683,"stackoverflow.com",2,"2019-04-16 10:06:35+03","2024-05-17 05:14:27.38347+03","I have found what was the problem I used the default KMS key and I had to use a custom key Then it worked "
55730680,55699768,"stackoverflow.com",0,"2019-04-17 18:02:03+03","2024-05-17 05:14:28.957413+03","At least exporting the require mongoose seems like something that is not needed What worked for me was just following the official guidelines httpsmongoosejs comdocslambda html and using serverlessplugintypescript httpsgithub comprismaserverlessplugintypescript"
56606440,55704912,"stackoverflow.com",2,"2019-06-15 03:34:04+03","2024-05-17 05:14:29.471504+03","According to this article the Serverless framework does support native GraalVM images The given example is with Kotlin but the same technique should work for plain Java There are a number of caveats due to using GraalVM native images "
60920644,60905099,"stackoverflow.com",1,"2020-03-29 23:58:49+03","2024-05-17 05:19:49.785355+03","You may want to take a look at cftotf which will convert cloudformation into terraform That will import your stack foobarbaz from AWS and convert it into terraform and print it to stdout "
55744299,55744177,"stackoverflow.com",1,"2019-04-18 13:39:53+03","2024-05-17 05:14:30.390668+03","Firstly you do not need to deploy lambda into a specific VPC you only need to do it if you need Lambda functions to specifically access resources that are only available within your VPC If you do not have a use case for this you can just remove the VpcConfig from your lambda resource Secondly if you do need it to be inside a specific then in Cloudformation you are not linking it to a VPC instead you are linking it to the subnets and security groups inside that VPC So in your new VPC make sure you have the relevant security groups and subnets created and then place those IDs into the above snippet Reference httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawsresourcelambdafunction htmlcfnlambdafunctionvpcconfig"
55749204,55749067,"stackoverflow.com",1,"2019-04-18 18:24:57+03","2024-05-17 05:14:31.231809+03","Maybe the Azure Functions Runtime could be a solution for your issue You can also use a Visual Studio Subscription which depending on the subscription include some azure volume This volume can be used for testing purpose and moving to another tennant on azure is not hard "
55769953,55768817,"stackoverflow.com",0,"2019-04-20 06:22:21+03","2024-05-17 05:14:31.796353+03","Thank you for looking at this Chetan Ranpariya yes this seems to have been the problem Not sure why lots of posts are recommending promisify there must be an approach where this works I guess but I just do not have the magic The code for the lambda that works is and the Jest test for it is"
55772323,55769484,"stackoverflow.com",2,"2019-04-20 12:55:17+03","2024-05-17 05:14:32.575163+03","From what you have said the problem looks like your sendMail function is not await able i e it does not return a promise or await transport sendMail This is why the two console logs in the callback do not happen because the lambda function finishes executing before the callback is called You should check nodemailers documentation here httpsnodemailer comaboutexample for more information but it looks like the following code should work fine n b I guess the reason it works locally is because you container is not precisely the same as the remote lambda environment In the remote lambda environment once your handler returns all code execution gets frozen and so your callback does not have time to happen But your local container continues to run in the background which means your callbacks run even though your handler has finished "
61313379,55774522,"stackoverflow.com",2,"2020-04-20 03:17:37+03","2024-05-17 05:14:33.124055+03","To use a variable you need to use field SecondsPath instead of Seconds It works the same way as Result vs ResultPath "
55784338,55784236,"stackoverflow.com",0,"2019-04-21 19:03:33+03","2024-05-17 05:14:33.891207+03","ok I found the answer I had to parse the string to integer"
55791071,55784236,"stackoverflow.com",0,"2019-04-22 10:57:34+03","2024-05-17 05:14:33.892202+03","To avoid such issues I like to use dynamodbsql open source which enable working with Dynamo using SQL syntax Select Example Insert Example"
55788910,55785733,"stackoverflow.com",3,"2019-04-22 06:49:19+03","2024-05-17 05:14:34.513124+03","Yes Install the package using npm and follow the JavaScript documentation within your React app Here is my configuration to use Cognito and API Gateway This lets you call your serverless backend endpoints like so The JWT configuration allows your serverless endpoints to use the authorizer type COGNITO_USER_POOLS to authorize endpoint usage "
55788857,55787756,"stackoverflow.com",3,"2019-04-22 06:39:37+03","2024-05-17 05:14:35.497429+03","TLDR const event body JSON stringify eventStub Your production code works because the code is written correctly for the expected payload structure where event is an object and event body is a JSONparsable string In your first test you are passing event not as an object but a JSONparsable string event body is undefined because event as a string does not have body as a parameter Your test should be const event body JSON stringify eventStub note that its an object with a body attribute that is a string In your second attempt you are passing in an object and then when you try to JSON parse the object it is throwing an error Pay attention to the error Unexpected token o in JSON at position 1 The o is position 1 at object like u is position one for undefined "
55814560,55810868,"stackoverflow.com",5,"2022-04-14 14:59:39+03","2024-05-17 05:14:36.108905+03","The requests that are registered asynchronously are sent in the destructor of the HttpTransport instance or when PHP shuts down as a shutdown function is registered In OpenWhisk we never shut down as we run in a neverending loop until the Docker container is killed Update You can now call clientflush and do not need to worry about reflection main now looks like this Original explanation As a result to make this work we need to call the destructor of the transport property of the Hub s client Unfortunately this private so the easiest way to do this is to use reflection to make it visible and then call it This will make the transport property visible so that we can retrieve it and call its destructor which will in turn call cleanupPendingRequests that will then send the requests to sentry io The main therefore looks like this Incidentally I wonder if this Sentry SDK works with Swoole"
55813107,55810868,"stackoverflow.com",2,"2019-04-23 17:05:20+03","2024-05-17 05:14:36.111549+03","Function runtimes are paused between requests by the platform This means any background processes will be blocked if they are not finished when the function returns It looks like the asynchronous HTTP request does not get a chance to complete before the runtime pauses You will need to find some way to block returning from the function until that request is completed If the Sentry SDK has some callback handler or other mechanism to be notified when messages have been sent you could use that"
55813938,55813231,"stackoverflow.com",2,"2019-04-23 17:52:53+03","2024-05-17 05:14:37.118284+03","This is the code used by Serverless to load the configuration httpsgithub comserverlessserverlessblobmasterlibutilsgetServerlessConfigFile jsL9 Relevant excerpt Note that from the CLI servicePath is set to the current working directory Looking at the code my guess is that you may have a serverless json which takes precedence over serverless yaml The command serverless print will show your resolved configuration httpsserverless comframeworkdocsprovidersawsclireferenceprintprint "
55818073,55817470,"stackoverflow.com",1,"2019-04-23 22:21:31+03","2024-05-17 05:14:37.991726+03","In the gist that you have shared there are a number of changes I suggest avoiding reading the extracted tar file in memory where you can stream stream its contents directly to the S3 bucket "
58254959,55839068,"stackoverflow.com",7,"2019-10-06 09:56:23+03","2024-05-17 05:14:39.503558+03","I had the exact same issue and for me the solution was to indent route properly like this So basically change to"
55849828,55839068,"stackoverflow.com",2,"2019-04-25 15:56:34+03","2024-05-17 05:14:39.504558+03","After going through literally everything related to Serverless I realised my Serverless version is not the most recent one do not ask me how that happened I ran yarn add serverless to get a project specific version of it yesterday Instead the version was 1 35 and thus had no support for API Gateway websockets Perhaps I had installed it globally at a previous point and neglected to remove it from my global npm packages The fact it was failing to recognise the tags silently did not help the debugging process and I might when I get chance contribute to the project by adding a validation run on the Serverless yml files so unsupported options are flagged in the console As it was running npm install g serverless fixed the problem and now the API Gateway gets deployed correctly Thank you Alex and Hugo for responding "
57044747,57043556,"stackoverflow.com",1,"2019-07-15 20:41:30+03","2024-05-17 05:15:46.210288+03","Please add console log for detailed logging via cloudwatch and use xray Some typical problems with cloudfront a lot of time to propagate to edge locations maybe u need recreate your cdn logs from lambda locates in invoked region"
55880742,55852551,"stackoverflow.com",9,"2019-04-27 15:54:00+03","2024-05-17 05:14:41.069551+03","Running the package command will generate 2 compiled Cloudformation templates in your service directory one for stack creation and another for the stack update They will be available in the folder serverless as cloudformationtemplatecreatestack json and cloudformationtemplateupdatestack json respectively "
55875902,55852551,"stackoverflow.com",0,"2019-04-27 02:12:52+03","2024-05-17 05:14:41.071551+03","I found a plugin that will output a SAM Template from serverless which solved this problem Here is the link httpsgithub comsapessiserverlesssam"
59893667,55897150,"stackoverflow.com",10,"2020-01-24 11:31:31+02","2024-05-17 05:14:43.782035+03","in the package json add debug script VS code lunch json Then start debugging from VS code"
55913365,55897150,"stackoverflow.com",4,"2019-04-30 05:53:53+03","2024-05-17 05:14:43.78403+03","The warning you are seeing is a deprecation warning the legacy debugger debug has been deprecated since Node 7 7 0 The correct way to attach a node debugger to serverless offline is by using inspect "
75331191,55897150,"stackoverflow.com",0,"2023-02-03 05:25:49+02","2024-05-17 05:14:43.786031+03","If you have a valid sample event in JSON format AND you are OK with debugging one function at a time then here is a configuration that has worked great for me It enables breakpoints and stepthrough debuggin exactly as you would expect"
56259231,55920819,"stackoverflow.com",0,"2019-05-22 17:31:54+03","2024-05-17 05:14:44.684603+03","You can go to cloudwatch to see more specific logs"
68036530,55920819,"stackoverflow.com",0,"2021-06-18 17:00:45+03","2024-05-17 05:14:44.685825+03","when I am hitting the end point url on browser Json response is coming like message Internal server error This suggests that the functions is setup correctly but when AWS spins up your code and executes it an error is thrown You options to debug and get to the bottom of this include in order of decreasing efficiencyROI IMHO "
75186967,55920819,"stackoverflow.com",0,"2023-01-20 18:43:39+02","2024-05-17 05:14:44.686822+03","If anyone still has this issue just change the events variable in serverless yml honestly I dont know why event http ANY doesnt work but the following works well Check this url"
55950362,55947884,"stackoverflow.com",2,"2019-05-16 15:54:31+03","2024-05-17 05:14:46.842547+03","I would argue the way serverless is written is a much cleaner way to deploy to different stages Whilst API Gateway does allow different stages under the same API Gateway this leaves much more room for accidentally doing something you didnt want to do e g accidentally tearing down your production API instead of dev Also best practice is to have each stage in its own AWS account This allows you to better lock down your production environment at an account level to avoid accidental changes This is beneficial for all your AWS resources not just API Gateway If you follow best practices and have an AWS account per stage your problem is mute as you will have an API Gateway in each of your staging accounts If these best practices are not for your you can always revert back to normal CloudFormation templates to force each stage to be a different deployment under the same API Gateway "
55952990,55951606,"stackoverflow.com",1,"2019-05-02 15:56:27+03","2024-05-17 05:14:47.573908+03","I am not sure where to properly integrate Cloudflare Assuming this is the crux of the question here this is what it might look like using the notation you provided The basic idea is that you will want Cloudflare to be the first thing your DNS Route53 resolves to so it can properly serve cached content before it ever reaches your application Which in this case would start at API Gateway "
55978563,55959385,"stackoverflow.com",0,"2019-05-04 03:44:29+03","2024-05-17 05:14:48.693617+03","From the doc this is something you can try"
55970024,55968432,"stackoverflow.com",0,"2019-05-03 15:40:24+03","2024-05-17 05:14:50.385765+03","Ok so I have found a solution I just did this It dint work because it reached the end of the handler function and did not care that there was other callbacks waiting I also learned that you could await an async function and not just a Promise "
55970342,55968678,"stackoverflow.com",1,"2019-05-03 15:59:10+03","2024-05-17 05:14:50.869406+03","I think the problem is that RDS and Lambda are in different regions which means they are also in different VPCs as a VPC cannot span across multiple regions Although you can enable Inter VPC Peering httpsaws amazon comvpcfaqsPeering_Connections Consider that when you deploy a lambda function in a VPC it will not have internet access as long as you do not attach a NAT Gateway to that VPCsubnet If the RDS is open to the world and does it really need to be you can try to deploy in the same region without a VPC and verify if that works "
73142221,55971241,"stackoverflow.com",0,"2022-07-27 20:25:01+03","2024-05-17 05:14:51.912464+03","This error occurred to me and I got solutions and finally the examplebelwebpack4 was used in serverlesswebpack I do not know why the error happens with settings for you But for me using the example I send here below solved my problem And maybe it will help yours or other colleagues who face this httpsgithub comserverlessheavenserverlesswebpackblobmasterexamplesbabelwebpack4webpack config js Alright I needed to install some packages after cloning the webpack config but this is simple Good luck to you and everyone "
55985646,55978262,"stackoverflow.com",2,"2019-05-06 17:39:49+03","2024-05-17 05:14:52.941558+03","This is the normal behavior of CloudFormation CloudFormation only updates a resource when its properties change Since the properties RequestMappingTemplateS3Location and ResponseMappingTemplateS3Location do not change CloudFormation does not update your AppSync Resolver even though those S3 locations points to new content One way to solve your problem is to use the aws cloudformation package command of the AWS CLI It allows you to define your template with local files Running returns a copy of your template packaged template replacing references to local artifacts with the S3 location where the command uploaded the artifacts The S3 location name key depends on the content uses MD5 Hence with this strategy the property RequestMappingTemplateS3Location changes if the content referenced by the S3 location changes After that you can deploy your template with aws cloudformation deploy Note this is the same as using AWS SAM CLI sam package is an alias of aws cloudformation package If working with the serverless framework another solution is to use the serverlessappsyncplugin which allows to specify mapping templates inline or in a file "
57930557,56147854,"stackoverflow.com",2,"2019-09-14 00:29:12+03","2024-05-17 05:14:55.81623+03","Make sure you have configured your API Gateway to allow binary media to be passed through "
61003588,56147854,"stackoverflow.com",1,"2020-04-03 04:03:47+03","2024-05-17 05:14:55.81723+03","Add this to serverless yml See httpsstackoverflow coma610034989585130 Hope this help "
56167161,56160686,"stackoverflow.com",1,"2019-05-16 14:08:49+03","2024-05-17 05:14:56.609989+03","It is not a great answer but you have hit a known and reported bug httpsgithub comserverlessserverlessissues6133 API Gateway tags setting Stage Variables not Tags Stage variables have different limitations to tags and specifically for your case they do not support Looking through the releases reverting back to version 1 14 1 may be a workaround for this "
56190345,56176642,"stackoverflow.com",0,"2019-05-17 19:33:34+03","2024-05-17 05:14:58.222562+03","s3 getSignedUrl was not resolving until after the lambda had finished resolving so the callback never resolved I needed to wrap s3 getSignedUrl in a promise to make it work"
56217536,56181544,"stackoverflow.com",0,"2019-05-20 12:00:55+03","2024-05-17 05:14:59.255539+03","To be able to define an AWS IoT Topic Rule with an ErrorAction that will also show up as a trigger event on AWS Lambda the configuration should look somewhat like this"
57068163,57048634,"stackoverflow.com",3,"2019-07-17 06:34:00+03","2024-05-17 05:15:46.718491+03","You should be able to do something like"
74745372,73903039,"stackoverflow.com",0,"2022-12-09 17:51:03+02","2024-05-17 05:31:29.303706+03","esbuild automatically includes node_modules If you do not want that to be included you can add esbuildnodeexternals plugin "
58223231,56183525,"stackoverflow.com",0,"2020-06-20 12:12:55+03","2024-05-17 05:15:00.384128+03","You need to deploy the jar on layer in AWS Lambda layers section AWS Lambda Layers You can configure your Lambda function to pull in additional code and content in the form of layers A layer is a ZIP archive that contains libraries a custom runtime or other dependencies With layers you can use libraries in your function without needing to include them in your deployment package Following are the steps to use AWS lambda layers Once you complete writing your function make sure the pom xml contains the artifacts and mavenshadeplugin Run Maven Please read further on following link"
56190927,56190791,"stackoverflow.com",3,"2019-05-17 20:17:46+03","2024-05-17 05:15:01.512846+03","You have incorrect indentation Policies attribute belongs to Properties not to AssumeRolePolicyDocument which is the case in your document unindent the whole Policies section by one "
56204225,56190791,"stackoverflow.com",0,"2019-05-19 05:10:02+03","2024-05-17 05:15:01.514847+03","As mentioned in the official documentation Policies belongs to Properties and not to AssumeRolePolicyDocument"
56201975,56197622,"stackoverflow.com",2,"2019-05-20 00:46:00+03","2024-05-17 05:15:02.584442+03","If the entry point of your service is API Gateway you can configure Sampling Rules and limits on the AWS XRay console or using API to control the number of requests that are sampled by XRay See this article for an introduction to sampling in XRay httpsaws amazon comblogsawsapigatewayxray Let me know if you have further questions regarding this Update Sampling rules may be specified only in XRay httpsdocs aws amazon comxraylatestdevguidexrayconsolesampling html This allows you to limit the number of traces no matter how many API Gateway or EC2 instances you have for handling your requests Small caveat As of today this mode of sampling is supported only if the entry point is API Gateway or if you have the 2 0 version of XRay daemon running on your instances EC2 or otherwise If the entry point is lambda this sampling effect is not supported today but will be supported soon In your case it seems you are using API Gateway as your entry point so you can definitely configure sampling rules in XRay console and have that take effect globally across all your API Gateway endpoints You can also configure different sampling rules for different URLs like auth is sampled at 5 TPS and products is configured for 1 TPS with different reservoirs based on your usecase "
56210125,56209473,"stackoverflow.com",1,"2019-05-19 20:04:09+03","2024-05-17 05:15:03.419172+03","You are missing the sqs attribute under your events section I am not really sure what you meant with pendingsqs as that is not a valid Serverless Frameworks keyword If you want to configure the batch size like you did above just put it under the sqs item in the events list like so This is all documented here"
56236518,56233675,"stackoverflow.com",15,"2019-05-21 13:40:35+03","2024-05-17 05:15:04.413997+03","The URL of your API Gateway Endpoint will change when you recreate the CloudFormation for your service This can happen when You remove the stack sls remove and recreate it sls deploy You rename your service name in your serverless yml "
56240136,56238704,"stackoverflow.com",8,"2019-05-21 17:08:52+03","2024-05-17 05:15:04.926381+03","You can do the following It works because serverless uses CloudFormation under the hood See also Return Values of AWSCognitoUserPool and Ref "
56242716,56242715,"stackoverflow.com",14,"2020-03-06 15:27:23+02","2024-05-17 05:15:05.734227+03","In general accessing decrypting AWS SSM parameter store values requires these 3 permissions Heres a real world example that only allows access to SSM parameters relating to my lambda functions distinguished by following a common naming conventionpattern it works under the following circumstances All parameters use the following naming convention a appnameorappnamespace serverless lambdafunctionnamethenwhateverelseyouwant b lambdafunctionname must begin with sls So let us say I have an app called myCoolApp and a Lambda function called slsmyCoolLambdaFunction Perhaps I want to save database config values such as username and password I would have two SSM parameters created myCoolAppserverlessslsmyCoolLambdaFunctiondevdatabaseusername plaintext myCoolAppserverlessslsmyCoolLambdaFunctiondevdatabasepassword SecureString Then in my serverless yml file I might reference these two SSM values as function level environment variables like so Or even better if I want to be super dynamic for situations where I have different config values depending on the stage I can set the environment variables like so With this above example if I had two stages dev prod perhaps I would create the following SSM parameters myCoolAppserverlessslsmyCoolLambdaFunctiondevdatabaseusername plaintext myCoolAppserverlessslsmyCoolLambdaFunctiondevdatabasepassword SecureString myCoolAppserverlessslsmyCoolLambdaFunctionproddatabaseusername plaintext myCoolAppserverlessslsmyCoolLambdaFunctionproddatabasepassword SecureString "
58493810,56242715,"stackoverflow.com",4,"2019-10-21 23:38:29+03","2024-05-17 05:15:05.737228+03","I suggest to use AWS SDK to get SSM parameters in code instead of saving in environment file i e env It is more secure that way You need to assign permission to the role you use with actionssmGetParameter and resource point to the parameter in the SSM Parameter store I use serverless framework for deployment Below is what I have in serverless yml assumming parameter names with pattern stage myproject e g devmyprojectusername qamyprojectpassword two useful resources are listed below where to save credentials wireless framework IAM doc"
69661489,56242715,"stackoverflow.com",0,"2021-10-21 14:52:42+03","2024-05-17 05:15:05.739228+03","In the case you are using codebuild in a cicd pipeline dont forget to add the ssm authorization policies to the codebuild service role when we are talking about ssm we have to differenciate between secretsmanager and parametstore "
56275261,56274094,"stackoverflow.com",0,"2019-05-23 15:28:59+03","2024-05-17 05:15:06.67697+03","It looks like the webpack serverless uses is none standard webpack configuration You can use serverless package instead of webpack directly "
56286940,56283412,"stackoverflow.com",2,"2019-05-24 09:25:23+03","2024-05-17 05:15:08.255643+03","Use cloudformations GetAtt function to access the return value from the DBCluster creation Now you can access the RDS FQDN hostnamedomain name in the node js code like so You can see more about the Cloudformation return values for DBCluster resources here httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawspropertiesrdsdatabaseinstance htmlawspropertiesrdsdatabaseinstancereturnvalues"
56305498,56304157,"stackoverflow.com",6,"2019-05-25 16:57:36+03","2024-05-17 05:15:08.764273+03","You need to specify the RoleArn property of the AWSCognitoUserPoolGroup resource See AWSIAMRole on how to define a role you will have to declare AWSIAMPolicy resources and reference them in your role "
59203910,56319766,"stackoverflow.com",1,"2019-12-06 00:01:25+02","2024-05-17 05:15:09.892713+03","Just want to point out that if you are looking to load different SSM paths based on environment you can achieve this many ways outlined here I have had a pleasant time loading through json files for example Hope this helps whoever else lands here from a search"
68683884,56319766,"stackoverflow.com",1,"2021-08-06 18:12:48+03","2024-05-17 05:15:09.893985+03","Due to a restriction in YAML it is not possible to use the shortcut syntax for a sequence of intrinsic functions See the Important section in the docs for reference Try this"
56377005,56322315,"stackoverflow.com",0,"2019-05-30 14:03:13+03","2024-05-17 05:15:10.635167+03","You can specify the Lambda function name explicitly using the name field Example With this config file your deployed Lambda function will have the name myfunc See line 129 in httpsserverless comframeworkdocsprovidersawsguideserverless yml Using the name of an already existing Lambda function will not work you still have to delete old the Lambda function beforehand "
57569492,56373536,"stackoverflow.com",14,"2019-08-20 11:23:09+03","2024-05-17 05:15:12.572941+03","Assuming your backend is deployed on lambda since serverless tag Each invocation will leave a container idle to prevent cold start or use an existing one if available You are leaving the connection open to reuse it between invocation like advertised in best practices With a poolSize of 25 and 100 max connections you should limit your function concurrency to 4 Reserve concurrency to prevent your function from using all the available concurrency in the region or from overloading downstream resources More reading httpswww mongodb comblogpostoptimizingawslambdaperformancewithmongodbatlasandnodejs"
57585396,56373536,"stackoverflow.com",7,"2019-08-21 09:08:52+03","2024-05-17 05:15:12.57441+03","You could try couple of things In a serverless environment as already suggested by Gabriel Bleu why have such a high connectionLimit Serverless environment keeps spawning new containers and stopping as per requests If multiple instances spawn concurrently it would exhaust the MongoDB server limit very quickly The concept of connectionPool is x number of connections are established every time from every node instance But that does not mean all the connections are automatically released after querying After completing ALL the DB operation you should release each connection individual after use mongoose connection close Note Mongoose connection close will close all the connections of connection pool So ideally this should be run just before returning the response Why are you setting explicity autoReconnect to true MongoDB driver internally reconnects whenever the connection is lost and certainly is not recommended for short lifespan instances such as serverless containers If you are running in cluster mode to optimize for performance change the serverUri to replica set URL format MONGODB_URImongodbusernamepasswordhostOnehostTwohostThree ssltrueauthSourceadmin "
57669042,56373536,"stackoverflow.com",5,"2019-08-27 09:48:22+03","2024-05-17 05:15:12.577408+03","There are so many factors affecting the max connection limit You have mongoDB hosted on Atlas and as you mentioned the backend is lamda means you have a serverless environment "
56413253,56406121,"stackoverflow.com",1,"2019-06-02 11:03:29+03","2024-05-17 05:15:16.440757+03","No Cognito only generates UUID format ID you can get the user ID from JWT no need put it in the URL If you insist on the numbers user ID you could create a mapping table in the DynamoDB or RDS "
56413560,56413437,"stackoverflow.com",12,"2019-06-02 11:52:03+03","2024-05-17 05:15:17.536582+03","It turns out that I had to add the following in my serverless yml "
56823483,56416115,"stackoverflow.com",1,"2019-06-30 11:31:15+03","2024-05-17 05:15:18.589301+03","Maybe you are missing a line from tsconfig add this below emitDecoratorMetadata true Credits to the God of Nestjs Mr Kamils reply httpsstackoverflow coma501218866301493"
56430508,56429705,"stackoverflow.com",2,"2019-06-03 18:36:57+03","2024-05-17 05:15:19.227957+03"," I just answered a very similar question here Yes this is explained in the Express docs under Error Handling Express comes with a builtin error handler that takes care of any errors that might be encountered in the app This default errorhandling middleware function is added at the end of the middleware function stack If you pass an error to next and you do not handle it in a custom error handler it will be handled by the builtin error handler the error will be written to the client with the stack trace The stack trace is not included in the production environment To override this handler refer to the section in the Express docs titled Writing error handlers It explains Define errorhandling middleware functions in the same way as other middleware functions except errorhandling functions have four arguments instead of three err req res next For example You define errorhandling middleware last after other app use and routes calls So in your case if you wanted to respond with a 400 and some JSON you might write something like this"
56503212,56460986,"stackoverflow.com",6,"2019-06-08 06:35:33+03","2024-05-17 05:15:20.093205+03","I solved the problem by running npm install on a Linux box instead of my Mac In my case I set up AWS CodePipeline which runs all the build scripts on an EC2 instance Could also have solved this using other hosted CI pipeline a Docker container etc "
63905156,56460986,"stackoverflow.com",0,"2020-09-15 18:31:05+03","2024-05-17 05:15:20.095205+03","I had the same problem with Pusher After reading aaronbragers answer I realized the problem and try this It worked for me "
56568080,56468432,"stackoverflow.com",0,"2019-06-12 21:09:50+03","2024-05-17 05:15:21.177013+03","Just in case it helps the only way I could get it to work was to comment out babelloader as such Not sure if this is the best workaround but it helped me solve the current issue I was working on "
56526173,56521463,"stackoverflow.com",2,"2019-06-10 15:11:44+03","2024-05-17 05:15:23.829665+03","One of the variables used for value in your YAML configuration might be the wrong type selfservice is not defined in the YAML but is being referenced in"
59147188,56742825,"stackoverflow.com",0,"2019-12-05 18:31:52+02","2024-05-17 05:15:25.892612+03","According to the documentation you can put the Output section as a list item under resources Note there is a small error in the documentation do not forget do indent Value under CognitoUserPoolId Update This could be another option with each part in a separate file Make sure you begin the resources file with Resources and the output file with Outputs serverless yml resources yml outputs yml"
56768307,56760376,"stackoverflow.com",0,"2019-06-26 11:20:37+03","2024-05-17 05:15:26.83158+03","You need to add to your role the AmazonTextractFullAccess policy as described in the official documentation"
56799096,56798618,"stackoverflow.com",1,"2019-06-28 01:26:11+03","2024-05-17 05:15:28.176097+03","This is not a fix for your problem but to simplify and correct minor issues with your IAM role"
56853831,56843940,"stackoverflow.com",1,"2019-07-02 16:40:00+03","2024-05-17 05:15:29.448891+03","Serverless is a great choice is you want to set up a simple REST API application Using Express would also be a good choice API Gateway and Serverless also now supports websockets so it should be pretty easy to create a websocket application When it comes to socket io however you will need to do a bit of research before diving in Websocket support on API Gateway is a relatively new concept and there are not too many resources online on it The combination with Lambda can be a little difficult to grasp at first As for socket io there are even less I personally recommend running a EC2 instance running socket io for your MVP I think it would be easier "
56876415,56843940,"stackoverflow.com",1,"2019-07-03 21:56:07+03","2024-05-17 05:15:29.450891+03","There are several reasons to choose a serverless infrastructure over nonserverless In many cases these align very closely with 5 Pillars of the AWS WellArchitected Framework Serverless architectures offer great While your proposed project does appear to fit well within the FaaS framework infrequent and unpredictable workload with low resource requirements the disadvantages of serverless notably the more complex and difficult to test architecture and vendor lockin can make it challenging to rapidly prototype and deploy a MVP As your product favors an engineering tradeoff toward time to market a nonserverless approach will most likely enable you to release a MVP quickly with minimal headache"
56927761,56927559,"stackoverflow.com",0,"2019-07-08 05:37:16+03","2024-05-17 05:15:32.666072+03","If you want to call a function like an endpoint you need to have your application running There is a plugin called serverlessoffline to help you on this In case you want to call your function using serverless options the command is called INVOKE httpsserverless comframeworkdocsprovidersawsclireferenceinvokelocal"
57826199,57693867,"stackoverflow.com",1,"2019-09-16 16:31:09+03","2024-05-17 05:16:25.62677+03","This is because serverless offline performs require cache invalidation at each invocation This is not the case on AWS Lambda You can disable this behavior with skipCacheInvalidation but it seems to be gone anyway "
61826720,56843940,"stackoverflow.com",0,"2020-05-15 22:19:30+03","2024-05-17 05:15:29.452892+03","There are some serverless frameworks that work on AWS lambda function By my realworld experience there are some notices on each of them AWS Amplify httpsdocs amplify aws a full stack serverless solution for developers It is quite easy to use at the begining Cons Is that over the time your maintainance cost is higher on deployment part It is very slow to deploy a stack on AWS when you just need to change a piece of code It will download all the stack files to local then upload again Serverless framework httpswww serverless com is less complex rich plugins and nano function oriented The downside of this framework is every code function will be used the same across lambdas When the project is bigger the code size is bigger therefore your lambdas cold start is slower Simplify framework httpsgithub comsimplifyframeworkcodegen sounds a lightweight but rich functionalities It allows you build your CICD inside your project This concept is similar with AWS CDK has just released a couple of days in May You design your API using OpenAPI specs swagger specs that will be reused and standardized your APIs across tools and processes No vender lockin has been desiged carefully Today you are in AWS but tomorrow it will be on your onpremise servers You choose what fits to your solution There is no one fit all "
56877989,56876764,"stackoverflow.com",0,"2019-07-04 00:12:09+03","2024-05-17 05:15:30.328706+03","Use the aws cli "
56920896,56919052,"stackoverflow.com",0,"2019-07-07 12:28:59+03","2024-05-17 05:15:31.07437+03","Why does it do that I dont know How can I disable it I can sort of help you with that I dont know how to stop the POST route from being generated but I can show you how to stop anything other than SNS from invoking your function You can attach a resourcebased IAM policy to your lambda function Specifically you can attach a policy that denies all access to invoke the lambda function except for allowing SNS to invoke it The policy would look something like this"
56924440,56919052,"stackoverflow.com",0,"2019-07-07 20:18:41+03","2024-05-17 05:15:31.076371+03","After toying with it I see that it is only created by Serverless Offline and not pushed live I used by sls to invoke the Lambda by itself using sls lambda invoke "
68815149,56927848,"stackoverflow.com",1,"2021-08-17 12:45:33+03","2024-05-17 05:15:33.489508+03","Is it an option using AWS Cognito or CloudFront We did that with an Enterprise application which uses OIDC OAuth 2 0 It does implement just Authentication for now Give a look in these links httpsaws amazon comdeblogsawsbuiltinauthenticationinalb httpsdocs aws amazon comelasticloadbalancinglatestapplicationlistenerauthenticateusers html"
56934367,56930584,"stackoverflow.com",0,"2019-07-08 14:53:56+03","2024-05-17 05:15:34.477634+03","I think you will need to pass the Subnet IDs in as parameters and reference those "
56944264,56940915,"stackoverflow.com",2,"2019-07-09 04:27:25+03","2024-05-17 05:15:35.57741+03","Figured a way around this just needed to prepare the app with async"
60649085,56940915,"stackoverflow.com",0,"2020-03-12 09:04:49+02","2024-05-17 05:15:35.578443+03","Did you add this in the server js so it looks like this"
57726029,56970358,"stackoverflow.com",1,"2019-08-30 14:36:55+03","2024-05-17 05:15:37.775171+03","As mentioned by Matthew Fellows you can configure the AWS Lambda Client to invoke your custom URL since it does regular HTTP request under the hood Use AWSLambdaClientBuilder and do You need to set ContentType applicationjson in request headers or else Pact fails to parse header since it is empty string by default for some reason Use withRequestHandlers for that and create consumer contracts by calling lambda functions through this client it should request Pact mockserver instead of AWS However on the producer side you need to The idea is that you have a proxy HTTP server that uses your serverless handler function for returning responses"
56978769,56970358,"stackoverflow.com",1,"2019-07-11 00:24:42+03","2024-05-17 05:15:37.778172+03","This is something that is not yet directly supported by Pact Option 1 At the moment you would need to write two separate sets of tests to cover both request and response aspects of the interaction one initiated by the consumer and another from the provider option 2 untested Invoke lambda is just an HTTP POST request via the AWS lambda service so it could in theory be tested via a regular HTTP requestresponse See httpsgist github combethesquec858e5c15649ae525ef0cc5264b8477c for somethinking on a reqres style interaction and join us at slack pact io to chat on it "
56978166,56971070,"stackoverflow.com",2,"2019-07-10 23:44:40+03","2024-05-17 05:15:38.425971+03","I read this page a few times and I realize that I was not using the correct way so I change my YML code to this And I also change my authorizer source to Authorization "
56979679,56974236,"stackoverflow.com",1,"2019-07-11 02:17:31+03","2024-05-17 05:15:39.419182+03","After desperate hours spent I have come up with the solution For anyone who comes across the same issue here is a solution that worked for me Below is just demonstrating that Below is an example for sending headers using API of awsamplifyapi and Auth of awsamplifyauth "
60453070,56975689,"stackoverflow.com",0,"2020-02-28 15:47:22+02","2024-05-17 05:15:40.211673+03","Try to update if local"
57268707,56991822,"stackoverflow.com",1,"2019-07-31 10:09:55+03","2024-05-17 05:15:41.352782+03","Since your question contains several different aspects I will split my answer into two parts The setup you describe deploying lambda functions to two different regions one being somehow non standard the Chinese one might create problems on the techical side as well I would suggest starting with a simple one region setup first until you get the ICP problem fixed maybe using some Chinaaware CDN provider Or you try with a standard AWS region closer to China for this case some people recommend the Singapore region "
57001359,57001273,"stackoverflow.com",4,"2019-07-12 09:33:51+03","2024-05-17 05:15:41.971137+03","Your callback nullresponse will not wait for those two callback functions to finish You can use Promise and use Promise all objs then function to wait for all promises finish and run "
57001702,57001273,"stackoverflow.com",2,"2019-07-12 10:00:51+03","2024-05-17 05:15:41.972336+03","Welcome to Worlds Javascript world Callback hell We have some options for your case Callback hell async lib Promise asyncawait Callback hell Call a async function in a callback Async lib async You can use waterfall function to do actions in order and parallel if order is not matter "
57001448,57001273,"stackoverflow.com",2,"2019-07-12 09:41:03+03","2024-05-17 05:15:41.973602+03","Try wrapping two quotation calls in Promise then utilise Promise all to wait for both of them to be completed then return the result to the callback"
57002017,57001273,"stackoverflow.com",1,"2019-07-12 10:33:56+03","2024-05-17 05:15:41.974672+03","You could also try using util promisify and the async await syntax For example We can also do something similar but without async await"
57029358,57028536,"stackoverflow.com",1,"2019-07-17 02:03:34+03","2024-05-17 05:15:44.835354+03","The 200 resources is a soft limit which you can write to AWS and ask them to relax In some cases they will let you deploy more resources than 200 if you apply for your case It happened to my project and what I did was splitting the API into several projects according to logical grouping of the modules Specifically I moved out the Users API to a Serverless project of its own and kept the Resource API in another project Then I also used a technique like the following to reduce the consumption of resources Instead of declaring a new handler for every REST verb one for post a resource and another for get a resource etc I put them in the same handler but as different events under the same handler "
57034572,57034365,"stackoverflow.com",9,"2019-07-15 09:53:40+03","2024-05-17 05:15:45.529304+03","Your problem is not with the serverless offline plugin but with the Serverless Framework instead it is not installed thus you get sls or serverless command not found Just run npm install serverless g Keep in mind that depending how you have set up npm you may need sudo permissions to install packages globally "
62560895,57073327,"stackoverflow.com",7,"2023-07-27 22:58:45+03","2024-05-17 05:15:47.572801+03","After struggling with this issue by myself I found the solution that worked for me Assume that we have a secret XYZ_CREDS where we store user and password keyvalue pairs AWS Secrets manager stores them in JSON format user test password xxxx Here is how to put user and password into Lambda function environment variables I am using Serverless 1 73 1 for deploying to CloudFormation Hope this helps others "
57582703,57073327,"stackoverflow.com",1,"2019-08-21 02:33:55+03","2024-05-17 05:15:47.574593+03","Given that the name of your secret in secrets manager is correct I think you might have an a after manager before the decryption "
77067873,57073327,"stackoverflow.com",1,"2023-09-15 04:57:41+03","2024-05-17 05:15:47.575658+03","I have created a serverless plugin since I faced the same issue You check out the plugin here httpsgithub comrobinthomasserverlessawssecrets You can have your environment variables that look like this The plugin will then load the secret from AWS Secrets Manager and then replace values of MYSQL_USERNAME and MYSQL_PASSWORD So when you access process env MYSQL_USERNAME within your lambda the secret is already available "
61563203,57073327,"stackoverflow.com",0,"2020-05-02 20:03:17+03","2024-05-17 05:15:47.577661+03","Secret manager stores in key valuejson format So specify the variables individually Eg otherwise pass secret manager name and decrypt using awssdk in the code"
57090032,57080184,"stackoverflow.com",3,"2019-07-18 12:53:55+03","2024-05-17 05:15:48.604873+03","Have you tried to override the cloudformation template Override AWS CloudFormation Resource you can specify a custom ressource with your provider name where you override the properties MinimumCompressionSize of AWSApiGatewayRestApi Example serverless yaml you can add links that can help httpsdocs aws amazon comfr_frAWSCloudFormationlatestUserGuidetransformawsserverless html httpsdocs aws amazon comfr_frAWSCloudFormationlatestUserGuideawsresourceapigatewayrestapi htmlcfnapigatewayrestapiminimumcompressionsize"
62446000,57088357,"stackoverflow.com",13,"2020-08-14 06:28:12+03","2024-05-17 05:15:49.595055+03","I hit the same problem Error Cannot find module jmespath and solved it Do you use awssdk via node_modules like follows If so you just remove awssdk from node_modules remove awssdk for yarn remove awssdk for npm awssdk has been included to lambda since nodejs10 see httpsdocs aws amazon comlambdalatestdglambdaruntimes html"
59617354,57088357,"stackoverflow.com",1,"2020-01-06 20:45:46+02","2024-05-17 05:15:49.596655+03","Try to put in front of your module name I changed my name from require xxx to require xxx and it worked again In my case the local module file i wanted to add xxx js is on the same level as the index js file "
62155693,57088357,"stackoverflow.com",1,"2020-06-02 18:36:29+03","2024-05-17 05:15:49.597653+03","I would also like to add that as a preliminary step before adjusting your path check and verify your dependencies listed in your package json s files I have seen this error Runtime ImportModuleErrorerrorMessageError Cannot find module something occur in the aws cloudwatch logs It happened because my project has multiple subprojectssubfolders with their own package json files Ensure that module is properly referenced in the subprojects package json In local dev you might have the dependency cited in your baseglobal package json and thinking it works but when you deploy the lambda the npm install that occurs during build does not include the newly required module because its not referenced in the local subprojects package json "
69377462,57088357,"stackoverflow.com",1,"2021-09-29 16:33:56+03","2024-05-17 05:15:49.599994+03","In the root serverless project folder"
65328274,57088357,"stackoverflow.com",0,"2020-12-16 19:34:23+02","2024-05-17 05:15:49.600992+03","Well encountered the same issue in our project too It was issue in the same of file import Our file name xabc js and in imported as Xabc js VScode was not complaining and was showing proper import"
66289068,57088357,"stackoverflow.com",0,"2021-02-20 09:27:13+02","2024-05-17 05:15:49.602035+03","For me the problem was that this jmespath library was actually missing and some others too from uploaded node_modules folder Verified this by downloading zip as the source was too big and did not found there jmespath nor awssdk libraries Solved it by installing dependencies with npm i and reuploading lambda with updated node_modules folder "
72986985,57088357,"stackoverflow.com",0,"2022-07-15 00:47:42+03","2024-05-17 05:15:49.603032+03","If nodejs folder is not there in Lambda Layer create nodejs and any file so that while executing the function lambda will create zip file with entire node_modules packages inside nodejs folder of Layer If it is empty it will not execute I hope it will resolve the issue In my case it is working fine "
57157683,57117775,"stackoverflow.com",4,"2019-07-23 09:10:25+03","2024-05-17 05:15:50.707459+03","Take the advantage of the model being a separate component to the library and uploaded the model in an S3 bucket Before initialising spaCy I download the model from S3 This is accomplished by the method below And the code using spaCy looks like this"
57155701,57118962,"stackoverflow.com",13,"2019-07-23 05:02:26+03","2024-05-17 05:15:51.737238+03","The procedure of setting up permissions for a lambda function which rotates AWS Secrets Manager secrets is explained in the docs [1] To put it in a nutshell you need two steps Note The function name is referenced in the DependsOn attribute It is also referenced in the condition StringEquals and the attribute FunctionName as arnawslambda selfcustom region selfcustom accountId function selfservice selfprovider stage rotateKeys Keep in mind to change them if you change your function name Here is how the serverless yml file should look like You have to replace yourservicename yourregion youraccountid and upload your rotation code using e g the package include attributes Note There are templates for the lambda function which update the secrets [2][3] Please also keep in mind to configure your VPC correctly for the lambda function being able to access the AWS Secrets Manager service over the network [4] [1] httpsdocs aws amazon comsecretsmanagerlatestuserguiderotatingsecretsrequiredpermissions html [2] httpsdocs aws amazon comsecretsmanagerlatestuserguiderotatingsecretscreategenerictemplate html [3] httpsgithub comawssamplesawssecretsmanagerrotationlambdas [4] httpsdocs aws amazon comsecretsmanagerlatestuserguiderotationnetworkrqmts html"
57119186,57118962,"stackoverflow.com",13,"2021-04-01 16:49:25+03","2024-05-17 05:15:51.740239+03","I had the same issue today I ran this and it worked for me httpsdocs aws amazon comsecretsmanagerlatestuserguidetroubleshoot_rotation html"
76098097,57118962,"stackoverflow.com",2,"2023-04-25 09:15:13+03","2024-05-17 05:15:51.741239+03","I got this resolved by adding following aws_lambda_permission resource"
57149343,57118962,"stackoverflow.com",2,"2019-07-22 18:34:22+03","2024-05-17 05:15:51.741849+03","Your policy is incorrect The service is secretsmanager but the action you defined is stsAssumeRole which is from AWS Security Token Service A full access policy would be But you should limit the actions and the Resource the lambda can use For this you can use the policy builder which can be found in IAMPolicies After creating a policy in the editor you can click on the JSON Tab and see the format Then you need to adapt it to your serverless yaml format I hope I can help you Dominik"
57301958,57283519,"stackoverflow.com",1,"2019-08-01 07:45:42+03","2024-05-17 05:15:52.619258+03","code response"
57449865,57407062,"stackoverflow.com",1,"2019-08-11 15:14:47+03","2024-05-17 05:16:05.418898+03","Assuming you are following this example you should add the resources part to your serverless yml httpsgithub comserverlessexamplesblobacfa06e6a93c1cb6e5d9306e65675d1acdec5eb3awsnodeauth0customauthorizersapiserverless ymlL36 Also I would first test the API in POSTMAN or any other similar tool to make sure everything works as expected before handling CORS "
57449795,57408181,"stackoverflow.com",3,"2019-08-11 15:01:59+03","2024-05-17 05:16:06.443494+03","You can use chromeawslambda to either package chrome with your Lambda or create a Lambda Layer to avoid the package size I did something similar here based on chromeawslambda"
57351649,57292228,"stackoverflow.com",2,"2019-08-05 05:42:06+03","2024-05-17 05:15:54.806238+03","Unfortunately the Serverless Framework does not support customizing API Gateway default responses natively yet There is an existing issue in the repository if you are interested in following it For now you will have to use CloudFormation to achieve this There are a number of options you can use to customize error responses You will want to know specifically what ResponseType you want to change For your use case it looks like the MISSING_AUTHENTICATION_TOKEN Response Type is what you are looking for This is an example you can adapt and add to your serverless yml via the resources section In ResponseTemplates you can set the JSON output as you see fit This example simply set an error property which will contain the value of the property customErrorMessage of the Lambda authorizer output context property You can learn more about this here You can also set it to be a static string if you want to but being able to dynamically change the error message in your code is rather nice "
57300949,57298750,"stackoverflow.com",1,"2019-08-01 05:02:17+03","2024-05-17 05:15:55.643143+03","You need to add example com into CloudFront CNAMEAlternative name filed and also in DNS to point it to CloudFront when url changes as project example com CloudFront finds the distribution based on the host header and if it cannot find it will give you 403 error "
57301430,57301087,"stackoverflow.com",6,"2019-08-01 06:26:37+03","2024-05-17 05:15:56.715391+03","You can use Cloudformation intrinsic function Sub to create index arn"
57301425,57301087,"stackoverflow.com",0,"2019-08-01 06:25:17+03","2024-05-17 05:15:56.716701+03","After referring to these docs httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawsresourcedynamodbtable html and these httpsdocs aws amazon comamazondynamodblatestdeveloperguideapipermissionsreference html I was able to figure out that the required format for referencing an index table is arnawsdynamodbregionaccountidtabletablenameindex Further in order to not hard code all the values in in my case because I have several staging environments you can do a join like so Where table name is defined in your custom block "
57322977,57322645,"stackoverflow.com",20,"2019-08-02 11:51:22+03","2024-05-17 05:15:57.710104+03","Apparently the right syntax is this These were the errors"
57450221,57340767,"stackoverflow.com",3,"2019-08-11 16:13:38+03","2024-05-17 05:15:58.659765+03","How are you importing the missing packages in your Lambda function You might need to force include them in your serverless yml"
57363235,57350873,"stackoverflow.com",3,"2019-08-05 20:13:30+03","2024-05-17 05:15:59.289769+03","Since you seem to be asking about both Terraform and Serverless Framework here I am assuming you are looking for a general answer rather than specifically how this would be solved with a particular tool One way to solve this problem is to decouple your build process from your deploy process by adding a version selection mechanism in between This just means that somewhere in your system you have a value that can be written by your build process and read by your deploy process which indicates what is the current artifact for each of your Lambda functions When your build process completes successfully it can write the information about the artifact it built into the appropriate location and then trigger your deployment process Your deployment process will then read the artifact information and use it to decide what to deploy If you have made no changes to the current artifact metadata for a particular function then the deploy process can see that and not do anything If a particular artifact is flawed in some way and you only notice once it is deployed you can potentially set the artifact metadata back to the previous one and rerun the deployment process to roll back If you choose a data store that retains historical versions you will also have a log of changes to the current artifact which might be useful to understand circumstances that lead to an incident Without getting into specifics it is hard to say more about this For Terraform in particular the artifact metadata store ought to be something that Terraform can read using a data source To show a real example I am going to just arbitrarily choose AWS SSM Parameter Store as a location for that artifact metadata store The technical details of this will vary a lot depending on your technology choices If you do not use Terraform then you will either use a feature similar to data sources in your other tool or you would write some wrapper glue code that can itself retrieve the necessary information and pass it into the tool as an argument The main thing regardless of technology choices is that there is an explicit record somewhere of what is the latest artifact for each function which is updated by your build step and read by your deploy step This pattern can apply to other artifact types too such as AMIs for EC2 docker images etc "
57351266,57350873,"stackoverflow.com",0,"2019-08-05 04:18:51+03","2024-05-17 05:15:59.293771+03","Seems you have added label of terraform serverlessframework I called it sls and awslambda So all of them work for you terraform Terraform itself will care of the differences which lambda need be updated But it is not lambda friendly if you need install related packages serverless framework sls it is good to use to manage lambda functions but as side effect it has to be managed with api gateway together I am not sure if sls team has fix this issue or not Need some confirmations SLS will take care of installing related packages The bad part is sls cannot diff the resources to be deployed and to be planned Bad part is cfn cloudformation does not have diff feature as well furtherly it does not have proper tools to manage its aws cli commands you have to use others such as shell scriping Ansible or even Terraform to manage coudformation templates updates You can take these and think with your teams skill sets Always choice the tools which you and your team are most confident "
57386974,57377077,"stackoverflow.com",2,"2019-08-07 07:28:37+03","2024-05-17 05:16:00.191215+03","Your problem is that WebPack failed to include the file in its transitive closure when trying to find out all the files you need due to dotenv importing it dynamically You could use a WebPack plugin to explicitly include your env file such as this one "
57401992,57399032,"stackoverflow.com",0,"2019-08-07 23:51:00+03","2024-05-17 05:16:01.34239+03","A couple of suggestion comes to mind returns the number of milliseconds elapsed since January 1 1970 000000 UTC"
57450886,57403579,"stackoverflow.com",6,"2019-08-11 17:51:42+03","2024-05-17 05:16:03.429062+03","I have copied pasted your code and added relevant Lambda functions and it works for me I have tested the PreSignUp with the following command aws cognitoidp admincreateuser region region userpoolid userpoolid username phone While not showing in the AWS Console Lambda UI the triggers do show up in the CognitoUser PoolsdevuserpoolTriggers which is confusing Example repo httpsgithub comerezrokahserverlesscognitotriggers"
61152798,57403579,"stackoverflow.com",1,"2020-04-11 09:09:19+03","2024-05-17 05:16:03.430062+03","I found the issue in your serverless yml You are missing an indentation under cognitoUserPool I tried it both ways and it works with the additional indentation "
75824507,57403579,"stackoverflow.com",0,"2023-03-23 16:48:12+02","2024-05-17 05:16:03.431374+03","For existing pools please use existingtrue and forceDeploy true as in the article here httpsforum serverless comthowtospecifyanexistingcognitouserpoolinservrelessyml241218"
57469565,57404025,"stackoverflow.com",4,"2019-08-13 02:53:42+03","2024-05-17 05:16:04.692168+03","This will be a short answer for what is a somewhat longer discussion on Python imports You can do the research yourself on the hectic and confusing battle between relative and absolute imports as a design for a python project The Gist It is necessary to understand that the base of the python importing for SLS functions IS where the serverless yml file exists I imagine that it is similar to having a main py that calls the other files that are referenced as functions in the sls yml For my case above I did not structure the imports using absolute imports when I had my issues I switched all of my imports to have absolute paths so when I moved the package around it would continue to work The error that I was given Runtime ImportModuleError Unable to import module somefunction attempted relative import with no known parent package was really poor to describe the actual issue The error should have included that the packages being used by somefunction were not found when attempting a relative import because that was the actual problem that needed fixing Hopefully this helps someone else out someday Let me know if I can provide more information where I have not already "
57459651,57404025,"stackoverflow.com",2,"2019-08-12 14:00:08+03","2024-05-17 05:16:04.694168+03","I think you need to change your handler property from to like my folder structure is then my template will be for more details check here httpsserverless comframeworkdocsprovidersawsguidefunctions"
59709167,57408181,"stackoverflow.com",2,"2020-01-13 01:55:24+02","2024-05-17 05:16:06.445495+03","chromeawslambda indeed is big 40MB adding to the deployment package using Layer could potentially reduce the package size but also could increase the size because the 250MB unzipped limit includes the layer and the lambda code If you use chromeawslambda then definitely do NOT use puppeteer instead use puppeteercore for a smaller size I did a very similar setup this hopefully it helps 1"
57981793,57411532,"stackoverflow.com",3,"2019-09-18 00:20:32+03","2024-05-17 05:16:08.15581+03","Try removing your build folder before deploying "
71912133,57423611,"stackoverflow.com",3,"2022-04-18 16:05:02+03","2024-05-17 05:16:08.390195+03",""
65415445,57423611,"stackoverflow.com",1,"2020-12-22 22:31:45+02","2024-05-17 05:16:08.391195+03","You can use a serverless plugin serverlesspluginresourcetagging it will tag your Lambda function Dynamo Tables Bucket Stream API Gateway and CloudFront resources The way it works is you have to provide stacksTags having your tags inside under the Provider section of serverless "
57449734,57425977,"stackoverflow.com",2,"2019-08-25 10:00:40+03","2024-05-17 05:16:09.387632+03","The layers value is an array per the documentation httpsserverless comframeworkdocsprovidersawsguidelayersusingyourlayers Should work "
57449573,57430901,"stackoverflow.com",1,"2019-08-11 14:26:23+03","2024-05-17 05:16:09.99324+03","NOTE S3 bucket gives you option for static web hosting"
57438418,57437050,"stackoverflow.com",8,"2019-08-10 03:36:43+03","2024-05-17 05:16:11.939868+03","I think that the problem is that in your Lambda function declarations you are referring to the IAM role as role arnawsiam123456789012rolelambdaIAMRole This is an absolute ARN and is how you would indicate an IAM role or other resource that was created and managed outside of your serverless yml template In your case the quickest fix is to simply replace role arnawsiam123456789012rolelambdaIAMRole with role lambdaIAMRole The latter refers to an AWS resource declared inside the template An even better fix assuming that all of your Lambda functions will have the same role is to remove your lambdaIAMRole declaration entirely and then remove all role arnawsiam123456789012rolelambdaIAMRole properties from the Lambda functions The role declaration adds nothing over the default IAM role that the Serverless Framework will implicitly generate for you and assign to the Lambda functions This is one of the things that makes the framework valuable it provides good defaults to save you the time and effort Examples here "
57484177,57452377,"stackoverflow.com",0,"2019-08-19 22:18:35+03","2024-05-17 05:16:12.903701+03","OK so here is a workaround without using the events part in the serverless syntax we can do this only using raw CloudFormation syntax Also according to the serverless team it is actually a bug that should be fixed and deployed on August 14th 2019 Hope it helps "
57466227,57466056,"stackoverflow.com",1,"2019-08-13 10:39:46+03","2024-05-17 05:16:15.497179+03","Because AWS Lambdas run on node and the version of node AWS Lambda use do not support import keyword More info on NodeJS plans to support importexport es6 es2015 modules EDIT As Michael states in the comments you need to install the proper packages Either by using npm or looking where the package should be I guess you should follow sapper svelte instructions properly import would fail the same way as require as the package do not exists Is not an import vs require problem but a nonexistent package problem "
57474225,57468191,"stackoverflow.com",0,"2019-08-13 11:50:35+03","2024-05-17 05:16:17.047058+03","Not sure if this is the only problem in your code but your handler signature should be one listed in the doc "
57481535,57468191,"stackoverflow.com",1,"2019-08-13 19:04:46+03","2024-05-17 05:16:17.049059+03","Alright so it took a lot of trial and error but this is the method signature that works I really wish the documentation for golang was better Thanks to Clement for pointing me in the right direction "
57481522,57480766,"stackoverflow.com",2,"2019-08-13 19:25:59+03","2024-05-17 05:16:17.279791+03","You can pass command line arguments via the schedule event input like so Although there is some discussion on this bref github issue about whether using the cli console application is the best solution vs writing PHP functions that bootstrap the kernel and do the specific thing you want the command to do "
57499527,57498259,"stackoverflow.com",7,"2019-08-14 20:35:57+03","2024-05-17 05:16:18.395942+03","I found the problem I ran a test that set env AWS_REGIONuswest1 and then tried to deploy in the same terminal instance My Serverless template sets region optregion envAWS_REGION useast2 so I usually just let it default because that is the desired region But because envAWS_REGION had been set by the test it was deploying to a region that in fact did not have the stack I was referencing The alternatives would be to run tests in useast2 or preferably to always pass in the region when running deploy e g sls deploy stage dev region useast2 "
65635470,57529281,"stackoverflow.com",0,"2021-01-08 21:44:13+02","2024-05-17 05:16:19.530717+03","This is probably that you are missing some provider configuration to let CloudFormation deal with the log versioning and retention I suggest you to add this following configurations at the provider section of the serverless yml file In additional you can check it out on httpsforums aws amazon commessage jspamessageID623472 discussion "
63099281,57529281,"stackoverflow.com",1,"2020-07-26 14:07:24+03","2024-05-17 05:16:19.531717+03","I think it is related to ApiGateway permissions solved by adding permission to role iam roles selected relevant code build role policies attach policy select API gateway read write"
57530117,57529281,"stackoverflow.com",2,"2019-08-16 22:28:15+03","2024-05-17 05:16:19.533718+03","This is a permissions error The user that you are using to deploy the serverless project does not have permissions to logsDescribeLogGroups To resolve this issue you will need to give the user that is deploying the service permissions to logsDescribeLogGroups on resource arnawslogsuseast1346468483688loggrouplogstream"
57534822,57534298,"stackoverflow.com",0,"2019-08-17 12:00:17+03","2024-05-17 05:16:20.581903+03","Refer the below snippet Try using the absolute path instead of relative __dirname Update If you want to use AWS S3 instead of those npm packages you can use AWSSDK for nodejs to read the content of S3 file You can refer this tutorial for reading file from S3 "
57560084,57558482,"stackoverflow.com",2,"2019-08-19 18:47:48+03","2024-05-17 05:16:21.670635+03","Two possible solutions externals [nodeExternals ] will remove all external dependencies from the bundle and includeModules true will still add them to the zip under node_modules and Same webpack config js but this time with includeModules false as we do not need the dependencies in the zip file they are loaded with the layer Layer structure Repository is available here httpsgithub comerezrokahserverlesswebpackfirebase See the differences in package size"
57561071,57560369,"stackoverflow.com",3,"2019-08-19 20:00:05+03","2024-05-17 05:16:22.645904+03","When using KeySchemaElements the HASH keytype must come before the RANGE keytype In your YAML on you GSI for spinnedprimary you have to put the HASH keytype before the RANGE keytype switch them around so that the HASH is the first keytype in that element "
57568562,57568482,"stackoverflow.com",3,"2019-08-20 10:23:29+03","2024-05-17 05:16:23.563617+03","If removing and redeploying the service is an option then you can use"
57576212,57575458,"stackoverflow.com",1,"2019-08-20 18:31:02+03","2024-05-17 05:16:24.666873+03","Your servreless yml will set the environment variable for the function but not for the process env of the scripts run by serverlessscriptableplugin You will need to save it as an output for your stack using something similar to this Then in your script extract that value from the stack something like this I am not sure how saving the value in parameter store will allow you to access it locally though If you want to invoke the function locally you can use Or use dotenv for any other JavaScript code"
57707059,57694397,"stackoverflow.com",21,"2019-08-29 12:24:05+03","2024-05-17 05:16:26.448311+03","OK I found a solution to get this working Apparently you need to set AWS_SDK_LOAD_CONFIG to a truthy value such that the Session will be created from the configuration values from the shared config awsconfig and shared credentials awscredentials files then execute with"
57707817,57700391,"stackoverflow.com",5,"2019-08-29 13:06:18+03","2024-05-17 05:16:28.416434+03","Instead of sharing just the Lambda function per each new authorizer you can share the authorizer itself Api Gateway stack And in your other stacks This is done from memory CloudFormation docs so please forgive for any compilation errors Source httpsserverless comframeworkdocsprovidersawseventsapigatewayshareauthorizer"
57748059,57725786,"stackoverflow.com",1,"2019-09-03 19:33:43+03","2024-05-17 05:16:29.412131+03","In almost all cases when you call a method xyz on an AWS client object and it fails with xyz is not a function the problem is that you are using an old version of an SDK that does not actually support that method Upgrading to the latest AWS SDK version will fix this problem "
57726096,57725786,"stackoverflow.com",0,"2019-08-30 14:42:21+03","2024-05-17 05:16:29.414131+03","When initializing dynamodb client new AWS DynamoDB DocumentClient please pass options atleast region parameter to DocumentClient function "
58526284,57726007,"stackoverflow.com",1,"2019-10-23 18:25:34+03","2024-05-17 05:16:30.168987+03","This is happening due to the work of serverlesspluginoptimize which I have had to remove in the end and simply use a lambda layer for my dependencies "
57730521,57729921,"stackoverflow.com",0,"2019-08-30 19:47:56+03","2024-05-17 05:16:30.804457+03","You need to match the Logical IDs for the bucket and lambda permission See this example taken from the docs httpsserverless comframeworkdocsprovidersawseventss3custombucketconfiguration"
63992585,57733581,"stackoverflow.com",21,"2020-10-02 17:14:55+03","2024-05-17 05:16:31.817426+03","I had this issue and if anyone comes across it this github comment fixed my issue You can run sls offline start noPrependStageInUrl or add the following to your serverless yml file According to the comment I had this problem with anything 6 this was due to the fact that it now defaults to appending the staging name to the url path To revert to the old way you need to add noPrependStageInUrl to the cli or in the serverless file custom serverlessoffline noPrependStageInUrl true to revert to previous setting I am testing it his out but dherault the functionality is not reflecting what is actually happening in AWS I was using serverlessoffline 6 7 0 and my index handler was as below And my serverless yml Apologies this is not exactly a great answer but hopefully someone comes across this and it is a solution to their problem "
58226113,57733581,"stackoverflow.com",12,"2019-10-03 22:59:31+03","2024-05-17 05:16:31.820427+03","Looks like you have got a whitespace issue in your serverless yml file Try indenting path and method under the http block"
57779803,57733581,"stackoverflow.com",0,"2019-09-04 02:33:16+03","2024-05-17 05:16:31.821974+03","for setup a quick example using serverless template output step1 so for generated a new nodejs example with an api step2 install serverlessoffline step3 in serverless yml step4 start local server github Example serverless yml to define an api in your serverless framework you need to respect yaml format and in the path variable you dont need to start with hello just hello will work "
75947530,57733581,"stackoverflow.com",0,"2023-04-06 11:47:57+03","2024-05-17 05:16:31.823268+03","Just pointing out that methods above the path behave differently than other way around for all other methods except GET facepalm route not found working"
57733979,57733950,"stackoverflow.com",0,"2019-08-31 02:01:05+03","2024-05-17 05:16:32.405609+03","Apparently I forgot to append ping to the URL as in the Github example As documented at httpsdocs aws amazon comapigatewaylatestdeveloperguideamazonapigatewayusingstagevariables html So I entered the URL with ping and got the expected result"
57738954,57737904,"stackoverflow.com",3,"2019-09-01 23:29:59+03","2024-05-17 05:16:33.923456+03","Your RDS instance is accepting inbound connections on 3306 from the LambdaSecurityGroup which is fine for anything with the LambdaSecurityGroup SG attached to it but you also need to allow connections from your VPNSecurityGroup Change your RDSSecurityGroupBlock to look as follows and that should allow you to connect to RDS from your VPN As a side note the VPNSecurityGroup is accepting connections from anywhere for 3306 22 1194 443 943 This may be intentional but given that these are exposed for management purposes it would not be best practice You should give serious consideration to scoping the CidrIps for those ports to trusted CidrIp sources to avoid any potential unwanted exposures You may also with to consider removing the 3306 block from there all together as it would seem to be unnecessary to have that port open on the VPN itself EDIT As per the OPs comments in addition to the above you also need to change PubliclyAccessible to False to resolve the issue "
57786267,57737904,"stackoverflow.com",0,"2019-09-04 13:13:02+03","2024-05-17 05:16:33.926458+03","I would like to give a full answer to the question since the title implies a problem just with mysql client not able to connect RDS Along with hephalump changes I had to do more two changes to also enable my lambdas to connect to RDS and now I am able to connect mysql client and also lambdas I had to create a new IAM Role for my lambdas The important bit here to solve the problem seems to be this Then I had to remove my iamRoleStatements from my provider and add the new role role LambdaRole And now I need to add my lambdas to the right security group So I changed my VPC on my provider to be I have updated the gist with the latest changes "
57742639,57740262,"stackoverflow.com",0,"2019-09-01 04:35:20+03","2024-05-17 05:16:34.450768+03","In my experience the intrinsic functions in cloudformation work fine inside serverless yml but not the yaml short form i e use FnGetAtt over GetAtt As long as you use the full syntax for yaml it should work Try this untested "
65016106,57740310,"stackoverflow.com",18,"2020-11-26 06:26:54+02","2024-05-17 05:16:35.457593+03","Add the following in the custom section Like this"
57740333,57740310,"stackoverflow.com",6,"2019-08-31 20:43:56+03","2024-05-17 05:16:35.458594+03","AWS Elastic Beanstalk did not yet include python3 7 31082019 try to downgrade your python version to 3 6 and try again "
61180518,57750299,"stackoverflow.com",1,"2020-04-13 04:55:40+03","2024-05-17 05:16:38.319552+03","From Oct 2019 you can add wildcard custom domains to API Gateway Then you can add domain xyz as the custom domain of your endpoint needs a certificate from AWS Certificate Manager ACM that is active for domain xyz too That allows you to issue subdomains to your tenants dynamically without any modifications to DNS anymore You can route tenants to their servicesor do a custom logic per tenant by using the parameter event requestContext domainPrefix lambda as an example "
57757291,57753966,"stackoverflow.com",1,"2019-09-02 15:53:10+03","2024-05-17 05:16:39.328996+03","It is not possible to have this information It is not logged However you can have inputs from stackdriver with a request like that However it is not per request it is per time slot Not really match your wish It is not possible to do more "
57758634,57758234,"stackoverflow.com",3,"2019-09-02 22:01:02+03","2024-05-17 05:16:40.449881+03","The cafile environment variable must be set before invoking login On windows powershell"
57864342,57762595,"stackoverflow.com",0,"2019-09-10 08:09:12+03","2024-05-17 05:16:41.379561+03","Try to set default nix proxy environment variables in the form http s username [email protected] proxy_port "
57870470,57762595,"stackoverflow.com",0,"2019-09-10 15:06:37+03","2024-05-17 05:16:41.381562+03","If it is a secure proxy that you use it might be the proxy certificate or certificates CA that you need to set as cafile "
57920107,57918079,"stackoverflow.com",1,"2019-09-13 11:52:42+03","2024-05-17 05:16:52.992598+03","You can use this plugin In your serverless yml And in postDeploy js Another option is this plugin but you will need to use a forked version as described here "
57792013,57791307,"stackoverflow.com",7,"2019-09-04 19:05:30+03","2024-05-17 05:16:42.457509+03","Lambda functions deployed with serverless do not default to your IAM user credentials as far as I know They use the IAM rolepolicy that you supply in serverless yml plus basic CloudWatch Logs permissions which are autogenerated by serverless The problem is that your Lambda function is using temporary credentials from STS via an assumed IAM role to generate the presigned URL The URL will expire when the temporary session token expires or earlier if you explicitly indicate an earlier timeout If you use IAM user credentials rather than temporary credentials via an IAM role you can extend the expiration to 7 days with signature v4 or end of epoch with the potentially deprecated signature v2 So you need to supply your Lambda function with IAM user credentials possibly through environment variables or AWS Parameter Store or AWS Secrets Manager For more see Why is my presigned URL for an Amazon S3 bucket expiring before the expiration time that I specified Also there are a couple of minor coding issues here"
57796363,57796295,"stackoverflow.com",0,"2019-09-05 01:34:52+03","2024-05-17 05:16:43.453069+03","AWS Step Functions seems pretty well targeted at this usecase of tying together separate AWS operations into a coherent workflow with welldefined error handling Not sure if the pricing will work for you can be pricey for millions operations but it may be worth looking at Also not sure about performance overhead or other limitations so YMMV "
57803798,57796295,"stackoverflow.com",0,"2019-09-05 13:48:32+03","2024-05-17 05:16:43.454662+03","You can simply trigger the next lambda asynchronously in your lambda function after you complete the required processing in that step So the first lambda is triggered by an HTTP call and in that lambda execution after you finish processing this step just launch the next lambda function asynchronously instead of sending the trigger through SNS or Kinesis Repeat this process in each of your steps This would guarantee single time execution of all the steps by lambda "
57804013,57796295,"stackoverflow.com",0,"2019-09-05 14:00:36+03","2024-05-17 05:16:43.45666+03","Eventful Lambda triggers SNS S3 CloudWatch generally guarantee atleastonce invocation not exactlyonce As you noted you would have to handle deduplication manually by for example keeping track of event IDs in DynamoDB using strongly consistent reads or by implementing idempotent Lambdas meaning functions that have no additional effects even when invoked several times with the same input In your example step 4 is essentially idempotent providing that the function does not have any side effects apart from storing the altered copy and that the new copy overwrites any previously stored copies with the same event ID One service that does guarantee exactlyonce delivery out of the box is SQS FIFO This service unfortunately cannot be used to trigger Lambdas directly so you would have to set up a scheduled Lambda to poll the FIFO queue periodically as per this answer In your case you could handle step 5 with this arrangement since I am assuming you do not want to submit the same resource to the target API several times So in summary heres how I would go about it With this arrangement even if Lambda B is occasionally executed twice or more by the same S3 upload event there is no harm done since the FIFO SQS queue handles deduplication for you before the flow reaches Lambda C "
57812367,57796295,"stackoverflow.com",0,"2019-09-05 23:08:09+03","2024-05-17 05:16:43.45966+03","AWS Step function is meant for you httpsdocs aws amazon comstepfunctionslatestdgwelcome html You will execute the steps you want based on previous executions outputs Each taskstep just need to output a json correctly in the wanted state httpsdocs aws amazon comstepfunctionslatestdgconceptsstates html Based on the state your workflow will move on You can create your workflow easily and trigger lambdas or ECS tasks ECS tasks are your own lambda environment running without the constraints of the AWS Lambda environment With ECS tasks you can run on Bare metal on your own EC2 machine or in ECS Docker containers on ECS and thus have unlimited resources extensible limits As compared to Lambda where the limits are pretty strict 500Mb of disk execution limited in time etc "
57809718,57807871,"stackoverflow.com",0,"2019-09-05 19:35:29+03","2024-05-17 05:16:44.503613+03","I was able to do it through asyncawait Here is my code"
57825352,57825211,"stackoverflow.com",1,"2019-09-06 19:03:57+03","2024-05-17 05:16:45.438806+03","You can definitely use Cognito from Lambda Source have done it You may not be able to use the AWS Cognito JS SDK from Lambda nicely though The AWS Cognito JS SDK appears to be designed for clientside applications where fetch is a builtin You have installed nodefetch but the SDK is not loading it because it does not think it needs to because it is expecting it to be builtin I see two options This thread has a good description of the same issue and some workarounds probably the best one for you is in your script which should make it appear as a builtin to the SDKs code without hacking up the internals "
57839840,57834849,"stackoverflow.com",0,"2019-09-08 10:06:15+03","2024-05-17 05:16:46.459443+03","You should either use the callback argument to return a response Or use promises"
58003021,57847738,"stackoverflow.com",2,"2019-09-19 05:47:31+03","2024-05-17 05:16:47.266255+03","Was a result of me brain farting around node being asynchronous and hitting the callback before it finished processing "
57849727,57847738,"stackoverflow.com",1,"2019-09-09 10:43:26+03","2024-05-17 05:16:47.267255+03","You should return a response in your lambda to see the result Also you do not need a Lambda to to publish a message to an SQS Queue You can set up an API Gateway service proxy to directly map a request to a Queue This plugin makes it very easy I am one of the collaborators and also saves you the cost of the Lambda invocations "
57855217,57855181,"stackoverflow.com",5,"2019-09-09 16:39:40+03","2024-05-17 05:16:48.179078+03","You can use AWS CloudFormation intrinsic functions like this"
57873778,57872576,"stackoverflow.com",4,"2019-09-10 18:29:39+03","2024-05-17 05:16:49.183498+03","You can use the same function and handler for your get and your post Use Gos builtin HTTP router or use a thirdparty one such as Gorilla Mux or Chi as shown in the example code below because that is what I had handy In essence you are building a Go HTTP server but in Lambda So follow the details for setting up a Go web server and take a look at AWSs API Gateway Proxy "
73156734,57872576,"stackoverflow.com",0,"2022-07-28 20:10:29+03","2024-05-17 05:16:49.185649+03","You should not need to build yourself a Go Server in Lambda as you already have API Gateway served to you from serverless framework I used AWS CloudFormation SAM and I used HTTP API Gateway Not REST but it should function in a similar way First of all you need to make it into 1 Lambda functions for 2 events like this Inside your lambda you should have"
57878136,57877852,"stackoverflow.com",1,"2019-09-10 23:59:40+03","2024-05-17 05:16:50.050547+03","The processLargeSpreadsheet function is returning before the stream callbacks are called which causes the lambda function to terminate and therefore you do not see those logs when running in lambda When running locally the callbacks still get executed even though processLargeSpreadsheet returns To fix this you can wrap the stream callbacks in a promise and await that like this You may also need to await processLargeSpreadSheet in the handleSplitSpreadsheet as well "
59650469,57888140,"stackoverflow.com",3,"2020-01-08 19:16:40+02","2024-05-17 05:16:51.026254+03","According to this giving the authorizer a name lets you use intrinsic functions to refer to the ARN"
57924696,57906573,"stackoverflow.com",0,"2019-09-13 16:37:47+03","2024-05-17 05:16:51.937765+03","You do not need GetAtt in Sub here Sub can resolve things like MyStateMachine Arn So you can just do something like In some cases you might still need to pass in parameters for instance when you need to call functions like ImportValue For instance if your state machine is defined in a different stack and is exported via otherstackMyStateMachineArn output you can do the following"
62512019,57920904,"stackoverflow.com",1,"2020-06-22 13:09:42+03","2024-05-17 05:16:54.906277+03","First those are warnings not errors A Serverless error would be something like This specific error happens because in the custom tag I have declared var optstage something which should be changed like I think in your case you need to update region like this I could not reproduce the last warning though but I reckon ENV variables should be defined in bitbucketpipelines yml or similar CI pipeline YAML under args or variables and then they can be accessed using envVAR "
57924138,57923950,"stackoverflow.com",3,"2019-09-13 16:01:22+03","2024-05-17 05:16:55.91739+03","You can use this plugin that allows setting up API Gateway service proxies very easily I am one of the collaborators serverless yml example"
58002349,57938583,"stackoverflow.com",0,"2019-09-19 03:53:38+03","2024-05-17 05:16:57.082798+03","I found the answer to my problem posting it here hopefully will help someone with the same problem The Problem was that I was using query to extract the id It was working fine in local but in the Server Side using Express you need to pass that as a parameter like this Then on the React component you can catch it and use it getInitial Props as req params id"
58184798,58137023,"stackoverflow.com",0,"2019-10-01 15:28:11+03","2024-05-17 05:16:57.89554+03","Because of Cache Capacity setting is global per stage it is not possible to set it per endpoint So the plugin is going to check this parameter only in the servelerless global configuration ignoring it at the endpoint level It means that the right configuration is"
58178325,58164754,"stackoverflow.com",0,"2019-10-01 08:03:56+03","2024-05-17 05:17:01.110781+03","If you cannot get this to work using pure Serverless framework functionality you could try an alternative approach Setup the bucket so that SNS notification is triggered on object creation You can then hook up your Serverless lambda to trigger from SNS See the docs here httpsdocs aws amazon comen_pvAmazonS3latestdevwaystoaddnotificationconfigtobucket html Then use SNS filtering to look for zip or whatever else you want to use to trigger only on the files you are interested in httpsserverless comframeworkdocsprovidersawseventssns good luck "
58200015,58182371,"stackoverflow.com",0,"2019-10-02 13:52:21+03","2024-05-17 05:17:02.025654+03","I found the answer my self I was confused in AWS we do not run it as server as we need to do in Azure Like in AWS we jut run serverless invoke command and it run the function and respond with an output but it does not work in Azure In Azure we need to run the server first by using the command and then we will be able to use our functions as a server Like we can make the calls as we are used to do with express or any other normal server "
58184018,58183513,"stackoverflow.com",0,"2019-10-07 14:13:42+03","2024-05-17 05:17:03.053814+03","I fixed it by generating a base64 zip instead of uint8array Before After"
58192307,58184322,"stackoverflow.com",2,"2019-10-02 00:10:06+03","2024-05-17 05:17:03.770335+03","The first hostname you want to set is called REST API id and is generated by API Gateway when creating the API The API used to create APIs in API Gateway does not offer the ability to specify the REST API id so no there is no way to specify the id The reason for that is probably that these ids are used as part of a public facing domain name As this domain name does not include an identifier for the AWS account it belongs to the ids have to be globally unique so AWS generates them to avoid collisions As AWS puts it emphasis by me For an edgeoptimized API the base URL is of the http[s] restapiid executeapi amazonaws comstage format where restapiid is the APIs id value generated by API Gateway You can assign a custom domain name for example apis example com as the APIs host name and call the API with a base URL of the httpsapis example commyApi format For the option to create a custom domain name you should consider that there is even some more complexity associated with it as you must provision a matching SSLcertificate for the domain as well While you can use ACM for that there is currently the limitation that SSLcertificates for CloudFront distributions which edgeoptimized API Gateway APIs use behind the scenes need to be issued in useast1 The option you already mentioned to export the API endpoint in the CloudFormation stack as output value and use that exported value in your other stack would work well As you noted that would create a dependency between the two stacks so once you deployed project 2 which uses the output value from project 1 you can only delete the CloudFormation stack for project 1 after the project 2 stack is either deleted or updated to not use the exported value anymore That can be a feature but from your description it sounds like it would not for your use case Something similar to exported stack output values would be to use some shared storage instead of making use of CloudFormations exported output values features What comes to mind here is the SSM Parameter Store which offers some integration into CloudFormation The integration makes it easy to read a parameter from the SSM Parameter Store in the stack of project 2 For writing the value to the Parameter Store in project 1 you would need to use a custom resource in your CloudFormation template There is at least one sample implementation for that available on Github As you can see there are multiple options available to solve your problem Which one to choose depends on your projects needs "
58292085,58184322,"stackoverflow.com",0,"2019-10-08 21:26:48+03","2024-05-17 05:17:03.775489+03","Question is it possible to construct and set the API Gateway id Answer No see the other answer to this question I was able to get the service endpoint of project 1 into the serverless yml file of project 2 though to finally construct the full URL of the service that I needed I am sharing this because it is an alternative solution that also works in my case In the serverless yml of project 2 you can refer to the service endpoint of project 1 via service_url cfservicenamestage ServiceEndpoint Example cfmyfirstservicedev ServiceEndpoint CloudFront exposes the ServiceEndpoint that contains the full URL so including the AWS Gateway REST API id More information in Serverless Framework documentation httpsserverless comframeworkdocsprovidersawsguidevariablesreferencecloudformationoutputs It seems that Serverless Framework is adding this ServiceEndpoint as stack output "
59217879,58188246,"stackoverflow.com",3,"2019-12-10 18:47:28+02","2024-05-17 05:17:04.710243+03","The short answer is yes Amazon state The Lambda teams policy is to support Long Term Support LTS versions of a runtime so NET Core 3 0 will not be natively supported on AWS Lambda That doesnt mean you cant use NET Core 3 0 on Lambda today though With the Amazon Lambda RuntimeSupport NuGet package you can use any version of NET Core including 3 0 This is possible because one of the great features of NET Core is the ability to package up an application as a completely self contained deployment bundle The following blog post shows how to use Amazon Lambda RuntimeSupport httpsaws amazon comblogsdeveloperannouncingamazonlambdaruntimesupport We have successfully deployed a Net Core 3 0 Web Api to AWS Serverless Heres how I did it Edit the project file to include the AWSProjectType with a value of Lambda in the PropertyGroup collection You can then deploy the ASP NET Core project to Lambda by rightclicking the project in Visual Studio and selecting Publish to AWS Lambda More info here httpsaws amazon comblogsdevelopernetcore30onlambdawithawslambdascustomruntime Note that this process may error if the webapi project you are publishing has references to other assemblies This is because in the deployment step 5 it attempts to rename all assemblies to bootstrap then you will need to Rename your projects assembly name in the csproj file to bootstrap without quotes Modify awslambdatoolsdefaults json so that the line becomes Note also that logging seems to have changed from Net 2 1 I tried to implement logging as per Net Core 2 1 That is and similar code in LambdaEntryPoint cs However this does not work Looking at the code it is trying to find the IConfiguration ImplementationInstance and this is null According to this httpsgithub comaspnetAspNetCoreissues14400 this breaking change is by design To get logging working in Net Core 3 you need to add code to Startup Configure "
63567961,58195895,"stackoverflow.com",2,"2020-08-24 22:59:52+03","2024-05-17 05:17:05.719565+03","I know that has been a while since this question was asked but I wanted anyway to write an answer Using the resource policy Using a private endpoint with a resource policy and a VPC Endpoint At the moment of writting this solution I am using the first example with extra IP Addresses because is the solution that I have in order to test with out QA Team the endoints from a fixed IP address I hope that this solution helped someone "
58199336,58198261,"stackoverflow.com",1,"2019-10-02 13:01:16+03","2024-05-17 05:17:06.369597+03","I was able to fix this by providing endpoint in the ApolloServer config Does not make much sense to me either "
58208270,58206844,"stackoverflow.com",1,"2019-10-02 22:35:54+03","2024-05-17 05:17:07.286683+03","DynamoDB has sub resources that often need access To ensure that you are also addressing those sub items I would recommend adding a wildcard onto the end of the resource To do this I prefer to use the serverlesspseudoparameters plugin you can install it quickly with serverless plugin install name serverlesspseudoparameters and then use it to more cleanly describe the resource like"
58224583,58223285,"stackoverflow.com",0,"2019-10-03 21:02:49+03","2024-05-17 05:17:07.980527+03","You can reset the users password by calling an admin API call not through the JWT token httpsdocs aws amazon comcognitouseridentitypoolslatestAPIReferenceAPI_AdminResetUserPassword html This will prompt the user for a new password This API call is to set a password for that particular user httpsdocs aws amazon comcognitouseridentitypoolslatestAPIReferenceAPI_AdminSetUserPassword html but I prefer the first option In order to change user attributes such as email birthday use httpsdocs aws amazon comcognitouseridentitypoolslatestAPIReferenceAPI_AdminUpdateUserAttributes html So all these are done using the Cognito Service inside the Lambda not to be confused with the JWT tokens "
58224767,58224566,"stackoverflow.com",1,"2019-10-03 21:53:02+03","2024-05-17 05:17:08.958014+03","At a very quick glance it looks like the indentation of your serverless yml file may be off Can you try again with a tab below the http array item Also you will need to remove the shorthand for method and path and instead use them separately The snippet below should work"
58235190,58233783,"stackoverflow.com",0,"2019-10-04 14:06:01+03","2024-05-17 05:17:09.373119+03","From the doc httpswww twilio comdocssmsquickstartnode twilioClient messages create does not have a callback params but returns a Promise Try to execute the Lambda with If this works then that was the issue Otherwise could be process env TWILIO_PHONE_NUMBER is not set correctly or appointment meta contact number may be missing or invalid Maybe print those also Twilio could also not have been correctly set up Make sure accountSid and authToken are set "
58241089,58240739,"stackoverflow.com",1,"2019-10-04 20:48:03+03","2024-05-17 05:17:10.36468+03","I think it is how you are setting stage This works for me when I run sls print s dev sls print s dev outputs"
58260382,58248023,"stackoverflow.com",0,"2019-10-06 22:01:14+03","2024-05-17 05:17:11.403381+03","serverlessstepfunctions depends directly on the Serverless Framework My guess is that it is resolving to the newest compatible version Try adding a new entry in your package json which should ensure that serverlessstepfunctions will not install a newer version of the framework "
58271899,58268430,"stackoverflow.com",6,"2019-10-07 17:42:15+03","2024-05-17 05:17:13.126173+03","Wherever you return a response from your Lambda function you need to include the specific header CORS requests The cors true option you add to serverless yml only helps make sure that the OPTIONS preflight requests work Do not forget that this includes nonsuccess responses as well For example"
58294935,58277011,"stackoverflow.com",6,"2019-10-09 01:48:21+03","2024-05-17 05:17:14.275617+03","I have figure out what was wrong The server side was Ok The issue on testing it on Postman was the Token I was using cognitoUser signInUserSession accessToken jwtToken but supposed to be cognitoUser signInUserSession idToken jwtToken Everything working as expected now "
64014672,58279355,"stackoverflow.com",6,"2020-09-22 20:23:17+03","2024-05-17 05:17:15.284298+03","I have just encountered this and overcome it I also have a lambda for which I want to attach an s3 event to an already existing bucket My place of work has recently tightened up AWS Account Security by the use of Permission Boundaries So i have encountered the very similar error during deployment If you read Using existing buckets on the serverless site it says NOTE Using the existing config will add an additional Lambda function and IAM Role to your stack The Lambda function backsup the Custom S3 Resource which is used to support existing S3 buckets In my case I needed to further customise this extra role that serverless creates so that it is also assigned the permission boundary my employer has defined should exist on all roles This happens in the resources section If your employer is using permission boundaries you will obviously need to know the correct ARN to use Some info on the serverless Resources config Have a look at your own serverless yaml you may already have a permission boundary defined in the provider section If so you will find it under rolePermissionsBoundary this was added in I think version 1 64 of serverless If so you can should be able to use that ARN in the resources sample I have posted here "
65078722,58279355,"stackoverflow.com",0,"2020-11-30 19:58:26+02","2024-05-17 05:17:15.288299+03","For testing purpose we can use"
59076325,58279355,"stackoverflow.com",1,"2019-11-27 20:18:49+02","2024-05-17 05:17:15.289299+03","For running sls deploy I would suggest you use a roleuserpolicy with Administrator privileges If you are restricted due to your InfoSec team or the like then I suggest you have your InfoSec team have a look at docs for AWS IAM Permission Requirements for Serverless Framework Deploy Heres a good link discussing it httpsgithub comserverlessserverlessissues1439 At the very least they should add iamCreateRole and that can get you unblocked for today Now I will address your individual questions can anyone confirm if that is the only solution if I want to use existing true Apples and oranges Your S3 configuration has nothing to do with your error message iamCreateRole must be added to the policy of whateverwhoever is doing sls deploy Also what is 1M5QQI6P2ZYUH in arnawsiamaccount_idrolebrazelambdasdevIamRoleCustomResourcesLambdaExec1M5QQI6P2ZYUH Is it a random identifier Does this mean that serverless will try to create a new role every time I try to deploy the function"
58291092,58281834,"stackoverflow.com",2,"2019-10-08 20:15:10+03","2024-05-17 05:17:16.028818+03","In order to use Google cloud functions please install the serverlessgooglecloudfunctions plugin You can generate a sample serverless yml using serverless create template googlenodejs path gcp It should look like this Heres the full quickstart guide httpsserverless comframeworkdocsprovidersgoogleguidequickstart"
59561890,58286081,"stackoverflow.com",2,"2020-01-02 12:35:11+02","2024-05-17 05:17:16.80462+03","AWS CDK allows to use custom resources which are executed lambda functions that run AWS SDK actions and allow consuming the results for further processing This would allow the following approach to search for the existing CF Distribution in typescript than you can search for the distribution using typescript Same can be done in the other cdk languages It should be possible to use the same find predicate as in the link you provided Sadly it does look like there is no way to import the Distribution into AWS CDK at the moment i will raise a feature request on the github repo I will update as soon as it is possible to import a CloudFront distribution "
66893480,58286081,"stackoverflow.com",1,"2021-04-02 00:23:00+03","2024-05-17 05:17:16.807621+03",""
58289387,58289295,"stackoverflow.com",1,"2019-10-08 18:19:55+03","2024-05-17 05:17:17.51659+03","You could create a function for path groupsdata and return 400 Bad Request But 404 is not a Bad Request code "
58311679,58311334,"stackoverflow.com",9,"2020-06-05 06:37:45+03","2024-05-17 05:17:18.418564+03","According to the documentation to get the Account Id you can use external js files "
58385197,58385146,"stackoverflow.com",0,"2019-10-15 02:21:50+03","2024-05-17 05:17:26.7342+03","It is recommended that you use a packages json for each service rather than try to use a single one shared across multiple services You can get additional details about the use of package json and the serverless framework here I have tried various techniques to share a single node_modules directory but the solutions are fragile and eventually something will go wrong as I introduce more dependencies and start to version the services I do try to share as much as possible across my services but have determined it is not worth the effort to share a single node_modules directory "
58962267,58311334,"stackoverflow.com",3,"2019-11-21 15:44:36+02","2024-05-17 05:17:18.419627+03","For anyone using Serverless with an assumed role where your IAM users are defined in a master AWS account and you are trying to deploy in a child account using a role from that child account the documented solution the one in the accepted answer above does not work This setup in described in detail here httpstheithollow com20180430managemultipleawsaccountswithroleswitching When using serverless with an awsprofile that is configured to assume a role defined in another account sts getCallerIdentity returns the account info of your master account from the default profile and not the account of the assumed role To get the account ID of the assumed role which is where we are deploying to I did the following Edit Found an even better way that is simpler than the one presented in Serverless docs and also works fine with assumed roles"
58311575,58311334,"stackoverflow.com",1,"2019-10-09 23:10:29+03","2024-05-17 05:17:18.421854+03","You should be able to access them below as per below example httpsserverless comframeworkdocsprovidersawsguidevariables"
58311676,58311334,"stackoverflow.com",1,"2019-10-09 23:18:18+03","2024-05-17 05:17:18.423268+03","It seems like your syntax is wrong Try Because at least in the example that you provided you are using AWSAccountId Notice the hashtag in your one"
59076090,58313959,"stackoverflow.com",0,"2019-11-27 20:05:16+02","2024-05-17 05:17:19.907879+03","I use a setup just like yours and it works great I think the error is correct in a way The CloudFrontDistribution as you pasted it is incomplete Therefore it was not created The error is poorly communicated to you because it looks like the plugin caught it the problem rather than the core app To prove my theory go ahead and try launching without the plugin being activated and you should see better errors related to an invalid AWSCloudFrontDistribution resource To resolve the problem configure a valid CloudFormation resource Use the example from here for inspiration httpsgithub comsilvermineserverlessplugincloudfrontlambdaedgeexamplecloudfrontstaticsiteserverlessconfig"
58368443,58315266,"stackoverflow.com",0,"2019-10-14 02:01:38+03","2024-05-17 05:17:20.009543+03","Need a NAT gateway IGWs are only for machines with public IPs already Second question posted to figure out how to mix NAT gateways with machines that have public IPs and an IGW in the same VPC "
62886393,58341292,"stackoverflow.com",2,"2020-07-14 03:39:42+03","2024-05-17 05:17:21.142715+03","Serverless functions are meant to be small selfcontained functions SQLAlchemy is an ORM which allows you to manipulate database objects like objects in python If you are just writing a few serverless functions that do you are average CRUD operations on a database you are better off writing the SQL by composing the strings and directly executing that through your database driver which you will have to install anyways even if you are using sqlalchemy If you are building your own framework on top of AWS Lambda then perhaps consider sqlalchemy "
64030658,58341292,"stackoverflow.com",1,"2020-09-23 17:53:12+03","2024-05-17 05:17:21.145716+03","From what I read SQLAlchemy performs in memory caching of data it has read and uses that for future calls Based on what you are doing it would be good to check out the SQLAlchemy caching strategy so another Lambda does not change the data from under the first lambda with SQLAlchemy "
58342195,58341292,"stackoverflow.com",4,"2019-10-12 12:11:10+03","2024-05-17 05:17:21.146716+03","Yes but you might want to consider AWS Dynamodb or other options first If SQL is your requirement then you can go for an RDS instance with an SQL compatible backend It all depends on your specific needs in this case It is not a general requirement If you just need to store some data and connect the data storage to AWS Lambda then DynamoDB is usually a good choice SQL can be preferred if you already have a relational database or if you need to perform certain SQL queries which are easier to do with SQL than a nonrelational model "
58343117,58342126,"stackoverflow.com",2,"2019-10-11 17:13:06+03","2024-05-17 05:17:21.703038+03","Do not know if there is a specific way to do this in serverless however just add a call to the AWS CLI like this to your build pipeline Where initialdata json looks something like this httpsdocs aws amazon comamazondynamodblatestdeveloperguideSampleData LoadData html"
58344665,58342126,"stackoverflow.com",0,"2019-10-11 18:48:13+03","2024-05-17 05:17:21.705039+03","A more Serverless Framework option is to use a tool like the serverlesspluginscripts plugin that allows you to add your own CLI commands to the deploy process by default httpsgithub commvilaserverlesspluginscripts"
58353225,58346601,"stackoverflow.com",1,"2019-10-12 14:57:31+03","2024-05-17 05:17:22.775009+03","There are two options that I can think of If you really want the endpoints the way you wrote you can use a general endpoint root endpoint and inside that handler check if the request is for admin and then redirect or do some other special case for that Give up on having a root endpoint and use two normal endpoints"
58346761,58346601,"stackoverflow.com",0,"2019-10-11 21:35:01+03","2024-05-17 05:17:22.777009+03","you should look into httpsdocs aws amazon comen_pvapigatewaylatestdeveloperguideapigatewayswaggerextensions html basically for open api 3 0 you would define a security key on your route the definition of such key is within components securitySchemas"
58373946,58368437,"stackoverflow.com",0,"2020-06-20 12:12:55+03","2024-05-17 05:17:23.811503+03","As it is not possible to attach public IP addresses to Lambda functions you have to launch them in private subnets and forward internet traffic to a NAT gatewayinstance to let your functions access the Internet It looks like you have only created public subnets in your VPC As you have already suggested you need to create private subnets that hosts your lambda functions Private subnet RT 0 0 0 00 NAT GW Public subnet RT 0 0 0 00 IGW"
58385363,58382779,"stackoverflow.com",59,"2019-10-15 02:45:32+03","2024-05-17 05:17:25.78022+03","Ive faced this issue with Serverless The issue is caused by how Serverless handles tracks and deploys your functions When you deleted the function you effectively changed the state of your application manually and it is effectively out of sync To resolve this comment out the function that is refusing to deploy and run sls deploy When its finished uncomment the function and sls deploy again This time it should deploy your function "
66713226,58382779,"stackoverflow.com",11,"2021-03-19 19:48:42+02","2024-05-17 05:17:25.782221+03","Use sls remove to remove everything from the stack and then run sls deploy"
68933365,58382779,"stackoverflow.com",4,"2022-04-07 14:05:43+03","2024-05-17 05:17:25.783041+03","Caution This will delete and recreate the whole stack including Databases if you have any so try at your own risk Use following command serverless remove stage environment region region eg serverless remove stage dev region useast1 The point is if you accidentlymanually delete anything from the stack then you break the stack You have to remove it in with above command and clean it up Another workaround would be to recreate the deleted component manually but again I suggest go with the above flow and now to break it "
66107830,58382779,"stackoverflow.com",1,"2021-02-08 21:09:52+02","2024-05-17 05:17:25.784614+03","Check your region filter on the menu at the top of the AWS console Make sure it is the same as the region you deploy to in serverless yml"
64750738,58382779,"stackoverflow.com",0,"2020-11-09 13:24:16+02","2024-05-17 05:17:25.785881+03","I had the same issue Cannot recall if it was because I changed the function name or because I added to serverless yml in the provider section I did this because I wanted to switch to the new API gateway naming Serverless was still tracking the old name so in order to reset this and be able to deploy again I did these 3 steps After that the serverless deploy command worked again "
60914904,60912592,"stackoverflow.com",0,"2020-03-29 16:31:04+03","2024-05-17 05:19:51.464151+03","You basically answered your question I dont have any iamRoleStatements You need to ensure proper IAM roles are supplied for lambda service and API gateway They do not come preconfigured "
59075902,58611513,"stackoverflow.com",0,"2019-11-27 19:52:27+02","2024-05-17 05:17:28.122582+03","No that is not possible as you described it However I do have an idea for you to consider but first a warning I must remind you what you seem to already know It is a generally unwise to update your configuration using multiple tools and methods Choose only one and stick with it Now to my idea When faced with these types of challenges I like to use a keyvalue store to glue multiple toolsmethods together Most people use AWS Parameter Store which is in the Systems Manager area Now this process that you mentioned is outofscope of serverless yaml can be updated with the awscli aws ssm putparameter parametername pathtothisserviceCloudFrontAliases value alias1alias2alias3 or AWS Console by a human person "
58615764,58615308,"stackoverflow.com",3,"2019-10-29 23:35:20+02","2024-05-17 05:17:28.648322+03","SAM is basically an extension of Cloudformation If you know SAM you basically know Cloudformation SAM can and should be used in conjunction w Cloudformation for testing locally Serverless is an abstraction layer on top of Cloudformation It helps to expedite app creation and deployment It falls short if you are doing more advanced configurations I always lean towards Cloudformation or SAM because it is provided by the CSP ie AWS This means everything new will be automatically available rather than waiting for an abstraction layer like serverless to bake in support "
58623545,58615308,"stackoverflow.com",2,"2019-10-30 21:33:59+02","2024-05-17 05:17:28.650322+03","I recommend the Serverless Framework The advantages are several There is a sample project at my gitlab profile if you are interested for GoLang but the principles are the same as for other runtimes httpsgitlab commontaoawslambdagogitlabsls"
58632662,58615308,"stackoverflow.com",2,"2019-10-30 21:47:37+02","2024-05-17 05:17:28.651696+03","LostJon is absolutely correct in that the support for both CloudFormation and SAM is immediately available as it is natively provided by AWS I am leaning to CloudFormation for the reason that is has a deterministic outcome which is desired in a DevOps landscape There are no transformations unless you explicitly add transformations to your CloudFormation stack When you deploy a CloudFormation template with SAM you will be able to view the CloudFormation template in the AWS console but you can also choose to show the rendered template e g after transformations so that you can learn what SAM does for you underwater You could copy the rendered resources and use them as pure CloudFormation This allows you to speed up development while not losing the benefit of knowledge of the underlying configuration "
71722369,58623612,"stackoverflow.com",9,"2022-04-03 04:39:36+03","2024-05-17 05:17:29.773682+03","Aurelia W mentioned it in a comment but I wanted to post it since it was the answer for me Make sure your function name matches what is in serverless yml It is too bad the error is not more descriptive "
58627650,58623612,"stackoverflow.com",8,"2019-10-30 16:29:50+02","2024-05-17 05:17:29.775683+03","Okay there seems to be a few things going on here First it is expected that your handler returns a function that can be called by lambda In this case you are calling the handler instead of returning the function to be called by lambda To solve this problem remove the parenthesis after run in task js There is another issue lambda handlers need to be asynchronous functions So you will need to add async in front of the function exported as run in analytics task js Now you can remove the callback argument The alternative would be to omit async but actually call the callback Since we tend to prefer newer node syntax let us remove the callback arg in favor of async "
58628774,58625457,"stackoverflow.com",0,"2019-10-30 17:28:09+02","2024-05-17 05:17:30.533696+03","Try the following"
58634039,58625457,"stackoverflow.com",0,"2019-10-30 23:40:59+02","2024-05-17 05:17:30.535697+03","Bars answer is good but I also figured that I could install all the dependencies with savedev flag and that is it "
68758198,58641916,"stackoverflow.com",3,"2021-08-19 09:41:30+03","2024-05-17 05:17:31.543491+03","Insert this permission under your provider tab in serverless yml"
59861158,58641916,"stackoverflow.com",2,"2020-01-24 19:35:21+02","2024-05-17 05:17:31.544466+03","To send message from AWS Lambda to SQS you need IM role attached to your Lambda to send messages to SQS Because in AWS you need permission for accessing other services resources"
58659179,58641916,"stackoverflow.com",1,"2019-11-01 14:22:15+02","2024-05-17 05:17:31.545768+03","Not sure if the wildcard is missing your permissions but from the code you posted you should only need the permissions to send message "
58658768,58645923,"stackoverflow.com",1,"2019-11-01 13:49:47+02","2024-05-17 05:17:32.658949+03","have a look at the httpsgithub commikesouzaserverlessimportapigateway from what you describe might help you with your use case"
59075738,58647423,"stackoverflow.com",0,"2019-11-27 19:42:07+02","2024-05-17 05:17:33.727099+03","I will do my best to address several areas of your post in order of their appearance serverlessoffline creates a docker image to launch a lambda Incorrect Serverless Framework and its plugins serverlessoffline etc have absolutely nothing to do with Docker or Docker related technologies I used localhost3306 for the db host but it does not work From your post I am gathering that you simply do not have a MySQL service running on your local machine Is that what you need Reply to this post and I will try to help or simply google examples of how to installstartconfigure a MySQL server I tried port forwarding to connect the database via public ip address which does not work I assume you are talking about the popular ssh L trick to connect to a remote database over SSH connection From your post I am gathering that you simply are not performing this operation correctly Do you need help doing that Reply to this post and I will try to help or simply google examples of how to use SSH Port Forwarding to connect to a MySQL database "
58659970,58659211,"stackoverflow.com",3,"2019-11-01 15:23:22+02","2024-05-17 05:17:34.876985+03","I am a Solutions Architect at Serverless Inc The scenario you mention only applies if you have an account at dashboard serverless com have set up a deployment profile in that account connected to your AWS account via a role you create in AWS and then set up one of your services to have an applicable app and org setting in the serverless yml Then on the CLI you run serverless login you login into your Serverless account which stores credentials on your local machine there is an alternate headless method for CICD systems etc Now when you run serverless deploy the framework sees the app and org settings sees you are logged in confirms that your user account is associated to that org and app checks to see if there is an access role set in the associated deployment profile and if so retrieves temporary access credentials from your AWS account for that specific deployment If you would like to see a video going through the process see here httpswww youtube comwatchvKTsWDCXvxqU"
58669206,58668987,"stackoverflow.com",3,"2019-11-02 09:46:51+02","2024-05-17 05:17:35.604719+03","It turns out that this was an indenting issue in my serverless yml The example blog did not indent path method or cors following http Thank you to httpsstackoverflow coma58226113296047 for pointing me in the right direction Side note after restarting my computer and trying this blog example from scratch I also received the error message Hopefully this helps someone else who is also going through the daylong troubleshooting I just went through "
58695727,58694087,"stackoverflow.com",4,"2019-11-04 16:44:01+02","2024-05-17 05:17:36.535689+03","You can use Signed URLs to achieve this Basically after your user successfully authenticated by Cognito your Lambda function will generate s3 presigned url for object requested and reply it back Example from AWS Documentation Also take a look at this AWS forum thread"
58729190,58694087,"stackoverflow.com",1,"2019-11-06 13:31:38+02","2024-05-17 05:17:36.53769+03","The only way that i know to do this is give the role access to the cognito user httpsdocs aws amazon comIAMlatestUserGuidereference_policies_examples_s3_cognitobucket html"
58704671,58703761,"stackoverflow.com",10,"2019-11-05 06:48:09+02","2024-05-17 05:17:37.636498+03","Lambda layers give you the ability to pull additional code and content into the execution environment of your AWS Lambda function By default the Node js Lambda runtime environment provides you with a version of the AWS JavaScript SDK But as a best practice and indeed because you may wish to use newer features of recent versions of the SDK that are not part of the version that is automatically included in the Lambda execution environment you can package up your desired AWS JavaScript SDK version into a Lambda later so that it will override the version of the SDK present in the execution environment Heres how This Lambda layers size is 6 1 megabytes as reported by the output from aws lambda publishlayerversion above "
58719915,58715708,"stackoverflow.com",1,"2019-11-06 00:05:06+02","2024-05-17 05:17:38.453737+03","By default in production mode Cube js disables dev server capability and it is why you do not see any Playground working at path httpscube devdocsdeploymentproductionmode Please use REST API to test your deployment httpscube devdocsrestapi "
58725074,58724832,"stackoverflow.com",1,"2019-11-06 09:28:14+02","2024-05-17 05:17:40.184267+03","Serverless as of now does not supports log retention for API Gateway However there are external pluginsworkaround that can be used to the same Links below httpsgithub comserverlessserverlessissues1918 httpsgithub comserverlessheavenserverlessawsaliasissues57"
58752840,58733599,"stackoverflow.com",1,"2019-11-07 18:14:21+02","2024-05-17 05:17:42.153351+03","Figured out the trigger for the bug When the lambda function zip uploaded is too large after the first time it times out it never recovers My solution was to carefully strip out the unnecessary dependencies to make the package smaller I created a repository using a docker container for people to reproduce the issue more easily httpsgithub compedrohbtpbugawslambdainfinitetimeout Thanks for the messages in the comments I appreciate whoever takes time to try to help here in SO "
70922049,58740591,"stackoverflow.com",3,"2022-01-31 08:35:52+02","2024-05-17 05:17:43.171836+03","Make a vercel json and paste the redirect rules code Save the file in your root directory then redeploy "
58753252,58747587,"stackoverflow.com",3,"2019-11-07 18:35:21+02","2024-05-17 05:17:44.191095+03","First you will need to export the resources from the corresponding nested stack like this To import the resource in other stack you will need to use the intrinsic function FnImportValue like this For more information check the AWS documentation"
58962200,58752602,"stackoverflow.com",1,"2019-11-20 21:44:44+02","2024-05-17 05:17:45.215227+03","You need to properly provide each possible browser support for those fonts Also on your lambda function replace the applicationxfontttf line by these Cheers"
58794542,58790372,"stackoverflow.com",1,"2019-11-11 03:34:26+02","2024-05-17 05:17:46.005273+03","Great question In order to share existing API Gateways you will need to declare the associated restApiId in your serverless yml file along with the restApiRootResourceId You can read more in the docs here You can also follow this excellent blog post"
58866176,58796904,"stackoverflow.com",2,"2019-11-14 22:41:29+02","2024-05-17 05:17:46.980584+03","Edit your serverless yml Try to add the vpc on the functions or in the provider section In specific function In all project To get the the security groups and the subnest you need to go to the VPC service on the aws console and find this information Visit the serverless documentation sls vpc docs"
58844086,58796904,"stackoverflow.com",1,"2019-11-13 21:17:57+02","2024-05-17 05:17:46.982584+03","If the VPCs were created through Cloudformation templates you can export a stack output value for each of your VPC Ids Then within your serverless yml file you can use the builtin intrinsic function FnImportValue to read the value of the exported vpc id If you are not using Cloudformation or cannot export the vpc id then I would suggest to add it as a parameter to serverless I do not personally see harm in having a parameter for VPC ID "
58800056,58799364,"stackoverflow.com",2,"2019-11-11 13:00:16+02","2024-05-17 05:17:47.854731+03","To get objects from S3 it is important to have permission to list the bucket you want to get the objects from In your iamRoleStatement add the permission to do so"
58800420,58799497,"stackoverflow.com",3,"2019-11-11 13:24:49+02","2024-05-17 05:17:48.958475+03","FnJoin is an intrinsic function and it appends a set of values into a single value separated by the specified delimiter If a delimiter is the empty string the set of values are concatenated with no delimiter You can use different intrinsic functions inside join and pseudo parameters as well For more information check the official AWS documentation"
58799960,58799497,"stackoverflow.com",1,"2019-11-11 12:51:42+02","2024-05-17 05:17:48.960269+03","FnJoin is a function in CloudFormation to concatenate strings Ref is another function to reference elements created in CloudFormation In you example FnJoin is used to concatenate strings to create the ARN of the S3 bucket whose format is arnawss3bucketname and is needed to give permissions to the bucket in Lambda "
58960015,58818746,"stackoverflow.com",11,"2019-11-20 19:19:25+02","2024-05-17 05:17:49.978048+03","I use serverless package for the purpose of testing variable resolution syntax checking plugin configuration etc If you have no use for the zip it creates you can just trash it afterwards "
58857717,58818746,"stackoverflow.com",1,"2019-11-14 15:16:19+02","2024-05-17 05:17:49.980049+03","You can use plugin serverlessoffline to run lambdas if that is what you are trying to do Usage serverless offline"
58857805,58837918,"stackoverflow.com",1,"2019-11-14 15:21:17+02","2024-05-17 05:17:50.828752+03","Yes This is very much possible Assuming you are using deploying from the same AWS account and Region Instead of manually creating resources use serverless to deploy these resources on AWS and use You can directly import these variable names in your main serverless yml file and set them to ENVIRONMENT variables like OPTION 2 Easier approach Or you can simply use plugin serverlesspluginsplitstacks"
58844398,58839769,"stackoverflow.com",2,"2019-11-13 21:38:41+02","2024-05-17 05:17:51.531156+03","One solution would be to globally configure your lambda functions to time out at the maximum 15 minutes currently Then your handler would need to fork your lambda process and have the parent process kill the child process which is where your actual application code will be after the userspecified amount of time "
58846000,58845165,"stackoverflow.com",0,"2019-11-13 23:37:24+02","2024-05-17 05:17:52.627021+03","Gah it was a bug in serverless appsync offline plugin It was emulating the return by fetching the items that it was writing and had a bug whereas it did not deal with composite keys "
58884290,58861573,"stackoverflow.com",0,"2019-11-15 22:32:45+02","2024-05-17 05:17:53.557653+03","The deployment event occurs when a deployment is started completed etc You can use Github Api to create a deployment POST reposownerrepodeployments further details here httpsdeveloper github comv3reposdeploymentscreateadeployment Alternatively you can use to run your workflow only if something is merged to master "
58873139,58871398,"stackoverflow.com",1,"2019-11-15 10:38:50+02","2024-05-17 05:17:55.050699+03","The problem appears to be in your serverless yml In particular in the functions specification The combination of path and method as well as the function name must be unique for each function So the serverless yml should look like"
60239935,58879822,"stackoverflow.com",2,"2020-02-15 16:56:36+02","2024-05-17 05:17:55.528865+03","Just ran into this exact issue and took a look into httphandler js in nodeslackevents All we have to do is store the raw request body as rawBody before serverlesshttp parses it serverlesshttp lets you transform the request before it is sent to the appgreat opportunity for a fix"
59243159,59177484,"stackoverflow.com",0,"2019-12-09 07:48:37+02","2024-05-17 05:18:14.96654+03","You can use the environment variables to construct the ARN value In your case you can define a variable in your provider section like below You might need to modify a little bit according to your application See this and serverless variables for aws for example "
58882178,58879822,"stackoverflow.com",0,"2019-11-15 19:42:27+02","2024-05-17 05:17:55.53075+03","I am not sure how to solve your problem exactly but I do know what is causing it The library you are using serverlesshttp parses the JSON body sent by Slack This causes an error to be thrown because the slackapisdk expects to parse the raw request body itself Could you try removing serverlesshttp and just respond to the API Gateway event"
58891789,58890867,"stackoverflow.com",1,"2019-11-16 16:36:45+02","2024-05-17 05:17:56.532384+03","Well I ended up taking some code from the AppSync emulator package I am not sure it covers it does a full parsing but it does the job for me "
58996271,58993864,"stackoverflow.com",7,"2019-11-22 16:27:24+02","2024-05-17 05:17:57.511317+03","Your use case simply is not possible with one API call if you want to stick with a serverless solution A possible serverless solution would be a 3 step process for the client Step 1 Call api1 to get a signed url for S3 This would point to a Lambda that creates a UUID and uses that UUID to construct a signed URL for S3 i e uses the UUID as the filename of the file being received The response would be the URL and the UUID Step 2 PUT file to s3 using the signed URL Step 3 Call api2 and pass the UUID and what ever other parameters are required This api also points to a Lambda which now knows where the file is thanks to the UUID and has whatever other parameters are required to process the file and give a response "
58996250,58993864,"stackoverflow.com",4,"2019-11-22 16:26:15+02","2024-05-17 05:17:57.512634+03","Have an endpoint on your API gateway that generates presigned URLs for uploading files to S3 Then your client application can invoke that endpoint to get the presigned URL after which it can upload the file to S3 Then you can have a Lambda function that is triggered by new objects in your S3 bucket that will read the file from S3 and process it To post extra parameters along with the file you have a few options Option 1 Post the extra parameters along with the initial request for the presigned URL Have the Lambda function that generates the presigned URL store those parameters somewhere like DynamoDB along with the S3 object key the presigned URL is being generated for Then when the other Lambda function is triggered by the new object appearing in S3 it can look up those extra parameters from DynamoDB Option 2 When uploading the file to S3 via the presigned URL your application can add extra header fields to the upload that will be stored on the S3 object as metadata "
59009388,59009255,"stackoverflow.com",4,"2019-11-23 17:58:06+02","2024-05-17 05:17:58.579305+03","First of all the structure of the code is wrong The header of Lambda function should have a certain structure either using async function or nonasync function Since you are using nonasync code in your example I will show you how to do the later In this case Lambda will finish only when callback is called or when it times out Similarly you can use async function but you will need to restructure your code accordingly Here is an example taken from official docs Note how the promise wrapper is used "
66592143,59009255,"stackoverflow.com",0,"2021-03-12 01:40:36+02","2024-05-17 05:17:58.581306+03","For AttributesToGet do not use username because it is one of the fields that always gets returned The following are members of the Attributes array and can be used in the AttributesToGet field sub email_verified phone_number_verified phone_number email e g "
59025356,59010177,"stackoverflow.com",2,"2019-11-25 06:43:08+02","2024-05-17 05:17:59.586545+03","I was able to solve this issue the lambda SQS triggers are getting created now Main Problems Observe the spaces before sqs events include exclude in the below serverless yml file sqs arnawssqsuseast1672851574246connectDeviceSQSDemo I have modified the serverless yml file it is working now "
59075471,59019870,"stackoverflow.com",1,"2019-11-27 19:23:24+02","2024-05-17 05:18:00.623501+03","I see you want to deploy a serverless website for your Static HTML content powered by AWSs S3 CloudFront and WAF services And that you seek to use the Serverless Framework to launch these resources However you make no mention of Lambda Serverless Frameworks primary use case is to deploy Lambda functions and resources related to these Lambda functions This includes the ones you mentioned S3 CF WAF etc I think you should take a look at Terraform for a configuration management tool Terraform does not specialize in anything and it is designed to handle everything I understand the confusion Serverless is a term that covers a few areas of modern cloud computing "
59028991,59025222,"stackoverflow.com",1,"2019-11-25 11:46:49+02","2024-05-17 05:18:01.50694+03","There is no such service and there is good reason for that API Gateway do actually wait until DynamoDB respond but DynamoDB respond immediately with a not found result Therefore the logic you are asking for would be more like This is not natively supported but you can do as you suggested use a Lambda to poll continously until you get your result Or even simpler Let the client call API Gateway continously until the result is found"
59161084,59034415,"stackoverflow.com",0,"2019-12-03 18:08:25+02","2024-05-17 05:18:02.622567+03","I see you are trying to resolve the error Trying to populate non string value into a string for variable In my experience this means that the variable is empty What happens if you hardcode the cognitoUserPoolId at the end of the ARN Is the error resolved I suspect it would be Moving forward from there you should take a closer look at how you declare that variable Your usage of FnImportValue may not working as intended Also I would definitely run your YAML through a validator There are too many extra blank lines and extra spaces before the colons e g COGNITO_USER_POOL_ID selfprovider cognitoUserPoolId These may be causing problems Keep your YAML formatting tidy "
59209728,59044024,"stackoverflow.com",1,"2019-12-06 16:17:33+02","2024-05-17 05:18:03.614628+03","The main features of AWS Lambda and Cloud Function can be found in httpscloud google comdocscompareawscomputefaas_comparison I can include the information of what I know that is Google Cloud Functions Triggers Cloud Functions can be triggered in two ways HTTP request or Eventtriggered Events and Triggers The events are things that happen into your project A file is updated in Cloud Storage or Cloud Firestore Other events are a Compute Engine instance VM is initialized or the source code is updated in your repository All these events can be the trigger of a Cloud Function This function when triggered is executed in a VM that will receive a HTTP request and context information to perform its duty Autoscaling and machinetype If the volume that arrives to a Cloud Function increases it autoscales That is that instead of having one VM executing one request at a time You will have more than one VMs that server one request at a time In any instance only one request at a time will be analyzed If you want more information you can check it on the official documentation "
59047581,59046596,"stackoverflow.com",3,"2019-11-26 11:24:23+02","2024-05-17 05:18:04.127932+03","It is really hard to tailor the permissions needed by the serverless In my opinion the developers should have full AWS access at least to test environments in order to learn and experiment If you are worried about accidental damage to production resources you can only make changes to production via CICD tools You may still have to temporarily grant permission to developers to do the initial configurations "
59241641,59177484,"stackoverflow.com",0,"2019-12-09 04:00:07+02","2024-05-17 05:18:14.968542+03","you can do this reference httpsgithub comandymac4182serverless_example"
59179934,59179144,"stackoverflow.com",1,"2019-12-04 17:54:37+02","2024-05-17 05:18:15.543367+03","So I found the error with the help of Andrewlohr Npm indicates the line where it stopped working For me it is parcel build index html To get more details about why parcel build index html didnt work I simply run the command line in the console without using npm Parcel told me the error came from a word on my css To simplify npm will just indicate where the problem comes from in the package Do not hesitate to execute the line yourself"
60141412,59046596,"stackoverflow.com",2,"2020-02-09 23:16:11+02","2024-05-17 05:18:04.129933+03","What you would want to do is use the Permissions Boundary feature provided by AWS With this feature the effective permissions the devs serverless stacks would have is the intersection of the policies defined in the IAM role and the permissions boundary For example lets say that you have allowed your devs to create IAM roles and policies for Lambda functions You also have defined a permissions boundary with only S3 read access Now if the devs were to create a serverless stack with Lambda permissions with the AdministratorAccess policy they would have to include the permissions boundary in the role and the effective permissions that your Lambda functions would have is just S3 read access There is a good blog post explaining this implementation in CloudFormation you can modify it to work with Serverless templates Hope this helps "
74565154,59046596,"stackoverflow.com",0,"2022-11-24 20:52:02+02","2024-05-17 05:18:04.132934+03","One developer friendly approach would be using several accounts combined with locking down the production account more than the testing account s developers use As mentioned in captainblacks previous answers a Permissions Boundary can be one tool to restrict potential damage in production We use it in addition to function specific roles SAST scanning in our CICD environment and enforce all newly created roles to adhere to the defined boundary in production For the Serverless Framework you can implement it like this [account] is your account id This way all roles serverless creates will inherit the permissions boundary from the provider settings "
59076106,59063801,"stackoverflow.com",1,"2019-11-27 20:06:32+02","2024-05-17 05:18:05.784434+03","This can be done as below Each individual would set their credentials under their own machine as below If you have different credentials for different stage then above serverless snippet can be implemented in parameterized way as below serverless yml Create configuseast1dev yml and configuseast1prod yml"
59075323,59063801,"stackoverflow.com",0,"2019-11-27 19:12:31+02","2024-05-17 05:18:05.786434+03","It sounds like you already know what to do but need a sanity check So I will tell you how I and everyone else I know handles this We prefix commands with AWS_PROFILE env var declared and we use stage names E g AWS_PROFILEmycompany sls deploy stage shailendra Google aws configure for examples on how to set up awscli that uses the AWS_PROFILE var We also name the stage with a unique ID e g your name This way you and your colleagues all have individual CloudFormation stacks that work independently of eachother and there will be no conflicts "
59293004,59072873,"stackoverflow.com",0,"2019-12-11 21:46:52+02","2024-05-17 05:18:06.755069+03","This is now supported in Serverless 1 59 0 httpsgithub comserverlessserverlessreleasestagv1 59 0 code change httpsgithub comserverlessserverlesspull7031 "
62684523,59086311,"stackoverflow.com",14,"2020-07-01 23:01:52+03","2024-05-17 05:18:07.799564+03","The issue I had was for one of the lambdas I had the abovementioned bucket as the event source so when some bucket is added as event source it actually creating that bucket as well therefore when it runs the actual creation related cloudformation it is saying the bucket already exists So I fixed it by only keeping the event source and removed the actual declaration of that bucket "
68286742,59086311,"stackoverflow.com",10,"2021-07-07 16:13:20+03","2024-05-17 05:18:07.800584+03","If you add existing true to the S3 config in your serverless yml file it will not try to create the S3 bucket like the below"
59414576,59086311,"stackoverflow.com",6,"2019-12-19 19:45:26+02","2024-05-17 05:18:07.80199+03","Anything involving CloudFormation or any other infrastructureincode is fussy and the error messages can mislead meaning there are a ton of things that can cause this problem see issues on GitHub like this one But in my experience the most common causes of these kind of problems are are not the preexisting bucket but problems with AWS credentials permissions or region that give misleading error messages To fix these or at least rule them out But as I mentioned CloudFormation is fussy There may be other problems to solve but try these first You may try them and still be beating your head against the wall but it will more likely be the right wall Hope this helps "
74057019,59086311,"stackoverflow.com",1,"2022-10-13 17:03:49+03","2024-05-17 05:18:07.803983+03","Try to use Conditional statements and pass them as a Parameter to create the bucket or not Follow the sample condition flow to decide wheather to create a resource or not See this for more details"
75433040,59086311,"stackoverflow.com",1,"2023-02-13 09:40:20+02","2024-05-17 05:18:07.805378+03","When deploying the BucketName must be unique across all regions So if anyone has already created a bucket with localbucketdev it will throw Try to just the BucketName to be unique I hope that helps "
76239543,59086311,"stackoverflow.com",1,"2023-05-12 22:52:31+03","2024-05-17 05:18:07.807376+03","Make sure the name you chose for the bucket is not a reserved name in AWS I had same problem with BucketName analyticss3bucket Changed the name and everything is fine"
59094290,59091832,"stackoverflow.com",1,"2020-10-07 15:27:44+03","2024-05-17 05:18:08.429701+03","serverlessdotenv currently does not have a blacklist option but they do have a whitelist option Example from httpsgithub comcolynbserverlessdotenvpluginpluginoptions If you use include only the variables you specified will be included Example from httpsgithub comcolynbserverlessdotenvpluginpluginoptions "
59110834,59100563,"stackoverflow.com",1,"2019-11-29 22:50:57+02","2024-05-17 05:18:09.572441+03","The AWS CLI is picking up credentials from either your shell or your awscredentials file If you want to Serverless and AWS to use default add export AWS_PROFILEdefault to your bashrc or equivalent shell config file "
61649006,59143125,"stackoverflow.com",1,"2020-05-07 05:46:35+03","2024-05-17 05:18:10.59357+03","After reading through the source code I was able to get an SNS hosted in AWS to trigger my local function as the documentation was not very clear Here is the configuration I used Example usage Use ngrok ngrok http 4002 Setup config like so"
59160928,59143125,"stackoverflow.com",0,"2019-12-03 17:59:22+02","2024-05-17 05:18:10.595267+03","After reading the docs at httpswww npmjs compackageserverlessofflinesnsinstallation I learned that those values are optional They are advanced settings for a more advanced configuration Simply delete them from your config and you will be good to go This will work just fine"
59180299,59155894,"stackoverflow.com",1,"2019-12-04 18:13:15+02","2024-05-17 05:18:11.931953+03","I see you are running into a problem which could be summarized as Environment Variables are always strings This is always handled in the code using for example process env and forcing the value to be an Integer Apparently the maintainers of serverlessstepfunctions have not done that yet I see you created a GitHub Issue to notify them of the problem thanks In the meantime until they fix it perhaps you should consider using another method to set different configuration data per stageenvironment Heres a code example that can work for you Read here for more detailed explanation on a blog article I found httpswww jeremydaly comhowtomanageserverlessenvironmentvariablesperstage"
59165459,59162562,"stackoverflow.com",1,"2019-12-03 23:16:32+02","2024-05-17 05:18:13.261178+03","Your issue may be as simple as having dynamodbGetItem This is a different permission than what listing all ie query or scan would be"
59163006,59162872,"stackoverflow.com",1,"2019-12-03 20:08:30+02","2024-05-17 05:18:13.726449+03","I am assuming you are behind a corporate firewall of some kind If so there are some documented options here httpsgithub comserverlessserverlessissues3256 First I would try NODE_TLS_REJECT_UNAUTHORIZED0 sls deploy If that works you can try some of the permanent solutions I linked to "
61834163,60685063,"stackoverflow.com",9,"2020-07-29 07:28:52+03","2024-05-17 05:19:32.222933+03","Found a solution with a custom CloudFormation resource template Edit Now working with serverlessdomainmanager "
59200751,59199386,"stackoverflow.com",1,"2019-12-05 19:52:47+02","2024-05-17 05:18:16.291836+03","The concept of an inlineCode parameter is supported by AWSServerlessFunction but not serverlessframework The YAML you pasted is not a 11 mapping to the AWSServerlessFunction it is specific to sls itself Store your code in filesdirectories until the sls team adds support for inlineCode I did not see any feature requests for it I am sure they would be glad to get one from you "
59209396,59199386,"stackoverflow.com",1,"2019-12-06 10:22:50+02","2024-05-17 05:18:16.293837+03","The workaround is to define lambda function deginition as normal cloudformation resource like that"
59377113,59211022,"stackoverflow.com",0,"2019-12-17 17:12:48+02","2024-05-17 05:18:17.346918+03","As jarmod in the comment said you are missing the kmsGenerateDataKey Here I am gonna show you what exact need to add to your existing yaml shown above And it is worth to note that if your code literally just use s3PutObject to upload you do not need to add EncryptDescribeKey permissions See httpsaws amazon compremiumsupportknowledgecenters3accessdeniederrorkms If your code involves multipart upload you do need kmsDescribeKey kmsEncrypt and more permissions like kmsReEncrypt kmsGenerateDataKey See details httpsdocs aws amazon comAmazonS3latestdevmpuAndPermissions html"
59375443,59211022,"stackoverflow.com",0,"2019-12-17 15:35:55+02","2024-05-17 05:18:17.349084+03","Try this If this works you know you were missing some action Its then a painful process of finding that missing action or if your happy just leave the s in "
59230759,59218643,"stackoverflow.com",0,"2019-12-08 00:23:44+02","2024-05-17 05:18:19.265122+03","As user mailtobash said we are missing a little bit of context to answer your question accurately Can you add what is inside req body when you send a request to your POST students endpoint You can get that value printed in your terminal when you send a request if you use console log req body in the code of your endpoint If you cannot see logs in your development environment you can have the lambda return req body before executing the rest of your code like so You will then be able to inspect that response in Postman httpswww getpostman com or in your client if you it is a webbased application you can go into the dev tools of your browser and look at the details of the request in the Network tab Knowing what is in requestBody will be very helpful In any case the error message indicates that studentId is the partition key of STUDENTS_TABLE and it is missing Partition keys are required and must be unique in a DynamoDB table Additionally if you are using an existing value dynamoDB put will overwrite the existing record but that behavior would not trigger an error It seems very likely that the reason why you get that error message from DynamoDB is because req body studentId is either undefined an empty string ie or any other value that DynamoDB would consider as missing when it comes to set the value of a partition key Do not hesitate to take a look at AWS documentation on primary keys httpsdocs aws amazon comamazondynamodblatestdeveloperguideHowItWorks CoreComponents htmlHowItWorks CoreComponents PrimaryKey"
59262815,59227110,"stackoverflow.com",7,"2020-09-08 18:26:28+03","2024-05-17 05:18:20.230628+03","Bit of a hackwork around but you can remove the cognito user pool from your serverless yml Deploy to AWS this will remove the pool Then add the details back into serverless yml including the changes you need then deploy to aws "
59244128,59243751,"stackoverflow.com",1,"2019-12-09 09:13:44+02","2024-05-17 05:18:22.064207+03","AWS API Gateway only accepts proxy syntax Link then I think serverless fw just support proxy and any If you want to just create a function to handle 2 api endpoint in this case the endpoints are POST authregister I think so POST authlogin Then you have setting in serverless yml like"
59316021,59243751,"stackoverflow.com",0,"2019-12-13 06:09:47+02","2024-05-17 05:18:22.066208+03","Thanks hoangdv your suggestion almost fixed the problem The issue was with path It should have been path auth proxy instead of path auth "
59262235,59250196,"stackoverflow.com",4,"2019-12-10 09:28:30+02","2024-05-17 05:18:22.888946+03","Found the issue facepalming because it took me hours to find it I had two problems My main lambda function had an async in front of it but I was implementing it with callbacks Removing the async fixed it My response format was missing the headers and isBase64Encoded fields Including that removed the 502 error see below Helpful links httpsdocs aws amazon comapigatewaylatestdeveloperguidesetuplambdaproxyintegrations htmlapigatewaysimpleproxyforlambdaoutputformat if you are using serverless framework httpsgithub comdheraultserverlessofflineissues405 If using API Gateway make sure your lambda functions response looks like the code snippet below Otherwise it will throw a 502 Malformed Lambda Function error "
59253259,59250196,"stackoverflow.com",1,"2019-12-09 18:48:19+02","2024-05-17 05:18:22.891946+03","If you are using proxy integration you need to be careful because for every possible syntax error it will throw an internal server error Can the error be not setting the headers here If not it might probably be a syntax error "
66753342,59250196,"stackoverflow.com",0,"2021-03-22 22:25:08+02","2024-05-17 05:18:22.892947+03","In our case we are using serverless API Gateway Lambda Our main app ts exports an async handler wrapped by serverless Ex EnvironmentService is just a service layer for the environment config Before modifying any code the 502 Bad Gateway Error log was showing this AWS CloudWatch Our solution was to override the default aws provider timeout in serverless yml S3Service file call to getObject "
59276168,59275935,"stackoverflow.com",1,"2019-12-11 00:16:16+02","2024-05-17 05:18:24.300382+03","This is a pretty good looking monorepo to me You are exactly correct about handling different environments Because Serverless applications are generally payperuse it is very easy and affordable to spin up multiple environments for testing QA Dev etc In addition to the environment variance you brought up many people also split application environments across different AWS accounts as well The Serverless Framework will handle naming your functions buckets and tables across stages It can also easily deploy to several different accounts on a perstage basis You should not need to adjust your directory structure at all You can deploy to a new stage with sls deploy stage stage name One question I receive often is how to share certain aspects of your business logic across your microservices For that I prefer to create another directory called shared and build a new npm package inside that directory with necessary shared logic Then in each microservice I use NPM to install the package by referencing the path This allows the Serverless Framework to find and install your local package when deploying or invoking the function locally The eventual downside of monorepos in my opinion is the complexity of whatever CICD solution you are using Most travisci circleci are not usually aware of monorepos for serverless functions and you can find yourself writing very complicated scripts to handle skipping services that did not change coordinating tests etc For that I would recommend trying out the new Serverless CICD tools Hope I have helped "
59758359,59279604,"stackoverflow.com",1,"2020-01-15 21:27:57+02","2024-05-17 05:18:24.951418+03","This flag config filename was introduced in the 1 45 serverless version So in order for this flag to work you need to update serverless "
59325500,59279604,"stackoverflow.com",0,"2019-12-13 17:29:04+02","2024-05-17 05:18:24.953419+03","This seems like a bug in the Serverless Framework but I think it is because you are passing f functionName to sls deploy Try again but remove f functionName as it is not needed on deployment a deployment will deploy all functions declared in your serverlessFileName yml file "
59300317,59299996,"stackoverflow.com",1,"2019-12-12 10:20:54+02","2024-05-17 05:18:26.084199+03","The HTTP API is not yet supported by the Serverless Framework although we are currently working on it You can track the effort here httpsgithub comserverlessserverlessissues7052"
59374724,59320980,"stackoverflow.com",0,"2019-12-17 15:13:39+02","2024-05-17 05:18:26.938784+03","Welcome to step functions They are unfortunately very bloated when written down I dont know if this helps but for something simple like this I will use one lambda function for all my tasks You can define your step functions with additional parameters to tell your function what to execute In your lambda function just setup some simple if statements I cant quite tell what youre trying to do in your question but you can also output variables from your lambda which then trigger different steps You can use choice states for this e g This is not a full solution as it would be too complicated to write down here But hopefully gives you some additional ideas to play with Also depending on how your state machine runs this method provides a performance boost as you minimise lambda cold starts "
59468086,59365944,"stackoverflow.com",1,"2019-12-24 13:23:55+02","2024-05-17 05:18:28.055408+03","You can try below steps to run sls package in command line task to create a deployment package and then use Azure Function App task to deploy to azure 1install specific version nodejs using Node js tool installer task _ 2 install serverless using npm task to run custom command 3 use npm task to run install command to install dependencies _ 4 Use command line task to run sls package to create the deployment package _ 5 use azure function app deploy task to deploy the deployment package"
59367509,59365944,"stackoverflow.com",0,"2019-12-17 06:08:55+02","2024-05-17 05:18:28.057408+03","Right now the Serverless Framework thinks you are trying to deploy your application using the Serverless Dashboard which does not yet support Azure I am not sure because it have not posted your serverless yml file but I think you will need to remove the app and org attributes from your serverless yml configuration file Then it will stop asking you to log in "
67675710,59365944,"stackoverflow.com",0,"2021-05-24 19:29:33+03","2024-05-17 05:18:28.059409+03","Using the serverless framework to deploy a function through DevOps gave me the same issue The problem is that the sls deplopy command will build package and deploy the code but will ask you for credentials each time you run the pipeline I solved this using the command serverless package in the build task after that I deployed the zip that was generated for the command with a normal web app deploy task "
59591228,59544511,"stackoverflow.com",0,"2020-01-04 15:40:26+02","2024-05-17 05:18:29.135364+03","I have figured this out and would like to share here for people who have similar confusion Based on the concept of Cognito Identity Pool authorizer aws_iam is always required You should assign proper policies to each IAM roles to define their accessibilities And through event requestContext identity cognitoAuthenticationType you can determine if the invoker is authorized or not its value could be either authenticated or unauthenticated "
59562267,59555246,"stackoverflow.com",1,"2020-01-02 13:04:54+02","2024-05-17 05:18:30.266206+03","I did this about a year back building a tool to essentially use Chrome Driver to walk a site in Lambda The issues I uncovered is because of issues with the included binaries of the underlying Lambda environment I cannot remember the exact details about how I corrected the issues I had but it was related to a specific version of the Chrome driver that did not require a specific set of libraries This version could be used in Lambda successfully What that version is I cannot remember but maybe that will point you in the right direction "
59653967,59555246,"stackoverflow.com",1,"2020-01-08 23:19:53+02","2024-05-17 05:18:30.267206+03","I am researching it right now I know for sure the guy from Ukraine did it with ProtractorJS He started 5k tests and they finished within 3 minutes the longest test time execution The video is in Russian language but most slides are in English I will update you if I will be able to implement it soon Or will simply add a post to qalesson com Keep trying "
59684066,59555246,"stackoverflow.com",1,"2020-01-10 16:54:20+02","2024-05-17 05:18:30.269207+03","You working with hard and complex task actually and it has a lot of hidden underwater rocks I have some similar project for mocha httpsgithub comXotabu4embarrassinglyparallelmocha but i did not finished it to be ready for public usage It also works with serverless framework as yours project First your issue is because you are running code with ES6 importexport which node 8 does not support The easiest solution would be just switch to old good requiremodule exports so you will not be needed webpackbabel to transform your code Another thing check this package it not documented at all but might point you to some solutions httpsgithub comwebdriveriowebdriveriotreemasterpackageswdiolambdarunner Btw you should also try Google Functions i know boris osipov did some experiments as well httpsgithub comBorisOsipovwdiogcloudfunctionexample"
59573016,59572960,"stackoverflow.com",0,"2020-01-03 05:59:21+02","2024-05-17 05:18:31.053433+03","The fix is quite trivial facepalm "
59587430,59585650,"stackoverflow.com",5,"2020-01-04 04:28:47+02","2024-05-17 05:18:31.949112+03","Have a look at this well documented AWS Blog on how to change attributes of Amazon Cognito user pool after creation httpsaws amazon compremiumsupportknowledgecentercognitochangeuserpoolattributes In Summary You have to recreate a new user pool with new attributes that you want and then use a lambda function to migrate users Sadly this seems to be the only way "
59617418,59616781,"stackoverflow.com",0,"2020-01-06 20:57:38+02","2024-05-17 05:18:32.901305+03","There are multiple reasons why the metro bundler does not start on it is own You have to be on the same network or there might be an issue with the adb Can you please refer to this webpage for more clarity on how to run the app on device It helped me out a lot while having issues "
59878782,59616781,"stackoverflow.com",0,"2020-01-23 14:38:27+02","2024-05-17 05:18:32.90329+03","The issue is with the react native cli I uninstalled the reactnativecli and use npx reactnative runandroid to run the app This solved the issue "
59880793,59616781,"stackoverflow.com",0,"2020-01-23 16:25:27+02","2024-05-17 05:18:32.904532+03","try this from your command prompt"
59626821,59619509,"stackoverflow.com",2,"2020-01-07 13:48:01+02","2024-05-17 05:18:34.498934+03","Try to add another propertie called certificateArn you can find certicateArn in the certificate manager detailed view of the domain"
61131532,59619509,"stackoverflow.com",0,"2020-04-10 01:54:52+03","2024-05-17 05:18:34.500702+03","It is actually notice the underscore "
59620643,59619965,"stackoverflow.com",0,"2020-01-07 02:23:11+02","2024-05-17 05:18:35.106321+03","Does your IAM have Read and Write Access to that Database and security group connected to the instance Is this the only table that seems to be the issue I had a similar issue ended up having to give my DB instance full read access "
59642341,59640726,"stackoverflow.com",5,"2020-01-08 10:53:24+02","2024-05-17 05:18:36.127096+03","This is correct you can access the parameter you defined in the serverless yml from event[pathParameters][title] inside the lambda hope this helps"
59681547,59673720,"stackoverflow.com",2,"2020-01-10 19:18:17+02","2024-05-17 05:18:37.236678+03","There is a communitysupported plugin that allows you to add the AWS Request Validator API Gateway functions to your Serverless project httpsserverless compluginsserverlessreqvalidatorplugin Here is an example from OP"
59684889,59673720,"stackoverflow.com",2,"2020-01-10 17:49:26+02","2024-05-17 05:18:37.238679+03","In addition to the other answer if you are looking for body validation you could also use the builtin request validation based on draft04 of JSON schema It is built into the Serverless Framework Docs httpsserverless comframeworkdocsprovidersawseventsapigatewayrequestschemavalidators Example httpsgithub comfernandomcschemavalidationdemo"
64335974,60685063,"stackoverflow.com",4,"2020-10-13 16:09:24+03","2024-05-17 05:19:32.225934+03","Thanks tpschmidt for the helpful response Just to add in case you want a BasePathMapping for your websocket domain add ApiMappingKey to the Cloudformation provided above E g "
59678430,59676343,"stackoverflow.com",1,"2020-01-14 11:23:47+02","2024-05-17 05:18:38.370435+03","The global setup does not work the way you are expecting it to work If you see the logs your beforeAll logs are coming after your test executes You should use different way to setup and teadown Jest has concept of globalSetup and globalTeardown and I guess that fits better in your case As part of this you can start and stop your server The config will look like this Read more here httpsjestjs iodocsenconfigurationglobalsetupstring And your bootstrap will looks like this"
59804424,59676343,"stackoverflow.com",0,"2020-01-18 22:14:09+02","2024-05-17 05:18:38.372435+03","I solved my issue using this config with help of 1st answer using globalSetup and globalTeardown Read more here httpsjestjs iodocsenconfigurationglobalsetupstring bootstrap js Find a sample on this link here httpsgithub combilalshaslstestjest P S globalTeardown do not works if test fails I will post solution once I have it "
61055419,59684624,"stackoverflow.com",1,"2020-06-20 12:12:55+03","2024-05-17 05:18:39.221422+03","This plugin silvermineserverlessplugincloudfrontlambdaedge will not help if you want to use an existing cloud front distribution It is only helpful if you are going to create a new one This issue has been already reported and as per the forum this functionality they are not supporting "
59742266,59684624,"stackoverflow.com",0,"2020-01-14 23:49:11+02","2024-05-17 05:18:39.224423+03","Lambda with ServerlessFramework is quite easy We use this plugin Please go directly to the plugin authors website for complete examples httpsgithub comsilvermineserverlessplugincloudfrontlambdaedge"
66919739,59684624,"stackoverflow.com",0,"2021-04-02 16:02:45+03","2024-05-17 05:18:39.225437+03","Base on your implementation you have a wrong indentation so I think it wont really attach it to your cloudfront Having a wrong indetation will not create an events on your lambda function so intead of this Do this I hope that this will solve your problem Because I encounter this wrong indentation myself and wander why it is not being implemented properly "
59727493,59690519,"stackoverflow.com",0,"2020-01-14 06:24:06+02","2024-05-17 05:18:40.244827+03","This actually happened to me previously I tried adding integration lambda to the serverless yml like so See if that works for you You can also try removing the cors true For some reason it works for me without the said flag "
59710564,59710528,"stackoverflow.com",5,"2020-01-13 06:26:03+02","2024-05-17 05:18:41.392808+03","The serverless yml will not be uploaded to the lambda function code therefore you would not have access to that file you can do this in few ways Define environment variables for lambda in serverless yml then you can access the environment variables in lambda Store the parameters in SSM parameter store and reference it in serverless yml Then in the lambda code Hope this helps Reference httpsserverless comframeworkdocsprovidersawsguidevariables"
59736494,59715831,"stackoverflow.com",2,"2020-01-14 17:06:10+02","2024-05-17 05:18:42.404652+03","I am not sure from your post but I gather that you want a way to handle canary deployments so that you could easily roll back changes If that is not the case could you edit your question and provide a bit more clarity If that is the case I would recommend following this guide and using the canarydeployments plugin which will automatically create aliases for new versions and allows you to define how traffic is shifted between deployed versions "
59791117,59735832,"stackoverflow.com",1,"2020-01-17 18:17:14+02","2024-05-17 05:18:43.499164+03","I was able to resolve the issue by using this CF template"
59762297,59762109,"stackoverflow.com",1,"2020-01-16 04:42:45+02","2024-05-17 05:18:45.305169+03","Looks like adding a GitHub secret for the private key and certificate works Just paste the certprivate key text into a GitHub secret e g Secret SIGNED_CERT Value BEGIN CERTIFICATE END CERTIFICATE Then in the GitHub Action Workflow Working directory if the serverless yml is not at that root level of the project "
59790027,59778964,"stackoverflow.com",1,"2020-01-17 17:10:34+02","2024-05-17 05:18:46.296187+03","You can do this by setting the value of response template However this is not done using an applicationjson key like request is you just set template directly Return a string foo Return JSON"
60203913,59778964,"stackoverflow.com",0,"2020-02-13 11:01:00+02","2024-05-17 05:18:46.298188+03","This is what I do to return mock response data "
59845750,59793303,"stackoverflow.com",3,"2020-01-21 19:08:32+02","2024-05-17 05:18:47.41513+03","What is your serverless version I am suspicious that you are using serverless version not supporting stream event syntax you are using For example maximumRetryAttempts is supported from version 1 60 0 serverless usually just ignore the not supported syntax not returning any error Try to check if your serverless version support what you want in here or just upgrade to the latest version and try again In addition you can check cloudformation file serverless create to deploy your project in serverlesscloudformationtemplateupdatestack json Check if the cloudformation is created as you expected with the file Edit I found MaximumRecordAgeInSeconds seems not supported now in serverless This is opened issue "
62287216,59793303,"stackoverflow.com",1,"2020-06-09 19:20:38+03","2024-05-17 05:18:47.418132+03","I just sent a PR implementing property MaximumRecordAgeInSeconds for Kinesis and DynamoDB streams httpsgithub comserverlessserverlesspull7833"
67958163,59800426,"stackoverflow.com",4,"2021-06-13 17:23:54+03","2024-05-17 05:18:48.535566+03","Serverless goes nuts when files are imported from outside the project directory To solve this problem you can now use projectDir"
59813915,59813541,"stackoverflow.com",2,"2020-01-19 22:02:01+02","2024-05-17 05:18:49.373804+03","Answering my own question it looks like I should have imported by output name not output export name which is bit weird and all the docs I have seen point to export name but this is how I was able to make it work replaced this authorizerId myAppservices selfprovider stage ExtApiGatewayAuthorizer selfprovider stage with authorizerId myAppservices selfprovider stage ApiGatewayAuthorizerId "
70180192,59813541,"stackoverflow.com",0,"2021-12-01 09:00:45+02","2024-05-17 05:18:49.375805+03","If you come across Trying to request a non exported variable from CloudFormation Stack name myAppservicestest Requested variable ExtApiGatewayAuthorizertest when exporting profile i e It must be done on the terminal window where you are doing sls deploy not on another terminal window It is a silly mistake but I do not want anyone else waste their time around that"
59907095,59824913,"stackoverflow.com",1,"2020-01-25 09:05:15+02","2024-05-17 05:18:50.414418+03","What errors do you get What does the generated serverlesscloudformationtemplateupdatestack json have for the export and import values I usually find it easier to use the internal Serverless CloudFormation property reference So where you are trying to import the SQS ARN do this cfSTACK_NAME InitializeAuthenticationQueueArnId where STACK_NAME is the name of the CloudFormation stack generated by the Serverless deployment this has the SQS ARN output Using this method the way you reference the value to import is via the CloudFormation key and not the export name which is admittedly confusing "
59841682,59841419,"stackoverflow.com",0,"2020-01-21 14:56:48+02","2024-05-17 05:18:52.508491+03","One way to help mitigate this issue is to not use the same AWS account for all your stages Take a look at the AWS Organisations feature that helps you create sub accounts to a master account and if you use Serverless Framework Pro even on the free tier you can easily have specific stages deploy to specific AWS accounts Each sub account has its own set of resources that do not affect other accounts You could even take this further if you have multiple ways of breaking things across multiple accounts perhaps you can break it up per Category"
63716424,59841419,"stackoverflow.com",0,"2020-09-03 06:36:18+03","2024-05-17 05:18:52.51056+03","Here is an example of a single CloudWatch rule with multiple targets each either an AWS Lamdba function or Lambda alias "
59842848,59842748,"stackoverflow.com",17,"2020-11-18 15:16:33+02","2024-05-17 05:18:53.281068+03","[202009 updated answer] As of v1 78 0 serverless now supports maximumRetryAttempts and maximumEventAge [202001 original answer] At the time of posting Serverless Framework does not yet support the configuration of Lambda retry counts Note that the underlying feature became available from AWS Lambda on Nov 25 2019 It is being tracked by Serverless as issues7012 "
59907114,59894351,"stackoverflow.com",0,"2020-01-25 09:08:06+02","2024-05-17 05:18:56.143652+03","By default Serverless will exclude devDependencies "
60011366,59904227,"stackoverflow.com",1,"2020-01-31 23:30:04+02","2024-05-17 05:18:56.8763+03","Fortunately the series of CLI commands you linked to can be reproduced in CloudFormation which can then be dropped into the Resources section of your Serverless template In this example a GET to function1 will invoke a lambda function while a GET to will return a 301 to a wellknown search engine Tested with Notes ApiGatewayRestApi is by convention the logical name of the API Gateway Stage resource created by Serverless on account of the http event Relevant CloudFormation documentation ApiGatewayMethod ApiGatewayMethod Integration EDIT This answer is not as verbose and uses an http event instead of the Resources section I have not tested it but it may also work for you "
60038431,59904227,"stackoverflow.com",0,"2020-02-03 13:34:49+02","2024-05-17 05:18:56.878621+03","Managed to achieve it with referring one function twice But see also reply of Mike Patrick looks more universal"
59907685,59907099,"stackoverflow.com",5,"2020-01-25 10:44:25+02","2024-05-17 05:18:57.779432+03","You can do it without having to pass secret key and access key by using aws IAM role Assign that role to your lambda trying to access your s3 bucket Remember to give your IAM role created for lambda permission to access s3 bucket "
59907183,59907099,"stackoverflow.com",0,"2020-01-25 09:21:18+02","2024-05-17 05:18:57.780868+03","what is the acl set on the object if it is set to public read access you do not have to pass credentials But make sure those objects does not have security or privacy concerns httpsboto3 amazonaws comv1documentationapilatestreferenceservicess3 htmlobjectacl"
59958573,59932690,"stackoverflow.com",42,"2020-02-20 18:47:48+02","2024-05-17 05:18:58.896726+03","FUNCTION_ERROR_INIT_FAILURE plainly means there is something wrong with the functions handlercode that i am trying to deploy wc is why provisioned lambdas cannot start upinitialize The way to resolve this is to test wo provisioned concurrency option first Once you are able to push your lambda error s will surely flow into your CW logs The best way though is to test your lambda locally using serverlessoffline plugin or serverless invoke if it works properly You can also package your app and invoke it with serverless cli to detect issues on packaging In my case there is a runtime error where my code bundle is looking for a require that is not part of bundle This is undocumented on AWS lambda as of now Jan 29 2020 "
59974254,59959002,"stackoverflow.com",1,"2020-04-07 17:02:11+03","2024-05-17 05:19:00.336443+03","It is possible to deploy a CDK app to an existing CloudFormation stack although it would be very difficult to achieve for nontrivial stacks since CDK apps usually involve many resources The cdk diff command will be your best friend You can name your stack in the CDK app using the same name as the existing stack Then you can iteratively addremove resources and run cdk diff to check your success in matching the current deployment CDK will additionally create metadata resources that will be added to the stack in addition to the currently existing resources Matching resource names can be difficult CDK automatically names many of the resources in a way that will not match you existing stack Following the instructions on CDK Escape Hatches you can access lower level CFN Resources directly and modify the name If a Construct is missing a feature or you are trying to work around an issue you can modify the CFN Resource that is encapsulated by the Construct All Constructs contain within them the corresponding CFN Resource For example the highlevel Bucket construct wraps the lowlevel CfnBucket construct Because the CfnBucket corresponds directly to the AWS CloudFormation resource it exposes all features that are available through AWS CloudFormation The basic approach to get access to the CFN Resource class is to use construct node defaultChild Python default_child cast it to the right type if necessary and modify its properties "
59976573,59969883,"stackoverflow.com",1,"2020-01-30 00:31:42+02","2024-05-17 05:19:00.848012+03","I actually think in this case it makes more sense for me to create a pub sub function and then use pub sub messages via the scheduler to trigger the function My main worry was a http endpoint that could be public The pub sub approach side steps this altogether "
60213237,60189853,"stackoverflow.com",0,"2020-02-13 19:12:59+02","2024-05-17 05:19:01.655907+03","This way there is no way for aws elasticsearch to know who you are You need to sign your request before sending it to aws elasticsearch You can use a package called httpawses which basicsally reads the aws credentials from your ec2lambda and signs the request for you Your code will look like this"
60218941,60197050,"stackoverflow.com",3,"2020-02-14 03:51:49+02","2024-05-17 05:19:02.634598+03","It looks like this is not really supported by the framework but it can be hacked together by ab using statusCodes in your serverless template Moving the response template to under a status code and providing a pattern for that status code accomplishes what I think you are after The syntax Note Both the pattern and the template must be present That is up to you ultimately I am calling it a hack because The offending code from libpluginsawspackagecompileeventsapiGatewaylibmethodintegration js I think for this to work under response template the code in the first if would need to behave more like the code in the second if "
72725795,60207668,"stackoverflow.com",5,"2022-06-23 09:57:52+03","2024-05-17 05:19:03.783845+03","Using const AWSXRay require awsxraysdk worked for me"
72778418,60207668,"stackoverflow.com",1,"2022-06-28 00:20:29+03","2024-05-17 05:19:03.785846+03","Please use This was explained on a similar issue on the AWS XRay JS SDK repo httpsgithub comawsawsxraysdknodeissues491"
60200251,60200117,"stackoverflow.com",3,"2020-02-13 05:57:42+02","2024-05-17 05:19:03.791847+03","In myAuthorizer handler function use callback style instead of using asyncawait style "
60263277,60228274,"stackoverflow.com",3,"2020-02-17 15:13:11+02","2024-05-17 05:19:04.796543+03","So I fixed it It means the role of deploying lambda does not have permission So it boils down to the fact to give it the role First confirm if you have the role Check in the image where to look for the role Once you do not find it Which you most likely will not Take the Role name and goto IAM Roles and Search for the role name and add AWSLambdaVPCAccessExecutionRole to the selected role This should give it the required permission Now try deploying the SLS and it should work Once you have the role edit it by adding the"
60250863,60228274,"stackoverflow.com",2,"2020-02-16 18:55:21+02","2024-05-17 05:19:04.798739+03","Although the user that you have created to deploy this lambda function has Administrator access the lambda function itself needs networking permissions if you are deploying it into a VPC Try adding these permissions in the provider block of your serverless yml template If that works you will want to deploy a more limited permission structure for your production environment "
60238704,60236910,"stackoverflow.com",2,"2020-02-15 14:20:47+02","2024-05-17 05:19:06.614994+03","If you can provide a name for your state machine add the stage you are deploying to as a part of that state machine name I usually name my resources with multiple variables to namespace them better so for example you could use selfservice myStateMachineName optstage dev In this way your statemachine will always be unique per service even if you reuse it on the same account and if you pass a stage during deployment such as serverless deploy stage mystage it will be unique or default dev if a stage is not specified "
60253027,60244846,"stackoverflow.com",0,"2020-02-16 22:44:44+02","2024-05-17 05:19:07.932263+03","I have resolved the issue just by chance I deleted my entire node_modules directory and ran npm install I believe because I am using the serverlessbundle package wires were getting crossed I can now run tests successfully using the serverlessbundle test command For reference here are my custom serverlessbundle settings in my serverless yml file"
60244904,60244874,"stackoverflow.com",0,"2020-02-16 05:09:00+02","2024-05-17 05:19:08.581495+03","try using const require and see if that works if it does what version of nodejs do you have installed and did you set it up to use es6 imports ES^ imports are still experimental as of node 13 httpsnodejs orgapiesm htmlesm_introduction httpsnodejs orgdistlatestv13 xdocsapimodules htmlmodules_module_exports"
60269338,60249157,"stackoverflow.com",2,"2020-02-17 21:37:48+02","2024-05-17 05:19:09.449939+03","Turns out there was nothing wrong with the above it is correct it is was me being a banana and not matching the full name of the table with the environment in the application i e notes table becomes devnotes for instance maybe the above will help someone "
60307098,60271261,"stackoverflow.com",0,"2020-02-27 07:01:11+02","2024-05-17 05:19:10.275333+03","In order to run serverless remove you will need to have the serverless yml file available which means the actual repository will need to be cloned or that file needs to get to GitLab in some way It is required to have a serverless yml configuration file available when you run serverless remove because the Serverless Framework allows users to provision infrastructure using not only the frameworks YML configuration but also additional resources like CloudFormation in AWS which may or may not live outside of the specified app or stage CF Stack entirely In fact you can also provision infrastructure into other providers as well AWS GCP Azure OpenWhisk or actually any combination of these So it is not sufficient to simply identify the stage name when running sls remove you will need the full serverless yml template "
61192631,60276092,"stackoverflow.com",0,"2020-04-13 19:48:06+03","2024-05-17 05:19:11.318072+03","I see you have added handler as handler publicindex php in serverless yml file but your file name is test php It seems to me like a typing mistake "
60287329,60285572,"stackoverflow.com",7,"2020-02-18 20:29:44+02","2024-05-17 05:19:12.43444+03","It is totally possible to mix the two and I have had to do so a few times How this looks actually ends up being simpler than it seems First off if you think about whatever you do with the Serverless Framework as developing microservices without the associated infrastructure management burden that takes it one step in the right direction Then what you can do is decide that everything that is required to make that microservice work internally is defined within that microservice as a part of the services configuration in the serverless yml whether that be DynamoDB tables Auth0 integrations Kinesis streams SQS SNS IAM permissions allocated to functions etc Keep that all defined as a part of that microservice Terraform not required Now think about what that and other microservices might need to interact with more broadly They are not critical for that services internal operation but are critical for integration into the rest of the organisations infrastructure This includes things like deployment IAM roles used by the Serverless Framework services to deploy into CloudFormation Relational Databases that have to be shared amongst multiple services and resources networking elements VPCs Security Groups etc monolithic clusters like ElasticSearch and Redis all of these elements are great candidates for definition outside of the Serverless Framework and work really well with Terraform Any resource would be able to connect to these Terraform defined resource as needed unlike that hard association such as Lambda functions triggered off of an API Gateway endpoint Hope that helps"
60316471,60311113,"stackoverflow.com",2,"2020-02-20 11:19:32+02","2024-05-17 05:19:13.508613+03","You can modify your configuration file If you specify the configuration file you can modify the name from res to return in it If you did not specify the configuration file the configuration file should be serverless yml yaml js json Please refer to the screenshots below Here provide two links screenshot1 screenshot2 for the screenshots above but I cannot find serverless yml for python sample the second screenshot is for nodejs you can also find more information about serverless in these two links Hope it helps"
60315494,60314382,"stackoverflow.com",1,"2020-02-20 16:07:16+02","2024-05-17 05:19:14.436875+03","S3 is returning a 403 because your items are private Your bucket policy should be "
68406363,60344618,"stackoverflow.com",10,"2023-02-10 02:27:42+02","2024-05-17 05:19:15.321854+03","You need to add an inline access policy like this to the SQS queue Source httpsdocs aws amazon comAmazonS3latestuserguidegrantdestinationspermissionstos3 html NOTE placeholder like region in the JSON need to be replaced Your queue can also use ServerSide Encryption In which case you also need to add a policy to the Customer KMS MUST be Customer KMS key default AWS keys will not work please read Why arent Amazon S3 event notifications delivered to an Amazon SQS queue that uses serverside encryption "
60357375,60344618,"stackoverflow.com",5,"2020-02-23 00:17:19+02","2024-05-17 05:19:15.323165+03","A lot of AWS configuration allows you to connect services and they fail at runtime if they do not have permission however S3 notification configuration does check some destinations for access In this case you have not allowed S3 to send messages to SQS It should be something like"
72643776,60344618,"stackoverflow.com",3,"2022-06-16 12:44:38+03","2024-05-17 05:19:15.324649+03","This tutorial helped me to solve the Unknown Error API response httpswww youtube comwatchvS7SFw8mMMTM As a summary what needed to be done is to delete this part in the access policy so the access policy will look like this Hope it helps"
71088561,60344618,"stackoverflow.com",2,"2022-02-12 15:39:52+02","2024-05-17 05:19:15.326647+03","This errror may be predominantly due to encryption enabled in the sqs queue The solution is either disable encryption in sqs or else use an encryption key with proper permissions to key the encryptdecrypt s3 notification "
71047233,60344618,"stackoverflow.com",1,"2022-02-09 11:44:33+02","2024-05-17 05:19:15.327647+03","I tried to create the event manually in the AWS console and got the same problem It seems this problem occurs if we register an event to encrypted SQS I tried to disable the SQS encryption to solve this problem Maybe for encrypted SQS further configuration is needed "
69267869,60344618,"stackoverflow.com",0,"2021-09-21 14:09:58+03","2024-05-17 05:19:15.329261+03","I was facing the same issue to give cross account access to publish S3 update message to my SQS It can be referenced from httpsdocs aws amazon comAmazonS3latestuserguidegrantdestinationspermissionstos3 html I have a CDK package and the following worked for me And we can set up the policy "
77062505,60344618,"stackoverflow.com",0,"2023-09-07 22:55:45+03","2024-05-17 05:19:15.33067+03","StringEquals awsSourceAccount bucketowneraccountid Deleting this string worked for me if you need additional assistance please see the following Youtube httpswww youtube comwatchvS7SFw8mMMTM"
77507894,60344618,"stackoverflow.com",0,"2023-11-28 21:29:59+02","2024-05-17 05:19:15.332668+03","I was getting similar error as OP For me the issue was with CloudFormation stack Execution Role I was trying to Update S3 adding sending Event Notification to Lambda Function but Stack Failed to create with similar Error Message Solution was to create Role for CloudFormation with Full permissions to Lambda and S3 of course only part of these were needed but it proves that issue was not in definitions in my case but rather in Stack Execution be aware Other thing that can help Occured for me same Error message Add an ResourceBased Permission to S3 so it can execute that function"
70409497,60344618,"stackoverflow.com",1,"2021-12-19 09:00:46+02","2024-05-17 05:19:15.334274+03","The AWSSourceAccount should be your Account ID Top right of the page clicking on your username See AWS Documentation"
73639620,60344618,"stackoverflow.com",1,"2022-09-07 20:39:31+03","2024-05-17 05:19:15.335272+03","While I would definitely check the policy on your SQS queue as the first thing the second thing would be to open your browsers developer tools and look at the actual request that the console sends back In our case the response identified a specific queue that no longer existed even though the AWS console UI just showed the generic some error happened message Additionally the queue causing the error was not the queue we were trying to target with our new rule it turns out that all the event notifications on a bucket are managed as one package So adding a new event notification is really updating the existing set of event notifications If your existing set of event notifications contains references to Queues or other targets that no longer exist in our case a QA environment that had been deleted long ago the API request to update the the set of notifications fails because it references queues that no longer exists The solution is to first delete the event notification rules that reference nonexistent targets then you can add a new rule "
60386875,60353154,"stackoverflow.com",0,"2020-02-25 04:55:46+02","2024-05-17 05:19:16.228647+03","The dataapiclient does not officially support Postgres yet httpsgithub comjeremydalydataapiclientissues27"
60362184,60361929,"stackoverflow.com",0,"2020-02-23 14:27:54+02","2024-05-17 05:19:17.017963+03",""
60361999,60361929,"stackoverflow.com",0,"2020-02-24 03:19:28+02","2024-05-17 05:19:17.019085+03","You are using callbackWaitsForEmptyEventLoop which basically let lambda function thinks that the work is not over yet Also you are wrapping it in promise but not resolving it You can simplify this logic using following inbuilt promise function on awssdk"
64098741,60368059,"stackoverflow.com",0,"2020-09-28 11:41:56+03","2024-05-17 05:19:17.885853+03","Try to use the Lambda Layers for your SAM application "
60401467,60387211,"stackoverflow.com",31,"2020-02-26 02:50:46+02","2024-05-17 05:19:19.472504+03","This is a great question I like to set the queue URL as an ENV var for my app So you have named the queue TheQueue Simply add this snippet to your serverless yml file Serverless will automatically grab the queue URL from your CloudFormation and inject it into your ENV Then you can access the param as"
60401567,60387211,"stackoverflow.com",4,"2020-02-25 21:04:35+02","2024-05-17 05:19:19.473504+03","You can use the Get Queue URL API though I tend to also pass it in to my function The QueueUrl is the Ref value for an SQS queue in CloudFormation so you can pretty easily get to it in your CloudFormation This handy cheat sheet is really helpful for working with CloudFormation attributes and refs "
60436480,60387211,"stackoverflow.com",1,"2020-02-27 17:21:43+02","2024-05-17 05:19:19.475098+03","I go a bit of a different route I personally do not like storing information in environment variables when using lambda though I really like Aaron Stuyvenberg solution Therefore I store information like this is AWS SSM Parameter store Then in my code I just call for it when needed Forgive my JS it has been a while since I did it I mostly do python There is probably some deconstruction of the returned data structure I got wrong but this is roughly what I do In python I wrote a library that does all of this with one line "
60420131,60419715,"stackoverflow.com",0,"2020-02-26 21:06:39+02","2024-05-17 05:19:19.76222+03","According to the official doc the signalr connection string needs to be on the binding schema httpsgithub comAzureazurefunctionssignalrserviceextension Define the SignalR Setting using the following AZ CLI Command httpslearn microsoft comenusazureazuresignalrsignalrconceptauthenticateoauth"
60442189,60439849,"stackoverflow.com",11,"2020-02-27 23:45:49+02","2024-05-17 05:19:20.961046+03","I am not sure of the best practice but the below works for me Provide functionName in the serverless function and when you want the arn you can form it test yaml When referencing"
60456267,60439849,"stackoverflow.com",6,"2020-02-28 19:00:59+02","2024-05-17 05:19:20.963046+03","Ok with thanks to Dinush for putting me on track I achieved a functional solution as follows Note The notation requires the plugin serverlesspseudoparameters I still think it is a shame that I could not simply use Ref and FnGetAtt as I would be able to if the lambda was created via a vanilla CloudFormation AWSLambdaFunction resource but this works for now Hope this helps someone else Thanks again Dinush for setting me on this course "
60457924,60457609,"stackoverflow.com",3,"2020-02-28 21:14:05+02","2024-05-17 05:19:22.764107+03","I believe this is a bug in the Standalone version of the Serverless Framework I have opened a PR to address this httpsgithub comserverlessserverlesspull7412 In the meantime this bug is caused by the fact that Serverless Framework v1 65 0 was just released You can resolve this by upgrading which will cause this erroneous codepath to not be called Thanks "
60461204,60457758,"stackoverflow.com",0,"2020-02-29 03:27:57+02","2024-05-17 05:19:24.080728+03","I forgot to include a separate userwritten file secret_manager py that had the logic for getting secret from secretmanager It had nothing to do with the PyPi binary "
60498569,60480496,"stackoverflow.com",1,"2020-03-03 02:45:50+02","2024-05-17 05:19:24.900711+03","You wont be able to use Ref as the resource was not created within this stack By setting vpc configuration in either the functions or provider block you are referencing existing resources If the existing resource was created using CloudFormation you could export it and make it available for import in this stack but if it was created manually its not possible "
60506537,60506343,"stackoverflow.com",3,"2020-03-03 13:45:07+02","2024-05-17 05:19:26.482594+03","As your error message what did you get you are mixing callback style with asyncawait style then it throws a warning I prefer use asyncawait This mean handler function always is a async function with async keyword then instead of call callback function just return the result and you no need callback parameter in handler function In error case just throw the error without trycatch block In simple"
60507223,60507222,"stackoverflow.com",3,"2020-03-03 14:24:07+02","2024-05-17 05:19:27.483555+03","After spending hours found that the latest version of uuid has a breaking change Hope this would help anyone who is facing the same problem If you are importing and using uuid as This will not work anymore as per the document here You will need to import it as or use a custom namespace This will solve the problem "
73795103,60507222,"stackoverflow.com",0,"2022-09-21 06:39:16+03","2024-05-17 05:19:27.485555+03","I fixed this issue using node 14 x AWS Lambda as follows"
66151716,60512454,"stackoverflow.com",0,"2021-02-11 11:07:52+02","2024-05-17 05:19:28.478453+03","nextjsapp component slsnext [email protected] inputs bucketName S3 bucket name use this in servereless yml and s3 bucket should be created in usesat1 N virginia and then deploy then a cloud front is created where the bucket is your s3 bucket after note the ID of cloud front and change the serverless yml as nextjsapp component slsnext [email protected] inputs bucketName S3 bucket name cloudfront distributionId"
60673518,60667491,"stackoverflow.com",0,"2020-03-13 17:50:42+02","2024-05-17 05:19:30.309675+03","Try to check if the Prod is configured in your AWS CLI Maybe in some point in the past you did something like aws configure and defined the default account check httpsdocs aws amazon comclilatestuserguideclichapconfigure html"
65625079,60685063,"stackoverflow.com",0,"2021-01-08 10:45:09+02","2024-05-17 05:19:32.226934+03","I had to make a couple of minor adjustments to have a Lambda in a private VPC with WebSockets support and a custom domain For your HostedZoneId check httpsdocs aws amazon comgenerallatestgrapigateway html"
60702836,60698968,"stackoverflow.com",2,"2020-05-12 01:17:03+03","2024-05-17 05:19:34.313198+03","Just needed to move nuxtpurgecss from my devDependencies to my dependencies since it is needed for the build "
65227524,61365361,"stackoverflow.com",1,"2020-12-10 04:06:32+02","2024-05-17 05:20:09.852165+03","Cognito UserPools are not unfortunately on the list of supported resource types for importing into CloudFormation templates httpsdocs aws amazon comAWSCloudFormationlatestUserGuideresourceimportsupportedresources html"
60691284,60684526,"stackoverflow.com",1,"2020-03-15 11:08:03+02","2024-05-17 05:19:31.331506+03","If you want the connection to remain after for subsequent requests to teh warmed Lambda function you should open the connection outside of the Lambda function itself within the same scope as the consts you declare at the top of your example The Serverless Framework is not the one closing anything this is a feature of Lambda itself In addition you are using a function called dbConnectAndExecute where it would probably be more useful to connect outside the function and then execute within i e break up that function This way your connection remains open but the execution is discarded after the Lambda executes One word of caution Watch out for too many open connections on your MongoDB cluster This is one of the reasons I prefer to use a service like DynamoDB where connections just do not exist everything is performed over an API call If you have 1000 simultaneously executing Lambda functions each will have its own connection and may cause failure in queries later "
61073892,60703916,"stackoverflow.com",2,"2020-04-07 09:19:58+03","2024-05-17 05:19:35.314801+03","It sounds like a good usecase for Localstack Localstack will spin up a local Docker instance that can act as a local AWS endpoint and support a lot of services and functionality out of the box Positives Negatives See this code as an example where a Lambda runs in its own Docker container but needs to access an offline EC2 instance during tests httpsgithub comspulecmotoblobmasterteststest_awslambdatest_lambda pyL55 Full disclose I am a Moto collaborator which is used by Localstack under the hood "
60707273,60706549,"stackoverflow.com",4,"2020-03-16 15:53:19+02","2024-05-17 05:19:36.312735+03","I guess body is already a json so if you do json parse again it will throw this error If you remove the json parse it should be ok "
63202117,60726608,"stackoverflow.com",1,"2020-08-01 08:30:28+03","2024-05-17 05:19:38.18158+03","The getJson function returns a callback so you cannot return from this Call the handler callback like this"
60735957,60735652,"stackoverflow.com",4,"2020-03-18 10:37:43+02","2024-05-17 05:19:39.194057+03","Based on the comments the problem was explicit deny From the docs If the code finds even one explicit deny that applies the code returns a final decision of Deny Basically explicit deny always wins over any allow "
60742078,60741608,"stackoverflow.com",1,"2020-03-18 16:56:08+02","2024-05-17 05:19:40.23697+03","This functionality would require the ability to inspect the current path in the structure Moreover it would require string manipulation since the source key is ServiceA but the target key is ServicecA The docs mention neither the ability to inspect the current path nor any functionality to insert that c into a given string So it seems to be very much not possible to do "
60768384,60768383,"stackoverflow.com",2,"2020-03-20 06:03:47+02","2024-05-17 05:19:40.982699+03","It so happens that despite rest of the System being on uswest2b region Oregon AWS Certificate Manager needs to be created in useast1a Atleast that is what worked for me "
60810532,60799506,"stackoverflow.com",0,"2020-03-23 11:06:36+02","2024-05-17 05:19:41.966062+03","I believe you might have missed to add VPC endpoints for API Gateway Ensure you create a VPC interface endpoint for API GW and use it in the API you create This will allow API requests to lambda running within VPC References Hope this helps "
68843109,60799506,"stackoverflow.com",0,"2021-08-23 11:10:22+03","2024-05-17 05:19:41.968063+03","SOLUTION We found a solution in our case the solution is that you cannot use wildcard certificates So if you make a certificate for the exact API URL it does work Obviously this is not a nice solution for everyone and everything but when you do have this problem and must deploy you can make it work this way Also maybe good to mention we did not have any additional problems with the below mentioned case I also wanted to add a post I found that might also have lead to this setup not working even if we do get the network to play nice But again if you or anyone has a working setup like the above mentioned one let me know how you did it TLDR If anyone wants to understand what was going on with API Gateway take a look at this thread It basically says that API Gateway processes regular URLs like aaaaaaaaaaaa executeapi useast1 amazonaws com differently than how it processes Custom Domain Name URLs like api myservice com So when API Gateway forwards your API request to your Lambda Function your Lambda Function will receive different path values depending on which type of your URL you used to invoke your API source Understanding AWS API Gateway Custom Domain Names"
60836763,60836613,"stackoverflow.com",2,"2020-03-24 20:01:03+02","2024-05-17 05:19:43.843947+03","The serverless folder is created by serverless to alocate the cloud formation files You should not handle them manually and the folder and its content should not be included in source control The serverless yml is the source of truth for the deployment so it should do the same if running with the same environments The AWS accountprofile can be set using the AWS cli Given all the devs use the same account or use accounts with the same level of permissions each one of you should be able to run deployremove If you project uses a env file or environmental variables each member of the team has to include them in their environment "
62048798,60859587,"stackoverflow.com",0,"2020-05-27 20:06:22+03","2024-05-17 05:19:44.747766+03","In case you are using NodeJs if not maybe something like this exists in your programming language I think you should take a look at this plugin httpswww npmjs compackageyamlinc It allow to compose YAML files using include tag Hope this helps you "
60958205,60869768,"stackoverflow.com",1,"2020-03-31 22:16:26+03","2024-05-17 05:19:45.698499+03","To deal with env variables in my lambdas i use this plugin httpswww npmjs compackageserverlessdotenvplugin It looks for a env file use it as a reference to your env variables Hope it helps "
60897869,60885819,"stackoverflow.com",5,"2020-04-01 20:49:45+03","2024-05-17 05:19:46.777932+03","One of the reason could be docker image you have used in gitlabci yml file But you want to deploy python lambda with some modules Try to use image which has both configuration python as well node like this image nikolaikpythonnodejs or if you want some specific version then you can do this also after you got error i tried to replicate it in my account so now i made some changes in serverless yml as well in gitlabci yml file here you can change service name in serverless yml and aws region in gitlabci yml after this it deployed in my account and worked fine and here is the test result Null is there because function is not returning any value but it printed the array here is my cloudwatch log Hope after these changes you will able to make it working "
60886284,60885819,"stackoverflow.com",0,"2020-03-27 14:43:20+02","2024-05-17 05:19:46.780933+03","It seems like you missed step about adding serverlesspythonrequirements plugin to serverless yml Try adding it as mentioned blog"
60890290,60888306,"stackoverflow.com",0,"2020-03-27 18:36:31+02","2024-05-17 05:19:47.895712+03","Unfortunately wildcards are not possible here Defining streams as a Lambda input in Serverless result in CloudFormation that looks like this The resource of type AWSLambdaEventSourceMapping uses the property EventSourceArn to specify a single source as per the documentation You could add new event source mappings programmatically by writing another Lambda Function that takes a CloudWatch event as its input whenever a new Table is created and then adds an EventSourceMapping for that table to your Lambda Function This will probably have scaling limits as the number of event source mappings per Lambda is most likely limited although I could not find the number "
60902291,60902127,"stackoverflow.com",1,"2020-03-28 16:57:03+02","2024-05-17 05:19:48.855863+03","Authorization needs to be implemented serverside Your serverless app will need to check for each request whether the user is authorized or not Any authorization features in your angular app only provide security through obscurity When your angular app connects to your serverless backend it does so through a http request It is theoretically relatively trivial to capture such a request and change it Nothing you do in your Angular app can protect you from this which is why you need to verify every single request The only functionality you should implement in your angular app is showing or hiding UI Elements depending on the users group and permissions Keep in mind though that this is only to make the app more accessible and does not increase security anyone can easily modify the apps code in their browser to show UI Elements that are meant to be hidden Make sure your server is secure and all requests are authorized in the backend You can find out more about the options of securing your serverless app here httpsdocs aws amazon comserverlessapplicationmodellatestdeveloperguideserverlesscontrollingaccesstoapis html"
74065843,74065616,"stackoverflow.com",0,"2022-10-14 10:35:23+03","2024-05-17 05:44:31.692206+03","Try add docker io in registriesskippingtagresolving How to add check here httpsknative devdocsservingconfigurationdeployment"
60926300,60919500,"stackoverflow.com",3,"2020-03-30 11:03:59+03","2024-05-17 05:19:52.524563+03","What you are likely seeing is that the server is returning an error before it reaches your return statements In other words API Gateway is returning a 502 or 403 as these are the most common errors and those specific returns have no CORS headers present by default in API Gateway I would recommend using CloudWatch to inspect the specific Lambda invocations to determine if your Lambda function itself is erroring out and correct that You can also force your API Gateway to return the CORS headers for all responses In the resources section of the serverless yml file you can do the following and add any additional entries for other status codes"
60921460,60921152,"stackoverflow.com",0,"2020-03-30 01:19:38+03","2024-05-17 05:19:53.584073+03","What you want is using CloudFormation Nested Stacks they allow you to reference another CloudFormation stack using the AWSCloudFormationStack resource type You can then provide parameters as an input to your nested stacks and retrieve their outputs once they are deployed this is how you communicate between main and sub stacks Below is an example of a nested stack reference Note that TemplateURL can point to S3 Bucket links or to a file on your filesystem Once you want to deploy your stacks they need to be packaged using the AWS CLI "
61028903,60921152,"stackoverflow.com",0,"2020-04-04 16:34:49+03","2024-05-17 05:19:53.586074+03","serverlesspluginsplitstacks is the plugin to be use Docs is here serverless split stack it will help you to split the stack into nested stacks max limit is 20 "
76940917,60932559,"stackoverflow.com",0,"2023-08-20 22:00:22+03","2024-05-17 05:19:54.625812+03","I had the same issue I solved it by adding multipartformdata in the API gateway settings Setting multipartformdata in the Binay Media Settings allows the file to be actually written to the stream Thus should fix your problem too since I can see you have the same flow Alsodo not forget to deploy the API again Let me know if it does not work and add details if any "
60936931,60936623,"stackoverflow.com",2,"2020-03-30 21:19:10+03","2024-05-17 05:19:55.743592+03","I have had some good success using the serverlesspluginsplitstacks plugin You can split your stacks by a few methods but I would imagine for your situation you would want to split per Lambda"
60960020,60959720,"stackoverflow.com",0,"2020-04-01 00:16:18+03","2024-05-17 05:19:56.381391+03","According to httpsdocs aws amazon comIAMlatestUserGuidelist_amazondynamodb htmlamazondynamodbtable you need to add the ARN for the table to the dynamodbQuery IAM statement "
60973632,60959720,"stackoverflow.com",0,"2020-04-01 17:46:58+03","2024-05-17 05:19:56.383392+03","The problem was the indentation The iamRoleStatements must be a child of the provider Here my working config"
65724189,60968313,"stackoverflow.com",0,"2021-01-14 19:42:45+02","2024-05-17 05:19:57.519406+03","Try this"
60976975,60973151,"stackoverflow.com",1,"2020-04-02 20:02:51+03","2024-05-17 05:19:58.520962+03","You could actually create a variable in custom with the arn of the sns topic then just use the arn on the places you need Or you can use the plugin httpsgithub comsilvermineserverlesspluginexternalsnsevents to basic reference the topic name If you have only 1 serverless yml and do not want to have a separate cloudformation file I would use the first option EDIT To use the arn follow the instructions on serverless httpsserverless comframeworkdocsprovidersawseventssnsusingapreexistingtopic since you have the sns topic on the same serverless yml you can even ignore the snsTopicArn variable and build it like one of the suggestions using Ref which should be a better option for you full example"
61002651,60973151,"stackoverflow.com",1,"2020-04-03 02:11:18+03","2024-05-17 05:19:58.522963+03","Finally got it I end up removing my SNS TOPIC declaration from resource section of serverless yml added under iamRoleStatements something like this And added variables in the custom section then mapped this to the lambda function events For reference link"
66396004,60997930,"stackoverflow.com",1,"2021-02-27 07:51:11+02","2024-05-17 05:19:59.637683+03","I have not tried this myself but I think you need to use a proxy integration to get the multiValueQueryStringParameters field httpsaws amazon comblogscomputesupportformultivalueparametersinamazonapigateway httpsdocs aws amazon comapigatewaylatestdeveloperguidesetuplambdaproxyintegrations html Note when selecting an integration you can choose the payload version If you select 2 there are no such proxy restrictions but instead of getting multiValueQueryStringParameters your values are concatenated together in a commadelimited list filePath abcd"
74296188,60997930,"stackoverflow.com",0,"2022-11-03 00:18:32+02","2024-05-17 05:19:59.640684+03","The correct property name is multiValueHeaders for alb in serverless to support multi value query parameters But for some reason the serverless deploy does not update the relevant Multi value headers attribute in target group even if we added this property to serverless yaml But we can manually update the attribute as in httpsdocs aws amazon comelasticloadbalancinglatestapplicationlambdafunctions htmlenablemultivalueheaders After this though we start to get the multiValueQueryStringParameters in the event object but the usual field queryStringParameters is not available Also it is mandatory that response object has to add additional multipleValueHeaders property as per httpsdocs aws amazon comelasticloadbalancinglatestapplicationlambdafunctions htmlmultivalueheadersresponse to make things works properly "
61034329,60998835,"stackoverflow.com",2,"2020-04-04 23:25:51+03","2024-05-17 05:20:00.837878+03","im aware what trigger the issue very strange bug has very strange solution Try lo disable Serverless Framework Enterprise if its enabled you can just comment the tenant and app rows into your serverless yml file and deploy the app again I think that theres a bug in the last version of the serverlesssdk "
61013639,61013073,"stackoverflow.com",2,"2020-04-03 17:03:37+03","2024-05-17 05:20:00.868333+03","You can use serverless print which will print it to your CLI docs"
61308602,61292542,"stackoverflow.com",1,"2020-04-19 20:22:16+03","2024-05-17 05:20:02.841753+03","From what I understand you should be able to handle this by spliting the gateway Would recommend to first create a serverless yml with the deployment of shared parts API Gateway and authorizer The remaining parts would split depending on the amount of resources There is also an explanation here httpsserverless comframeworkdocsprovidersawseventsapigatewayeasiestandcicdfriendlyexampleofusingsharedapigatewayandapiresources You should also consider export your authorizer arn into something that you can reuse later in the other serverless yamls maybe ssm"
61353664,61314096,"stackoverflow.com",0,"2020-04-22 00:34:02+03","2024-05-17 05:20:03.853272+03","putfunctioneventinvokeconfig accepts ARNs which means one could query CFN based on stacknames which would correspond to the repo that it was deployed from However I decided to use listfunctions to query for Lambdas and then listtags because our deploys are tagged by repo names It seemed like a better option than to query CFN also CFN output ARNs contain a suffix which means putfunctioneventinvokeconfig will not run on them Then I can run the text output through a for loop in bash and use putfunctioneventinvokeconfig to add the maximumeventage setting "
61331781,61331780,"stackoverflow.com",0,"2020-04-20 23:47:52+03","2024-05-17 05:20:04.92513+03","Custom properties are available at Thus for my foo property I would use or"
61350511,61348586,"stackoverflow.com",0,"2020-04-21 21:26:30+03","2024-05-17 05:20:06.661538+03","The stubbed function is never called in the codesample you provided If you actually call the funtion with the test should pass as expected "
61457676,61360504,"stackoverflow.com",2,"2020-04-27 14:24:37+03","2024-05-17 05:20:08.810037+03","and regarding your folder structure if you can do something like this and give references for those function in main serverless yml file like this then you do not need to go inside each folder to execute serverless yml file will be info for more info you can check here"
61429179,61399578,"stackoverflow.com",1,"2020-04-25 19:48:49+03","2024-05-17 05:20:10.534105+03","Your iam roles seem to be configured correctly Double check your indentation on the roles are correct at first I would say it is probably missing one space at least comparing the environment definitions"
61492086,61408391,"stackoverflow.com",1,"2020-04-29 03:10:25+03","2024-05-17 05:20:11.592425+03","I was able to fix this contacted serverless support and they said there was a bug and now it has been fixed Just need to update serverless and it was resolved "
61441047,61408391,"stackoverflow.com",0,"2020-04-26 15:48:07+03","2024-05-17 05:20:11.594426+03","Looks like you enabled the i option of curl Has curl an alias"
61432681,61413855,"stackoverflow.com",2,"2020-04-26 00:11:02+03","2024-05-17 05:20:12.677846+03","As far as I know serverless blocks the number of versions to 5 from this PR httpsgithub comserverlessserverlesspull4134files Did a quick test and you can backup the builds to other bucket and when you want to rollback you just need to move them again to serverless bucket then if you do list and rollback you can use them again This is actually an interesting case that could be easily done in a serverless plugin "
61416856,61413855,"stackoverflow.com",0,"2020-04-25 19:57:11+03","2024-05-17 05:20:12.679623+03","what is the framework you are using to deploy I am using the serverless framework and this framework tried to hold all versions which I deployed by default That pushed me to use a special plugin for clearing the storage after every deploy because storage is limited 75 GB httpsserverless compluginsserverlesspruneplugin"
61432823,61430215,"stackoverflow.com",7,"2020-04-26 00:25:07+03","2024-05-17 05:20:13.568866+03","This normally means that it can not find the method that is the starting point to execute For example on your serverless yml you can have something like this this would mean that it is required to have a userFile in the same folder of the serverless yml with the method handler exported Note that it does not need to be named handler function just needs to have the same name defined on the serverless yml"
71425396,61430215,"stackoverflow.com",0,"2022-03-10 16:01:06+02","2024-05-17 05:20:13.570453+03","I ran into the same error when launching a lambda locally using AWS sam with Webstorm Turns out a previous run had not correctly stopped and destroyed the docker container running the lambda Stopping and removing said docker container fixed the problem for me "
61451415,61447000,"stackoverflow.com",2,"2020-04-27 06:48:54+03","2024-05-17 05:20:14.347586+03","Just add corstrue under function events for all events to access them via cross origin in your serverless yml file like and redeploy the lambda "
61478545,61477850,"stackoverflow.com",1,"2020-04-28 13:57:00+03","2024-05-17 05:20:15.276804+03","Quick search through this doc did not amount to anything so a solution I have in mind would be this"
61487816,61485067,"stackoverflow.com",1,"2020-04-28 21:54:27+03","2024-05-17 05:20:16.49116+03","you can solve this by adding the plugin httpsgithub comjanders223serverlessofflinessm if you are feeling more adventurous you can also use localstack httpsgithub comlocalstacklocalstack note that free version does not support everything"
61515180,61514264,"stackoverflow.com",0,"2020-04-30 06:11:17+03","2024-05-17 05:20:17.893711+03","You can use a workaround to get your way through these syntax caveats In this case I would suggest you to create a custom node to set variables you would want to reuse You can then reference these variables using Serverless Framework syntax only to avoid that error like so"
61536014,61535864,"stackoverflow.com",1,"2020-05-01 05:41:06+03","2024-05-17 05:20:19.046689+03","I am sorry I found a solution Protocol parameter should be lowercase sms The code should be like this "
62736794,61614430,"stackoverflow.com",9,"2020-07-05 07:21:18+03","2024-05-17 05:20:20.889966+03","You can send parameters via query string as follow Then you can get that param from event queryStringParameters in the function handler Hope that helps "
61627127,61614430,"stackoverflow.com",1,"2020-05-06 06:53:11+03","2024-05-17 05:20:20.890759+03","Send the value as header in your connect call and retrieve it in backend to store alongside connectionId In my lambda the headers looked like"
61619800,61619575,"stackoverflow.com",1,"2020-05-05 20:58:17+03","2024-05-17 05:20:22.532412+03","The issue is with your lambda function You have to send back the SUCCESS or FAILURE signals back to the CFN Since your lambda function is nots sending any signals its waiting for Timeout 2 hours and the Cloudformation gets failed Please use cfnresponse module in your lambda function to send the SUCCESSFAILURE signals back to your Cloudformation For more details httpsdocs aws amazon comAWSCloudFormationlatestUserGuidecfnlambdafunctioncodecfnresponsemodule html"
61632539,61619575,"stackoverflow.com",1,"2020-05-06 13:10:09+03","2024-05-17 05:20:22.534332+03","I finally managed to find a solution to the issue albeit it is not explaining the strange behavior with the charts that I explained in the question My problem was similar to what Abhinaya suggested in her response The Lambda function was not sending the signal properly because of a programming error Essentially I took the code from the documentation the one for Python 3 second fragment starting by the end and apparently I mistakenly removed the line for retrieving the ResponseURL Of course that was failing A sidecomment about this be careful when using Pythons cfnresponse library or even the code snippet I linked in the documentation It relies on botocore vendored which was deprecated and no longer exist in latest botocore releases Therefore it will fail if your code relies on new versions of this library as in my case A simple solution is to replace botocore vendored requests with the requests library Still there is some strange behavior that I cannot understand On creation the Lambda function is not recording anything to CloudWatch and there is this strange behavior in the charts that I explained in my question However this only happens on creation If the function is manually invoked or is invoked as part of the delete process when removing the CFN stack then it does write to CloudWatch Therefore the problem only occurs in the first invokation apparently Best "
61939271,61624741,"stackoverflow.com",1,"2020-05-22 09:00:51+03","2024-05-17 05:20:22.71203+03","This is because one of your dependencies is trying to access the Event object which is not available by default server side Since you are already using domino you can try adding these lines to your server ts to make Event object available"
61638231,61637362,"stackoverflow.com",1,"2020-05-06 17:48:30+03","2024-05-17 05:20:23.505362+03","Normally you do not develop and deploy a lambda function in isolation instead it is one part of your cloud infrastructure That can include other lambdas S3 buckets databases API Gateways IAM roles environment variables and much more Serverless framework is allows you to write your infrastructure as code For AWS services it translates serverless yaml config files into AWS cloudformation files and from there deploys any new or updated services you define You lambda function is just one part of that A major benefit of writing and deploying this way is that you can use your favourite editor locally and can check your code into version control i e git This is not just for your lambda code but also your infrastructure config i e serverless yaml and associated files "
61984218,61957969,"stackoverflow.com",1,"2020-05-26 20:27:12+03","2024-05-17 05:20:38.374605+03","I was able to deploy the Azure functions by breaking down the deployment in two parts Package the project using serverless Deploy the packaged zip file using Azure CLI Hope this helps someone who is facing the same issue Edit Looks like this issue might get resolved in upcoming releases Link httpsgithub comserverlessserverlessazurefunctionsissues469"
62392738,62347645,"stackoverflow.com",0,"2020-06-15 19:31:30+03","2024-05-17 05:21:03.859654+03","I found an article about sharing resources between different serverless projects and it seems that we can just define the resource as S3SharedBucketArtifacts instead of NewResource and that will do the trick Code will be Reference How to reuse an AWS S3 bucket for multiple Serverless Framework"
61638337,61637362,"stackoverflow.com",1,"2020-05-06 17:53:14+03","2024-05-17 05:20:23.507363+03","The Serverless Framework is more than just a replacement for the AWS Console GUI You can definitely set everything up via the AWS console for a Serverless application but how do you share that with your team What if you wish to deploy that repeatedly into multiple applications The Serverless Framework gives you a configuration file usually called serverless yml where you define all the services within AWS and other vendors there is support for more than just AWS and then you use the CLI to perform functions on this configuration file such as deploy invoke and lot more Then there are the Serverless Framework plugins designed by the community around the project to make other tasks even easier such as unit testing configuration of S3 buckets CloudFront and domains to make frontend deployment easier and a lot lot more Lastly but most importantly there is a professional product provided in addition to the open source framework that you can use to add on monitoring deployment management troubleshooting optimisation CICD and too many other benefits to list here "
61640550,61637362,"stackoverflow.com",0,"2020-05-06 19:43:36+03","2024-05-17 05:20:23.510364+03","Definitely if you are doing a big project the Serverless framework has a lot of benefits imagine you developing an MVC c project with notepad How do you feel about that The framework are done to make our life for developers very much easier "
62129787,61638792,"stackoverflow.com",1,"2022-09-07 16:11:30+03","2024-05-17 05:20:24.184489+03","worked for me Found the answer here httpswww richdevelops devbloghowdoigetmyapigatewayurl"
61664917,61644434,"stackoverflow.com",0,"2020-05-07 21:17:34+03","2024-05-17 05:20:25.244999+03","So the problem has simple solution just update your serverless I used 1 63 0 which gave me this problem "
61650211,61648983,"stackoverflow.com",1,"2020-05-07 07:57:54+03","2024-05-17 05:20:26.361327+03","Marcins suggestion works but is somewhat tedious Fortunately here comes the serverless framework to the rescue The following example uses Python 3 8 but can easily switch it to 3 7 Prerequisites Then use sls deploy to build dependencies in docker container and deploy to AWS using CloudFormation Run sls invoke f dlib log and you will get something like this Cheers "
61687419,61648983,"stackoverflow.com",0,"2020-05-08 23:16:48+03","2024-05-17 05:20:26.363329+03","ok I solved this by increasing the pipeline size to 2x and it succeeded"
61673283,61654624,"stackoverflow.com",0,"2020-05-08 09:26:35+03","2024-05-17 05:20:27.402215+03","The event object also contains two extra structures afaik a requestContext object and a pathParameters object The requestContext object is just the context object included in the event object for testing purposes and for our general convenience The pathParameters object is a list of fields that are extracted from paths with special tokens A handler like that would take a request with the URI scratchnotes1234 and return a pathParameters object as follows Of course both objects can be combined to invoke our API offline with the Serverless framework which would look like this"
61659296,61655999,"stackoverflow.com",1,"2020-05-07 16:42:26+03","2024-05-17 05:20:28.912193+03","The easiest way to do this is to actually take the Lambda function out of the loop entirely for managing the upload process S3 has a feature that allows you to generate a temporary set of credentials to allow a user to upload a file directly to the S3 bucket This has a lot of benefits the biggest being reduced cost of Lambda streaming binary data and billing per 100ms as well as far more reliable uploads just in support of file sizes and error management What this means is that instead of the user uploading with a form via API Gateway to Lambda the process from a frontend looks a lot more like Hope that helps "
61729787,61681230,"stackoverflow.com",7,"2021-08-31 19:27:00+03","2024-05-17 05:20:29.931599+03","To upload to bucket I am just using this way I see you are using two Effect Allow maybe problem it is there Try to use just one Or you can try to use effect who upload first just for testing I am assuming that your user permissions are enabled If not that is for sure Enable permission on AWS IAM "
63126994,61681230,"stackoverflow.com",0,"2020-07-28 23:23:30+03","2024-05-17 05:20:29.933482+03","I had similar sounding issue and it turned out to be I was missing s3PutObjectTagging privilege on my Lambda role My Node js app is adding tagging when it uploads below are the parameters I pass to S3 put Funny thing is when I ran the code from my local it uploaded fine only when the Node js ran in the cloud environment is when it complained For full details checkout my reply here "
62028628,61875318,"stackoverflow.com",1,"2020-05-26 21:10:20+03","2024-05-17 05:20:32.497377+03","you need to set your custom headers on the cors httpswww serverless comframeworkdocsprovidersawseventsapigatewayenablingcors for example"
61891832,61876285,"stackoverflow.com",1,"2020-05-19 16:08:17+03","2024-05-17 05:20:33.135248+03","The Serverless Framework added support for existing S3 buckets last year All you need to do is add a regular S3 event just like normal as documented but add the property existing true to the configuration of the event httpswww serverless comframeworkdocsprovidersawseventss3usingexistingbuckets"
61909524,61890421,"stackoverflow.com",2,"2020-05-20 12:24:31+03","2024-05-17 05:20:34.177117+03","Ok found it myself You can do it like this I found it in this blog post httpsserverlessguru comresourcesblog20200303_AmazonAPIGatewayHTTPAPIswiththeServerlessFramework7be95f305318"
61891788,61890421,"stackoverflow.com",0,"2020-05-19 16:06:01+03","2024-05-17 05:20:34.179118+03","You never need to do this with the Serverless Framework If you just attach an HTTP event to a function it will automatically create the API for you The same is true of the new HTTP API If you need to add additional configuration there are a lot of options built into the Serverless Framework to perform that customisation httpswww serverless comframeworkdocsprovidersawseventsapigatewayapigateway but you can also do so using CloudFormation in the resources section of the serverless yml file Bundling all required resources as much as possible into the same service is generally preferred when developing these services as it makes for a more portable and easily deployable service Being able to easily deploy a service in any environment has advantages in testing and developer productivity as you can have two or more developers working on the same service but testing off seperate instances reducing conflict in development "
61905193,61905026,"stackoverflow.com",1,"2020-05-20 07:34:31+03","2024-05-17 05:20:35.604982+03","This is not a direct answer but for using regular CloudFormation tools ideally one would like to get the transformed raw CloudFormation template which is an outcome of serverless transformations of the SAM template For now there is no build in dedicated functionality for that in SAM toolset However there has been a GitHub issue already made for such a feature The issue also indicates that sam validate debug is a workaround on getting the raw template not ideal though Thus having this template with some manual fixes a regular CloudFormation deployment can be attempted "
61924296,61918130,"stackoverflow.com",0,"2020-05-21 01:54:32+03","2024-05-17 05:20:36.452329+03","The framework allows you to customise resources it creates and you can always configure any resources you manually create through the Resources block Both allowing you to set NotificationConfiguration which allows for SNS subscription The documentation also details how you would customize a framework created S3 bucket "
61938121,61936928,"stackoverflow.com",5,"2020-05-21 18:33:29+03","2024-05-17 05:20:37.320941+03","I found an answer Turns out you need to install tsconfigpathswebpackplugin Then in webpack config js you add the following inside webpack config js NOTE be sure to use the plugins inside the resolve section There is a plugins a the root however TsconfigPathsPlugin only works in resolveplugins I hope this helps you if you experience this same issue "
61968334,61967803,"stackoverflow.com",2,"2020-05-23 09:31:06+03","2024-05-17 05:20:39.181753+03","I think you can exclude dev deps like this Spinning up docker and inistalling deps from within it is actually a better idea Even though it takes some more time to spin it up before deployment Some deps might have compiled binaries and if you yarn install them locally and then deploy they might not work on a remote server Aside from this have you tried parcel bundler and with serverlesssimpleparcel plugin Here is a sample configuration for it goes into serverless yml into the custom block "
62008387,61971889,"stackoverflow.com",0,"2020-05-25 21:21:06+03","2024-05-17 05:20:40.351458+03","but it looks like I am missing an important part here That was the point The lambda itself has not much todo with particular statusCodes So I either may log each status code and let datadog parse it accordingly Or that is the solution I went for I can leverage APIGateway for monitoring status codes per lambda "
62014307,61998239,"stackoverflow.com",2,"2020-05-26 06:43:53+03","2024-05-17 05:20:42.281882+03","You can use the serverless package command packages your entire infrastructure into the serverless directory This is where you could see the results of any local variables Note that any CloudFormation variables e g Fn config will not have been compiled as this is handled by CloudFormation at deployment time "
63572735,62020995,"stackoverflow.com",1,"2020-08-25 08:48:57+03","2024-05-17 05:20:44.81185+03","As per AWS all the lambdas has only incoming connection lambda can not call any http for us for that we need to have NAT Gateway using NAT Gateway your lambdaserverless will get internet access and it can make outgoing http calls "
62703610,62021834,"stackoverflow.com",1,"2020-07-03 16:40:13+03","2024-05-17 05:20:45.911504+03","If you have repeated properties in your yml file would be good to use an anchors But unfortunately you can not define it on the provider level In order to keep your function declarations clean to define your ARN in custom section like"
62051769,62031276,"stackoverflow.com",4,"2020-05-27 22:55:00+03","2024-05-17 05:20:46.971839+03","Based on OPs feedback in the comment changing kinesis PutRecord to kinesis PutRecord should work "
62037793,62035220,"stackoverflow.com",2,"2020-05-27 10:38:22+03","2024-05-17 05:20:47.874885+03","I think you could use a CloudFormationRef See the documentation at Serverless httpswww serverless comframeworkdocsprovidersawsguidelayers To use a layer with a function in the same service use a CloudFormation Ref The name of your layer in the CloudFormation template will be your layer name TitleCased without spaces and have LambdaLayer appended to the end EG"
62074534,62055659,"stackoverflow.com",5,"2020-05-29 00:06:56+03","2024-05-17 05:20:49.5808+03","I found out that my original code was actually pretty close The only issue was that there was a hyphen in front of the FunctionName key and and extra on lambdaArn This allows the lambda to be passed in as a dynamic resource "
63951975,62066641,"stackoverflow.com",1,"2020-09-18 11:24:37+03","2024-05-17 05:20:50.715335+03","I had the same errors as you did and by mistake I found the solution In circleCI serverless was reading the npmrc file that was containing the authorization token for private npm packages but it was not reading the local project npmrc file that was containing the path for the private company packages So accidentally copy the private path to the npmrc and magically it the deployment t was successful After that I just update my circleCI step to get both pieces of information in the npmrc"
62082232,62066641,"stackoverflow.com",0,"2020-05-29 11:59:01+03","2024-05-17 05:20:50.820744+03","Ive come up with a solution but it meant i had to move away from the serverless action "
62095399,62093365,"stackoverflow.com",1,"2020-05-30 01:20:02+03","2024-05-17 05:20:51.473356+03","There is a space in your layer ARN Your config in YAML looks like this in JSON where it becomes more apparent"
62165236,62121378,"stackoverflow.com",3,"2020-06-03 07:28:38+03","2024-05-17 05:20:52.341168+03","The logs written to CloudWatch by your Lambda function seem to be Base64 encoded and compressed with the gzip format This looks like the behavior of the serverless enterprise plugin after issues regarding high costs for CloudWatch were opened To retrieve the original logs you can try running the following Node js code and pass in the encoded log string "
62122170,62121378,"stackoverflow.com",0,"2020-05-31 23:35:43+03","2024-05-17 05:20:52.445772+03","These are accessible in CloudWatch logs If you access in the region you execute the Lambda in it should have a log group of awslambdafunctionname Then a stream of the save revision"
62302518,62143878,"stackoverflow.com",1,"2020-06-10 14:32:05+03","2024-05-17 05:20:53.424391+03","You can try scp command for the same scp i directorytoabc pem [email protected] pathtofile yourlocaldirectoryfilestodownload abc pem is key for your computer you can try with the password as well scp [email protected] pathtofile yourlocaldirectoryfilestodownload"
62250275,62150124,"stackoverflow.com",0,"2020-06-07 22:03:11+03","2024-05-17 05:20:54.017399+03","This means the user credentials you are using to run the framework the one you configured with the serverless config credentials do not have the proper IAM permissions Make sure the user you are using has the IAMFullAccess resource policy "
69534211,62150124,"stackoverflow.com",0,"2021-10-12 04:48:00+03","2024-05-17 05:20:54.019399+03","You have role arnawsiamidrolerolename Maybe you need to replace this with a real role name "
62721541,62157032,"stackoverflow.com",0,"2020-07-03 22:20:48+03","2024-05-17 05:20:55.047004+03","This is how i defined the IoT Endpoint on awssdkgov2 On the original awssdkgo you set it on the aws Config struct directly I found the solution on a test case on httpsgithub comawsawssdkgov2"
62181482,62179995,"stackoverflow.com",4,"2020-06-03 22:47:10+03","2024-05-17 05:20:56.04654+03","for some reason it seems that is unable to find the packages when installed from a sub folder but doing all manually seems to work fine I had this problem for around 17 hours and then decided to go all manual instead of using the package serverlessgithubaction"
62182773,62181576,"stackoverflow.com",1,"2020-06-04 00:10:24+03","2024-05-17 05:20:57.041734+03","Recently I was getting the same problem I updated the version of serverlessoffline plugin and worked out for me"
62184095,62181587,"stackoverflow.com",0,"2020-06-04 01:57:08+03","2024-05-17 05:20:58.53261+03","Found the error Hopefully this will be helpful for others The error was due to exceeding the max cookie size of 4096 bytes in the application request headers We solved it by stripping out erroneous cookies that were passed through some 3rd party services we were using which has long encoded strings for cookie values "
62207260,62200289,"stackoverflow.com",14,"2020-06-05 05:43:37+03","2024-05-17 05:20:59.125517+03","Rather than creating an IAM Role in Account B the cleanest method would be From Basic examples of Amazon SQS policies The following example policy grants AWS account number 111122223333 the SendMessage permission for the queue named 444455556666queue1 in the US East Ohio region This is much easier than assuming a role "
62205937,62205158,"stackoverflow.com",1,"2020-06-05 02:49:23+03","2024-05-17 05:21:00.028698+03","Action should be aligned with Effect"
62373705,62337496,"stackoverflow.com",1,"2020-06-14 17:12:58+03","2024-05-17 05:21:02.060134+03","What are you actually testing for here What is the network latency between your client and the server If you want to calculate execution time on the Lambda and any roundtrips to DynamoDB it needs to happen inside the function "
62338680,62338069,"stackoverflow.com",3,"2020-06-12 09:27:21+03","2024-05-17 05:21:02.898365+03","Refer to this Create two separate s3 buckets for your NextJs and React app Attach them to a CloudFront distribution Attach a lambda function to your CloudFront distribution and route requests to different origin based on whether the request uri startsWith public or not "
62537228,62534433,"stackoverflow.com",2,"2020-06-23 17:34:13+03","2024-05-17 05:21:21.428929+03","As of writing this HTTP API does not have support for Execution logs httpsdocs aws amazon comapigatewaylatestdeveloperguidehttpapivsrest html"
62582170,62536275,"stackoverflow.com",2,"2020-06-25 21:38:10+03","2024-05-17 05:21:22.624351+03","I was missing the statusCodes property "
62441577,62399918,"stackoverflow.com",0,"2020-06-18 05:55:01+03","2024-05-17 05:21:04.837021+03","The solution for this was to recreate the project with from the scratch and reinstall the packages sls create template awsnodejstypescript path projectname in this case the webpack config json generate looks like this and package json deployment both offline and cloud works ok "
62425763,62425616,"stackoverflow.com",1,"2020-06-17 12:27:09+03","2024-05-17 05:21:05.73678+03","I believe that instead of it should be s3GetObject applies to objects Your original Resource is a bucket "
62444186,62436640,"stackoverflow.com",3,"2020-06-18 10:02:07+03","2024-05-17 05:21:06.654464+03","As Serverless manages your resources via a CloudFormation Stack you could probably be able to import the lambda function within the UI Import Existing Resources into a CloudFormation Stack and do the deploy afterwards again I did not try this and there is most probably a better solution though Edit precondition is that you successfully created your stack before adding your desired function "
62442606,62441788,"stackoverflow.com",0,"2020-06-18 07:53:23+03","2024-05-17 05:21:07.88062+03","I do not have a windows machine but typically the way I do this is packaging functionality of the framework httpswww serverless comframeworkdocsprovidersgoogleguidepackaging At least for MacOS I just include the directories I want relative from where the serverless yml files are and they are included into the directory of the deployed package Hope that helps "
62469267,62450701,"stackoverflow.com",0,"2020-06-19 14:27:12+03","2024-05-17 05:21:08.344918+03","node_modules folder is generally packaged with your code provided it exists in your directory does it If node_modules is not installed on your local machine then you can create it by using the npm install command This command will install all the dependencies listed in the packagelock json file or package json sorry I am not a node guy But it definitely sounds like you are not uploading your node_modules folder because it is not on your local machine You have to initialize the directory first "
62461310,62461238,"stackoverflow.com",0,"2020-06-19 03:07:56+03","2024-05-17 05:21:09.398332+03","All the environment variables defined under the environment node are available at any js file using process env variable_name In your case to access the customerDef variable you should use process env customerDef You can do the same with the BucketName and Bucket URI "
71994042,62461238,"stackoverflow.com",0,"2023-02-14 20:47:59+02","2024-05-17 05:21:09.400085+03","If you have your variables in environment key you can reference them by process env yourVariable in every part of your project"
62476755,62462138,"stackoverflow.com",3,"2020-06-19 21:48:08+03","2024-05-17 05:21:11.467118+03","Serverlessoffline does not support request validation httpsgithub comdheraultserverlessofflineissues369 Better test it out seperately on apigateway by defining mock response This will ensure lambda response in case incorrect based on the integration type is not causing any misguidance "
62466517,62462138,"stackoverflow.com",2,"2020-06-19 11:41:48+03","2024-05-17 05:21:11.468119+03","There are a few things to look out for here"
62469573,62465770,"stackoverflow.com",4,"2020-06-19 14:44:27+03","2024-05-17 05:21:12.571103+03","This feature is not supported in apsoutheast2 Cannot do much until AWS provides that support in the new regions From docs Available regions for Amazon SES are US East N Virginia useast1 US West Oregon uswest2 and Europe Ireland euwest1 Amazon SES does not support email receiving in the following Regions Asia Pacific Mumbai Asia Pacific Sydney Canada Central Europe Frankfurt Europe London South America Sao Paulo and AWS GovCloud US "
66079891,62465770,"stackoverflow.com",1,"2021-02-06 19:35:48+02","2024-05-17 05:21:12.572504+03","Did you verified the email from which you want to send emails in the SES console After verifying you will see the correct arn from which to set I had one more strange mistake I had to remove the EmailConfiguration From field When I had this field set even same as verified email it caused me a similar error "
66988838,62465770,"stackoverflow.com",0,"2021-04-07 18:04:24+03","2024-05-17 05:21:12.573948+03","I have not tested this yet although am about to but I do not see why you would not be able to use a Lambdabased Custom Email Sender to route through the Sydney region as per the instructions here Configure Cognito to send emails through third party such as SendGrid the proper way"
62479064,62478642,"stackoverflow.com",3,"2020-06-20 00:56:01+03","2024-05-17 05:21:13.663566+03","It appears that the default values for serverless yml are According to the error message these are the values that you need to use"
65415755,62482418,"stackoverflow.com",1,"2020-12-22 23:00:03+02","2024-05-17 05:21:14.336547+03","Lambda functions can only operate for a maximum of 5 seconds for viewer request functions and 30 seconds for origin request functions so it is probably not practical to have the entire Next js app on Lambda You could have individual portions of the apps logic on Lambda but you also need to think about total package size because there are restrictions there as well "
62494563,62487716,"stackoverflow.com",1,"2020-06-21 07:59:38+03","2024-05-17 05:21:15.351453+03","Looking at the AWS Documentation I would say your issue is not with using the variable selfcustom_stage but in that you are providing a string for the name without the topleveldomain accompaniment Try adding the suffix to the name e g com "
62513353,62487716,"stackoverflow.com",0,"2020-06-22 14:27:21+03","2024-05-17 05:21:15.353454+03","It turns out that the real name I was using instead of myapp was not accepted because it contained number characters I have removed them and it works now It do not know why it would not accept number characters though as the documentation says they can be used "
62490662,62490205,"stackoverflow.com",1,"2020-06-20 22:26:39+03","2024-05-17 05:21:16.346532+03","event pathParameters id is failing because event pathParameters is undefined The error message is telling you that undefined has no property called id Try let id event pathParameters event pathParameters id 1 instead"
62499722,62499623,"stackoverflow.com",2,"2020-06-21 17:11:06+03","2024-05-17 05:21:18.507818+03","AWS Lambda supports two invocation types RequestResponse will wait until the Lambda invocation is finished before returning the response Event will invoke the Lambda function and then return a success status indicating that the function was successfully started You can read more about this here "
62499952,62499623,"stackoverflow.com",0,"2020-06-21 17:31:59+03","2024-05-17 05:21:18.509608+03","In actual fact Lambda functions are almost always requested asynchronously For your specific use case you can have the lambda be triggered when a file is uploaded to s3 instead of invoking it programmatically This means every time a file is uploaded to s3 the lambda is triggered and will process the file That way no connection is open for the duration of that processing The function can then notify it once it has completed "
62540393,62525285,"stackoverflow.com",2,"2020-06-23 20:23:26+03","2024-05-17 05:21:20.466345+03","After further research I found this link Splitting Your Serverless Framework API on AWS I ended up reworking my original approach following what is in the article above The piece I was missing was having a root or base serverless file which is used to create your routing in AWS API Gateway and expose those placeholders as output which your subsequent child serverless files consume as input for wiring up your child lambda functions to routes under the API Gateway umbrella "
62546187,62534433,"stackoverflow.com",5,"2020-06-25 04:07:27+03","2024-05-17 05:21:21.426674+03","Execution logs are not available with HTTP APIs because they only support proxy integrations with Lambda and HTTP endpoint So ideally there would be no transformation in requestresponse going inout of API Gateway Whatever request parameters client sends can be seen at the integration so enable logging on LambdaHTTP side to see the request details If you want to use access logs to troubleshoot errors for HTTP API there is a new context variable context integrationErrorMessage that will give the error message in a similar format we see in execution logs for the traditional REST API "
62564189,62560559,"stackoverflow.com",0,"2020-06-25 00:14:53+03","2024-05-17 05:21:23.603541+03","I could not fix this with serverless So I decided to sls deploy without pymongo and once serverless generated the requirements zip file I copied that file elsewhere and once again ran sls deploy but this time with only pymongo and pymongo[srv] in requirements txt That generated requirements zip containing pymongo and its dependencies I merged files from this requirments zip and the one requirements zip generated from the first sls deploy This way I got all other dependencies opencv2 numpy joblib etc and pymongo in one requirements zip file After that I zipped the source code plus the merged requirements zip file and manually uploaded the zip to s3 It came down to 128MB zipped Pointed my lambda function to use this deployment package from S3 and it worked I got pymongo along with opencv2 and other dependencies But a drawback is that you have to upload to S3 and update the function yourself Until this is problem is fixed I am going to have to use this hack "
65302160,62560559,"stackoverflow.com",0,"2020-12-15 10:11:46+02","2024-05-17 05:21:23.606542+03","Use layers Pack functions individually "
62585684,62566500,"stackoverflow.com",2,"2020-06-26 02:17:34+03","2024-05-17 05:21:25.39863+03","What I was missing it seems was that the Serverless Offline pluginor at least my configuration of itdoes not emulate asynchronous lambda invocation in this way Note in my Lambda constructor I had a localhost endpoint giving away that it was offline Once I deployed to an AWS instance it worked perfectly If anyone has a configuration that is worked for them with Serverless Offline and async Lambda invocation it might be worth posting here for future internauts "
68326897,62566500,"stackoverflow.com",0,"2021-07-10 13:34:57+03","2024-05-17 05:21:25.401631+03","a configuration that is worked for them with Serverless Offline and async Lambda invocation it might be worth posting here for future internauts Just in case someones looking for a working config Try async true for both invoker and invoked functions in serverless yml InvocationType should be Event in lambda invoke parameters iam role statements lambdaInvokeAsync FunctionName for offline invocation should come in the format serviceNamestagefunctionName so all together it looks like handler tested in node15 based on httpswww serverless compluginsserverlessofflineusagewithinvoke "
62576587,62571583,"stackoverflow.com",0,"2020-06-25 16:32:56+03","2024-05-17 05:21:27.37157+03","It looks like the error is line 26 where the bucket name has caps in it httpsgithub comOWASPDVSAblobb26c8a744293cd192383e4a61e0699563505c5a8backendsrcfunctionsprocessingsls ymlL26"
62648856,62604033,"stackoverflow.com",0,"2020-07-24 02:52:20+03","2024-05-17 05:21:28.350474+03","It is a bit tricky to do so using a serverless framework but I solved it by combining cloud formation with the serverless framework I have the answer here to another question which contains a full description of how to do so How to access AWS CloudFront that connected with S3 Bucket via Bearer token of a specific user JWT Custom Auth I do not want to repeat everything again here and also I found the question so important and facing many people without a concrete solution so pleas let me know in case you are facing any issue The approach is to just create the function inside the serverless yml then inside the cloud formation you can do all the magic of creating the versions roles and another function that will help you publish your arn and use it dynamically Here is my Serverless yml Here is my resourcess3cloudfront yml But you will find the full description in my other questions answer "
62636742,62629026,"stackoverflow.com",0,"2020-06-29 14:05:06+03","2024-05-17 05:21:29.120372+03","It should not be necessary Just add your console log to your code and you will see it echo out in CloudWatch when you test it in Lambda If you would like to improve the feedback loop consider using a tool like Studio developed by the team at Serverless designed to give you a dev mode experience You just need to connect to the Serverless Framework Dashboard httpswww serverless comframeworkdocsdashboardenablingthedashboardonexistingserverlessframeworkservices then on CLI run serverless dev and now when you save your Lambda handler code it will automatically detect that upload it into AWS in seconds and give you a postman style interface to test Just click the studio on the top menu of the dashboard to access it "
62631752,62631647,"stackoverflow.com",2,"2020-06-29 09:04:06+03","2024-05-17 05:21:30.023252+03","The MyManagedPolicy policy is correct There is nothing wrong with its syntax yaml nor the PolicyDocument To verify it I created my own template with your MyManagedPolicy and it deployed without any issues However upon tests with serverless I can confirm that this does not work with the same issue as OP reported Upon further investigation it was identified that the serverless incorrectly specifies Version 20121017 as The solution was to use date in quotes "
74296581,62635178,"stackoverflow.com",1,"2022-11-03 01:15:47+02","2024-05-17 05:21:31.003283+03","DataTraceEnabled is only supported for WebSocket APIs See httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawspropertiesapigatewayv2stageroutesettings html Since you mention the API you are creating is an HTTP endpoint setting DataTraceEnabled false or removing the DataTraceEnabled option from your template should fix this error "
71059015,62648402,"stackoverflow.com",0,"2022-02-10 04:15:28+02","2024-05-17 05:21:31.623578+03","In my case the problem was because the browser was sending additional headers like ContentType but this header was not configured when generating the presigned URL The solution that I chose was to set the header to applicationoctetstream at the moment of generating the presigned URL and on the browser I forced the ContentType header to be applicationoctetstream Obviously this solution was the best for me at the moment "
62648587,62648402,"stackoverflow.com",2,"2020-06-30 03:31:34+03","2024-05-17 05:21:31.625579+03","I believe the problem is that you should be using s3 createPresignedPost instead of s3 getSignedUrlPromise and then you must use the POST method and not PUT when sending the request I think s3 getSignedUrlPromise requires you to provide the Body of the object at the time that you sign the request while s3 createPresignedPost allows any body to be sent With s3 createPresignedPost you can also add Conditions to do certain things e g restrict the max size of the object See"
62658028,62651090,"stackoverflow.com",1,"2020-06-30 15:58:26+03","2024-05-17 05:21:32.734597+03","This answer assumes you created all of your resources manually in the AWS console You could create a new CloudFormation stack and import your API Gateway and any supporting resources lambdas databases etc by following this guide httpsdocs aws amazon comAWSCloudFormationlatestUserGuideresourceimportnewstack htmlresourceimportnewstackconsole Now open the stack in the CloudFormation console go to the Template tab and click on View in Designer In the page that opens up click the Create Stack button Now this part is a bit of a hack but in the browsers URL you will see the current AWS region replace this with the region that you want to deploy to and press enter If you do not know the region name this list is a good reference Now you can step through the wizard to setup your new stack You will be asked to enter Parameter values you can just copy them from the original stack that you created "
62832076,62827429,"stackoverflow.com",2,"2020-07-10 13:12:14+03","2024-05-17 05:21:33.325075+03","While your lambda function is allowing the ZClientTimezone header the builtin options method of AWS is not In order to allow this you can do the following Then add any other headers you are also sending "
62869955,62836558,"stackoverflow.com",1,"2020-07-13 08:55:05+03","2024-05-17 05:21:34.693392+03","The stringified response you are sending will not work with the schema but that response is not specific to serverless its the response required by the api gateway proxy Try returning non stringified result and use another middleware to format the response Check out autoproxyresponse Or write your own to handle the final formatting "
62852113,62838650,"stackoverflow.com",1,"2020-07-11 20:05:33+03","2024-05-17 05:21:34.785095+03","Call the SSM client from your code You can put this outside of your default handler export which will call it only on lambda launch which might be less than on invoke depending on traffic But as far as redeploying it can be extremely fast I personally separate my stuff into multiple services and redeploying lambda is a separate process than say DynamoDB setup so it goes fast "
62846060,62838650,"stackoverflow.com",0,"2020-07-11 09:47:17+03","2024-05-17 05:21:34.787094+03","Environment variables are designed to be set at deployment because they pertain to the environment of the Lambda If you need values that are changed regularly there are many other options DynamoDB sticks out as the perfect solution to that issue If all you need is a Key Value store which is what environment variables are then DynamoDB absolutely excels Its super fast and way cheaper than SSM that can be used to do the same thing "
62839426,62839323,"stackoverflow.com",2,"2020-07-10 20:31:41+03","2024-05-17 05:21:35.761891+03","If the principle you specify is an IAM user you need to add this as the value of the AWS key Ref AWS JSON Policy Elements Principal"
62852055,62849298,"stackoverflow.com",2,"2020-07-12 19:43:51+03","2024-05-17 05:21:36.780082+03","AppSync plugin exposes a variable GraphQlApi which you can tap into for your environment variables which are automatically accessible in your lambdas In your config In your lambda Or lambda config directly"
66482256,62856931,"stackoverflow.com",2,"2021-03-04 22:04:25+02","2024-05-17 05:21:37.405694+03","AWS support clarified the problem There is a Via header where an extra value is added each time CF is involved and there is a limitation that the header cannot include more than 2 CF references The API Gateway generated by Serverless is EdgeOptimized so it involves going through the nearest CF Edge Location and the number of CF distributions involved in the request becomes 3 and that is not allowed The solution was using a regional API Gateway instead of an EdgeOptimized API gateway reducing by 1 the number of CF distributions involved "
67158494,62877754,"stackoverflow.com",0,"2021-04-19 11:28:31+03","2024-05-17 05:21:37.931278+03","You cannot mix API Gateway mapping between RestApis and Websocket APIs under a single custom domain In other words it could be expressed like we cannot use the same domain or subdomain for RestAPI and WebSocket "
62879353,62878199,"stackoverflow.com",0,"2020-07-13 18:38:31+03","2024-05-17 05:21:38.963459+03","In cloud formation you can add a BucketName property which can be referenced outside of cloudformation "
62897261,62883665,"stackoverflow.com",0,"2020-07-14 17:22:00+03","2024-05-17 05:21:39.91886+03","I do a lot of testing with a similar setup and this is how I approach things The goal is to find a balance between complete mocks and testing against a running server For dynamoDb elasticsearch I use the docker instances available Before every test in my Jest setup I clear the db tables and elasticsearch indices Just wrote some simple code to go through a list of tables and wipe them In each test I seed the necessary tables ensuring that the data will not conflict with other tests since Jest runs in parallel by that I mean if I am going to have a test to fetch all entries by user then I ensure that this user ID is only used in that test seed to prevent another test seed from affecting results I have wrappers for all the aws services I use such as Cognito SNS SQS etc which abstract a bit of aws for me I do unit tests for these using nock I especially use the nock recorder rec command which lets me capture the response from aws I do these tests for all these root level calls This ensures my code works against live servers but is repeatable without a dependency on these servers For all other unit tests and by unit tests here I am referring to tests that just test a function and do not involve running sls offline for ex I will use jest mocks and mock the aws method Since my low level aws wrappers were tested and recorded with nock now I just mock things for easy testing as I feel confident the mock methods work as expected For integratione2e testing when using sls offline things are trickier since I cannot mocknock when things are loaded in a running server For this I provide each aws service with a custom endpoint every aws service has a configuration that accepts this parameters defaulting to null it will use the real aws server but you can create a local httpexpress server to mock the responses Check out this SO question where I answered how to do this with Cognito the process is the same for all other services It takes a bit of work to get this whole testing framework setup but once do not it works pretty flawlessly and also provides a test setup that CI servers can run I should also mention the amazing work of LocalStack which provides local docker servers for most aws services I personally prefer to run just what I need hence my above setup and also they do not provide local abstraction for everything I needed but it is a great option to explore and it may fit your needs well It is a big stack however my laptop freaks out if I try to run it "
62913942,62891056,"stackoverflow.com",4,"2020-07-15 14:35:48+03","2024-05-17 05:21:40.98653+03","The invoke command was executed from a directory which did not have a serverless yml file Adding this minimal yaml file activated the plugin But in order to run the name parameter in invoke must be the name described in the serverless yml file In the example below the correct value for the name parameter is aStateMachine I first did the uncorrected assumption that the name was the same as the name parameter under the state machine Amusing that you are in the same directory as the above serverless yml file A working invoke to a step function could look something like The above example explain the error message in the question It is however much more convenient to build a solution where the invoke command is executed from the directory where you have the serverless yml file "
62925837,62904125,"stackoverflow.com",0,"2020-07-16 03:54:12+03","2024-05-17 05:21:41.80903+03","It was so obvious I wanted to delete my question but I will leave the answer here just go to the folder that contains your node_modules you will find a package json there and run npm install save for the new packages you need your serverless yml include should consider it when running local with no issues Viola "
62916148,62908940,"stackoverflow.com",1,"2020-07-15 16:36:03+03","2024-05-17 05:21:42.726588+03","You have two options here "
62929693,62927699,"stackoverflow.com",2,"2020-07-16 10:31:49+03","2024-05-17 05:21:43.754701+03","You have complete control over the permissions added to that role You can add an iamRoleStatements section to your serverless yml file under provider that describes the permissions you wish to apply to the role applied to functions It would look something like You can find out more in the official documentation here httpswww serverless comframeworkdocsprovidersawsguideiamiam"
62929827,62927699,"stackoverflow.com",1,"2020-07-16 10:47:08+03","2024-05-17 05:21:43.756702+03","You can use iamRoleStatements to give that permission to your Lambda function The following template worked for me"
67061540,62935080,"stackoverflow.com",15,"2022-02-02 20:38:24+02","2024-05-17 05:21:44.623091+03","Better late than never Hope this helps you or someone searching for this problem When you configure SQS to trigger Lambda the DLQ is supposed to be configured on the SQS Since it would not be an Asynchronous invocation Notice the Note section in the link Source Hence your serverless yaml needs to declare the ReddrivePolicy in the main queue to refer to the DLQ Below maxReceiveCount is set to 5 as per AWS Documentation "
62935243,62935080,"stackoverflow.com",5,"2020-07-16 15:56:16+03","2024-05-17 05:21:44.625093+03","To give you some background Dead Letter Queue is just that a normal SQS queue it is the configuration at AWS Lambda that informs it to push message to this Queue whenever there is any error while processing the message You can verify this from the management console by referring to the Deadletter queue service under Asynchronous invocation"
62966519,62935465,"stackoverflow.com",2,"2020-07-18 11:46:00+03","2024-05-17 05:21:45.690119+03","Yes you can I am not sure if you have already setup EFS on serverless But assuming that this have been done you can then explicitly tell your serverless lambda project what vpcs to connect to and what EFS IAM roles to use I do not have the details of the EFS setup but on my project this would look something like this under the functions section just make sure that you define your env vars to point to your libscode Finally in resources FYI for tensorflow you can bring it down to around 60MB using a combination of lambcdockerlambda and tensorflow packaging but in the long run you will be better off with EFS anyway just was worth mentioning "
63246698,62937452,"stackoverflow.com",10,"2020-08-04 15:16:19+03","2024-05-17 05:21:46.716536+03","I hit the same issue You need to specify the full domain name including the host in the DomainValidationOptions DomainName parameter and just specify the hosted zone id In my testing the Route53 validation record was added about a minute after running the stack and the domain successfully validated itslef after about 15 minutes "
64937486,62937452,"stackoverflow.com",3,"2020-11-21 00:04:54+02","2024-05-17 05:21:46.718536+03","If this is stuck as in progress for a long time it could be that you are using a Private Hosted Zone when you need to use the Public one Probably you do not use a private CA That process should take 23 minutes not more than that "
62943187,62937452,"stackoverflow.com",0,"2020-07-16 23:50:00+03","2024-05-17 05:21:46.719884+03","I just deployed the below template to CloudFormation and it successfully created the validation DNS records and authorised the certificate If you were to pass the parameters SiteDnsZoneNamemydomain io and SiteDnsZoneIdABCDEFGHIJKLMNOPQRSTU it would create a SAN cert that covers both mydomain io and mydomain io Note If you want to use a cert in CloudFront you have to deploy the cert in useast1 Note 2 Route53 needs to be hosting your DNS Zone but theres no requirement on AWS being the registrar Your domain can be registered with any provider so long as you use the AWS name servers provided by Route53 when you add the zone "
65173546,62951047,"stackoverflow.com",9,"2020-12-07 00:00:44+02","2024-05-17 05:21:47.658797+03","when you try to establish a connection you need to check if there is already a connection it can use this is my Database class used to handle connections"
62974443,62974231,"stackoverflow.com",1,"2020-07-19 01:50:58+03","2024-05-17 05:21:48.651381+03","You can view export value of selfprovider stage ApiGatewayAuthorizerOutput in your stacks console in Outputs tab or in CloudFormation console in Exports menu Ref in your context cannot be used as the imported value is from other stack If you just want to use imported value then there is no need for Ref You can try the following"
62975525,62975280,"stackoverflow.com",11,"2020-07-19 04:55:25+03","2024-05-17 05:21:49.686649+03","Based on the doc this should do the trick "
68596652,62976032,"stackoverflow.com",9,"2021-09-15 03:00:32+03","2024-05-17 05:21:50.619749+03","I had the same error I work with the CDK but I think the problem is the same with the Serverless Framework I solved it as follows This is how it worked for me ps you cannot replace the index with a new one as mentioned above you need to delete the old one and then add the new one Or you will end up with an error message like this one Cannot perform more than one GSI creation or deletion in a single update"
67859735,62976032,"stackoverflow.com",1,"2021-06-06 18:49:34+03","2024-05-17 05:21:50.622751+03","I have faced the same issue I tried to changed my ProjectionType but after that my serverless deployment started failing with the same error As mentioned in the question I also deleted my GSI from DynamoDb but that did not help me as well After Googling a lot I did not get a proper answer But this is how I resolved it finally That is it now if you run your yarn sls command again it will not stop you to let your new DynamoDb GSI projection settings get deployed One more tip it takes some time to create a new GSI on Cloud so please be patient and look for the Status Active on your DynamoDb table under the tab Indexes "
62991447,62990653,"stackoverflow.com",21,"2020-07-20 11:28:59+03","2024-05-17 05:21:51.362991+03","You can use AWSCloudFormationWaitConditionHandle for this Example"
62990705,62990653,"stackoverflow.com",10,"2020-07-20 11:27:25+03","2024-05-17 05:21:51.363992+03","The Resource section is required but you can create nonresource type of resource For example minimalist template with only a nonresource would be"
72387988,62990653,"stackoverflow.com",0,"2022-05-28 03:10:51+03","2024-05-17 05:21:51.365992+03","You can use create AWS SSM parameter using Terraform and reference them in your serverless framework That would do the job easily httpswww serverless comblogdefinitiveguideterraformserverless"
63066534,62999002,"stackoverflow.com",2,"2020-07-28 03:37:47+03","2024-05-17 05:21:52.422312+03","You will either have to put all your vars in the external file or import each var from the custom file one at the time as file config yml foo However you can also use js instead of ymljson and create a serverless js file instead allowing you to build your file programically if you need more power I have fairly complex needs for my stuff and have about 10 yml files for all different services For my offline sls I need to add extra stuff modify some other so I just read the yaml files using node parse them into json and build what I need then just export that Heres an example of loading multiple configs and exporting a merged one"
63021992,63010989,"stackoverflow.com",1,"2020-07-21 22:44:11+03","2024-05-17 05:21:53.489979+03","Use sequelize sync only for development purposes Use sequelize migrations for production There must be some way to run console command dbmigrate in your environment When you push all the migration code use that command to update your database to the last version "
63020330,63020039,"stackoverflow.com",2,"2020-07-21 20:55:20+03","2024-05-17 05:21:55.499938+03","A commonly used workaround is to define perstage values and use variable resolution to route to the specific one "
63023091,63022698,"stackoverflow.com",2,"2020-07-22 00:05:14+03","2024-05-17 05:21:56.510079+03","Absolutely You can manually include or exclude files in your serverless yml See more here httpswww serverless comframeworkdocsprovidersawsguidepackaging"
74286883,63022698,"stackoverflow.com",0,"2022-11-02 11:20:12+02","2024-05-17 05:21:56.512074+03","For those whom above explanations might not be matching this source also outlines it well httpsfilip5114 github ioslsmanagefunctionpackage"
63058560,63024267,"stackoverflow.com",2,"2020-07-23 19:07:27+03","2024-05-17 05:21:57.367491+03","It is not quite clear to me whether you have assumed an identity exchanged your ID token from the user pool for an STS token Confusingly cognitoidentity amazonaws comsub resolves to the ID pool identity ID not the subject ID in the ID token from the user pool See the Note section on this page httpsdocs aws amazon comIAMlatestUserGuidereference_policies_examples_s3_cognitobucket html For getting identity credentials have a look at httpsdocs aws amazon comcognitoidentitylatestAPIReferenceAPI_GetCredentialsForIdentity html"
65083150,63024267,"stackoverflow.com",1,"2020-12-01 02:36:39+02","2024-05-17 05:21:57.369492+03","It turns out you cannot use these variables if you are using AWS Gateway with Lambda You must access DynamoDB directly from the client application where you have registered for your identity using IAM auth with something like aws amplify I ended up using STS to assume the Cognitos group authenticated role in my lambda function and completely bypassing identity pools "
63042124,63037592,"stackoverflow.com",1,"2020-07-22 22:50:57+03","2024-05-17 05:21:58.95543+03","Looking at the actual code the TimeoutInMinutes parameter is not part of the parameters passed to the CreateStack API call so it is not yet supported But a feature request has already been opened for this One solution to overcome long deployment operations could be to split the serverless stack into multiple independent not nested stacks using the serverlesspluginadditionalstacks plugin "
63424728,63045817,"stackoverflow.com",2,"2020-08-15 12:54:20+03","2024-05-17 05:21:59.006466+03","I had a similar issue with v8 crashing during the serverless build Changing packages from hewmenserverlessplugintypescript to the original serverlessplugintypescript fixed it "
75461375,63045817,"stackoverflow.com",0,"2023-02-15 16:38:49+02","2024-05-17 05:21:59.008466+03","Please note that awsserverlessexpress says On 1130 [2022] the AWS Serverless Express library is moving to Vendia and will be rebranded to serverlessexpress [ ] has become obsolete Probably the same job you wanted to achieve does the following Vendina example vendia serveless basicstarternestjs"
63066456,63058395,"stackoverflow.com",0,"2020-07-24 06:43:10+03","2024-05-17 05:22:00.077332+03","I do not think this is a right or wrong answer people kinda create their preferences with this but imo yes it is fine It is what I do for integration tests I also spinup a docker with DynamoDb For SQS however you are going to have to emulate that For integration tests I spin up a simple server and mock the callsresponse I do this for SQSSNSCognito and a few other things that are either not available in serverless offline or do not provide the type of testing framework I desire You can check out one of my answers on mocking Cognito the same process applies to every AWS service which is very handy "
63323866,63323334,"stackoverflow.com",0,"2020-08-09 10:54:32+03","2024-05-17 05:22:00.386958+03","If you deploy with v option the bucket name will be provided in the Stack Outputs section For example"
63323677,63323500,"stackoverflow.com",4,"2020-08-09 10:32:44+03","2024-05-17 05:22:01.16279+03","They map directly to return values of AWSS3Bucket in CloudFormation As the link explains the use of"
63348742,63340608,"stackoverflow.com",2,"2020-08-11 01:15:33+03","2024-05-17 05:22:02.188772+03","From the AWS documentation The Toolkit provides the ability to convert one or more AWS CDK stacks to AWS CloudFormation templates and related assets a process called synthesis and to deploy your stacks to an AWS account The cdk synth does not do any additional validation on the underlying Cloudformation resources it simply converts the CDK code into Cloudformation templates You have to add in this functionality yourself before deployment One way to achieve this could be running a local SAM test suite "
63367561,63351445,"stackoverflow.com",0,"2020-08-12 02:09:14+03","2024-05-17 05:22:03.071606+03","The syntax you used is not importing variables from another stack According to the doc it is simply referencing CloudFormation stack output values as the source of your variables to use in your service It is the same as retrieving the output values from cloudformation using AWS CLI "
63390887,63362015,"stackoverflow.com",0,"2020-08-13 11:14:52+03","2024-05-17 05:22:05.180343+03","After doing more research and asking around I finally made sense of the answer that was provided to me on github that When running mock on a function which has access to a dynamodb table generated by API It will populate the env with fake values If you would like to mock your lambda function against your deployed dynamodb table you can edit the values in the sdk client so it can make the call accurately In summary if you are running things locally then you would not have access to your backend variables which you might try mocking I hope this helps someone Thanks "
63405429,63403922,"stackoverflow.com",4,"2020-08-14 04:54:01+03","2024-05-17 05:22:06.841332+03","I think the syntax should be"
63413786,63409922,"stackoverflow.com",2,"2020-08-18 01:27:12+03","2024-05-17 05:22:07.494776+03","you have not specified any integration type in your lambda therefore it will use the default lambdaproxy integration type In Lambda proxy integration when a client submits an API request API Gateway passes to the integrated Lambda function the raw request asis You cannot use mapping templates with lambdaproxy integration If you would like to transform your request or response you can choose lambda integration without proxy You are using an HTTP GET Therefore you can pass the data as a query string or path parameters Query string you pass the data in the url for example httpapi example comBooksid1 The query string parameters can be accessed inside the lambda as event queryStringParameters Path Parameters you can define a parameter in your serverless template as below then you can access the path parameters inside the lambda like event pathParameters Reference Lambda Proxy Integration"
63445445,63419422,"stackoverflow.com",0,"2020-08-17 09:12:21+03","2024-05-17 05:22:08.512487+03","My few cents here based on some experience in the last 6 months I started using a single Git repo with all lambda functions each in a folder During the first 2 months I had a single YML file to define all functions Then after some issues I moved to separate YML files inside each folder for lambda function "
63440278,63440238,"stackoverflow.com",3,"2020-08-16 21:00:03+03","2024-05-17 05:22:09.580942+03","You are missing the dynamodbUpddateItem action from your iamRoleStatements Should read"
63452304,63452092,"stackoverflow.com",4,"2022-06-03 16:22:52+03","2024-05-17 05:22:11.662785+03","iamRoleStatements is designed to contain the most common permissions needed for this service For example you have an API gateway and a bunch of lambda functions that all use DynamoDB to store the transactional data Almost all the lambda functions need to have permission to query DynamoDB so iamRoleStatements should be configured like this All the lambdas will get the same iamRoleStatements written above Now say if you have a special lambda function that needs completely different permission sets You can craft a role in the console and use the role option to overwrite the default role which contains iamRoleStatements "
63457160,63456912,"stackoverflow.com",1,"2020-08-17 22:06:43+03","2024-05-17 05:22:12.574866+03","I guess you need selfcustom assets targets bucket1 not sure if this nested assets will work Please check the example below is supposed to work "
67376823,63501703,"stackoverflow.com",1,"2021-05-04 02:04:45+03","2024-05-17 05:22:14.314869+03","If you would like AWS CloudFormation to issue an IoT certificate for you you need to tell it your own CSR Certificate Signing Request You can for example use openssl to generate a CSR for you Once you have your CSR you can then update your CloudFormation with it as below and it will work"
74659641,63508975,"stackoverflow.com",0,"2022-12-02 19:50:38+02","2024-05-17 05:22:16.037185+03","In your serverless common yml you must reference as if it were serverless yml In this case selfcustom stage does not exist but selfcustom common stage does exist "
63511512,63511383,"stackoverflow.com",4,"2020-08-20 21:52:58+03","2024-05-17 05:22:17.869696+03","You have typos in your resource type In both cases you have a single colon where you should have a double colon "
76010488,63511383,"stackoverflow.com",3,"2023-12-27 19:37:30+02","2024-05-17 05:22:17.870674+03","I ran into this issue and the solution was to add the following line to the top of my template yaml When using SAM specific resource types like AWSServerless you must define the AWSServerless transform in your Cloudformation Template To do so include the Transform property at the top of the template "
65986368,63511383,"stackoverflow.com",2,"2021-02-01 04:19:29+02","2024-05-17 05:22:17.872425+03","If you get the error Unrecognized Resource Types it means there is a syntax problem in your code Check for typos after Unrecognized resource types In this case it should be"
63519145,63518859,"stackoverflow.com",1,"2020-08-21 11:19:26+03","2024-05-17 05:22:18.689847+03","You can do that with serverlessoffline plugin It is pretty well described here httpswww npmjs compackageserverlessofflineusagewithinvoke"
63533751,63530643,"stackoverflow.com",0,"2020-08-22 10:40:33+03","2024-05-17 05:22:19.156358+03","I think I solved it by moving the API gateway specific IAM roles to the stack that contains the relevant endpoints That actually makes sense to me since the root stack does not need to know about api endpoint specific roles "
63556237,63537929,"stackoverflow.com",2,"2020-08-24 10:09:02+03","2024-05-17 05:22:19.932689+03","You can associate the destination bucket with a S3 event When the upload is finished you update the database with the URL There is two alternatives for that If you do not need to save any metadata only the url you can answer with the signedUrl and do not update the database at the first step You can find more information about this strategy in the S3 official docs "
65010750,63994864,"stackoverflow.com",0,"2020-11-25 21:24:56+02","2024-05-17 05:22:37.390523+03","Add dockerizePip command at the end then it will not show this error or delete the requirements txt then enter serverless deploy and then paste requirements txt and run again serverless deploy"
76694315,76694314,"stackoverflow.com",0,"2023-07-15 17:50:58+03","2024-05-17 05:51:46.45979+03","I got it It was simple mistake on my side I have to remove only this part of code fn spec replicas 1 see full valid code"
63562867,63557690,"stackoverflow.com",0,"2020-08-24 17:18:07+03","2024-05-17 05:22:21.187125+03","By default Serverless creates a bucket with a generated name like service nameserverlessdeploymentbuck1x6jug5lzfnl7 to store your services stack state Please go to S3 and find you deployment bucket There you could download your latest lambda archive and unzip it this is how could see if you really have all required modules uploaded Here is how to add extra dependencies not included by AWS httpsdocs aws amazon comlambdalatestdgpythonpackage htmlpythonpackagedependencies"
65522639,63565333,"stackoverflow.com",0,"2020-12-31 17:17:19+02","2024-05-17 05:22:21.928484+03","You can use a serverless plugin serverlesspluginresourcetagging it will tag your Lambda function Dynamo Tables Bucket Stream API Gateway and CloudFront resources The way it works is you have to provide stacksTags having your tags inside under the Provider section of serverless You can also update tags value using this plugin "
63584513,63581767,"stackoverflow.com",3,"2020-08-25 20:50:39+03","2024-05-17 05:22:22.76839+03","The problem is that S3 does not allow event notifications to have overlapping prefixes or suffixes From the S3 documentation about notifications Notification configurations that use Filter cannot define filtering rules with overlapping prefixes overlapping suffixes or prefix and suffix overlapping This means you either need to adjust your prefix in one of the filters e g use assetsimagesjpg and assetsimagespng as prefixes or if you do not want to have different prefixes then you can just use one notification config for prefix assetsimages and filter for the suffix within your Lambda function This can be done by checking the key of the S3 object data Of course the latter option has the disadvantage of always calling your Lambda function regardless of the suffix This can have negative consequences for your costs I advise you to read through the various notification examples in the S3 documentation maybe you get inspired how you can deal with this limitation in your case For the event name you have to use s3ObjectCreatedPut if you want to filter for PUT events You can have a look at the overview section of the S3 notification documentation where it is explained New object created events Amazon S3 supports multiple APIs to create objects You can request notification when only a specific API is used for example s3ObjectCreatedPut or you can use a wildcard for example s3ObjectCreated to request notification when an object is created regardless of the API used "
63598890,63590044,"stackoverflow.com",2,"2020-08-26 16:36:09+03","2024-05-17 05:22:23.810978+03","The name property needs to be defined for the buckets being created If this is omitted then CloudFormation will generate a name for the bucket If you do not specify a name AWS CloudFormation generates a unique ID and uses that ID for the bucket name Within a CFT the property is BucketName or if defined in the serverless file then this is just name CFT info here httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawspropertiess3bucket htmlcfns3bucketname Serverless info here httpswww serverless comframeworkdocsprovidersawseventss3custombucketconfiguration"
63618450,63610620,"stackoverflow.com",0,"2022-07-16 07:43:19+03","2024-05-17 05:22:24.596331+03","These methods are not coming from AWS DynamoDB API Gateway etc These are artifacts of Vue You can check out the Vue documentation on Reactivity for more in depth information "
63733476,63614466,"stackoverflow.com",2,"2020-09-04 03:54:11+03","2024-05-17 05:22:25.485585+03","I have recently been working on something like this and it covers some of the technologies that you are talking about specifically You could potentially use this serverless yml as a starting point httpsgithub comCompulsedblogblobmasterbackendserverless yml"
63622412,63622377,"stackoverflow.com",3,"2020-08-27 21:47:21+03","2024-05-17 05:22:26.691608+03","S3 is a universal namespace meaning each S3 bucket you create must have a unique name that is not being used by anyone else in the world This is because your bucket name makes up part of your S3 url which must be unique "
63707052,63641425,"stackoverflow.com",1,"2020-09-02 16:52:21+03","2024-05-17 05:22:27.292903+03","The path should point to a directory with a package json and node_modules directory Your repo should probably have a layers directory andor some other step in the deploy flow that builds the layer That way your code can still reference the modules in the normal way and your editorplugins will not complain "
63967937,63920446,"stackoverflow.com",0,"2020-09-19 13:54:30+03","2024-05-17 05:22:30.55315+03","You seem to be importing the API gateway you should add that configuration where you create the api gateway "
76407311,63920446,"stackoverflow.com",0,"2023-06-05 17:12:26+03","2024-05-17 05:22:30.55515+03","I had a similar error and you should check which was which Aws API type you are using as how logs are enabled are slightly different between if you are using REST API or HTTP Api In my case I was using the rest syntax when I needed to be using the http syntax"
63937280,63933864,"stackoverflow.com",1,"2020-09-17 14:41:46+03","2024-05-17 05:22:31.298357+03","Based on the comments and chat discussion There were a few issues causing problems"
63938139,63937962,"stackoverflow.com",2,"2020-09-17 15:34:54+03","2024-05-17 05:22:32.356411+03","You will need to provide the actual ARN of the table now that this stack is not part of your app you cannot reference its components Try something like this"
68440847,63939160,"stackoverflow.com",1,"2021-07-19 15:54:36+03","2024-05-17 05:22:33.366435+03","You can install serverlessfunctionsbasepath plugin and use Check out the guide on the plugin here httpswww serverless compluginsserverlessfunctionsbasepath If your serverless yml file and src in the directory you can add this to you serverless yml file and try deploy"
63944855,63939160,"stackoverflow.com",0,"2020-09-17 22:16:05+03","2024-05-17 05:22:33.368435+03","You can use serverlesspythonrequirements plugin for this It can be installed locally or on a pipeline with You can add this to you serverless yml file and try deploy Check out the guide on the plugin here httpswww serverless comblogserverlesspythonpackaging httpswww npmjs compackageserverlesspythonrequirements"
64572802,63978535,"stackoverflow.com",4,"2020-10-28 14:24:45+02","2024-05-17 05:22:34.452569+03","you can add multiple cloudwatchLog events and if you want to give different filter patterns to each log group you can use"
77120090,63978655,"stackoverflow.com",2,"2023-09-17 03:36:22+03","2024-05-17 05:22:35.552885+03","There are couple of ways to solve your problem Serverless ignores devDependencies when it is creating the zip file to upload Until Node js v16 awssdk v2 is already available in Lambda So do not bundle it Also if you are using Node js v18 better to update to latest version of AWS SDK awsask That means serverless shall create an individual zip file for each lambda function You can enable that by having the below in your serverless yml file You can do that by having the below in your serverless yml file You can add more patterns depending on your usecases "
71422151,63994864,"stackoverflow.com",1,"2022-03-10 11:55:59+02","2024-05-17 05:22:37.386522+03","Fix This is the problem with requirements txt file encoding This is an open issue as on date 10th March 2022 The serverless plugin rewrites the file into serverless directory it assumes UTF8 encoding when it reads the file The problem occurs as serverless attempts to read a file with another encoding then dump it into serverless folder Serverlesspythonrequirements issue"
76079869,63994864,"stackoverflow.com",1,"2023-04-22 16:24:01+03","2024-05-17 05:22:37.388522+03","I faced a similar issue while deploying a Serverless application on Windows and I was able to resolve it by changing the encoding from UTF8 to ANSI Here are the following these steps After saving the file in the ANSI format I was able to deploy the application successfully I hope this solution helps anyone who might be facing a similar issue on Windows I wanted to include these steps for those who might find them helpful "
64018593,64003575,"stackoverflow.com",2,"2020-09-23 01:39:36+03","2024-05-17 05:22:38.239021+03","The easy way to write the lambda is not covered in the proxy docs Basically just write it as This way the callback will not wait and the lambda will not timeout API Gateway will not then sent the 502 code "
64008491,64005368,"stackoverflow.com",0,"2020-09-22 14:11:40+03","2024-05-17 05:22:39.169111+03","So further to the path issue I have added in my update there is one MASSIVE gotcha I didnt pot it originally One of the documents I read said to add Turns out this was cuasing the issue My plugins now looks like"
64026525,64026416,"stackoverflow.com",0,"2020-09-23 14:15:16+03","2024-05-17 05:22:40.236288+03","This error basically means you do not have the right installation of python Some application needs python3 7 and you do not specify a version with apk add python3 Hence the latest is probably installed 3 8 This article deals with how to select a given python version for an agent in a bitbucket pipeline It basically boils down to Is there a reason you have to use Alpine Otherwise I would go for the pragmatic image above "
66523070,64026416,"stackoverflow.com",0,"2021-03-08 02:49:50+02","2024-05-17 05:22:40.239289+03","he solve with Similar problem"
64028063,64026784,"stackoverflow.com",0,"2020-09-23 15:43:15+03","2024-05-17 05:22:40.597823+03","How do I add my own lambda without going to AWS console manually and most importantly I hope you have serverless yml file with your function config Here is a template with possible configs httpswww serverless comframeworkdocsprovidersawsguideserverless yml If everything is setup deploying is super easy just by using serverless deploy httpswww serverless comframeworkdocsprovidersawsguidedeploying Here is a very simple one from serverless examples httpsgithub comserverlessexamplestreemasterawsnoderestapi How do I call it from my React app You need an exposed public endpoint either you take directly the one generated by API Gateway or you create custom domain and map it to your existing domain "
65013884,64048687,"stackoverflow.com",10,"2020-11-27 01:42:18+02","2024-05-17 05:22:41.487648+03","Edit I tried nuking my virtualenv uninstallingreinstalling serverlesspythonrequirements deletingrepulling the lambcilambda Docker image no dice Then I stumbled on this GH issue As hinted in it the experimental Docker feature Use gRPC FUSE for file sharing seems to be the cause I have switched it off in the Docker Preferences for now and deploys are successful "
64819633,64048687,"stackoverflow.com",1,"2020-11-13 12:53:54+02","2024-05-17 05:22:41.490649+03","I just had a similar problem See this issue for a possible solution In summary"
70708831,64060415,"stackoverflow.com",0,"2022-01-14 11:58:49+02","2024-05-17 05:22:42.615214+03","I only achieved it using a previously created JWT Authorizer for httpApi but must be similar with a custom Lambda Authorizer never used one More on this issue here AWS HTTP API Support IAM and Lambda authorizers 8210 feat AWS HTTP API Add support for custom Lambda authorizers 9192"
76582437,64060415,"stackoverflow.com",0,"2023-06-29 18:18:58+03","2024-05-17 05:22:42.617214+03","This configurations work today "
64546429,64074827,"stackoverflow.com",69,"2021-03-02 12:51:54+02","2024-05-17 05:22:43.488275+03","The issue is indeed indentation The indentation on this is weird so I will explain it below Every is a space Some pointers Another tip if you find your text editor is autoformatting your YAML file and replacing the space indentation with tabs or whatever add a editorconfig file to the root with these settings"
64250780,64074827,"stackoverflow.com",34,"2021-09-10 11:09:26+03","2024-05-17 05:22:43.490275+03","I had a similar issue The problem was with indentation Try to fix it like this"
76002502,64074827,"stackoverflow.com",0,"2023-04-13 09:59:04+03","2024-05-17 05:22:43.491276+03","I found the similer issue I solve by remove the prefix sapace from http events http path PostFunction method POST this is the correct one events"
64088265,64076430,"stackoverflow.com",0,"2020-09-27 15:04:14+03","2024-05-17 05:22:45.305016+03","Please try it in the following way"
64183744,64122628,"stackoverflow.com",0,"2020-10-04 13:26:58+03","2024-05-17 05:22:47.239405+03","I had the same issue You cannot change the server region afterwards I deleted everything serverless and nextserverless folders that were created and changed the serverless yml Add the following in your serverless yml file And deploy using npx serverless"
64131042,64124942,"stackoverflow.com",5,"2020-09-30 07:46:05+03","2024-05-17 05:22:47.953647+03","First of all please move out the plugins from provider"
64233667,64127315,"stackoverflow.com",0,"2020-10-06 23:56:05+03","2024-05-17 05:22:48.732326+03","As far as I know this use case is not presently possible with Lambda and API Gateway as the invocation to Lambda from API Gateway is of type RequestResponse and not of type Async Reference Once your code which in this case I believe is serverlessexpress calls context done the response is sent from Lambda to API Gateway and the execution context of the Lambda function is finished either frozen to be reused in the next invocation or recycled into Lambdas underlying infrastructure There are several options to invoke a new function to continue processing so your API can return to the user"
64191901,64127315,"stackoverflow.com",0,"2020-10-04 09:53:23+03","2024-05-17 05:22:48.735328+03","I would say that the AWS lambda function is terminating the execution after the response is written Are you using promises with S3 Should not the code to upload be Try that However I would also suggest moving the response render to the end of the route and handling things from the frontend For example"
64208312,64206992,"stackoverflow.com",3,"2020-10-05 16:50:19+03","2024-05-17 05:22:50.96337+03","Based on Serverless Framework documentation and my personal experience I would recommend this This will"
64210071,64210070,"stackoverflow.com",0,"2020-10-05 16:54:33+03","2024-05-17 05:22:51.729504+03","The issue was package include expects an array but I passed it a string In yaml an array called a sequence is denoted with a new line a dash and a space for each element YAML docs on defining sequences"
64230128,64229859,"stackoverflow.com",4,"2020-10-06 21:15:33+03","2024-05-17 05:22:52.606817+03","If you are running the app in a serverless environment traditional CRON is not going to be a good approach You should instead look into using something like AWS Cloudwatch to trigger recurring events on a timer through a webhookAPI endpoint This could trigger a Lambda which sends a payload to your API with some secure headers set so that you can verify that the request should actually be executed Alternatively you can set your serverless lambda to be triggered by external events besides just HTTP as well for example having Cloudwatch push a message onto EventBridge which in turn could safely trigger an execution of your lambda with some payload It would be up to you to configure an additional entry point for this in your NestJS app though"
64256439,64243798,"stackoverflow.com",1,"2020-10-08 08:41:04+03","2024-05-17 05:22:53.741639+03","Have you deployed the API Gateway Make sure in API Gateway you select the GET Method and under the action dropdowns select deploy A CORS issue can be debunked by seeing if you are able to reach the API Gateway from Postman but not the browser Since you are returning AccessControlAllowOrigin the browser will allow the request "
64266733,64251906,"stackoverflow.com",0,"2020-10-08 19:24:29+03","2024-05-17 05:22:54.854457+03","Well I found a workaround I replace Sub with Join This is what I had And this is the replacement However I would like to know if there is a better way "
64362875,64254669,"stackoverflow.com",0,"2020-10-15 02:34:48+03","2024-05-17 05:22:56.187859+03","Does not look like EventBridge supports multiple rules on a lambda Unless someone knows different but so far I have not found anything that indicates that is it is possible "
65558884,64254669,"stackoverflow.com",0,"2021-01-04 09:09:49+02","2024-05-17 05:22:56.189859+03","Yes It is possible with a little workaround Create rules as resources Add your lambda as target to the created rules Note Make sure that you have latest version of serverless framework"
66053382,64254669,"stackoverflow.com",0,"2021-02-04 22:30:44+02","2024-05-17 05:22:56.190859+03","This creates the rule in the event bus on EventBridge but it does not create the association between cloudwatch and lambda from the lambda perspective so when you put an event in the event bus it never triggers the lambda When you create the association between eventbridge and lambda using the default way It creates the rule in EventBridge and the Cloudwatch event association in the lambda you can check it in the lambda configuration TAB "
64499514,64262800,"stackoverflow.com",2,"2020-10-23 15:03:28+03","2024-05-17 05:22:56.640526+03",""
70259480,64262800,"stackoverflow.com",1,"2021-12-07 13:39:58+02","2024-05-17 05:22:56.64213+03","I checked with zipinfo requirements zip and found that macos dynlib where loaded instead of linux so files I fixed this by using dockerizePip nonlinux be aware that this will not be triggered if in the working dir a requirements zip already exists so git clean xfd before running sls deploy"
76464842,64262800,"stackoverflow.com",1,"2023-06-13 15:17:11+03","2024-05-17 05:22:56.644133+03","In case it is helpful to others I installed Docker on my dev machine only after installing serverlesspythonrequirements and deploying my app and I think this got me into a weird state I kept getting the same numpy error whatever I did My deploy only started working after I forcibly removed the serverlesspythonrequirements cache and redeployed I ran serverless deploy verbose to find out where the cache was Then removed it and then next serverless deploy worked "
64265952,64262800,"stackoverflow.com",0,"2020-10-08 18:37:32+03","2024-05-17 05:22:56.645299+03","Since you are using serverlesspythonrequirements plugin it will package the libraries for you In order words you do not need to do pip install t srcvendor r requirements txt nocachedir all that stuff manually To solve you problem remove srcvendor and the following two lines in data py Then sit back and let serverlesspythonrequirements do the work for you "
64270588,64269791,"stackoverflow.com",3,"2020-10-09 00:04:50+03","2024-05-17 05:22:57.581757+03","Turns out we were on a rather old version of serverless This was added in version 1 65 0 "
64426340,64426133,"stackoverflow.com",20,"2020-10-24 12:24:04+03","2024-05-17 05:22:58.360311+03","Based on the extra discussion in the comments It is not possible to make an ES domain totally public CloudFormation will not allow for that Thus there are three options to choose from Below I present three of them with in a sample serverless application This is just basic helloworld application it does not use the ES domain in any capacity but I use it to verify that each choice works and can be deployed using serverless framework without errors This will make your domain open for access to only individual IP address or IP CIDR range The example below limits access to one single IP address You can restrict access to your ES domain to selected IAM user or role This way only the given IAM userrole will be able to access the ES domain In the below I use lambda existing IAM role as a principle The function and its role must already exist The example here creates publicly accessible ES domain with finegrained controls that requires username and password This does not work in freetier I also hardcoded username and password which obviously would need to be modified and provided as a parameter from from SSM Parameter store in real application "
64433256,64432440,"stackoverflow.com",2,"2020-10-19 21:33:30+03","2024-05-17 05:22:59.37486+03","The const keyword was not added until draft 06 You should upgrade to an implementation that supports at least that version httpsjsonschema orgdraft06jsonschemareleasenotes htmladditionsandbackwardscompatiblechanges Otherwise you can use enum with a single value enum [public]"
68835736,64434002,"stackoverflow.com",0,"2021-09-03 15:55:23+03","2024-05-17 05:23:00.187339+03","I do not know yet why that is the issue but you cannot have two lambdas there pointing to the same path Better explained by example It would work if you change them to be other words or endpoints such as BUT this is obviously not feasible and not nicely readable looking at all endpoints from a user perspective In this case I would recommend to group many of those in one handler such as for example CreatePost UpdatePost DeletePost in one handler and then inside that handler differentiate between different method types POST PATCH DELETE via for example a switch or whatever Something like this You can get the method type from the event object of the lambda function handler This is fix the lambda cold start issue since the container the lambda function will run will be hit more often "
70563830,64434002,"stackoverflow.com",0,"2022-01-03 11:37:27+02","2024-05-17 05:23:00.18934+03","Seems to be an AWS issue where changing the endpoint variable causes an issue httpsgithub comserverlessserverlessissues3785"
68238798,64444314,"stackoverflow.com",2,"2021-07-03 21:03:16+03","2024-05-17 05:23:01.083404+03","Note that rds in the ARN was changed to rdsdb and dbproxy in the ARN was changed to admin the admin user of the database sls deploy and check the lambda You should see the proxy in the database proxy configuration section For example the Proxy ARN I copied from RDS was arnawsrdsuseast1123123123dbproxyblah123abc123abc and I edited it to be arnawsrdsdbuseast1123123123adminblah123abc123abc Also be sure your lambda is in the same Vpc as the RDS proxy or it will not be able to connect I am using"
64447929,64445104,"stackoverflow.com",0,"2020-10-20 18:02:32+03","2024-05-17 05:23:02.32557+03","Just add to your serverless yml A better practice is to include only the folders that relevant this the specific lambda function and not all the project files Taken from httpswww serverless comframeworkdocsprovidersawsguideserverless yml"
64460303,64445104,"stackoverflow.com",0,"2020-10-21 12:08:51+03","2024-05-17 05:23:02.32657+03","This seems to be coming from the custompackagingplugin Removing the plugin solves that issue but then your package zip explodes in size which is also a problem Amending the serverless yaml to unfortunately still yields the same error Error Error Cannot find module libcore at Function Module _resolveFilename internalmodulescjsloader js63615 at Function Module _load internalmodulescjsloader js56225 at Module require internalmodulescjsloader js69217 at require internalmodulescjshelpers js2518 at Object codebuildoutputsrc448056649srcnode_modulesarchiverindex js816 at Module _compile internalmodulescjsloader js77830 at Object Module _extensions js internalmodulescjsloader js78910 at Module load internalmodulescjsloader js65332 at tryModuleLoad internalmodulescjsloader js59312 at Function Module _load internalmodulescjsloader js5853 at Module require internalmodulescjsloader js69217 at require internalmodulescjshelpers js2518 at Object codebuildoutputsrc448056649srcnode_modulesserverlesscustompackagingpluginlibindex js518 at Module _compile internalmodulescjsloader js77830 at Object Module _extensions js internalmodulescjsloader js78910 at Module load internalmodulescjsloader js65332 at tryModuleLoad internalmodulescjsloader js59312 at Function Module _load internalmodulescjsloader js5853 at Module require internalmodulescjsloader js69217 at require internalmodulescjshelpers js2518 at requireServicePlugin usrlocallibnode_modulesserverlesslibclassesPluginManager js2712 at pluginsObject modules filter map name usrlocallibnode_modulesserverlesslibclassesPluginManager js14420 at Array map at PluginManager resolveServicePlugins usrlocallibnode_modulesserverlesslibclassesPluginManager js1418 at PluginManager loadAllPlugins usrlocallibnode_modulesserverlesslibclassesPluginManager js12740 at service load then usrlocallibnode_modulesserverlesslibServerless js8839 at tryCatcher usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleaseutil js1623 at Promise _settlePromiseFromHandler usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js54731 at Promise _settlePromise usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js60418 at Promise _settlePromise0 usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js64910 at Promise _settlePromises usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js72918 at Promise _fulfill usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js67318 at Promise _settlePromise usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js61721 at Promise _settlePromise0 usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js64910 at Promise _settlePromises usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js72918 at Promise _fulfill usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js67318 at Promise _resolveCallback usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js46657 at Promise _settlePromiseFromHandler usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js55917 at Promise _settlePromise usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js60418 at Promise _settlePromise0 usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js64910 at Promise _settlePromises usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js72918 at Promise _fulfill usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js67318 at PromiseArray _resolve usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise_array js12719 at PromiseArray _promiseFulfilled usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise_array js14514 at Promise _settlePromise usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js60926 at Promise _settlePromise0 usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js64910 at Promise _settlePromises usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js72918 at Promise _fulfill usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js67318 at Promise _resolveCallback usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js46657 at Promise _settlePromiseFromHandler usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js55917 at Promise _settlePromise usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js60418 at Promise _settlePromise0 usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js64910 at Promise _settlePromises usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js72918 at Promise _fulfill usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js67318 at PropertiesPromiseArray PromiseArray _resolve usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise_array js12719 at PropertiesPromiseArray _promiseFulfilled usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleaseprops js7814 at Promise _settlePromise usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js60926 at Promise _settlePromise0 usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js64910 at Promise _settlePromises usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleasepromise js72918 at _drainQueueStep usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleaseasync js9312 at _drainQueue usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleaseasync js869 at Async _drainQueues usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleaseasync js1025 at Immediate Async drainQueues [as _onImmediate] usrlocallibnode_modulesserverlessnode_modulesbluebirdjsreleaseasync js1514 at runCallback timers js70518 at tryOnImmediate timers js6765 at processImmediate timers js6585 "
64461165,64460245,"stackoverflow.com",1,"2022-09-16 17:31:54+03","2024-05-17 05:23:03.301782+03","We can provide a custom bucket for serverless using the plugin Go through this link use custom bucket in serverless Edit You can now directly specify deployment bucket in serverless without any plugin Refer httpswww serverless comframeworkdocsprovidersawsguideserverless ymldeploymentbucket"
64500825,64492570,"stackoverflow.com",1,"2020-10-27 17:42:48+02","2024-05-17 05:23:04.256514+03","The fastest way using the serverless framework might be to hook up a lambda function to the s3 event stream as outlined here Then I suppose you could just write to cloudwatch using console log but likely it would be simpler to skip cloudwatch and use this lambda function to trigger your step function The serverless yml file should look like this Additional context taken from below "
64493835,64492570,"stackoverflow.com",0,"2020-10-23 07:23:35+03","2024-05-17 05:23:04.258514+03","I have used the method described here before httpsdocs aws amazon comeventbridgelatestuserguidelogs3dataevents html In short you enable Cloudtrail configure Eventbridge to matchfilter the S3 resources and operations you are interested in and then use Lambda to log those specific events or can trigger other actions "
64542301,64492570,"stackoverflow.com",0,"2020-10-26 19:45:33+02","2024-05-17 05:23:04.259685+03","The solution was explained in the link above that Aaton Stuyvenberg pointed We needed to declare that the bucket existed The event would be an S3 event and the definition of the function should be The key existing true signal that a new bucket does not been to be created More about this is explained here Serverless Framework Documentation"
64493634,64493633,"stackoverflow.com",2,"2020-10-23 06:53:07+03","2024-05-17 05:23:05.072906+03","You will need to use the hooks in the lambdarust docker builder to install the necessary dependencies and include the library files in the packaged build First create a directory at the root of the project named lambdarust and in it create two files install and package Adjust the name of the library file for whichever is installed for you I added a line with echo ls usrlib64mysql in one of the scripts to see what files existed after the install "
64516435,64511315,"stackoverflow.com",0,"2020-10-24 20:56:57+03","2024-05-17 05:23:05.477845+03","The DynamoDB docker region is the same that the one configured in your AWS CLI The region in your Serverless Yaml has to be the same by default it is useast1 even if you work locally "
64519878,64515356,"stackoverflow.com",2,"2020-10-25 04:53:08+02","2024-05-17 05:23:06.441888+03","Turns out it was adding the binary media type multipartformdata on API Gateway this had to be deployed and it took a while before the change took effect and now I no longer get blank PDFs "
65633550,64538621,"stackoverflow.com",0,"2021-01-08 19:21:44+02","2024-05-17 05:23:07.477837+03","I guess you have been using an older serverlesswsgi version There has been an update that supports v2 payload "
64539335,64539200,"stackoverflow.com",1,"2020-10-26 16:42:30+02","2024-05-17 05:23:08.328068+03","Is not it accidentally deployed to another AWS account You can check arn of resources in sls logs add v if you dont see logs in detail and check if that lambda exists in proper account Arn has aws account id which is really usable to track where you deployed "
64543272,64542672,"stackoverflow.com",1,"2020-10-26 20:52:18+02","2024-05-17 05:23:09.400064+03","It turns out the problem was the addition to my serverless yml file When I removed this line the APIGatewayProxyHandler interface worked as expected "
64547116,64543066,"stackoverflow.com",4,"2020-10-27 03:22:41+02","2024-05-17 05:23:10.161286+03","I think this has to do with the case sensitivity of the Amazon States Language Try updating the StartAt with imageWasUploadedEvent"
64547041,64543433,"stackoverflow.com",1,"2020-10-27 03:11:13+02","2024-05-17 05:23:11.153367+03","It depends on the types of S3 events you are trying to trigger from The link you provided shows that you can use CloudTrail events to trigger CloudWatch event rules This would allow you to trigger on events such as creatingremoving buckets If you want to trigger off of things like objects being addeddeleted from a bucket then you will not be able to do it without writing some additional resources You can create S3 events to trigger SQS SNS or a Lambda function My suggestion would be to implement a lambda function that accepts an S3 event and kicks off your state machine as desired Then create an S3 event to trigger your lambda when the desired object event occurs "
64548562,64545924,"stackoverflow.com",0,"2020-10-27 07:03:51+02","2024-05-17 05:23:12.098403+03","First of all please move the awssdk to dev dependencies It is already installed in the AWS lambda container Second you may also use serverless plugin serverlesswebpack for more decreasing the size of lambda Besides that I cannot notice anything significant what may cause the build size to be different "
64762579,64553344,"stackoverflow.com",0,"2020-11-10 05:46:48+02","2024-05-17 05:23:12.892941+03","Your param UserPoolId in SDK calls is incorrect Will not be able to see that with only what you have posted "
64797861,64553344,"stackoverflow.com",0,"2020-11-13 05:49:38+02","2024-05-17 05:23:12.894941+03","Need another await in front of calling that function like this"
64613267,64559720,"stackoverflow.com",1,"2020-10-30 19:35:42+02","2024-05-17 05:23:13.899358+03","After studying how StepFunctions work I finally arrived to the conclusion that this is a wrong pattern In no place on the documentation of the plugins or Amazon it says that what I did is a pattern StepFunctions can be started by events on CloudWatch and those events could be originated on a change on S3 That is not what I did here There is no a direct link between an action on S3 and a StepFunction The link in AWS documentation could confuse someone to think otherwise Here is the title Starting a State Machine Execution in Response to Amazon S3 Events But the state machine does not start because an S3 events but because the CloudWatch log that this event generates A lambda proxy function is a good way of invoking an State Machine This is a easy to use pattern and very common as we can use it with SQS etc So the correct response to this question is The state machine does not start because it is never invoked We just called a lambda function that was used as the StartAt for an StateMachine This does mean I invoked the State Machine That is the reason why there is no logs for the state machine meanwhile there are correct logs for the lambda function Hope this response helps I will add more details and reference to this response BR"
64571027,64559720,"stackoverflow.com",1,"2020-10-28 12:34:04+02","2024-05-17 05:23:13.903359+03","Or it is that the Step Function itself is not working and the lambda function is working just a lambda function totally independent of the Step Function To verify whether lambda was invoked as part of step function or not cannot you just check execution history from the step function console Also unless you have explicitly configured s3 to publish events to lambda your lambda will not be automatically invoked upon uploading files to s3 What I have to do so the lambda function gets trigger as part of the Step Function To be able to call trigger step function on fileupload to s3 you can follow this tutorial httpsdocs aws amazon comstepfunctionslatestdgtutorialcloudwatcheventss3 html"
64568024,64564233,"stackoverflow.com",7,"2020-10-28 09:14:18+02","2024-05-17 05:23:15.712301+03","You can use jests automock by adding and then AWS DynamoDB DocumentClient will be a mocked class so you will be able to mock it is implementation And since we want it is get method to be a function that accepts anything as a first argument as we will not do anything with it within the mock implementation and a callback that we are expecting it to have been called with null and user we can mock it like this"
64565943,64564233,"stackoverflow.com",6,"2020-10-28 05:11:34+02","2024-05-17 05:23:15.713301+03","You could use jest mock moduleName factory options to mock awssdk module manually E g handler js handler test js unit test result source code httpsgithub commrdulinjestcodelabtreemastersrcstackoverflow64564233"
64598578,64579406,"stackoverflow.com",0,"2020-10-29 22:12:28+02","2024-05-17 05:23:16.76605+03","In the documentation for serverlesspluginsplitstacks it states Many kind of resources as e g DynamoDB tables cannot be freely moved between CloudFormation stacks that can only be achieved via full removal and recreation of the stage I am not 100 sure this is the error being thrown with a bad message but to test it out I would try applying your CloudFormation templates to an empty new AWS account and see if the succeed "
64599275,64599016,"stackoverflow.com",0,"2020-10-29 23:11:14+02","2024-05-17 05:23:17.805411+03","Alright so when you use STS Assume Role SessionToken is not optional and is needed to be included This is the answer "
64610845,64602303,"stackoverflow.com",3,"2020-10-30 17:00:27+02","2024-05-17 05:23:18.737545+03","Ref ApiGatewayRestApi ApiGatewayRestApi is a default Rest API logical ID How do I getreference API gateway restAPI id in serverless yml"
76278018,64602303,"stackoverflow.com",0,"2023-05-18 08:37:07+03","2024-05-17 05:23:18.739546+03","For socket endpoint you can use Also check your cloudformation in serverless to recheck these names "
64608182,64606487,"stackoverflow.com",1,"2020-10-31 14:32:14+02","2024-05-17 05:23:19.748502+03","You should be able to do dynamoose model example_user schema create false to get away from the need to create a table httpsdynamoosejs comguideModel"
64619414,64619300,"stackoverflow.com",2,"2020-10-31 12:56:52+02","2024-05-17 05:23:20.688287+03","Probably your friend uses gifted AWS credentials profile If you want to keep using the same profile name you can create the gifted credentials profile as shown in the linked docs Or use your own profile serverless deploy awsprofile profile name as shown here "
64688115,64670539,"stackoverflow.com",23,"2020-11-04 23:41:13+02","2024-05-17 05:23:23.962925+03","It is really simple to apply this change all what you need to do is to add this to your serverless yml file "
64768335,64670539,"stackoverflow.com",6,"2020-11-10 13:45:49+02","2024-05-17 05:23:23.964926+03",""
64777281,64670611,"stackoverflow.com",1,"2020-11-11 20:24:27+02","2024-05-17 05:23:25.0781+03","The answer is type coercion Had to downcast to unknown then upcast to the Sns event type Heres a minimal serverless ts from my setup"
64876054,64680652,"stackoverflow.com",1,"2020-11-17 15:19:25+02","2024-05-17 05:23:26.145816+03","In case anyone ever faces this issue After going over the docs one more time apparently what I initially tried to do is not possible According to CF docs For outputs the value of the Name property of an Export cannot use Ref or GetAtt functions that depend on a resource Similarly the ImportValue function cannot include Ref or GetAtt functions that depend on a resource "
65372922,64708459,"stackoverflow.com",0,"2020-12-19 20:11:28+02","2024-05-17 05:23:27.145212+03","Sorry for late response Not sure if you found a solution yet but I will answer anyway if that helps anyone I notice 2 things here So here what what used was iamRoleStatements because serviceRoleArn was misspelled The name of the generated resource confirms that GraphQlDsUsersRole As for why the policy is malformed this is because Resource is misspelled too It has to be singular even if you pass an array "
64724763,64724464,"stackoverflow.com",0,"2020-11-07 07:33:01+02","2024-05-17 05:23:29.191719+03",""
65011779,64897299,"stackoverflow.com",0,"2020-11-25 21:55:54+02","2024-05-17 05:23:30.217332+03","You can set up multiple http events for the same lambda function in your serverless yml file Each http event can have its own definition andor path to give you the opportunity to capture the event depending on which endpoint your user hits even with or without query params Define an http event for the function you need and declare one for each path you need with and without the query param The trick is going to be in the lambda itself where you are going to have to set up a few conditions or trycatch statements to deal with the fact that in some cases you will not have a queryparam and in others you will but that is not too hard to deal with "
64942334,64908730,"stackoverflow.com",0,"2020-11-21 13:05:14+02","2024-05-17 05:23:31.648036+03","Probably now what you hoped for but you cannot export client secret in CloudFormation explicitly Take a look at the return values from AWSCognitoUserPoolClient There you can only get the client ID What you could do is to create the client in another CF template and either create there a custom resource to read the secret and output it or have an intermediate step where you get this value with CLI and then pass it into serverless There is currently no other option "
64918213,64917954,"stackoverflow.com",3,"2020-11-19 20:56:01+02","2024-05-17 05:23:32.115171+03","Resorce is a typo of Resource Recommend trying the CloudFormation Linter in VSCode to see some of these errors inline while authoring templates along with autocompletion and documentation links"
64964708,64957832,"stackoverflow.com",1,"2020-11-23 10:17:07+02","2024-05-17 05:23:33.12592+03","What you will need to do is figure out if Runway has a way for you to add an environment variable for the build container The Serverless Framework looks for an environment variable called SERVERLESS_ACCESS_KEY to authenticate you to your Serverless account From a quick Google it looks like this can be done using env_vars as a parameter in your config"
65014927,64982641,"stackoverflow.com",1,"2020-11-26 03:27:27+02","2024-05-17 05:23:34.08934+03","Assuming you have a CloudFront Distribution custom resource in your serverless yml you can reference it into an environment variable "
65014907,64984521,"stackoverflow.com",1,"2020-11-26 03:23:14+02","2024-05-17 05:23:35.125984+03","You just use Typescripts import "
64988692,64987159,"stackoverflow.com",2,"2020-11-24 16:36:23+02","2024-05-17 05:23:36.397961+03","Great question The Serverless Framework is a project written in NodeJS Specifically sls plugin install basically just runs npm install under the hood This means that sls plugin install just fetches the plugin from NPM and installs it via adding it to the project package json and packagelock json I would guess you can likely run npm i g serverlesspythonrequirements to install the library globally for your system and then I suspect you could just declare the plugin in the plugins block of each projects serverless yml file and be done "
75868939,65002698,"stackoverflow.com",0,"2023-03-28 19:48:20+03","2024-05-17 05:23:38.231046+03","Closest you can get would be reusing a yaml anchor for deployment steps and match custom deployment environments with your services Bonus Bitbucket will track each microservice deployment independently I doubt BITBUCKET_DEPLOYMENT_ENVIRONMENT can be interpolated in the includePaths block so this is still quite verbose If you manage to smarten your deploy script to detect the file changes for you then you could reduce this to But beware there is a short limit to the amount of custom deployment environments you can have "
65118542,65033849,"stackoverflow.com",5,"2020-12-03 03:22:35+02","2024-05-17 05:23:39.576935+03","I was able to accomplish this utilizing a plugin called serverlessplugincanarydeployments httpsgithub comdavidgfserverlessplugincanarydeployments I found it from this blog post httpswww serverless comblogmanagecanarydeploymentslambdafunctionsserverlessframework Example Serverless com confirmed with me that there is no native way to do this but I think this plugin does what I need it to do with the caveat that deploymentSettings does not pass the configuration validation so you will get a warning on deploy "
65082502,65035066,"stackoverflow.com",1,"2020-12-01 01:13:02+02","2024-05-17 05:23:40.351776+03","You can use SecretsManager and you can import it from there into your function with something like this"
65152842,65066945,"stackoverflow.com",1,"2020-12-05 03:19:05+02","2024-05-17 05:23:41.318388+03","Atul Sharma wrote Just attach AmazonAPIGatewayAdministrator policy and that totally worked "
67154653,65119342,"stackoverflow.com",0,"2021-04-19 03:31:25+03","2024-05-17 05:23:42.995812+03","I found this to be just what would serve the purpose right httpsgithub comzotoiogeneratormonoserverless"
66272137,65123631,"stackoverflow.com",8,"2021-02-19 07:23:27+02","2024-05-17 05:23:44.052468+03","I got same warning when I used the value as 2 Integer I checked in aws cloudwatch log retention settings It lists some options as this image Then I changed the value to 3 which is in the list the warning message was disappeared "
75308902,65123631,"stackoverflow.com",1,"2023-02-15 05:10:40+02","2024-05-17 05:23:44.054468+03","The serverless plugin for setting logRetention internally uses the PutRetentionPolicy AWS api to set up the logRetentionPolicy by which we can set up a retention period The api accepts only a set of valid values for the retention days as you can see in this link Also to make it never expire we simply do not have to set it to any logRetentionPolicy Reference "
70095660,65127818,"stackoverflow.com",1,"2021-11-24 14:00:32+02","2024-05-17 05:23:44.610842+03","With the testing strategy you linked you can mock your requests and responses to external services to test different scenarios Nock is a library that can simplify mocking external requests in your tests Although it seems that this may not be work as a I imagine with serverlessoffline I found this answer that outlines a strategy for replacing endpoints that access external services when running tests "
65698470,65697616,"stackoverflow.com",0,"2021-01-13 10:53:11+02","2024-05-17 05:24:07.624102+03","So it depends on how you package your function below is my SAM template for function and the corresponding lambda layer And my directory structure for the packaging is"
65140304,65139557,"stackoverflow.com",0,"2020-12-04 10:36:28+02","2024-05-17 05:23:45.709014+03","Serverless uses CloudFormation under the hood and there is no direct way to share data between CloudFormation and terraform Most common way is to use SSM to share data in both directions so I think you are on the right track you can use data source to fetch value from SSM There is an overview how to use Serverless framework with terraform in this blog post"
65152109,65147827,"stackoverflow.com",1,"2020-12-05 01:23:51+02","2024-05-17 05:23:46.668425+03","Sadly you cannot do this easily as the bucket and sqs queue are out of CFNs control You could consider two options"
65156776,65147827,"stackoverflow.com",0,"2020-12-05 14:12:22+02","2024-05-17 05:23:46.670425+03","You can do this with EventBridge and CloudTrail See httpsaws amazon comblogscomputeusingdynamicamazons3eventhandlingwithamazoneventbridge"
65235608,65150973,"stackoverflow.com",1,"2020-12-10 15:37:38+02","2024-05-17 05:23:47.749979+03","Looks like you are referencing something called CustomIamRole in your state machine creation but I cannot see it being created anywhere in the yaml file Either create the role and use it in creation or remove the depends on part "
65160907,65157346,"stackoverflow.com",3,"2021-01-09 18:47:27+02","2024-05-17 05:23:48.786029+03","The main reason for slow coldstart times with a Java Lambda is the need to load classes and initialize objects For simple programs this can be very fast a Lambda that does nothing other than print Hello World will run in 40 ms which is similar to the Python runtime On the other hand a Spring app will take much more time to start up because even a simple Spring app loads thousands of classes before it does anything useful While the obvious way to reduce your coldstart times is to reduce the number of classes that you need to load this is rarely easy to do and often not possible For example if you are writing a webapp in Spring there is no way around initializing the Spring application context before processing a web request If that is not an option and you are using the Maven Shade plugin to produce an uberJAR you should switch to the Assembly plugin as I describe here The reason is that Lambda unpacks your deployment bundle so an uberJAR turns into lots of tiny classfiles that have to be individually opened Lastly increase your memory allotment This without question the best thing that you can do for Lambda performance Java or otherwise First because increasing memory reduces the amount of work that the Java garbage collector has to do Second because the amount of CPU that your Lambda gets is dependent on the memory allotment You do not get a full virtual CPU until 1769 MB I recommend that for a Java app you give it 2 GB the cost of the bigger allotment is often offset by reduced CPU requirements One thing I would not do is pay for provisioned concurrency If you want a machine up and running all the time use ECSEKSEC2 And recognize that if you have a bump in demand you are still going to get cold starts Update I spent some time over the holiday quantifying various performance improvement techniques The full writeup is here but the numbers are worth repeating My example program was like the OPs a do nothing that just created an SDK client and used it to invoke an API I ran this with different memory sizes forcing a cold start each time All times are in milliseconds As I said above the primary benefit that you get from increasing memory is that you increase CPU at the same time Creating and initializing an SDK client is CPUintensive so the more CPU you can give it the better Update 2 this morning I tried compiling a simple AWS program with GraalVM It took several minutes to build the standalone executable and even then it created a fallback image which has an embedded JDK due to dependencies of the AWS SDK When I compared runtimes there was no difference between running with standard Java Bottom line use Java for things that will run long enough to benefit from Hotspot Use a different language Python JavaScript perhaps Go for things that are shortrunning and need low latency "
65158836,65157346,"stackoverflow.com",3,"2020-12-05 17:50:32+02","2024-05-17 05:23:48.79171+03","Provisioned concurrency helps with The the code initialization duration you are having Other than that it targets to another overhead coming from execution environment setup for your functions code Refer to Turning on Provisioned Concurrency section here "
67836541,65157346,"stackoverflow.com",2,"2021-06-07 08:39:41+03","2024-05-17 05:23:48.792709+03","awslightweightclientjava is a standalone jar no dependencies and is less than 60K It was built with the exact purpose of reducing Java Lambda cold start times which it does considerably and it is easy to use though you might have to check the AWS API docs for your task I have found that with the AWS SDK S3 jar my cold start time was about 10s and with this lightweight client it is down to 4s this is with 512MB memory allocated Allocating 2GB memory to the Lambda yields a cold start time of 3 6s with the AWS SDK and down to 1s with the lightweight client The mere fact that the library is making https calls does bring about the loading of 2000 or so classes so it is hard to go a lot quicker than 1s unless there is some cool https library out there that is much more efficient in this regard "
72336158,65157346,"stackoverflow.com",2,"2022-05-22 12:43:13+03","2024-05-17 05:23:48.79471+03","Basically there are a set of recommendations that I am using as a cheat sheet every time I have to optimize lambda performance Use SDKv2 I have seen a lot of times when AWS SDKv1 and v2 are completely incompatible Migration from v1 to v2 can be easy but sometimes API changes are so huge that you simply cannot find the corresponding method in V2 But if you can then you better do so V2 introduces a lot of performance improvements so its a rule of thumb If its possible then use V2 whenever you can Using defined credentials provider AWS SDK has quite an interesting way of detecting the credentials It passes multiple steps trying to figure out the proper credentials until it finds or fails to find any All these steps take time and you save some milliseconds by specifying exact credentials providers Like this This way SDK wouldnt traverse all possible sources of creds and immediately detect the correct one Initialize everything prior to the execution Simple advice to follow You may want to simplify things and put all the initialization into the handler method Better dont do this but try to put as much initialization into the constructor as possible It may reduce latency for repetitive lambda invocations Reduce jar size To reduce lambda cold start one of the not obvious advice is to reduce the jar size Java developers usually dont care to include a few more libraries to avoid reinventing the wheel But in the case of lambda you better take a closer look at your pom xml and clean everything unnecessary Cause a bigger jar means a longer cold start Avoid using any DI I do not think you wanted to use any kind of DI But in case you do try to avoid it Lambdas purpose is to be small and lightweight And DI will dramatically increase cold start and it does not make a lot of sense to wire up 23 classes Use Tiered compilation Java Just in time compilation has such a cool feature as a tiered compilation introduced since Java 8 was released The purpose of JIT is to run the code and eventually reach the native code performance It cannot be done immediately But running the code and analyzing the hot spots JIT eventually interprets code almost as good as native This can be achieved by collecting the profiling information in the background This makes sense with your monolith application running in a servlet container for ages But shortlived Lambda cannot benefit from these optimisations and its better to turn it off completely To do this put these env variables how to put env vars For better understanding I would refer to oracle docs httpsdocs oracle comjavacomponentsjrockithotspotmigrationguidecompopt htmJRHMG119 Almost the same situation is with the region It took some time to figure out the region lambda is deployed and this time can be reduced by specifying the region explicitly Overall the resulting configuration should look like Use RDS Proxy to have connection pooling In case you are planning to use Lambda with RDS then this advice may help you as well otherwise skip it In normal Java applications it is common to use a pool of connections to reuse existing ones and save some time on establishing new ones RDS Proxy service is coming to the rescue when you are using Lambda Increase the memory allocated Simple yet powerful advice It can be that your Lambda can run out of memory with the standard 128 Mb allocated And it seems correct to increase memory in this case What is hidden and not obvious is that increasing memory allocated gives your lambda more CPU available So the combination of increased memory allocated and more virtual CPU power available of course decreases the execution time Giving your lambda more memory and CPU means increased costs But less execution time Instead of guessing which combination is better I propose to use this tool httpsgithub comalexcasalboniawslambdapowertuning Use provisioned concurrency Other guys already mentioned this Probably the most simple and easy way to solve the problem But it incurs additional costs Provisioned concurrency means AWS will keep execution context ready for you to be used thus decreasing lambda cold start You can specify the number of provisioned instances and enjoy lambda being warmed up for you There is exotic advice to use Graal VM but I think my answer is long enough "
69313162,65184833,"stackoverflow.com",4,"2021-09-24 13:02:27+03","2024-05-17 05:23:50.877812+03","For future reference if anyone is looking for an explanation on the process of binary media type support for serverless I personally have failed to find good internal serverless packages that support binary This is now all handled directly within serverlessAPIG To answer the main question I will answer this in phases 1 Your serverless package The biggest pitfall is that default Accept headers are set to which is why you see a lot of documentation surrounding setting your Binary Media Type to within APIG Do not do this this means that every request you send will be encoded which will mess up your cors approach and give you a false response Add this to your YAML and within you API in API gateway your api settings Binary media type you will see your MIME type added once deployed This is all you need to add to your serverless file 2 Your functions NODEJS Typescript Note here I am using the serverlesshttp package the reason this is important is because it allows you to set the accepted MIME types As you can see here we need to set the Content type and make sure we are returning a stream and piping that into our response This basically tells lambda we want this to be encoded into a base64 Why base64 Base64 encoding schemes are commonly used when there is a need to encode binary data that needs to be stored and transferred over media that are designed to deal with ASCII This is to ensure that the data remain intact without modification during transport If this is not done you will end up losing packets across APIG Hence why your data may not work if sent as a raw stream 3 Your client Accept header If you do not add an accept header to your request from the client you will get a base64 back if you add an accept header of applicationvnd mapboxvectortile this will tell APIG to decode the format on response Replace all cases of applicationvnd mapboxvectortile with your accepted binary media type and hey presto Flow for Serverless APIG binary media type"
65298372,65207520,"stackoverflow.com",2,"2020-12-15 15:51:18+02","2024-05-17 05:23:52.877478+03","To enable XRay tracing for all your Services Lambda functions you just need to set the corresponding tracing configuration on the provider level If you want to setup tracing on a perfunction level you can use the tracing config in your function definition Setting tracing to true translates to the Active tracing configuration You can overwrite this behavior by providing the desired configuration as a string Also note that you can mix the provider and functionlevel configurations All functions will inherit the providerlevel configuration which can then be overwritten on an individual function basis It is recommended to setup XRay tracing for Lambda with the aforementioned tracing configuration since doing so will ensure that the XRay setup is managed by the Serverless Framework core via CloudFormation You can get more granular and specify which resources you want traced as well Open your serverless yml and add a tracing config inside the provider section IMPORTANT Due to CloudFormation limitations it is not possible to enable AWS XRay Tracing on existing deployments which dont use tracing right now Please remove the old Serverless Deployments and redeploy your lambdas with tracing enabled if you want to use AWS XRay Tracing for Lambda Lastly do not forget to have the right IAM permission policies configured To enable XRay tracing for other AWS services invoked by AWS Lambda you MUST Install the AWS XRay SDK In your project directory run npm install s awsxraysdk Update your Lambda code and wrap AWS SDK with the XRay SDK Change To As of Release Serverless v140"
66198369,66096833,"stackoverflow.com",1,"2021-02-14 19:54:54+02","2024-05-17 05:24:36.696234+03","I solved this by base64 encoding my binary data and using the default handler which allows arbitrary datatypes"
71235671,66104739,"stackoverflow.com",3,"2022-02-23 12:59:33+02","2024-05-17 05:24:37.768536+03","Sls added support for stripping null properties about a year ago so now it is pretty straitforward"
66109596,66104739,"stackoverflow.com",1,"2021-02-08 23:25:10+02","2024-05-17 05:24:37.769536+03","Just try to keep it simple without additional yml configs and complicated ifstatement and take benefit of Conditional variable "
68963492,65207520,"stackoverflow.com",1,"2021-08-28 13:33:39+03","2024-05-17 05:23:52.880474+03","At the moment Lambda does not support continuing traces from triggers other than REST APIs or direct invocation The upstream service can be an instrumented web application or another Lambda function Your service can invoke the function directly with an instrumented AWS SDK client or by calling an API Gateway API with an instrumented HTTP client In every other case it will create its own new Trace ID and use that instead You can work around this yourself by creating a new AWSXray segment inside the Lambda Function and using the incoming TraceID from the SQS message This will result in two Segments for your lambda invocation One which Lambda itself creates and one which you create to extend the existing trace Whether that is acceptable or not for your use case is something you will have to decide for yourself If you are working with Python you can do it with awsxraylambdasegmentshim If you are working with NodeJS you can follow this guide on dev to If you are working with NET there are some examples on this GitHub issue "
65215034,65214210,"stackoverflow.com",5,"2020-12-09 12:32:51+02","2024-05-17 05:23:53.959974+03","Solution Add AwsExpressServerlessMiddleware to your setup during bootstrap Note The app use should be before app init Now the event and context object can be accessed Credits This answer on SO"
78117724,65214210,"stackoverflow.com",0,"2024-03-07 02:58:12+02","2024-05-17 05:23:53.961763+03","With the latest version of the codegenieserverlessexpress package you can easily access the API Gateway event context in your controllers without any additional middleware Nothing to do in handler function Simply add to your controller Source"
72341950,65235859,"stackoverflow.com",4,"2022-05-23 02:29:10+03","2024-05-17 05:23:55.069096+03","We had the exact same problem and contacted AWS support and the support says Edge optimised APIs are fronted by a default CF distribution which the WAF is not able to recognise To make WAF work for the default invoke URL you either need to change the endpoint type to Regional or create a custom CF distribution "
65236846,65235859,"stackoverflow.com",1,"2020-12-10 16:52:38+02","2024-05-17 05:23:55.070096+03","API Gateway can restrict access by API key CloudFront can send a custom secret xapikey header when it accesses the origin The absence of that header in other requests to the API Gateway will cause API requests to be rejected with 403 Forbidden See Protecting your API using Amazon API Gateway and AWS WAF parts one and two "
66592202,65237353,"stackoverflow.com",3,"2022-05-17 12:07:11+03","2024-05-17 05:23:55.826196+03","The problem is that they moved the path of the java runtime wrapper Base paths differ regarding install location NVM global installation NPM global installation Homebrew Replace 2 29 0 with installed serverless version "
65268133,65245897,"stackoverflow.com",8,"2020-12-12 20:01:29+02","2024-05-17 05:23:56.890384+03","If you are managing your deployment with plain CloudFormation and the aws command line interface you can handle this relatively easily using aws cloudformation package to generate a packaged template for deployment aws cloudformation package accepts a template where certain properties can be written using local paths zips the content from the local file system uploads to a designated S3 bucket and then outputs a new template with these properties rewritten to refer to the location on S3 instead of the local file system In your case it can rewrite Code properties for AWSLambdaFunction that point to local directories but see aws cloudformation package help for a full list of supported properties You do need to setup an S3 bucket ahead of time to store your assets but you can reuse the same bucket in multiple CloudFormation projects So let us say you have an input yaml with something like You might package this up with something like Which would produce an output yaml with something resembling this You can then use output yaml with aws cloudformation deploy or any other aws cloudformation command accepting a template To truly deploy with one command and ensure you always do deployments consistently you can combine these two commands into a script Makefile or something similar "
65248340,65245897,"stackoverflow.com",1,"2020-12-11 10:58:30+02","2024-05-17 05:23:56.893385+03","you can zip the file first then use aws cli to update your lambda function"
65252105,65245897,"stackoverflow.com",0,"2020-12-11 15:18:15+02","2024-05-17 05:23:56.894379+03","Within CloudFormation last 3 lines "
68323699,65245897,"stackoverflow.com",0,"2021-07-10 03:06:35+03","2024-05-17 05:23:56.89538+03","Re your comment The only issue is aws SAM or serverless framework create API gateway as default that I do not need it to be created For Serverless Framework by default that is not true The default generated serverless yml file includes config for the Lambda function itself but the configuration for API Gateway is provided only as an example in the following commented out section If you uncomment the events section for http then it will also create an API Gateway config for your Lambda but not unless you do "
65669708,65248377,"stackoverflow.com",0,"2021-01-11 17:27:02+02","2024-05-17 05:23:57.547617+03","Use HTTP connector HTTP PUT request method should work specify the full URL including filename and that should work"
65422692,65303230,"stackoverflow.com",2,"2020-12-23 12:24:16+02","2024-05-17 05:23:59.329427+03","I have experienced this too and had the same exact behaviour with chrome tabs crashing when running the serverless deploy command I narrowed it to serverless opening too many files more than the default limit you might have when running the deploy command which is causing the chrome tabs unable to use required resources and crashing Try to increase your ulimit and check if it happens again "
65328136,65312813,"stackoverflow.com",3,"2020-12-16 19:25:05+02","2024-05-17 05:23:59.979892+03","You need to specify that your job is a Fargate job in platformCapabilities with a Fargate version i would recommend 1 4 See Job Definitions section in httpsdocs aws amazon combatchlatestuserguidefargate html"
65328287,65312813,"stackoverflow.com",1,"2020-12-16 19:35:17+02","2024-05-17 05:23:59.981892+03","The launch might not be reflected already in the CFN Resource Spec They should be soon ish Here are the notes of someone who already did use Fargate on batch via CFN httpsgithub compplucfnfargatebatch"
65770687,65337078,"stackoverflow.com",0,"2021-01-18 10:01:25+02","2024-05-17 05:24:00.940899+03","The middleware wrapper cannot be a promise You can also remove the onError step and use the handler callback to short circuit the call stack "
65407406,65354597,"stackoverflow.com",0,"2020-12-22 12:54:19+02","2024-05-17 05:24:02.457708+03","So I was able to solve this with the following code I created a dummy certificate with SANs and Apparently nodeforge extracts that information and places them inside extensionRequest subjectAltNames I have these hardcoded since I am assuming these attribute names will not change but I cannot be 100 sure about it Hope this helps someone who might have gone through this issue Thank you "
68824505,65638381,"stackoverflow.com",1,"2021-08-18 01:04:54+03","2024-05-17 05:24:03.000965+03","I am using this plugin review it to deploy logically serverlesspluginsplitstacks"
65639734,65639686,"stackoverflow.com",2,"2021-01-09 07:42:29+02","2024-05-17 05:24:03.972853+03","You would have to setup a prefix as shown in the docs "
65660904,65660643,"stackoverflow.com",18,"2021-01-11 05:12:40+02","2024-05-17 05:24:05.058672+03","You need to provide the relative path to the handler this should work You may choose to omit and handler srcaccount handler should also work "
65774566,65661823,"stackoverflow.com",2,"2021-01-18 14:29:10+02","2024-05-17 05:24:05.975622+03","I have created a bash script for deploying stacks one after another Sequentially "
65675854,65675720,"stackoverflow.com",1,"2021-01-12 01:13:49+02","2024-05-17 05:24:06.89418+03","The only apparent issue in your AWSLambdaPermission is incorrect ARN You cannot have as ARN instead it should be Alternatively you can just remove the entire SourceArn property "
68349934,66104739,"stackoverflow.com",1,"2021-08-25 14:41:30+03","2024-05-17 05:24:37.770536+03","I had the same question Nowadays you can do this YAML example "
65792032,65699060,"stackoverflow.com",1,"2021-01-19 15:01:06+02","2024-05-17 05:24:08.742757+03","There is no need to create a function for each preflight endpoint When a browser receives a nonsimple HTTP request the CORS protocol requires the browser to send a preflight request to the server and wait for approval or a request for credentials from the server before sending the actual request The preflight request appears to your API as an HTTP request that To support CORS therefore a REST API resource needs to implement an OPTIONS method that can respond to the OPTIONS preflight request with at least the following response headers mandated by the Fetch standard With Serverless Framework you can do this in two easy steps Eventually if you use Javascript take a look to Middy middleware engine for use with Lambda It has a lot of nice middlewares and one is the cors middleware which automatically adds CORS headers to your functions "
65700643,65700292,"stackoverflow.com",2,"2021-01-13 13:04:58+02","2024-05-17 05:24:09.627056+03","You are missing the required field field for output value Here you may found how to set up Outputs in serverless framework Another examples"
65847536,65710262,"stackoverflow.com",15,"2021-01-22 17:00:23+02","2024-05-17 05:24:11.610495+03","In AWS Lambda a concurrency limit determines how many function invocations can run simultaneously in one region You can set this limit though AWS Lambda console or through Serverless Framework If your account limit is 1000 and you reserved 100 concurrent executions for a specific function and 100 concurrent executions for another the rest of the functions in that region will share the remaining 800 executions If you reserve concurrent executions for a specific function AWS Lambda assumes that you know how many to reserve to avoid performance issues Functions with allocated concurrency cant access unreserved concurrency The right way to set the reserved concurrency limit in Serverless Framework is the one you shared I would suggest to use SQS to manage your Queue One of the common architectural reasons for using a queue is to limit the pressure on a different part of your architecture This could mean preventing overloading a database or avoiding ratelimits on a thirdparty API when processing a large batch of messages For example let us think about your case where your SQS processing logic needs to connect to a database You want to limit your workers to have no more than 5 open connections to your database at a time with concurrency control you can set proper limits to keep your architecture up In your case you could have a function hello that receives your requests and put them in a SQS queue On the other side the function compute will get those SQS messages and compute them limiting the number of concurrent invocations to 5 You can even set a batch size that is the number of SQS messages that can be included in a single lambda "
71228075,65710262,"stackoverflow.com",0,"2022-02-22 22:45:28+02","2024-05-17 05:24:11.614495+03","Have you considered a proxy endpoint acting like a pool instead of limiting the concurrency of the lambda Also I think the way the lambda SQS communication happens is via some event pool and setting the concurrency lower than however many threads they have going will cause you to have to handle lost messages httpsaws amazon comrdsproxy"
68646306,65730620,"stackoverflow.com",19,"2022-08-19 10:55:27+03","2024-05-17 05:24:12.20315+03","There are a few ways to do conditional deployments in serverless yml some are more brittle than others and there are proscons to each but here is a list of methods I have collected NOTE We use a custom regex variable syntax to separate Serverless variables from cloudformation variable syntax All of the following snippets use this modified syntax If your conditional has two options you can write it as If your conditional is something you want to toggle ON or OFF you can write it as Using Serverless to selectively deploy entire files of resources using strings set via environment variables This is also a way to toggle deployment of many resources via conditionals conditional_file_A yaml can contain all the resources you toggle deployment of while conditional_file_B yaml can contain an empty Resources list If you do not want a file not found warning from serverless If you want to conditionally modify a json or yml file that will be sent deployed as a value for some CloudFormation key such as deploying a json file for parameter store or submitting buildspec yml files for CodePipeline you cannot actually use serverless syntax in these external files This is because serverless gets confused with it is own method of calling KeyValues from external files In order to use Serverless Variables in these files you can make them txt extensions as long as the actual file is formatted as proper json or yml"
65731471,65730620,"stackoverflow.com",9,"2021-01-15 08:33:00+02","2024-05-17 05:24:12.206115+03","You may check the following answers Conditional serverless yml based on stage Therefore you get something like"
69200747,65730620,"stackoverflow.com",3,"2021-09-16 02:23:04+03","2024-05-17 05:24:12.207745+03","Alternatively there is a Serverless Plugin Ifelse Add the plugin and then add your conditions see more info Serveless plugin ifelse"
65909966,65749618,"stackoverflow.com",1,"2021-01-27 00:06:56+02","2024-05-17 05:24:13.161641+03","It worked for me 1 install serverless as a devDependency with yarn or npm 2 run the deploy command once again 3 If the error stills look at your package json and chech if the serverless is as a devDependency If it is as normal dependency you can move it to devDependency 5 Delete the node_modules and run the command or"
65770437,65749618,"stackoverflow.com",0,"2021-01-18 09:37:24+02","2024-05-17 05:24:13.163642+03","As the message states it is looking for a module at UsersrasikrajDesktopawssocialmaxoldspacesize2048 The error makes sense because the odds of you actually having a file named maxoldspacesize2048 in the awssocial directory are remarkably low To me the filename maxoldspacesize2048 looks very much intended as a commandline argument to something "
67798327,65774612,"stackoverflow.com",2,"2021-06-02 05:55:14+03","2024-05-17 05:24:14.725858+03","The serverlessapigatewayserviceproxy plugin will create API Gateway endpoints backed by services other than Lambda including S3 Here is a complete example serverless yml that defines an S3 bucket of public static resources and creates an API method connected to it at path static asset A file app js placed in the bucket is made available at Since each service proxy integration has its own path a Serverless app can include a mix of proxy integrations and normal Lambda method handlers "
65782625,65774612,"stackoverflow.com",0,"2021-01-19 00:00:19+02","2024-05-17 05:24:14.727858+03","You can host you static files on the S3 httpsdocs aws amazon comAmazonS3latestuserguideWebsiteHosting html with the custom domain httpsdocs aws amazon comAmazonS3latestdevwebsitehostingcustomdomainwalkthrough html you can also use the CloudFront httpsdocs aws amazon comAmazonS3latestdevwebsitehostingcloudfrontwalkthrough html API Gateway can be configured with custom domain via CloudFront httpsdocs aws amazon comapigatewaylatestdeveloperguidehowtocustomdomains html"
65789538,65788920,"stackoverflow.com",4,"2021-01-19 12:21:27+02","2024-05-17 05:24:15.47088+03","To troubleshoot this issue you can use following steps Note It takes few minutes for completion of this path analysis OR Note It takes few minutes to load vpc network flow logs in aws cloudwatch"
65791057,65790802,"stackoverflow.com",2,"2021-01-19 13:57:07+02","2024-05-17 05:24:16.524221+03","Based on the docs the following should work"
66015866,65799053,"stackoverflow.com",0,"2021-02-02 20:52:10+02","2024-05-17 05:24:17.682239+03","I updated CustomErrorResponses to include 405 and I no longer get 405 Method Not Allowed error"
65822389,65820152,"stackoverflow.com",0,"2021-01-21 08:50:30+02","2024-05-17 05:24:18.652409+03","After adding S3 VPC endpoint this issue is resolved "
67400415,67398018,"stackoverflow.com",1,"2021-05-05 14:29:53+03","2024-05-17 05:26:04.347732+03","The GO1 15 runtime is not supported by the regular and beta version of Cloud Functions Use Go1 13 instead or use Cloud Run "
66054168,65832576,"stackoverflow.com",1,"2021-02-04 23:36:29+02","2024-05-17 05:24:19.632923+03","The first solution is not the cleanest one but will work for sure In the directory define your custom JS file ssmToNumber js with following script Now in serverless yml define following values And in the place where you want to have your number value from SSM simply invoke it this way Serverless Framework can start any JavaScriptTypeScript file during the build and put it is output into serverless yml file And this is exactly what we do here We define our ssmToNumber js script that simply reads SSM parameter from SSM and then converts it to integer and returns the value It knows which SSM path to use thanks to the custom ssmKeys section in serverless yml file Of course if you want to customise ssmToNumber js file to make it more verbose and fault tolerant then you simply need to edit the JavaScript file It requires more work Check out the official example for Serverless with Typescript As you can see it is possible to use serverless ts or serverless js instead of YAML file It would require from you some work about refactoring existing YML file but writing helper function that would convert it to number is very easy and elegant way to achieve your use case It have some downsides like problems with including directly other YAML templates but still you can define your CloudFormation YAML templates in separate files and then simply import those using import call from TSJS code "
65866711,65850866,"stackoverflow.com",0,"2021-01-24 04:48:44+02","2024-05-17 05:24:22.030663+03","Ok I figured it out When a new lambda function is created using serverless framework you can define the API type as shown above in my question Now since the stack is expecting that type I can also modify it in my globals aka provider and even use the id to share the resource across other lambda functions for example"
65858095,65857120,"stackoverflow.com",0,"2021-01-23 12:27:14+02","2024-05-17 05:24:22.371401+03","The team at Serverless has also released a tool called components one of which looks like its perfect for what you want to do it automatically handles everything on the cloud to get your React files hosted and a domain assigned too httpsgithub comserverlesscomponentswebsite"
65905535,65860137,"stackoverflow.com",3,"2021-01-26 18:46:45+02","2024-05-17 05:24:24.242641+03","Solution downgrade Node JS from version 15 to 13 Did not try 14 "
65935287,65860137,"stackoverflow.com",3,"2021-01-28 12:30:14+02","2024-05-17 05:24:24.243641+03","Thanks I downgraded nodejs from 15 7 0 to 15 4 0 and it is working fine now "
66886884,65860137,"stackoverflow.com",0,"2021-03-31 14:23:54+03","2024-05-17 05:24:24.244635+03","Use nvm to manage different versions of the node The issue was happening for me with node version v15 8 0 Resolved by downgrading system version to v14 15 5 using nvm Reference httpsforum serverless comtemptyfilesinuploadedzip13777"
65867567,65866680,"stackoverflow.com",3,"2021-01-24 07:55:09+02","2024-05-17 05:24:25.298929+03","Pagination can be done only if we have a marker In case of UI application it can be hard antipattern to keep store of lastevaluatedkey But you need to keep track of some information which DDB needs in order to know from where to start My recommendation will be to do the following "
65874336,65872319,"stackoverflow.com",1,"2021-01-24 20:22:16+02","2024-05-17 05:24:26.287036+03","Since you are making a query with a global secondary index you must specify the name of the index that you want to use and the attributes to be returned in the query results The result should be this"
65894578,65891964,"stackoverflow.com",2,"2021-01-26 03:10:57+02","2024-05-17 05:24:27.243135+03","I will try to give brief answers"
65897298,65896699,"stackoverflow.com",1,"2021-01-26 09:26:21+02","2024-05-17 05:24:28.259577+03","I am not 100 sure but I guess there is s3PutObjectAcl missing as your setting publicread for the object Edit probably be safe and also grant s3GetObjectAcl A lot of discussions and similar issues can be found here Getting Access Denied when calling the PutObject operation with bucketlevel permission"
65903506,65903505,"stackoverflow.com",0,"2021-01-26 16:51:29+02","2024-05-17 05:24:29.877062+03","Some people said to enhance the NodeJS memory with the code But i had no happy result I just ended up this error installing a newer version of NodeJS In my case v14 15 4"
65903666,65903665,"stackoverflow.com",0,"2021-01-26 17:00:58+02","2024-05-17 05:24:30.438489+03","After searching on web i found the solution What happens is that when deploying your application the serverless bundle all functions and upload every function with the size of all functions all together So if you have 50 function in your serverless yml file all them will be packaged and upload Let us imagine that you have 50 functions everyone with 1MB When deploying we will have a bundle with 50MB for every function instead of 1MB This can be easily fixed adding the property individually in our serverless file Documentation can be found here "
66040355,65908409,"stackoverflow.com",2,"2021-02-04 08:25:39+02","2024-05-17 05:24:31.354335+03","I guess you are mixing up the ways in which we configure api keys with and without the serverlessaddapikey plugin Following is a working serverless yml without using the plugin sls deploy output below If you still want to use the plugin you need to get the correct syntax You can use the below serverless yml "
67166238,66055156,"stackoverflow.com",0,"2021-04-19 19:57:55+03","2024-05-17 05:24:32.50838+03","add return servicePath line at the end of getBindPath function in D\a\telegrambot\telegrambot\node_modules\serverlesspythonrequirements\lib\docker js1529"
66065520,66064212,"stackoverflow.com",1,"2021-02-05 16:57:44+02","2024-05-17 05:24:33.356609+03","The following rules apply for naming S3 buckets S3_HTML_TEMPLATES selfservice htmltemplatess7adnjfnabbfvai7 S3_LANGUAGE_TEMPLATES selfservice languagetemplates12747dhadgdva S3_STATIC_WEBSITE selfservice staticwebsiteasjdu29ehq1i2 You can follow this doc to make it done Thanks"
66316441,66064212,"stackoverflow.com",0,"2021-02-22 15:16:40+02","2024-05-17 05:24:33.358609+03"," file env yml optstage BUCKET_UPLOADS was what I needed in the end I had misunderstood how the variables were being formed and fetched "
66071906,66071366,"stackoverflow.com",1,"2021-02-06 01:20:24+02","2024-05-17 05:24:34.125593+03","You should not need to do any changes to the API gateway as you are not physically placing function in a VPC so that it is not accessible from the internet The fact the lambda is associated with VPC or not affects only lambda not the api gateway integration with it Thus your API gateway will still be able to invoke the lambda function as before But now the function will be able to access RDS in a VPC and you will need to setup NAT gateway for the function to access internet if you need internet access "
66091323,66089671,"stackoverflow.com",2,"2021-02-07 20:03:16+02","2024-05-17 05:24:35.275014+03","I would recommend using S3 presigned URLs In your serverless code you use the S3 SDK to create the presigned URL You pass the URL back to the client from your Lambda The client then has a certain amount of time to use the URL to upload the image httpsdocs aws amazon comAmazonS3latestdevPresignedUrlUploadObject html"
66100689,66094737,"stackoverflow.com",1,"2021-02-08 13:24:12+02","2024-05-17 05:24:36.018498+03","Normally CloudFormation expects a Type parameter in a resource definition e g Type AWSLambdaFunction hence why you are seeing the error In your case you are using the Override AWS CloudFormation Resource functionality by Serverless though i e the name needs to exactly match the normalized function name Serverless assigns see the doc linked above in your case this would be TestLambdaFunction Change your code to"
66143106,66094737,"stackoverflow.com",1,"2021-02-10 20:45:42+02","2024-05-17 05:24:36.0205+03","I talked to some folks on GitHub As stated by yvesonline this method overrides auto generated cloud formation code Serverless has native support for EFS and it is recommended over the method described in the question native EFS in serverless looks something like this with fileSystemConfig existing within the function definition you can read more about serverless efs configuration here"
66122208,66121240,"stackoverflow.com",4,"2021-02-09 19:59:29+02","2024-05-17 05:24:38.796143+03","for dynamic generation you can use the AddPermission to add the necessary permissions If you are using serverless framework The above definition simply create the rule and add your lambda as the target It will take care of the necessary permissions as well whichever is needed Setting up event pattern matching EventBridge Use Cases and Examples Schedule"
66137461,66136190,"stackoverflow.com",1,"2021-02-10 15:03:57+02","2024-05-17 05:24:40.456452+03","We were struggling with the same problem How we solve it The lambda timeout is handled with middleware How to log timed out Lambda invocations middy We process the batch of messages from SQS and here popups the following cases a all messages were processed successfully return statusCode 200 and the messages will be deleted from SQS b some messages failed"
66159414,66142935,"stackoverflow.com",0,"2021-02-11 19:10:10+02","2024-05-17 05:24:41.253532+03","As mentioned by yvesonline the problem was the method This was fixed by simply replacing method get to method post"
66149080,66149079,"stackoverflow.com",2,"2021-02-11 07:10:34+02","2024-05-17 05:24:42.276908+03","The result of a TypeScript build will only include the files it compiles and accompanying dependencies The way to have Serverless include files not compiled by TypeScript in the build result is to use the include configuration For this specific example a solution with a glob pattern to include all EJS files under the src folder would look like this serverless yml"
66161709,66157734,"stackoverflow.com",0,"2021-02-11 21:42:20+02","2024-05-17 05:24:43.868609+03","sls remove will be referring to serverless yml file and will be removing the stack serverless directory is created when you do a sls deploy It will basically have some contents like zip file which is used to deploy it to AWS So you need not create an serverless directory manually for anything It is also not recommended to push this directory to git either "
66186755,66183371,"stackoverflow.com",1,"2021-02-13 17:27:10+02","2024-05-17 05:24:45.451249+03","You should not init DynamoDB with region localhost Either leave it out or change it to a proper region e g "
66329766,66188828,"stackoverflow.com",0,"2021-02-23 10:43:40+02","2024-05-17 05:24:45.684234+03","Are you passing it as if Yes then it should work else try going with redirect stage _nuxt for an if statement put this inside else I think it should work "
66215148,66210607,"stackoverflow.com",0,"2021-02-15 22:44:05+02","2024-05-17 05:24:46.78934+03","I am not sure you are using the DocumentClient API correctly For example you have and I have only ever used it this way Perhaps this is why the awssdk is complaining about a missing id key "
77543881,66210607,"stackoverflow.com",0,"2023-11-24 16:32:54+02","2024-05-17 05:24:46.791342+03","I had the same issue For me I had a wrong tableName I was using another table where the id was required Logging the params helped "
66213407,66211835,"stackoverflow.com",0,"2021-02-15 20:25:51+02","2024-05-17 05:24:47.589177+03","So I think lModel was just webpack minimizing the variable names I had a module where I was importing a mongoose model using MongooseModule forFeatureAsync I needed to add the model class to the providers exports array in that module and then import that module in app module ts where I was calling MongooseModule forRootAsync "
66221734,66221323,"stackoverflow.com",3,"2021-02-16 11:26:39+02","2024-05-17 05:24:48.544107+03","I would argue that MultiFactor Authentication inside of CICD Pipelines is generally not a good idea MFA is intended to be used to reduce the risk that credentials that are handled by regular people can be exploited when they are stolen They are a workaround for the fact that humans tend to be less careful with their own hardware and credentials than IT organizations dedicating resources to secure an environment It is a system designed in a way that it is not easy to automate for that reason You explicitly want a human to interact with it CICD pipelines have different characteristics They run in a dedicated environment and can be easier secured against outside access You can also create temporary credentials from a trusted source and easily feed them into a pipeline e g do an assume role and pass the credentials as environment variables Another option would be to have an access key and a secret access key that get periodically rotated More importantly the focus of a pipeline is on automation we explicitly want to automate as much as possible because human input is error prone In contrast to the regular workstations the pipeline is supposed to get artifacts and code from trusted entities whereas it is harder to ensure that on every developers machine That is why credential exfiltration is a lower risk even if you should not rule it out completely thus my suggestion of using temporary credentials tldr Do not use MFA in your pipeline use a different strategy to secure your credentials such as a role assumed by the CodeBuild project "
66224108,66223691,"stackoverflow.com",0,"2021-02-16 14:12:58+02","2024-05-17 05:24:49.504479+03","There are some options possible check if whats needs to be ignored PACKAGES is really ignored maybe you installed some packages that are taking too much space that is the main reasons for something like that "
66224563,66223691,"stackoverflow.com",0,"2021-02-16 14:43:32+02","2024-05-17 05:24:49.505794+03","Let me add also some points as Ethanolle pointed clean up your npm dependencies Consider using the webpack for creating your bundle serverlesswebpack plugin As an alternative for webpack you may get the benefit of using the more precise configuration of Serverless how to manage what to excludeinclude in a bundle"
66303957,66254217,"stackoverflow.com",24,"2021-02-21 17:33:49+02","2024-05-17 05:24:52.947501+03","You should put it at the root of your serverless yml file I have mine just after the service attribute "
66259567,66259088,"stackoverflow.com",3,"2021-02-18 14:02:30+02","2024-05-17 05:24:53.556048+03","The 11 10 value you are specifying for the EngineVersion property is processed as a number by the YAML engine since you are not using quotes so the trailing zeros are removed and the resulting value is 11 1 You need to specify the value in quotes like this 11 10"
66259414,66259088,"stackoverflow.com",3,"2021-02-18 13:58:23+02","2024-05-17 05:24:53.558043+03","It seems like Serverless ignores 0 in Engine Version that you are trying to set up Could you try to escape version with quotes It will look like this It should help to solve the problem and upgrade version successfully"
66992078,66261624,"stackoverflow.com",6,"2021-04-07 21:28:47+03","2024-05-17 05:24:54.657753+03","This is an issue posted on their github httpsgithub comCoorpAcademyserverlesspluginsissues171 Basically you have to set NODE_ENV to test At least that is what they suggested and what I did "
66283660,66283103,"stackoverflow.com",0,"2021-02-19 21:10:43+02","2024-05-17 05:24:55.668636+03","I will not be able to help with this answer on StackOverflow because our support team will need to work with you and your Serverless com organization specifically so please reach out via the support channels in the app or DM me in Slack Parameters can be configured at the Servicelevel and optionally overridden at the specific stage so it would help to verify that this parameter is set within the service or stage you are deploying to "
68150068,66283103,"stackoverflow.com",0,"2021-06-27 13:13:13+03","2024-05-17 05:24:55.670378+03","Hmmm seems like you are trying to create an environment variable there if I am not mistaken You are trying to assign value to your environment variable abc which is not defined There are few ways you can pass the value to your environment you could have an environment file where you can supply these parameters to your service like the following one envFilePath sharedenvironmentsenv json Then You will be able to pick the value and set it to the variable using file selfcustom envFilePath abc or another option you could directly type the value in your yml You can even read the value from a ssm parameter also "
67435486,67433939,"stackoverflow.com",5,"2021-05-07 16:03:49+03","2024-05-17 05:26:05.472345+03","After a lot of Googling I have finally found something useful httpswww serverless comblogstructuringarealworldserverlessapp"
66374715,66300248,"stackoverflow.com",0,"2021-02-25 20:52:09+02","2024-05-17 05:24:57.338512+03","Unfortunately since AWS XRay does not support tracing with kinesis per message level tracing would be hard However you may write custom code using XRay SDK to propagate context information manually I would recommend to check out XRay SDK language of your choice Heres some reference on that How can use the Java Kinesis Client Library together with XRay "
66305464,66304049,"stackoverflow.com",4,"2021-02-22 10:04:37+02","2024-05-17 05:24:57.837731+03","Unfortunately Lambda destinations only work with asynchronous Lambda invocations SQS invokes Lambda synchronously so Lambda destinations do not workapply here See httpsaws amazon comblogscomputeintroducingawslambdadestinations You are going to have to post a message to SQS programmatically within your Lambda in this case "
66316050,66309527,"stackoverflow.com",1,"2021-02-22 14:49:43+02","2024-05-17 05:24:58.563423+03","You are hitting the size limitations of your AWS Lambda deployment package see here It can be maximal 250 MB unzipped that is what your error message tells you Have a look at the section Dealing with Lambdas size limitations in the documentation of the serverlesspythonrequirements plugin In short what serverlesspythonrequirements recommendssupports is Compression works by adding the following in your serverless yml and then add this to your handler before using your dependencies But some users seem to have had issues with librosa and could not seem to make it work see this GitHub issue for example or this other SO question A suggestion in there was to split up your Lambdas and chain them i e one Lambda which has only librosa as a dependency maybe with compression of the dependency and then handing off for further processing to a second Lambda I do not know what you are trying to do if this is feasible for you "
66327722,66319392,"stackoverflow.com",0,"2021-02-23 07:47:32+02","2024-05-17 05:24:59.667595+03","sendMail is an async function You have to await it Replace"
66320439,66319917,"stackoverflow.com",0,"2021-02-22 19:23:16+02","2024-05-17 05:25:00.495144+03","I believe this may be a bug with serverlesss3sync since those commands are being loaded when the error is thrown I would recommend removing that plugin and trying again If you need that plugin you may have to open a bug ticket in the repo here"
66321760,66319917,"stackoverflow.com",0,"2021-02-22 20:56:44+02","2024-05-17 05:25:00.496626+03","It looks like the config is missing for serverlesss3sync Please check the documentation of the serverlesss3sync plugin and how to SETUP it "
70663130,66319917,"stackoverflow.com",0,"2022-01-19 10:57:26+02","2024-05-17 05:25:00.497623+03","Refer to the solution here httpsgithubmate comrepoar90nserverlesss3localissues294 There is a dependancy version mimatch between sls and the node serverless plugin serverlesswsgi in my case I had not used my sls since a year so it was dated Had to reinstall sls with curl o L httpsslss ioinstall bash httpswww serverless comframeworkdocsgettingstartedinstallasastandalonebinary"
66504010,66500621,"stackoverflow.com",2,"2021-03-06 11:00:42+02","2024-05-17 05:25:01.59872+03","The API Gateway currently does not offer a feature like that you would have to implement this yourself If I was to implement this I would probably use an inmemory cache like ElastiCache for Redis or Memcached as the storage backend for deduplications For each incoming request I would determine what makes it unique and create a hash from that Then I check if that hash value is in the cache already If that is the case it s a duplicate and I reject the request If it is not already in the cache I would add it with a time to live of n seconds The time interval in which I wish to deduplicate "
66530725,66521824,"stackoverflow.com",0,"2021-03-09 15:46:29+02","2024-05-17 05:25:03.183924+03","The libcurldevel openssldevel packages are missing use one of the lambcilambda Docker images to build and make sure all the dependencies are installed Change your serverless yml to Then add a Dockerfile with the following contents"
66530152,66527991,"stackoverflow.com",2,"2021-03-08 18:42:46+02","2024-05-17 05:25:03.5239+03","From what I gather you have a two entities Each restaurant may have multiple beers and each bear is either bottled or tap and also has a price You want to enable these access patterns I propose a single table with a Global Secondary Index GSI1 that looks like this When you now add items to the table you have to consider what type of entity you want to create Restaurants are pretty simple they just need an ID and a name From these information we can compute the keyattributes The beer always belongs to a restaurant so we need to have it is id it also always has a price and a name "
66557225,66556524,"stackoverflow.com",3,"2021-03-11 02:35:15+02","2024-05-17 05:25:04.57245+03","You can use Step Functions Step Functions is a serverless orchestration service that lets you combine AWS Lambda functions and other AWS services to build businesscritical applications Specifically Step Functions supports a Parallel state A Parallel state causes AWS Step Functions to execute each branch starting with the state named in that branchs StartAt field as concurrently as possible and wait until all branches terminate reach a terminal state before processing the Parallel states Next field Alternatively consider the Map state The Map state can be used to run a set of steps for each element of an input array While the Parallel state executes multiple branches of steps using the same input a Map state will execute the same steps for multiple entries of an array in the state input So Map supports dynamic parallelism while Parallel supports predefined static parallelism "
66620617,66560150,"stackoverflow.com",28,"2023-05-24 01:56:38+03","2024-05-17 05:25:05.692016+03","A better solution is to specify AWS_SDK_LOAD_CONFIG1 without messing with your credentials AWS_SDK_LOAD_CONFIG1 sls deploy Make sure you have the serverlessbettercredentials serverless plugin installed as it is needed for this to work Follow installation instructions at httpswww serverless compluginsserverlessbettercredentials"
66572781,66560150,"stackoverflow.com",26,"2021-03-10 23:01:11+02","2024-05-17 05:25:05.69389+03","This is a known issue with Serverless Serverless only checks awscredentials for the profile and not awsconfig There are multiple Serverless forum posts about this e g this one Change your awscredentials file to this and it should work"
68524626,66571130,"stackoverflow.com",0,"2021-07-26 07:48:51+03","2024-05-17 05:25:06.706728+03","You have a typo in next block It must be subnetIds instead Reference"
66580514,66580072,"stackoverflow.com",5,"2021-03-11 12:17:26+02","2024-05-17 05:25:07.623208+03","You should be able to just omit authorizer and your endpoint will be publicly accessible right after deployment Ensure that you are calling proper URL with proper path and method Could you share how are you calling the deployed endpoint"
66600096,66584623,"stackoverflow.com",1,"2021-03-12 14:39:27+02","2024-05-17 05:25:08.615568+03","Unfortunately no From the Serverless doc IMPORTANT You can only attach 1 existing S3 bucket per function AWS Lambda allows only one Amazon S3 bucket as an event source You can define s3 multiple times in events with a different event type but it still must be from the same bucket see here You can though use the handler you wrote multiple times to define multiple functions like this"
66592537,66591071,"stackoverflow.com",2,"2021-03-12 02:33:11+02","2024-05-17 05:25:10.288683+03","Your local path is localMountPath mntefs So in your code you should be using only this path not mntefsvcfs "
71311206,66592284,"stackoverflow.com",1,"2022-03-01 17:55:35+02","2024-05-17 05:25:11.236744+03","I was having the identical issue using Mailchimps Marketing API and solved it by routing traffic through an NAT Gateway Doing this allows your lambda functions that are within a VPC to reach external services The short version of how I was able to do this You can find out more at this link httpsaws amazon compremiumsupportknowledgecenterinternetaccesslambdafunction"
66593438,66592799,"stackoverflow.com",0,"2021-03-12 04:41:48+02","2024-05-17 05:25:12.415174+03","Can you try packaging installing your module in a containerised lambda env As sometimes modules packed in windows will not work on other os I had a similar issue with psycopg2 then installed and packaged in lambci docker and everything was working fine "
66602218,66597767,"stackoverflow.com",8,"2021-03-12 16:54:21+02","2024-05-17 05:25:14.343728+03","There is no downtime when updating a lambda function using the Serverless Framework simply by running sls deploy The function code is zipped and uploaded to Lambda and when that is completed CloudFormation will update the Lambda configuration to point to the new code There is no downtime in this process "
66604017,66602814,"stackoverflow.com",0,"2021-03-12 18:54:06+02","2024-05-17 05:25:15.397019+03","there is brief but dirty solution removing USER node and adding symbolic link on serverless"
66605248,66603054,"stackoverflow.com",0,"2021-03-12 20:59:33+02","2024-05-17 05:25:16.403264+03","I think your Dockerfile is not correct you are not copying requirements txt to let pip install the packages for you because of this the dependencies are not packaged It should be something like this Your Dockerfile gives this error I am not sure if you noticed this in your build logs while deploying your app One more thing you are not supposed to include the requirements txt directly in your Dockerfile assuming you are using serverlesspythonrequirements not any other variant Dockerfile This configuration will take the Dockerfile and add the requirements txt to it Native Code Dependencies During Build Cross Compiling"
66705010,66621777,"stackoverflow.com",7,"2021-03-19 10:46:25+02","2024-05-17 05:25:18.067421+03","So after careful research I did the following and this is apparently a pretty common issue so I recommend anyone else suffering from this issue do exactly the following Please remember this is with the serverlessnextjs component not just the serverless framework although the same may apply there This experience is always a good one to have Your by no means invincible to really stupid errors in your code And 99 of the time if you are stuck and thinking it is complex and that it is unrealistic that you will fix it step back and ask What is the dumbest thing it could be And it is always that "
69408248,66621777,"stackoverflow.com",1,"2021-10-01 18:13:45+03","2024-05-17 05:25:18.069422+03","For me using the Alpha version of Serverless solved this error message The Lambda function associated with the CloudFront distribution is invalid or does not have the required permissions serverless yml file We can find the latest version here httpswww npmjs compackageslsnextserverlesscomponent"
73748932,66633000,"stackoverflow.com",0,"2022-09-16 21:24:13+03","2024-05-17 05:25:19.904314+03","Why are you defining two policy statements Should be immediately under And you should have just 1 policy statement for the bucket "
66648142,66647945,"stackoverflow.com",3,"2021-03-16 04:05:45+02","2024-05-17 05:25:20.893313+03","The S3 permissions should be added to your CodeBuild CB project role not CodePipeline CP role The reason is that the CB container is the entity that actually tries to access the S3 not CP "
66688188,66654211,"stackoverflow.com",5,"2021-03-18 11:27:03+02","2024-05-17 05:25:21.653869+03","in the first error line gyp WARN EACCES current user nobody seems nodegyp is trying to use another user instead of root we can find the issue discussed in the package repository the most recent solutions seems to be to forceset the environment user to be used by npm install process"
75939494,66654211,"stackoverflow.com",1,"2023-04-05 15:35:19+03","2024-05-17 05:25:21.65586+03","For Ubanntu users use"
66708354,66665229,"stackoverflow.com",0,"2021-03-19 14:40:08+02","2024-05-17 05:25:22.719365+03","It depends what you are really trying to achieve here I think what you are referring to is a magic link httpsauth0 comdocsconnectionspasswordlessguidesemailmagiclink A magic link when clicked signs a user into your application The way to implement this is not by sending a usernamepassword encoded in a link but rather by sending an OTP one time passcode in the link The link you send would contain the path to an applications endpoint lets say example commagiclinkZXCBN123 You would need an application client that takes the OTP and performs a cognito signin I have implemented this myself and its not very well supported by Cognito When you send the email you create the OTP and persist it in some database When the client sends you the OTP you look it up and validate it You have to use the custom challengeresponse cognito triggers to verify that the code is correct and return tokens if it is "
66696882,66680982,"stackoverflow.com",4,"2021-03-18 20:19:01+02","2024-05-17 05:25:24.871782+03","The issue in your example is caused by invalid indentation in the config it should looks like this"
66689113,66684536,"stackoverflow.com",0,"2021-03-18 12:23:26+02","2024-05-17 05:25:25.677706+03","we would need more details about how you are testing the confirmation flow how your lambda is configured in the meantime I created a working example with Java 8 Serverless Framework with Amazon Cognito in CloudFormation you can see the details here httpsgithub comoieduardorabelo20210319cognitojavapostsignupconfirmation Pardon my Java I know nothing about it I am explaining how I tested the signup flow and the output in CloudWatch includes all necessary user details I hope it can help you debug your application "
66716797,66715295,"stackoverflow.com",1,"2021-03-20 01:29:42+02","2024-05-17 05:25:28.546046+03","You do not say what language this is written in I am assuming some Javascript variant You also do not bother to post the actual output from your incorrect function That would have been useful and probably would have saved you from posting the question at all In your first example you are using paramter placeholders This is not simple string substitution it is like passing parameters to a function It is used for values and not for structural elements of your query like column or table names So in your first example you are asking for SELECT DISTINCT literal value FROM table whose values are not used Since you do not know the difference between your two examples and this appears to be accessible from the general internet I urge you to read up on SQL injection and review your code as soon as possible Your second example leaves your entire database accessible to anyone who can post a request to that endpoint "
66719265,66718121,"stackoverflow.com",0,"2021-03-20 09:34:05+02","2024-05-17 05:25:30.26714+03","If you are using git you can track all changes by it In that case make sure that the gitignore file does not include the node_modules folder Dont forgot to add it back if you gonna remove it Otherwise If you are using the Visual Code you can rightclick on the node_modules folder and then choose to Find In Folder and then search console log for example If you are using the other IDE please search the same functionality for you are IDE "
66749166,66748915,"stackoverflow.com",2,"2021-03-22 17:31:44+02","2024-05-17 05:25:30.966974+03","A get operation in DynamoDB is only available on the primary key of the table Your table definition as shown in the serverless yml only includes a partition key aka HASH so the only way to use the get operation is to request a single item by the key which you have named VenueID If your intent is to have the sort key aka RANGE be the SK attribute and the partition key be the PK attribute then your table definition needs to look something like this Note that there is no mention of VenueID or BeaconAddr in the definition You would put the values as you have in the display above into those fields To add a GSI as you have displayed you would need to have this Note that there are separate columns here The data that goes into those columns is up to you to populate Finally when querying a GSI you cannot get on a GSI you must supply the index name in the query "
78098955,67433939,"stackoverflow.com",1,"2024-03-04 06:25:44+02","2024-05-17 05:26:05.473345+03","You can have one file per lambda just follow the official serverless docs scroll down to the word separate or here is my copypaste docs from 2024 and lambda file"
66759895,66759486,"stackoverflow.com",1,"2021-03-23 10:51:00+02","2024-05-17 05:25:31.385897+03","In the provided example you are using relative path to tmp unless you are working dir is it will try to open a file in an invalid not available for write path When trying to write to a file in tmp which is writable for Lambdas ensure you are providing proper ideally absolute path "
66954941,66949090,"stackoverflow.com",2,"2021-04-05 17:50:49+03","2024-05-17 05:25:31.990892+03","This appears to be a known permissions issue with the TypeScriptPlugin You may try following the advice of others here and removing the build and serverless directories and rerunning the command httpsgithub comprismalabsserverlessplugintypescriptissues170"
66953380,66953162,"stackoverflow.com",1,"2021-04-05 15:52:32+03","2024-05-17 05:25:32.954504+03","If you need indices on two attributes create two indices You cannot have one BeaconAddrindex index with two hash attributes Also I wonder if you have misunderstood something because naming your index BeaconAddrindex when it is NOT indexed on BeaconAddr seems odd What did you intend to do"
66956469,66956042,"stackoverflow.com",0,"2021-04-05 19:41:41+03","2024-05-17 05:25:33.539784+03","Buckets and IAM roles and permissions are global that is why they exist S3 Bucket names are also unique and global across all AWS accounts "
66957192,66957095,"stackoverflow.com",7,"2021-04-06 23:03:59+03","2024-05-17 05:25:35.113209+03","To achieve a dynamic service name based on the package json app name I took advantage of the variable import Serverless Framework feature httpswww serverless comframeworkdocsprovidersawsguidevariablesreferencevariablesinjavascriptfiles This loads the file package json and specifies that the name property should be applied to the service entry "
67024361,67011985,"stackoverflow.com",1,"2021-04-09 18:54:29+03","2024-05-17 05:25:36.333916+03","If you want to use one existing role for all functions in a Serverless app you will need to either specify iam role or iamRoleStatements I believe the framework is attempting to create a new role based on the iamRoleStatements you provided You can refer to the documentation for more information "
67075792,67024519,"stackoverflow.com",1,"2021-04-13 16:26:03+03","2024-05-17 05:25:37.798141+03","It turned out that the problem was that AWS was expecting a multifacto authentication MFA I resolved it by"
67028714,67024519,"stackoverflow.com",0,"2021-04-10 00:49:21+03","2024-05-17 05:25:37.799142+03","Try running the same command with AWS_PROFILEdefault serverless deploy v region euwest1 or serverless deploy v region euwest1 awsprofile default If that still does not work verify that your provided access keys actually do have permissions to create an s3 bucket by using the awscli "
68184688,67025000,"stackoverflow.com",0,"2021-06-29 22:37:30+03","2024-05-17 05:25:38.149932+03","This is not my preferred solution as I would like to use 256 but the way I got around this errorwarning was by changing the authMechanism from SCRAMSHA256 to SCRAMSHA1 in the connection string The serverlessbundle most likely needs to add this dependency into their package to enable support for Mongo 4 0 SHA256 my best guess You can specify this authentication mechanism by setting the authMechanism parameter to the value SCRAMSHA1 in the connection string as shown in the following sample code "
70698549,67032458,"stackoverflow.com",0,"2022-01-13 16:48:18+02","2024-05-17 05:25:39.278636+03","I had the same issue and the problem is that I have the setup in a monorepo using yarn workspaces and lerna and next was declared as a shared dependency in the root of the monorepo and the nextjs app I was trying to deploy did not have the dependency explicitly in its package json For serverless to run we need to have this dependency declared there as well in my case I simply included in dependencies and now serverless is capable to run the command as well "
67050979,67033976,"stackoverflow.com",1,"2021-04-12 02:10:44+03","2024-05-17 05:25:40.136614+03","Based on the comments The link provided in the question showcases a CloudFormation CFN template not serverless framework template Although there are similarities between the two they are different The code in the question is a mixture of both CFN and serverless templates which makes it not work in both systems Thus to solve the issue you have to check and find examples for serverless framework templates not CFN Then you modify your current template into valid serverless template "
67049323,67048280,"stackoverflow.com",2,"2021-04-11 22:27:52+03","2024-05-17 05:25:41.256625+03","You just have an indentation issue The ProvisionedThroughput property is part of the index not the Projection "
67177948,67049434,"stackoverflow.com",1,"2021-04-20 14:22:42+03","2024-05-17 05:25:42.520173+03","First try to fix the indentation in the custom section The domainName and basePath should be under customDomain baseName is optional so if you do not need it just do not include it "
67060006,67058655,"stackoverflow.com",1,"2021-04-12 18:00:32+03","2024-05-17 05:25:44.144934+03","An issue in terraformprovideraws terraformprovideraws 3 13 0 and later including 3 25 0 cause lambdas in a VPC to be updated on every apply 17385 From the documentation How to deploy and manage Lambda Functions Typically Lambda Function resource updates when source code changes If publish true is specified a new Lambda Function version will also be created publish flag aws_lambda_permission So every time you deploy a new version is being deployed which is referenced in the corresponding resource to update the policy Hence it is triggering updates every time In AWS Lambda function what is the difference between deploy and publish Depending on where you are deriving your context for deploy and publish normally deploy means redeploying your lambda with new code whereas publish is increasing your lambda version not redeploying code "
67283855,67058655,"stackoverflow.com",1,"2021-04-27 16:25:59+03","2024-05-17 05:25:44.146631+03","The problem I was facing boils down to several things Solution seems to be to not deal with permissions of Lambdas at all but instead provide API Gateway credentials aka Role with Lambda InvokeFunction rights that allow it to call Lambdas This way when an AWS Gateway integration Lambda is called it assumes the Role Permissions on Lambda side are not needed in such case My tests show that in that case the sequence of updating the Lambda is correct no unecessary recreations of resources for VPC lambdas and when a Lambda is being updated first a new version is deployed and then API Gateway shifts to it hence no downtime happens The production tests under certain load also confirmed that we do not see an outage in practice Heres the snippet for API Gateway configuration that permits Lambda invocations It follows a recipe found at httpsmedium comjun711 gawsapigatewayinvokelambdafunctionpermission6c6834f14b61 Note that last Resource instruction would allow all Lambdas to be called by this Role You might want to restrict these rights to a subset of lambdas for increased security and less of human error Having this Role set up I configure API gateway using a popular Module apigatewayv2 from serverless tf framework "
67076891,67076668,"stackoverflow.com",1,"2021-04-13 17:38:35+03","2024-05-17 05:25:45.055047+03","Add a build step which decrypts an encrypted json with Cloud KMS Or heres another one example doing about the same but with pgp "
67093767,67077413,"stackoverflow.com",0,"2021-04-14 17:34:59+03","2024-05-17 05:25:46.11829+03","It turned out I just did not have the permission to display the trigger s I am surprised that in this case AWS Console displays Triggers 0 as if it actually was empty in this section instead of clearly warning about permission as it usually does "
67088227,67080848,"stackoverflow.com",1,"2021-04-14 11:34:40+03","2024-05-17 05:25:47.203733+03","In resources you specify a CloudFormation template unless you want to use it in CloudFormation that is not where it should go I guess you want to add the variables as environment variables in your functions this is the most common approach in Serverless Framework You add them in custom for each stage and then you reference and insert them in the environment section "
67099492,67085089,"stackoverflow.com",2,"2021-04-15 00:43:15+03","2024-05-17 05:25:48.677609+03","I figured it out When I had the libGL so error that was the closest I was to success and adding the opencv fix actually just makes all imports break The solution is to put opencvpythonheadless in requirements instead of opencvpython because the headless version is made for servers that do not have graphics dependencies or something like that "
67086160,67085869,"stackoverflow.com",4,"2021-04-14 08:48:38+03","2024-05-17 05:25:49.098275+03","There is a reason for that If you ever wanted to deploy that function into production you can add stage prod which will change the name of the function to match the stage If you handled naming yourself you could end up with a conflict later where the deployment will tell you it cannot complete because that Lambda function with the name function1 already exists After saying all that you can control the name of the function in AWS explicitly as well Just add a name parameter to your function definition the same way you do handle You can see this on this serverless yml reference page which is usually my first port of call for these kinds of questions I usually have as well as that reference contains pretty much every configuration option available to you httpswww serverless comframeworkdocsprovidersawsguideserverless ymlserverlessymlreference"
67092182,67090756,"stackoverflow.com",1,"2021-04-14 15:52:37+03","2024-05-17 05:25:50.748003+03","I fixed the problem with running sudo npm install g [email protected]"
67097202,67095704,"stackoverflow.com",3,"2021-04-14 21:30:06+03","2024-05-17 05:25:51.245574+03","I guess it is not possible Serverless product uses the Cloud Functions API to deploy te Cloud Functions To set the allowunauthenticated you need to use the IAM API to add the cloudfunctions invoker role to allUsers gcloud CLI offers a convenient way to package the both API call in a single command line but external tool need to implement the same extra effort to achieve that That is why I am pretty sure that is not possible "
69380913,67095704,"stackoverflow.com",2,"2021-09-29 20:21:50+03","2024-05-17 05:25:51.247569+03","I was able to achieve this by using hooks You need to add custom scripts to serverless yml Note This should work given that the service account that you are using have the proper access like this"
67111606,67097701,"stackoverflow.com",0,"2021-04-15 18:40:47+03","2024-05-17 05:25:52.374463+03","Like all good answers in software it depends My theory for routing is to ask about requirements Can you look up a user without a group ID Can you look up a group without a user ID If the answer to either is yes then that resource deserves its own route Compare that with something like a multitenant system where an Organization has many Widgets A widget wholly belongs to an organization so you would never directly query that widget Given this we have In your example given that Groups have many users and Users have many groups many to many relationship I think your provided routing scheme is correct "
67114325,67111405,"stackoverflow.com",0,"2021-04-15 21:43:10+03","2024-05-17 05:25:54.400502+03","I dont know if I was hallucinating but it doesnt seem as if what I was originally doing can ever work This is a solution although it is ugly but posting here might help someone to understand what it is that I am trying to achieve which is to get the path and request body into the event with nothing else It is not pretty and if there is a more simplistic way to do this then I would appreciate knowing the answer "
67128494,67125589,"stackoverflow.com",1,"2021-04-16 18:57:35+03","2024-05-17 05:25:54.939872+03","Here some documentation that might be helpful for your case even though required to take a deep look into it Configure Serverless VPC Access which i think applies for setting up your local environment Cloud Run Quick start which contains how to built and deploy serverless services with GCP Cloud Run using node js python java etc "
67129214,67127740,"stackoverflow.com",0,"2021-04-16 19:53:36+03","2024-05-17 05:25:55.911453+03","If you want to conenct to an RDS instance from a Lambda function I assume this is what you mean by Serverless then check the details here httpsdocs aws amazon comlambdalatestdgservicesrdstutorial html To connect to RDS from code or a tool running on your development machine it does matter if the code is Java JavaScript Python etc you need to perform these tasks For information about setting up security group inbound rules see Controlling Access with Security Groups "
67132538,67132310,"stackoverflow.com",4,"2021-09-30 17:17:48+03","2024-05-17 05:25:56.87466+03","It looks like you may be missing the permissions The rule needs to have permission to invoke the lambda This tutorial shows the steps In particular check out the portion that covers Example for request in comments"
67170457,67170379,"stackoverflow.com",2,"2021-04-20 02:17:35+03","2024-05-17 05:25:58.58032+03","Why would an email need to be resent Due to a transient error or due to some business logic initiated by an end user The solution depends on the problem I would argue that if you detect the case where a message needs to be recent that you either A maintain separate resend email SQS queue OR B put a new message on the existing email queue with a field indicating that this is a resend Since SQS is pretty cheap I could lean towards A "
67171492,67170379,"stackoverflow.com",1,"2021-04-20 05:24:21+03","2024-05-17 05:25:58.582321+03","If my understanding is correct you have single SNS topic to do multiple processing and that is why multiple microservices are listening to it Please correct if that is incorrect If that is the case the better option is to a Segregate email processing keep a separate SNS topic for email purpose e g informuser and use it for retrying b If you cannot segregate then the only option in above scenario is to have state in the message itself That means having an attribute in the message to indicate that it is for retrying and then the listener microservice should process accordingly "
67209333,67172172,"stackoverflow.com",0,"2021-04-22 11:14:45+03","2024-05-17 05:25:59.725048+03","You have two options A Create a Proxy Lambda function which does a 301 redirect B On an existing Lambda define a second path and do the 301 redirect directly in serverless yml like this taken from here "
67189963,67189465,"stackoverflow.com",0,"2021-04-21 08:51:20+03","2024-05-17 05:26:00.296229+03","This is an error from AWS themselves It is likely that the clock on your local machine is not set to the correct time therefore the signature process fails "
67223586,67210893,"stackoverflow.com",0,"2021-04-23 06:27:33+03","2024-05-17 05:26:00.747385+03","Here is the yml for the second one But it didnt work functions mysqlHook handler handler mysqlHook events mysql onConnect Could I use a function that created by myself to do the trigger or set mysql into provider or resource to be used here"
67238157,67236751,"stackoverflow.com",3,"2021-04-24 03:12:24+03","2024-05-17 05:26:02.711536+03","If remember device suppresses the MFA prompt how long does it suppress it for Till the device is remembered Depending upon how you configure Cognito Userpool to remember devices and opting to suppress MFA for remembered devices the following are the possibilities For the SRP flow to work the client must have the correct deviceKey deviceGroupKey and randomPasswordKey If the client does not provide this information then it would be challenged with MFA If the user uses a browser in incognito mode or clears the LocalStorage it would be presented with the MFA challenge "
70000262,67383809,"stackoverflow.com",1,"2021-11-17 09:17:59+02","2024-05-17 05:26:03.581293+03","I was facing the same issue while using event schedule The input properties are included in the event argument passed to your handler function Here you can simply access your input param email via event object as below"
68821042,67383809,"stackoverflow.com",0,"2021-08-17 19:34:20+03","2024-05-17 05:26:03.583293+03","I figured out a solution using a schedule rather than eventBridge event Then define a class to receive the parameter Then the handler function receives the parameter class "
70465969,67433939,"stackoverflow.com",1,"2021-12-26 18:17:14+02","2024-05-17 05:26:05.474771+03","I think that you could separate your project into services Then inside of each service you can define one serverless yml You can see the best explanation here Serverless Framework Documentation Services"
76483715,67433939,"stackoverflow.com",1,"2023-06-15 19:27:49+03","2024-05-17 05:26:05.476001+03","I have been looking for this on google and found this post Also I find a youtube video that explain really well how to split a serverless file Split Serverless Files I hope it really helps to someone who looks for this in the future "
67694906,67444602,"stackoverflow.com",0,"2021-05-25 23:03:26+03","2024-05-17 05:26:08.186322+03","I ran into this issue and after debugging the code I found this httpsgithub comserverlessserverlessblob29f0e9c840e4b1ae9949925bc5a2a9d2de742271libpluginsawsprovider jsL129 Since by default AWS SharedIniFileCredentials does not return the roleArn by default sls assumes the profile is invalid The fix is to set AWS_SDK_LOAD_CONFIG1 as suggested in the comments That variable tells the AWS SDK to load the profile when you are using a shared config file Based on that I can assume that setting AWS_SHARED_CREDENTIALS_FILE might work as well since the other file should only contain the one profile "
67479549,67479514,"stackoverflow.com",5,"2021-05-11 04:44:38+03","2024-05-17 05:26:08.652268+03","For a resource you just add a Condition to include or exclude it Also watch your indentation Type and Properties should be at the same level "
67479552,67479514,"stackoverflow.com",1,"2021-05-11 04:45:11+03","2024-05-17 05:26:08.65427+03","You cannot make entire resource conditional using If Normally the following should be done using Condition "
67491135,67482144,"stackoverflow.com",0,"2021-05-11 19:52:21+03","2024-05-17 05:26:09.129469+03","I solved this by My ec2 instance is already publicly not accessible so turning of protected mode should not be a problem "
71795857,67496089,"stackoverflow.com",0,"2022-04-08 14:00:47+03","2024-05-17 05:26:11.161522+03","Should not the env variable REPORT_REQUEST_QUEUE_URL locally point to ElasticMQ E g http0 0 0 09324queuelocalreportrequest "
67496419,67496292,"stackoverflow.com",0,"2023-06-02 13:27:15+03","2024-05-17 05:26:12.134318+03","Put a placeholder for the reference in serverless yml file [serverlessscriptableplugin] LambdaFunctionARN THIS_IS_SET_IN_SCRIPT The scriptable plugin uses a hook The script completeLambdaAssociation js updates the LambdaFunctionARN by searching for the AWSLambdaVersion resource "
67504281,67502076,"stackoverflow.com",8,"2021-05-12 17:08:01+03","2024-05-17 05:26:13.064298+03","Okay I got it by replacing process env with env serverless yml"
67520364,67520056,"stackoverflow.com",1,"2021-05-13 16:47:24+03","2024-05-17 05:26:14.16375+03","the Azure functions support is currently not available in Db2 NET provider It will be available in the upcoming release of NET 5 and NET Core 3 1 See httpscommunity ibm comcommunityuserhybriddatamanagementcommunitiescommunityhomedigestviewerviewthreadGroupId6163MessageKey2494b111ffe74073a8ece7e1c485e6d0CommunityKeyf2e5dc34896d4e8e9678724907c4b9f5tabdigestviewerReturnUrl2fcommunity2fuser2fhybriddatamanagement2fcommunities2fcommunityhome2fdigestviewer3fcommunitykey3df2e5dc34896d4e8e9678724907c4b9f526tab3ddigestviewer"
67554060,67553537,"stackoverflow.com",29,"2021-05-16 10:45:37+03","2024-05-17 05:26:15.155533+03","Flask Werkzeug and other pallets projects just had a major update dropping python2 support and deleting _compat module And AWS hast resolve the capability issue yet The simplest fix will be downgrading Flask Werkzeug etc to the previous major version "
67570077,67569834,"stackoverflow.com",6,"2021-05-17 15:53:05+03","2024-05-17 05:26:16.177319+03","In addition to being installed globally via NPM serverless can also be a project dependency If you are working in a node project ensure that serverless is not listed in the package json file or in the packagelock json file If so run npm uninstall serverless and then npm install d [email protected] to install the framework in the project at the correct version "
68118942,67580041,"stackoverflow.com",0,"2021-06-24 18:48:15+03","2024-05-17 05:26:16.966946+03","I already knew it because I test on AWS console and it have no http header It is when I use postman "
67598485,67597444,"stackoverflow.com",4,"2021-05-19 10:10:38+03","2024-05-17 05:26:17.897473+03","Basically in my environment variable there is the wrong format named _X_AMZN_TRACE_ID It was not matching this pattern [azAZ] [azAZ09_] After that I had changed the variable name and now it is working fine "
67606521,67598301,"stackoverflow.com",2,"2021-05-19 18:44:30+03","2024-05-17 05:26:18.777646+03","Flink is designed to help you process and move data continuously between storage or streaming solutions It is not intended to and would not work well with websockets directly for these reasons When submitting a job the runtime serializes your logic and moves it to other TaskManager instances so that it can parallelize them These can be on another machine entirely Now if you were intending to service a websocket with that code it has just moved elsewhere TaskManagers can be stopped and restarted scaling event recovering from a checkpointsavepoint etc That is where your websocket connection will be cut Also the Flink planner can decide that your source functions need be read twice if it helps the processing This means that your websockets would need to maintain a history of messages received and make sure they are sent once to each operator instance This being said you can have a webserver managing the websocket piping messages back and forth to a Kafka topic which then Flink can operate on Since you are talking about AWS I suggest you learn about their Websocket API Gateway service I believe these can be connected easily with Kinesis which Flink can read from and write to easily "
68701033,67599191,"stackoverflow.com",5,"2021-08-08 16:00:26+03","2024-05-17 05:26:19.721204+03","This can happen when your Custom Domain Name has a base path mapped to an API that was created manually as in not by serverless deploy run Delete the offending API mapping and all should work for you "
67802408,67600166,"stackoverflow.com",0,"2021-06-02 12:24:59+03","2024-05-17 05:26:20.571696+03","I believe the problem in your case is the fact that the handler you have for serverlessstackoutput plugin is the same as for your function That plugin explicitly calls the handler httpsgithub comsbstjnserverlessstackoutputhandler which executes your function If you will drop use of the plugin or just configure a different handler for it the problem should disappear"
70886019,67611900,"stackoverflow.com",1,"2022-01-27 23:32:42+02","2024-05-17 05:26:21.673597+03","I got this error when I was using the serverlessstackoutput plugin with serverless and it was only when specifying a custom file handler at utilsome_handler js Incorrect way that caused an error Correct way handler is a function in utilsome_handler js that is exported via module exports See the serverlessstackoutput docs on npm"
71966843,67619133,"stackoverflow.com",1,"2022-04-22 12:54:37+03","2024-05-17 05:26:22.918528+03","strToBool only solves the problem that when a boolean value is received from a variable like e g SSM is actually returned as a String strToBool will then make the conversion But if your custom variable custom AwsXRayEnabled optstage has a hardcoded boolean value like following then strToBool is not necessary Your issue is actually Typescript as my guess is that your compiler says something like Type string is not assignable to type boolean And in a sense that is correct because you do have string defined with What Typescript does not know is that this string will be substituted by the Serverless framework templating engine to a boolean value So the solution is to tell Typescript that it will be a Boolean This can be achieved by simply doing Why do you first need to set unknown Well I am not a Typescript expert but Phpstorm does give an interesting explanation TS2352 Conversion of type string to type boolean may be a mistake because neither type sufficiently overlaps with the other If this was intentional convert the expression to unknown first "
67693271,67642109,"stackoverflow.com",0,"2021-05-25 20:58:00+03","2024-05-17 05:26:23.40045+03","In general continuous delivery best practice is to limit the size and complexity of your deployments and instead do frequent small deployments When applied to Serverless this means you should consider deploying new infrastructure separately from application code This behavior would prevent this issue and other limitations which are not always apparent but nonetheless crop up Consider deploying schema changes to your tables in a separate deployment from the application code Then once you have verified that the indices have been created successfully you can easily add new queries for those indices "
67644098,67643889,"stackoverflow.com",2,"2021-05-22 00:38:20+03","2024-05-17 05:26:24.034136+03","Second option is always better Creating multiple lambda function for each functionality Lambda latency depend on how many calls get from API gateway If you are using multiple endpoint and single lambda calls then it is going to bottleneck or high latency issue Plus lambda charge based on per lambda calls Each lambda 1 million request is free if you use one lambda for all use going to hit this limit early Recommendation is use different lambda function for each functionality and this is beauty of Micro Service Keep simple and light weight "
67644775,67643889,"stackoverflow.com",1,"2021-05-22 02:24:07+03","2024-05-17 05:26:24.037137+03","One of the main benefits of Lambda Functions is independently scalable pieces of functionality within your application You not only realize isolated scalability this way but you also realize cost savings potentially "
67661774,67645510,"stackoverflow.com",0,"2021-05-23 19:03:45+03","2024-05-17 05:26:24.762909+03","It looks like a sign is missing from your provider stage"
74495308,67646196,"stackoverflow.com",4,"2022-11-18 23:05:21+02","2024-05-17 05:26:25.745773+03","One recommendation Amazon presents is to use the sam tool to build the distribution by using a Docker container However in my situation I was not able to use docker in the build environment Amazon provides some other documentation on how to use pip to install requirements by passing very explicit command line flags to ensure the Lambda environments version is downloaded The buildpackage path will cause the dependencies to be downloaded and installed into that directory to allow for easy zip for upload into a Lambda These flags can also be used if you have a setup cfg or pyproject toml file by using as the resource to load rather than the explicitly named library I expect that as Amazon introduces new runtime environments and deprecates older ones the given platform and python flags will need to change "
67654678,67646196,"stackoverflow.com",3,"2021-05-23 01:26:42+03","2024-05-17 05:26:25.747774+03","I had a similar problem before that was resolved by running the deployment command from a linux machine I use a mac for development and I was trying to deploy my lambda function from my mac However when it was deployed some of the dependencies threw import errors From my experience it was due to the operating system that packages the dependencies differently when it runs in a mac or a linux environment Hence try running the serverless deployment command from inside a linux machine to see if that works In my case I set up a gitlab CICD pipeline to run the command inside the environment of gitlab pipeline and that resolved the problem "
72410573,67646196,"stackoverflow.com",3,"2022-05-27 23:08:49+03","2024-05-17 05:26:25.749774+03","What I did to fix a similar problem while trying to add a layer with the cryptography library to a lambda function was to use the same runtime and processor architecture in both the lambda function and layer For example my problem was that I had a lambda function running with Python 3 9 and in a arm64 architecture But I was creating the layer zip file running python 3 8 and in a x86_64 architecture Do not ask me why it was easier for me to recreate the lambda in Python 3 8 and in x86_64 rather than the other way around Anyhow as soon as I added the layer with the cryptography library to the lambda it ran smoothly So my theory is that you need to match both the runtime and architecture in order for a layer to work properly with a lambda function Additionally now that I look up my solution it is actually backed up by this article httpsdocs aws amazon comlambdalatestdginvocationlayers html see the notes in it "
75889113,67646196,"stackoverflow.com",2,"2023-03-30 16:49:08+03","2024-05-17 05:26:25.751775+03","I had exactly the same problem when deploying lambdas to AWS from Mac with M1 processor I fixed it by adding dockerRunCmdExtraArgs argument to the serverless config"
76369772,67646196,"stackoverflow.com",1,"2023-05-31 05:25:35+03","2024-05-17 05:26:25.752775+03","awslayerpython 310 cryptography"
74156138,67646196,"stackoverflow.com",0,"2022-10-21 18:45:03+03","2024-05-17 05:26:25.753775+03","I had similar errors after migrating my Lambda functions from python 3 6 to python 3 9 I use an amazonlinux docker container for development testing and deployment via serverless In cryptographys documentation the installation steps for Linux are not as straightforward as in macOS as cryptography ships manylinux wheels as of 2 0 Heres what you could try Upgrade pip and reinstall cryptography via pip again or Compile cryptography yourself youll need a C compiler a Rust compiler headers for Python if youre not using pypy and headers for the OpenSSL and libffiInstall these packages are redhatrpmconfig gcc libffidevel python3devel openssldevel cargo using your package manager and then run pip install cryptography nobinary cryptography In cryptographys FAQ page there is a section about AWS Lambda "
77656340,67646196,"stackoverflow.com",0,"2023-12-13 22:19:58+02","2024-05-17 05:26:25.755834+03","Encountered the same issue on Mac M1 when deploying Lambda functions with Serverless Framework Solved it by configuring dockerRunCmdExtraArgs with platform linuxamd64 in the serverless config along with setting dockerizePip nonlinux and dockerImage public ecr awssambuildpython3 9latest "
77835972,67646196,"stackoverflow.com",0,"2024-01-18 00:49:27+02","2024-05-17 05:26:25.757073+03","Others have already mentioned this but my case was also the issue of architecture x86_64 vs arm64 The Lambda runtime was for x86_64 Intel 64bit architecture but the Lambda layer created was for arm64 If you are using Docker to generate the Lambda layer e g httpsgithub compatrickm663awslambdalayergenerator you can specify the Lambda layer architecture version through platform keyword In my case I changed FROM python3 9slim to FROM platformlinuxx86_64 python3 9slim in Dockerfile "
67693065,67657937,"stackoverflow.com",0,"2021-05-25 20:43:57+03","2024-05-17 05:26:26.667005+03","Mocha does not invoke the Serverless Framework so those variables are never parsed from the ENV file because the serverless yml file is never readinterpolated Instead you should use something like dotenv to parse that file and set variables in your test environment Or just manually set a few directly on process env in a beforeAll function "
67669188,67668483,"stackoverflow.com",1,"2021-05-24 11:53:00+03","2024-05-17 05:26:27.76469+03","Is there any Serverless way to do this No I think so You can create an npm script to do that package json Now you can run npm run deploytest "
67676120,67668483,"stackoverflow.com",0,"2021-05-24 20:04:53+03","2024-05-17 05:26:27.766692+03","One option you could explore would be building your own plugin which hooks the deployinitialize lifecycle hook and runs npm i You can read more at the documentation"
67688787,67687754,"stackoverflow.com",1,"2021-05-25 16:15:27+03","2024-05-17 05:26:28.767349+03","This looks to be a bug with the serverlessrubypackage plugin as it is been reported by other users You may consider adding a comment here "
67716178,67703857,"stackoverflow.com",3,"2021-05-27 08:30:12+03","2024-05-17 05:26:29.797635+03","Found the issue turns out I need to add just after name since AWS WAF supports V2 Once I added it and redeployed the API Gateway got attached to the created WAF PS the name is the name of the ACL that we want to use "
76751022,67703857,"stackoverflow.com",0,"2023-07-24 05:03:28+03","2024-05-17 05:26:29.798969+03","Creating an ACL in a separate template and importing its name to serverless yml did not work The export was in the form aclnamehere 8e22cf49765b46157ad4bebb66ed5c6a REGIONAL While the template expects"
67726425,67721853,"stackoverflow.com",0,"2021-05-27 19:36:44+03","2024-05-17 05:26:31.805844+03","I suspect the variable interpolation is missing for your production stage I would suggest running serverless package and inspecting the generated cloudformation template to identify IAM Role Policy for the Lambda Execution verify it for completeness "
76769234,67734420,"stackoverflow.com",0,"2023-07-26 11:14:48+03","2024-05-17 05:26:33.093145+03","before the update you should delete the"
67742017,67741752,"stackoverflow.com",0,"2021-05-28 19:09:19+03","2024-05-17 05:26:33.266395+03","Execute the pwd command inside the container while running it Try The error showing sls not able to find the config file in the current working directory Either add your config file to your current working directory Include this copying in Dockerfile or copy it to specific location in container and pass config in CMD sls deploy config This command can only be run in a Serverless service directory Make sure to reference a valid config file in the current working directory"
68805398,67741752,"stackoverflow.com",0,"2021-08-16 18:22:11+03","2024-05-17 05:26:33.268395+03","Be sure that you have serverless installed Once installed create a service cd to folder with the file serverless yml This will deploy the function to AWS Lambda"
67989294,67934877,"stackoverflow.com",1,"2021-06-15 18:34:42+03","2024-05-17 05:26:34.937408+03","I reviewed the serverless docs and there doesnt seem to be an official way to do this However as a work around you can just parse your serverless yml file in your key js file using an npm package like httpswww npmjs compackageyaml Then do something like this When you parse serverless yml in your key js you can then just use normal not notation to get the params"
73660981,67934877,"stackoverflow.com",1,"2022-09-09 13:49:21+03","2024-05-17 05:26:34.938408+03","My recommendation is to use Serverless environment variables In your serverless yml file you have a section environment under provider Then those values will be populated for all your lambdas as process env varibles For example having this section in you serverless yml Then in your typescript files you can use this value as"
78181749,67934877,"stackoverflow.com",0,"2024-03-18 18:47:15+02","2024-05-17 05:26:34.940409+03","You should add the variable to the custom section and inside the imported function you use the resolveVariable method docs httpswww serverless comframeworkdocsprovidersawsguidevariablesreferencepropertiesinotherfilestextExporting20afunctionNote3A20the20method"
67948891,67943931,"stackoverflow.com",1,"2021-06-12 15:33:44+03","2024-05-17 05:26:35.832797+03","If I run your code locally following exception is displayed Microsoft Azure WebJobs Host Error indexing method upload Microsoft Azure WebJobs Host Unable to resolve binding parameter name Binding expressions must map to either a value provided by the trigger or a property of the value the trigger is bound to or must be a system binding expression e g sys randguid sys utcnow etc As mentioned in the error message you have to specify the variable in the trigger I guess binding to the queryparameter is still not possible in Azure Functions So you have to specify it in the route Run it locally upload [GETPOST] httplocalhost7071api name Complete code Please find the working code on GitHub"
67989001,67959096,"stackoverflow.com",1,"2021-06-15 18:16:48+03","2024-05-17 05:26:36.850444+03","Please open the AWS Console and inspect the table settings If the billing mode is set to pay per request try redeploying If that fails again please provided a detailed description serverless version plugin information and any other information that might help the team track it down and then open a bug ticket in the project"
67963538,67962125,"stackoverflow.com",4,"2021-06-14 03:30:28+03","2024-05-17 05:26:37.70304+03","In default VPC all subnets are public This means that even if you have NAT your lambda will not work To make it work you have to create a private subnet in your default VPC setup it route tables to the NAT located in a public subnet and then place your function in the private subnet "
67962591,67962125,"stackoverflow.com",0,"2021-06-13 23:55:36+03","2024-05-17 05:26:37.704676+03","You can give your VPC Lambda access to connect to the internet using either VPC end points or a NAT gateway like you have setup and is also described here httpsaws amazon compremiumsupportknowledgecenterinternetaccesslambdafunction However this gives your Lambda access to the internet it does not work in the other direction ie give the Lambda access to from the internet side to to your Lambda For that you need to use an API Gateway that fronts it etc "
77871787,67962125,"stackoverflow.com",0,"2024-01-24 11:22:34+02","2024-05-17 05:26:37.706155+03","The Lambda function within a VPC particularly within a public subnet faces limitations in accessing the internet This is primarily due to the default absence of a public IP for AWS Lambda functions leading to the rejection of requests by the Internet Gateway To address this issue you can take the following steps By implementing these steps the Lambda function within the public subnet can overcome the inherent lack of public IP and gain the necessary access to the internet Reference Links"
67977269,67977166,"stackoverflow.com",1,"2021-06-15 00:12:18+03","2024-05-17 05:26:38.449496+03","Every item listed in the functions block is a separate lambda function The example post you cited lays that out somewhat implicitly If you would like to combine the search and read APIs into the same function you will need to perform routing internally You can do that with several popular frameworks including Express httpapi or you can build your own Then you will simply proxy all requests to it"
68006871,68006047,"stackoverflow.com",2,"2021-06-16 19:42:18+03","2024-05-17 05:26:39.473002+03","You have nested params in another object This causes the method call to receive Try replacing dynamodb update params with just dynamodb update params Also with the document client you do not need to specify the type Key id will be fine "
68020563,68017528,"stackoverflow.com",1,"2021-06-17 16:39:39+03","2024-05-17 05:26:40.463439+03","If your authorizer function is part of another cloud formation stack you can use CloudFormation outputs to do this If not there are still several other variable solutions in the docs To do that conditionally you would combine it with a default For example"
68019211,68018892,"stackoverflow.com",0,"2021-06-17 15:19:39+03","2024-05-17 05:26:41.270135+03","You can use a Standalone Nest application and pass the event data directly to MyService"
75231923,68018892,"stackoverflow.com",0,"2023-01-25 11:28:30+02","2024-05-17 05:26:41.271135+03","You can use NEstJs standalone app and make your handler like this export const checkDeletion Handler async event any context Context After that call your handler from serverless yaml like"
68025384,68021722,"stackoverflow.com",1,"2021-06-17 22:45:22+03","2024-05-17 05:26:42.232405+03","The errors are warning you that you have got two indentation issues The first is in your serverless_common yml file The array items should be indented one place further The second is in your cron expression The oneline syntax is only if you are not using other arguments Since you want to pass enabled false you will need to use the multiline syntax Unfortunately the syntax you have chosen will not merge the two arrays You will have to reference each item in your array individually or rewrite your serverless yml into a serverless js file which allows you to be more programmatic "
68102404,68035352,"stackoverflow.com",1,"2021-06-23 18:04:11+03","2024-05-17 05:26:43.942607+03","I found serverlessSSMpublish plugin which is doing the job of writingupdating SSM just need to add this to serverless yml"
74122465,68261153,"stackoverflow.com",1,"2022-10-19 11:50:27+03","2024-05-17 05:27:03.933406+03","You can now use the environment key to add environment variables to your function For example Then you can access it using os environ in Python"
68072009,68040032,"stackoverflow.com",1,"2021-06-21 20:06:35+03","2024-05-17 05:26:45.357007+03","It is a deprecation warning which serves to inform you that in the next version of the Serverless Framework this specific resolver syntax is deprecated and will error as internally the process for resolving variables has changed You can adopt the new custom resolver very simply by changing the declaration in the serverless yml and modifying the function arguments in getcustomvalue js then you can set in your serverless yml to indicate you have migrated That will instruct the Serverless Framework to use the new resolver as indicated by the warning message The full overview is in the documentation"
68040942,68040700,"stackoverflow.com",1,"2021-06-18 23:11:37+03","2024-05-17 05:26:45.715775+03","You can output anything you would like in the resources section You can use all the same templating variables like Ref selfcustom myEndpointName and Join to clean it up"
68057601,68051340,"stackoverflow.com",2,"2021-06-20 18:52:47+03","2024-05-17 05:26:46.466374+03","Without seeing your application code I cannot be certain However I presume you are doing something like this I noticed in your serverless yml the environment block is at the root level That will not work so your environment variable is undefinednot set The environment block goes inside the provider block or inside the function block depending on if you want to set an env var per function or for all functions in your stack "
68051357,68051340,"stackoverflow.com",2,"2021-06-20 01:57:27+03","2024-05-17 05:26:46.468375+03","The error msg says that you are using Tablename not TableName "
72192450,68051340,"stackoverflow.com",0,"2022-05-16 11:40:38+03","2024-05-17 05:26:46.469496+03","I had some issues with tableName I resolved it using proper tableName My code looked like this"
68069795,68061842,"stackoverflow.com",5,"2021-06-21 17:31:39+03","2024-05-17 05:26:47.467739+03","Your permissions are goofed Speaking generally you do not want to install anything from npm with sudo You can sudo chown yourUseryourGroup R usrlocallibnode_modules If you are on macOS your group is probably staff You can find your group by running ls l in your home directory and looking at the label adjacent to your username A simple alternative is a best practice anyway install the serverless framework as a development dependency of your project And then instead of running serverless deploy you can run npx serverless deploy Then as you check in the package json file to version control other collaborators will use the same version of the serverless framework "
68071091,68070277,"stackoverflow.com",1,"2021-06-21 18:56:42+03","2024-05-17 05:26:47.948197+03","Simply add py to your excludeFiles block You can read more in the documentation Side note you may consider trying esbuild instead which is often 10x faster than webpack You can read more here"
68090821,68088344,"stackoverflow.com",2,"2021-06-23 05:17:15+03","2024-05-17 05:26:48.967358+03","The libraries in node_modules are required for your function to run in AWS Lambda That is why they are called dependencies and that is why they are zipped and uploaded to S3 because your function needs them in order to run If you are using specific libraries like typescript joi mocha or jest which are only meant for development you can ensure the Serverless framework will not package them by installing them as development dependencies like this serverless deploy is the deployment of lambda The serverlesspythonrequirements plugin is simply a packaging extension that gives python projects a similar workflow as nodejs projects with npm The only package available to you by Lambda is the awssdk"
68105292,68105052,"stackoverflow.com",3,"2021-06-23 21:22:46+03","2024-05-17 05:26:49.849569+03","Every key you use in any index GSI LSI etc needs to be listed in the AttributeDefinitions block So this should work You can read more in the documentation"
68426799,68117964,"stackoverflow.com",1,"2021-07-18 10:00:17+03","2024-05-17 05:26:51.506223+03","You must also pass the headers prop If you use the app Postman to make the request it will automatically pass that header prop Otherwise you will have to explicitly pass it Heres an example valid json you could pass"
68154874,68141464,"stackoverflow.com",0,"2021-06-27 23:06:11+03","2024-05-17 05:26:54.140984+03","Plugins are loaded in order from top to bottom serverlessoffline and serverlessdynamodboffline are a special case because the former depends on the latter but for the vast majority of plugins this is not an issue You can read more about it in the documentation Finally I would advise you to consider developing against the cloud instead of using things like severlessoffline You can read my thesis on the subject here "
68178620,68176433,"stackoverflow.com",1,"2021-06-29 15:22:13+03","2024-05-17 05:26:55.066151+03","no worries AWS have a solution httpsaws amazon comelasticbeanstalk Just one tiny point This service is free but the resources deployed whit it are not so be careful Your first job in AWS is to create Cloud Watch alarm when you reach price point I have alerts when my mountly bill reach 10 20 100 httpsdocs aws amazon comAmazonCloudWatchlatestmonitoringmonitor_estimated_charges_with_cloudwatch html"
68195757,68189184,"stackoverflow.com",4,"2021-06-30 16:30:01+03","2024-05-17 05:26:55.65526+03","It should work like this within your serverless yml you can reference env parameters with envkeyname and AWS Parameters using the paramkeyname syntax If you need to support both of them you just need to write envkeyname paramkeyname Heres an example"
68203523,68202859,"stackoverflow.com",0,"2021-07-01 05:22:33+03","2024-05-17 05:26:57.697049+03","How to create react app and deploy to CloudFront httpswww prisma iobloghowtousecreatereactappwithgraphqlapollo62e574617cff httpsdeveloper okta comblog20181011buildsimplewebappwithexpressreactgraphql"
69117638,68203167,"stackoverflow.com",0,"2021-09-09 15:08:56+03","2024-05-17 05:26:58.560161+03","Unfortunately there is no way just manually from aws console you may check it in AWS interface that should be in this package serverlesstypescript There is no description key inside provider "
68219494,68219493,"stackoverflow.com",2,"2021-07-02 07:06:32+03","2024-05-17 05:27:00.880949+03","I decided to post this with my own answer since it took me a few hours of wasted time to verify the policy was being created and determine what is going on I believe since S3 does not usually include a region definition and buckets are global the simpler specification works SQS and probably most other resources however need these things Once I determined this might be the issue it took a little research to get the correct syntax working"
68239652,68239470,"stackoverflow.com",0,"2021-07-03 23:17:48+03","2024-05-17 05:27:01.11672+03","I am not sure about what scale will you be operating But I think you can use cloudwatch events which ultimately triggers the lambda function So your users configuration can become a cloudwatch event with its specific rule and whenever the rule executes it triggers the lambda function I would suggest you to go through this httpsaws amazon comblogsmtbuildschedulerasaserviceamazoncloudwatcheventsamazoneventbridgeawslambda Looks like it is exactly what you are looking for "
74425402,68239470,"stackoverflow.com",0,"2022-11-14 01:42:58+02","2024-05-17 05:27:01.117719+03","Recently 10Nov2022 AWS launched a new service called EventBridge Scheduler and it is serverless I think this is what you are looking for Mainly in the schedule patterns you can choose In the recurring schedule you have two schedule types For both options in Recurring Schedule you can set the Timeframe as well "
71715354,68252134,"stackoverflow.com",1,"2022-04-02 09:25:11+03","2024-05-17 05:27:02.990865+03","Create a custom s3 bucket name variable e g And use it events"
68252391,68252134,"stackoverflow.com",0,"2021-07-05 10:44:25+03","2024-05-17 05:27:02.991973+03","You should use your bucket name not MyS3Bucket "
68332356,68261153,"stackoverflow.com",2,"2021-07-11 02:55:51+03","2024-05-17 05:27:03.932406+03","You can try interpolating stage name with the function name like this The function name can be obtained by handler code from a context parameter property function_name If you use the convention mentioned above you can obtain stage name in this way"
68469294,68465148,"stackoverflow.com",7,"2021-07-21 15:13:48+03","2024-05-17 05:27:04.74552+03","your layer configuration is correct from the Serverless Framework and TypeScript perspective the problem could be in the packing of the project itself e g internal of serverlessplugintypescript i would suggest trying another TypeScript plugin like serverlessesbuild using your tsconfig json example and samples from serverless yml I created an example here httpsgithub comoieduardorabelo20210721serverlesstypescriptlayers it is using esbuild for packing and transpile TypeScript to JavaScript and it is working as expected"
77377371,68465148,"stackoverflow.com",0,"2023-10-28 01:44:20+03","2024-05-17 05:27:04.747701+03","I strongly suspect you have run into this issue here httpsgithub comserverlessserverlessissues10326 Which looks like a bug in the plugin which should go away with version 3 which is currently in preview Although as mentioned here I ran into another bug with the 3 0 0pre 82c9bc17 prerelease where it produces this error which I worked around with this ugly hack"
68499037,68497679,"stackoverflow.com",1,"2021-07-23 15:15:03+03","2024-05-17 05:27:07.276899+03","What people tend to do is build a frontend using tools like React Vue Svelte etc that builds a frontend consisting of only HTML CSS and JS The JavaScript talks to your Serverless backend using HTTP requests The HTML CSS and JS can then be hosted in an AWS service called S3 and this can be deployed in any number of ways which is probably what the support meant when they said that You could upload the code manually or use a tool you can add to the Serverless Framework called Lift httpsgithub comgetliftlift which helps make it easier to upload these static files to S3 as a website "
68662218,68507854,"stackoverflow.com",2,"2021-08-05 10:30:12+03","2024-05-17 05:27:08.375303+03","I do not know what kind of code base u are using so I will add my code which I wrote in GO In essence you should connect to MSK cluster the same way as you would connect to some stand alone Kafka instance We are using brokers for connecting or better said writing to MSK cluster I am using segmentiokafkago library My function for sending event to MSK cluster looks like this My serverless yml Upper code addEvent belongs to functions postEvent in serverless yml If you are consuming from kafka then you should check functions processEvent Consuming event is fairly simple but setting everything up for producing to Kafka it crazy We are probably working on this for month and a half and still figuring out how everything should be set up Sadly serverless does not do everything for you so you will have to click trough manually in AWS but we compared to other frameworks and serverless is still the best right now I must warn you that we spend a lot of time figuring out how to properly setup VPC and other networking permission stuff My collage will write blog post once he arrivers from vacation I hope this helps you some how Best of luck UPDATE If you are using javascript then you would connect to Kafka similar to this"
70842957,68507854,"stackoverflow.com",0,"2022-01-25 05:01:39+02","2024-05-17 05:27:08.378304+03","The connection string which is called broker bootstrap string can be found my making an API call like aws kafka getbootstrapbrokers clusterarn ClusterArn See example here httpsdocs aws amazon commsklatestdeveloperguidemskgetbootstrapbrokers html Also here is a step by step walk through on how produceconsume data httpsdocs aws amazon commsklatestdeveloperguideproduceconsume html"
68610404,68546410,"stackoverflow.com",1,"2021-08-01 15:25:47+03","2024-05-17 05:27:09.239161+03","Your issue is very similar to this issue According to the post and from my experience No I do not think you can perform validation in apigateway level "
68623933,68549286,"stackoverflow.com",1,"2021-08-02 18:13:26+03","2024-05-17 05:27:10.024728+03","I m not familiar with ajv but I ve used middyvalidator Assuming you have a lambda function that expectes a JSON body and has the next simple code just returns the JSON body received My suggestion and what worked for me is the following Where yourSchema is the validation schema you built "
68925753,68580153,"stackoverflow.com",1,"2021-08-25 18:34:22+03","2024-05-17 05:27:10.936546+03","I dont see the role for roleArn entry declared in your DBProxy resource See the documentation httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawsresourcerdsdbproxy htmlicmpiddocs_cfn_console_designer Take note of the RoleArn in the CF snippet below"
75283280,68580153,"stackoverflow.com",0,"2023-01-30 12:44:07+02","2024-05-17 05:27:10.938547+03","I am unable to replicate the same error you had but I had the same error msg and managed to resolve it by going to Cloudwatch service and search for rdsproxy log group you will able to get a more granular error message The output for aws rds describedbproxytargets dbproxyname soproxyrdsdbproxy was very misleading In my case I never gave rds permission to assume my proxy role It can be quickly fixed with hope this will be helpful "
68585502,68585058,"stackoverflow.com",1,"2021-07-30 07:39:36+03","2024-05-17 05:27:11.476072+03","In your Resources you have to add AWSLogsLogGroup and AWSLogsLogStream But tags on AWSLogsLogGroup are not supported "
68594048,68591491,"stackoverflow.com",2,"2021-07-30 19:10:24+03","2024-05-17 05:27:12.359169+03","Build a whl file corresponding to package using within a parent directory Add the relative path to this whl file to used pip requirement file requirements txt for instance serverlesspythonrequirements will automagically pack this dependency within the deployed archive when doing sls deploy How cool is that huh "
68597751,68596791,"stackoverflow.com",2,"2021-07-31 01:41:45+03","2024-05-17 05:27:13.070809+03","This is probably because of your use of Please change it to Alternatively fix your spaces when using and your code are not aligned "
68603758,68596805,"stackoverflow.com",2,"2021-07-31 19:07:19+03","2024-05-17 05:27:13.627409+03","If all of your services share the same serverless yml file they live in the same cloudformation stack so what you are asking for does not work You can use serverless deploy function to update just one function but that skips cloudformation so new resources will not be created and your stack will drift You should use it during development but not for production deployments I strongly recommend splitting your stacks You can still use a monorepo and then you can conditionally deploy services depending on which files changed for a given commit "
68602415,68602298,"stackoverflow.com",1,"2021-07-31 20:55:01+03","2024-05-17 05:27:14.590894+03","I see a few potential problems with your template"
68611074,68610427,"stackoverflow.com",0,"2021-08-01 16:52:59+03","2024-05-17 05:27:15.723076+03","Okey guys I got it Since I am using typescript I have to import the serverlesstypescript plugin to convert all the ts files to js "
68615536,68613056,"stackoverflow.com",0,"2021-08-02 04:26:58+03","2024-05-17 05:27:16.609839+03","Heres how i resolved this one guys I had an extra layer configured who is path had no files in it By removing the emptylayerfolder or adding any files to it I was able to resolve this error message "
68772656,68769276,"stackoverflow.com",0,"2021-08-13 15:51:18+03","2024-05-17 05:27:28.683453+03","If you want to use JavaScript from within a Lambda function to upload an image to an Amazon S3 bucket try using the AWS SDK for JavaScript instead of posting to AWS API Gateway For information on how to use the AWS SDK for JavaScript to place files in an Amazon S3 bucket see Uploading photos to Amazon S3 from a browser "
68782117,68773548,"stackoverflow.com",0,"2021-08-14 12:42:13+03","2024-05-17 05:27:29.38022+03","It is the serverless Frameworks method of adding the trigger to call the lambda to the S3 bucket via a Custom Resource"
68779183,68777034,"stackoverflow.com",0,"2021-08-14 02:17:00+03","2024-05-17 05:27:30.218783+03","yeah like the comments say make your fake promises functions instead of const promises Then call it like so"
69207572,69196793,"stackoverflow.com",0,"2021-09-16 14:25:20+03","2024-05-17 05:27:46.168906+03","It turned out that the lambdas getting timeout out was the issue The message got lost in the huge volume of logs and interestingly timed out is not an error so filtering logs with ERROR did not work "
68619073,68615017,"stackoverflow.com",1,"2021-08-02 12:13:54+03","2024-05-17 05:27:17.540786+03","The Serverless Framework creates an S3 bucket as a way to get your service into AWS Instead of trying to push directly at the Lambda service it packages it in a zip uploads to S3 and then points at that S3 bucket for the deployment process to know where to find stuff You can specify your own S3 bucket which should be used to store all the deployment artifacts The deploymentBucket config which is nested under provider lets you e g set the name or the serverSideEncryption method for this bucket If you do not provide your own bucket Serverless will create a bucket which uses default AES256 encryption As for the API Gateway if you want to use an existing API Gateway resource no real need to though as they cost nothing unless there is traffic going through them you can share the same API Gateway between multiple projects by referencing its REST API ID and Root Resource ID in serverless yml as follows You should reconsider using CloudWatch at least at a basic level It is the only way you can get output from your functions unless you tie in a service that makes API requests which can add latency to your service CloudWatch does not add latency or at least so small as to be insignificant However if you really must turn CloudWatch off you cannot stop it creating the log group in ClouWatch but you can restrict the time for logs to live to 0 or a small number of days"
68627090,68627016,"stackoverflow.com",2,"2021-08-02 22:33:50+03","2024-05-17 05:27:18.515472+03","The 403 Forbidden is related to the user or service account who is deploying the function You will need to have at least the rolescloudfunctions developer in order to be able to deploy a Cloud Function Also you must also assign the user the Service Account User IAM role rolesiam serviceAccountUser on the Cloud Functions Runtime service account Ref httpscloud google comfunctionsdocsreferenceiamrolesadditionalconfiguration"
68634303,68627016,"stackoverflow.com",2,"2021-08-03 13:18:32+03","2024-05-17 05:27:18.517048+03","It got resolved by changing the project variable in serverless yml file I was mentioning the numeric id but it was expecting the alphanumeric id of the project that we see on the dashboard of GCP Somehow it was not throwing the exact error but after wasting lots of time in it got resolved Thanks everyone for answering this question "
68631857,68627016,"stackoverflow.com",1,"2021-08-03 10:20:29+03","2024-05-17 05:27:18.518046+03","The third party application use ADC Application Default Credential It could be by order of precedence The 1 is not a good practice and I do not recommend it The 2 is impossible on your workstation The 3 is the best perform a gcloud auth applicationdefault login to create the credential file and try again "
68710632,68633528,"stackoverflow.com",0,"2021-08-09 13:35:15+03","2024-05-17 05:27:19.100793+03","You should be able to define the function and a custom authorizer function for it in the same Serverless Framework service as is evident also from the documentation If you run into problems with a fresh deploy that could indicate a bug in Serverless Framework timing problems between functions in CloudFormation stacks have been common in it You can always try upgrading your sls In bigger projects it is common to isolate authorizer functions to their own stack referenced from several other stacks which sidesteps this internal dependency issue in that case you naturally deploy the authorizer stack first "
68658705,68641214,"stackoverflow.com",1,"2021-08-05 01:39:32+03","2024-05-17 05:27:21.148014+03","There is not a good way to get access to these environment variables if you are running the lambda code as a script The Serverless Framework injects these variables into the Lambda function runtime configuration via CloudFormation It does not insertupdate the raw serverless yml file nor does it somehow intercept calls to process env via the node process You will could use the scriptable plugin to run after package and then export each variable into your local docker environment But that seems pretty heavy for the variables in your env Instead you might consider something like dotenv which will load variables from a env file into your environment There is a serverlessdotenv plugin you could use and then your script could also call dotenv before running "
68702760,68689752,"stackoverflow.com",0,"2021-08-08 19:36:05+03","2024-05-17 05:27:21.799441+03","The logline you have posted here is the HTTP Access Log for API Gateway It is essentially like an apache or nginx access log containing just response code and path You will find the bug complete with stack trace by examining the Lambda execution logs for your function Log groups for API Gateway are prefixed with APIGatewayExecutionLogs_apiIdstageName You are looking for awslambdayourfunctionnamestageName "
68705149,68699104,"stackoverflow.com",0,"2021-08-09 01:22:10+03","2024-05-17 05:27:22.696488+03","Try running aws configure inside your conda environment httpsdocs aws amazon comclilatestuserguidecliconfigurequickstart html Also please do not run it using sudo if you need to do that you have probably installed something incorrect "
68708863,68701001,"stackoverflow.com",1,"2021-08-09 11:16:46+03","2024-05-17 05:27:23.796148+03","It looks like you want all of your requests to be processed sequentially In that case you can set a maximum concurrency to 1 and you will not have two lambdas running at the same time That being said it will not scale anymore and it kinda defeats the benefits of a serverless infrastructure "
68702732,68701001,"stackoverflow.com",0,"2021-08-08 19:33:22+03","2024-05-17 05:27:23.798148+03","Lambda is stateless compute so if you need shared state you will need to build that If your throughput is low dynamo is cheap enough and comes with consistency guarantees that may be effective for you If not redis would be a good option especially a managed a managed solution like redislabs which offers an HTTP API nicely suited for Lambda "
68764031,68750814,"stackoverflow.com",0,"2021-08-12 23:50:47+03","2024-05-17 05:27:25.716326+03","This is an error from the node process running the serverless command Likely caused by a firewall or system between yourself and the S3 bucket where these templates are stored You may try running NODE_TLS_REJECT_UNAUTHORIZED0 serverless which suppresses this error "
69007096,68754612,"stackoverflow.com",3,"2022-08-06 04:49:14+03","2024-05-17 05:27:26.759974+03","After a lot of digging I isolated the cause to having multiple incompatible versions of gracefulfs in the resolved node subdependencies In my case I am using the serverlesspythonrequirements plugin not serverlesss3sync but the error is the same Both plugins list gracefulfs as a dependency You can see from yarn list that there are three different versions of gracefulfs installed It seems that the stream objects created by one version of this library are not consumable by the other version With yarn I solved this by pinning gracefulfs to the latest version by adding this to my package json file For npm it should be possible to achieve the same thing using npmforceresolutions but it appears the syntax is slightly different I have not tested this Removing the lock file and node_modules folder and reinstalling might solve the problem but given the pain this has caused I was happier to pin the version in package json "
70322564,68754612,"stackoverflow.com",2,"2021-12-13 14:59:39+02","2024-05-17 05:27:26.762613+03","I got this error message twice from Serverless Framework The error message is not descriptive for the root cause The way I could resolve it was Now I could deploy the project again But the error message is misleading and should be considered an error in itself that the error message is a source of painful confusion "
69117505,68768994,"stackoverflow.com",0,"2021-09-09 14:37:29+03","2024-05-17 05:27:27.80138+03","There are 2 ways imho Make your endpoints private and then add 2 api keys if you just want to have separate endpoint that will be accessed only by devs Try to deploy your app to different stages prod sandbox It is not the best answer but maybe it will help you to did deeper on this subject "
68810052,68785753,"stackoverflow.com",2,"2021-08-17 04:08:18+03","2024-05-17 05:27:31.305516+03","After some thinking if you are still committed to Lambda as the compute solution I think your best option is to manage provisioned concurrency outside of the Serverless Framework entirely You have already got an orchestrator function which will enable provisioned concurrency you could try removing provisionedConcurrency from your serverless yml file adding another method in your orchestrator to disable provisioned currency in the evenings and verifying that you can deploy when your orchestrator has set your functions to either state If you are willing to throw away your orchestrator function AWS suggests using Application Auto Scaling which is very useful for exactly what you are doing hat tip to mpv That being said Lambda is not particularly well suited to predictable steadystate traffic If cost is a concern I would suggest exploring Fargate or ECS and writing a few autoscaling rules Your Lambda code is already stateless and likely is portable and has pretty limited networking rules There are other forms of compute which would be dramatically cheaper to use "
68890967,68826878,"stackoverflow.com",3,"2021-08-23 13:30:34+03","2024-05-17 05:27:33.399808+03","Your serverless yml file needs to be in the same folder where node_modules folder is located Also ensure you ran npm install and the serverless yml contains the following Follow the readme in httpswww npmjs compackageserverlessoffline"
68948798,68826878,"stackoverflow.com",1,"2021-08-27 09:16:22+03","2024-05-17 05:27:33.401803+03","Answering based on below statements my node_modules folder is not the root folder serverless yml it is in the same folder as node_modules Make sure you are running serverless offline or sls offline from the directory where the serverless yml is located If you need to run it from outside of the directory where the serverless yml is located you can change the script which uses serverlessoffline to first change the current working directory Ex cd path to the directory of serverless yml serverless offline Side note Also make sure the indentation of the serverless yml is correct"
68955061,68826878,"stackoverflow.com",0,"2021-08-27 17:24:41+03","2024-05-17 05:27:33.403302+03","Can you try running which serverless from your command line Did you try running with the global serverless executable not the one from node_modules "
68840437,68840427,"stackoverflow.com",2,"2021-08-19 02:40:11+03","2024-05-17 05:27:34.370692+03","you can prevent duplicate versions by using this plugin serverlesspruneplugin"
69274743,69094549,"stackoverflow.com",1,"2021-09-21 22:45:30+03","2024-05-17 05:27:35.364133+03","Your problem is not related to nx The issue here underlies in using bcrypt Alternatively you can use bcryptjs instead of bcrypt"
69098106,69097321,"stackoverflow.com",1,"2021-09-08 15:46:24+03","2024-05-17 05:27:36.23064+03","You have to set the Resource to and use condition keys to limit the access scope httpsdocs aws amazon comAmazonCloudWatchlatestmonitoringiamcwconditionkeysnamespace html So your policy statement might look something like"
69098236,69098235,"stackoverflow.com",0,"2022-02-10 17:21:02+02","2024-05-17 05:27:37.301729+03","Using some different key words I retrieved a second online search result quoting The plugin does not support working with the role property You have the following in your provider sections role arnfordeploymentrole Try removing this More info here This basically solves the issue and serverlessiamrolesperfunction worked after commenting out this The reason for this could be that previos versions serverless v2 24 0 used a different syntax than current ones Compare"
72126537,69098235,"stackoverflow.com",0,"2022-05-05 14:38:04+03","2024-05-17 05:27:37.303729+03","If you are using Bref framework make sure to update bref in composer and serverless globally to the latest version"
69176781,69103528,"stackoverflow.com",0,"2021-09-14 14:17:50+03","2024-05-17 05:27:38.351012+03","Answer found"
69112392,69112114,"stackoverflow.com",1,"2021-09-09 07:51:04+03","2024-05-17 05:27:39.262049+03","I will take approach of setting everything using awscli hence aws cli installation is imp "
69160524,69112375,"stackoverflow.com",0,"2021-09-13 12:41:06+03","2024-05-17 05:27:39.917963+03","The dotenv File is choosen based on your stage property configuration You need to explicitly define the stage property in your serverless yaml or set it within your deployment command This will use the env dev file Or you set the stage property via deploy command This will use the env prod file"
69113384,69112375,"stackoverflow.com",0,"2021-09-09 09:42:10+03","2024-05-17 05:27:39.919963+03","In your serverless yml you need to define the stage property inside the provider object Example"
75550264,69112375,"stackoverflow.com",0,"2023-02-23 22:58:30+02","2024-05-17 05:27:39.920964+03","As Feb 2023 I am going to attempt to give my solution I am using the Nx tootling for monorepo this should not matter but just in case and I am using the serverless ts instead I see the purpose of this to be to enhance the developer experience in the sense that it is nice to just nx run usersserve stagetest in my case using Nx or sls offline stagetest and serverless to be able to load the appropriate variables for that specific environment Some people went the route of using several env stage per environment I tried to go this route but because I am not that good of a developer I could not make it work The approach that worked for the was to concatenate variable names inside the serverless ts Let me explain I am using just one env file instead but changing variable names based on the stage The magic is happening in the serverless ts When one is utilizing the useDotenv true serverless loads your variables from the env and puts them in the env variable so you can access them envSTAGE Now I can access the variable with dynamic stage like so envDB_PORT_ selfprovider stage If you look at the env file each variable has the _stage at the end In this way I can retrieve dynamically each value I am still figuring it out since I do not want to have the word production in my url but still get the values dynamically and since I am concatenating this value envDB_PORT_ selfprovider stage then the actual variable becomes DB_PORT_ instead of DB_PORT "
69132450,69118164,"stackoverflow.com",1,"2021-09-10 15:47:20+03","2024-05-17 05:27:40.642336+03","Changed to And it fixed the problem"
69259509,69122943,"stackoverflow.com",0,"2021-09-20 21:53:24+03","2024-05-17 05:27:41.5315+03","I would keep only 1 serverless yml inside project Then have 4 functions inside serverless yml with handlers pointing to corresponding handler py this way you will have 4 lambdas This way you may use serverlessoffline with no problem and still have 4 microservices "
69127353,69127263,"stackoverflow.com",2,"2021-09-10 07:40:45+03","2024-05-17 05:27:42.208162+03","the closest we have in Serverless Framework is the serverless package command it will build and save all of the deployment artifacts in the serverless directory httpswww serverless comframeworkdocsprovidersawsguidepackaging you can inspect the CloudFormation and anything else e g code transpilation plugins etc generated by Serverless Framework in that folder"
69153358,69153336,"stackoverflow.com",1,"2021-09-12 19:34:09+03","2024-05-17 05:27:43.288296+03","In the subnets section the second is not a It is a different character Try to delete it and replace it with an normal hyphen The error occurs because it cannot parse the file as valid yaml due to that character "
69166014,69166004,"stackoverflow.com",0,"2021-09-13 19:16:12+03","2024-05-17 05:27:44.139471+03","andymac4182 has the Idea for the solution for your problem and it is actually pretty easy You have to define a method for each http element and for the last one this is missing method post Updated"
69167031,69166004,"stackoverflow.com",0,"2021-09-13 20:45:42+03","2024-05-17 05:27:44.140473+03","Serverless is a javascript project which is why you are seeing a javascript error It appears one of your functions is an HTTP endpoint but lacks any methods That seems to be the cause of the error Try adding method DELETE to your endpoint"
69208979,69208763,"stackoverflow.com",1,"2021-09-16 16:03:58+03","2024-05-17 05:27:47.288481+03","AWS has a limit on policy size Check this article for reference httpsaws amazon compremiumsupportknowledgecenteriamincreasepolicysize Check this AWS blog httpsaws amazon comblogsinfrastructureandautomationhandlingcirculardependencyerrorsinawscloudformation"
69210869,69208763,"stackoverflow.com",0,"2021-09-16 18:03:24+03","2024-05-17 05:27:47.290481+03","AWS is setting limit on few of the resources like IAM S3 etc Resources should not exceed whatever the limit is set You can submit a request to AWS Support to increase the limit Before that you can go to service quota in AWS to know the limit for AWS resources Based on that you can take a call to submit a request to AWS or follow the above document to reduce the size "
69218627,69208763,"stackoverflow.com",1,"2021-09-17 09:21:50+03","2024-05-17 05:27:47.291481+03","first create IAM role in your aws account with full access to the service that u want then do following serverless yaml"
69230910,69220587,"stackoverflow.com",1,"2021-09-18 04:33:46+03","2024-05-17 05:27:47.845208+03","I want the serverless framework to check if these resources exists at the region I choose This is not how Infrastructure as a Code IaC works CloudFormation nor terraform for that matter have any build in tools to check if a resource exists or not The IaC perspective is if its in a template than only the given templatestack can manage that There is nothing in between like it may exist or not Having said that there are ways to rearchitect and go around that The most common ways are Since the bucket is common resource it should be deployed separately from the rest of your stacks and its name should be passed as an input to the dependant stacks Develop a custom resource in the form of a lambda function The function would use AWS SDK to check for the existence of your buckets and return that info to your stack for further use "
69223660,69222959,"stackoverflow.com",1,"2021-09-17 15:45:28+03","2024-05-17 05:27:48.607409+03","When you implement CICD this thing must be automated we normally use a trigger that trap the git change event of say CodeCommit and execute a lambda function The lambda function then scans the files for changes and creates new layer and update all the lambda functions that uses this layer to use latest version of the layer Sharing the code written in python can change and use as per your needs "
69228177,69226440,"stackoverflow.com",4,"2021-09-17 21:41:12+03","2024-05-17 05:27:49.506221+03","According to docs In your configuration file set this providerlevel option Reference httpswww serverless comframeworkdocsprovidersawsguidefunctionsversioningdeployedfunctions By default the framework creates function versions for every deploy This behavior is optional and can be turned off "
76568871,69230278,"stackoverflow.com",0,"2023-06-28 00:52:35+03","2024-05-17 05:27:50.406009+03","Lambda now supports streaming via Lambda function urls This does not work with API GateWay httpsdocs aws amazon comlambdalatestdgconfigurationresponsestreaming html Sample"
77538624,69241370,"stackoverflow.com",0,"2023-11-23 19:03:51+02","2024-05-17 05:27:51.376921+03","Try capitalising the first letter of updateMetric "
69283690,69274047,"stackoverflow.com",0,"2021-09-22 14:53:55+03","2024-05-17 05:27:53.66934+03","Try using slsnextserverlesscomponent plugin httpswww npmjs compackageslsnextserverlesscomponentactiveTabversions It will do it out of the box "
69275336,69274991,"stackoverflow.com",1,"2021-09-21 23:47:24+03","2024-05-17 05:27:54.599746+03","As far as my knowledge goes each lambda is run in its own firecracker VM so even though the code is in the same zipfile lambda execution environments might not even be on the same VM AWS does some optimization depending on the nature of the workload So answer would be no not possible since AWS treats each lambda and lambda version in a completely isolated environment From the docs Execution environments are isolated from one another using several containerlike technologies built into the Linux kernel along with AWS proprietary isolation technologies They also have their own copy of the code and own tmp directory so nothing is shared really httpsdocs aws amazon comwhitepaperslatestsecurityoverviewawslambdalambdaisolationtechnologies html Best you can do is probably check why it takes so long to set up the connection in the first place 2 secs is more than average Sometimes simply increasing the lambda size to 1GB also drastically reduces start up times although it might not work for your case of course "
69287573,69274991,"stackoverflow.com",1,"2021-09-22 18:55:22+03","2024-05-17 05:27:54.602746+03","You would not be able to share network connections because discrete functions run in separate firecracker VM containers so those invocations and thus network connections may not be in the same underlying hosts You have got two options here The monolambda API pattern has many advantages as well as disadvantages you can read more about that here"
69292567,69291364,"stackoverflow.com",2,"2021-09-23 03:18:52+03","2024-05-17 05:27:55.125279+03","You can output execution ARN like Execution Execution Id doc Below is a simple demo with a Lambda function Nodejs Lambda function with name HelloFunction only for outputting to CloudWatch log Step function just put Execution Id to HelloFunction lambda Output will be like"
69355897,69304181,"stackoverflow.com",5,"2021-09-28 07:41:05+03","2024-05-17 05:27:56.114983+03","You just simply move them into the provider section They will be applied to all functions in the same service "
69304307,69304181,"stackoverflow.com",1,"2021-09-23 20:11:49+03","2024-05-17 05:27:56.115984+03","Use the Globals section in the SAM template For more details please go through this document httpsdocs aws amazon comserverlessapplicationmodellatestdeveloperguidesamspecificationtemplateanatomyglobals html"
69332100,69304467,"stackoverflow.com",6,"2021-09-26 08:24:02+03","2024-05-17 05:27:56.995521+03","You can do this See the source "
69305906,69305724,"stackoverflow.com",0,"2021-09-23 22:26:58+03","2024-05-17 05:27:57.947265+03","If I understand your problem correctly you are looking for a way to utilize external Python modules from your project within your Nuclio function If so the solution is less Nucliocentric but rather Dockercentric You need to make sure that your Python modules are available within the Docker environment There are two ways to do this Bake your code into your Docker image easiest and recommended From there you should be able to use it within your Nuclio function There is a page in the Nuclio docs on deploying a function from a Dockerfile httpsnuclio iodocslatesttasksdeployfunctionsfromdockerfile This approach is simpler as everything is in your Docker image However if your code changes you will need to rebuild your image can be automated with CICD pipeline Mount a directory with your Python modules to your container with a K8s PVC or Docker volume add to Python path and use as expected This approach is more complex and depends on your K8s environment However since the code is just being mounted within the container you do not need to rebuild your image if your code changes "
69396689,69314258,"stackoverflow.com",1,"2021-09-30 20:41:39+03","2024-05-17 05:27:59.38749+03","So you need this minimal setup"
69352132,69317152,"stackoverflow.com",1,"2021-09-27 22:15:27+03","2024-05-17 05:27:59.79288+03","False Alarm The directory WORKSPACE is generated because of Jenkins run and not because of the serverless framework The frame although was picking it up while packing and deploying the application thus making the lambda function bulk Excluding it as follows did the trick Maybe something good to know for people using serverless cli on Jenkins "
69544687,69542211,"stackoverflow.com",1,"2021-10-12 20:28:39+03","2024-05-17 05:28:10.467335+03","If you want to package each function individually you will need two things one of which you have already done Packaging individually is configurable globally or at a perfunction level so you can choose what is best for you You can find more information in the documentation"
69417316,69326889,"stackoverflow.com",1,"2023-03-02 17:02:59+02","2024-05-17 05:28:00.362871+03","As every other process that you want to debug you need to run it and somehow connect the debugger to it You need to remember that Serverless Framework is written in JSTS so it runs in Node js So you can debug it quite easily if you are developing your Lambdas in Node js as it is quite common environment 4 Make sure you have chosen the correct package json In my example I am using package script from package json but it could be also any other script that triggers serverless deploy or serverless print in the end And that is it Breakpoints should be triggered normally like when you debug your own JS code "
69417216,69360533,"stackoverflow.com",3,"2021-10-02 16:10:31+03","2024-05-17 05:28:02.319758+03","I would recommend simply using dotenv feature which is described in the documentation httpswww serverless comframeworkdocsenvironmentvariables Files structure Files content env dev env prod serverless yml Then you can easily use it from custom section as a normal Serverless Framework variable "
70424782,69363095,"stackoverflow.com",1,"2021-12-20 18:14:52+02","2024-05-17 05:28:03.25255+03","I ran into this same problem In the serverless yml I changed service that I had it as lambda_function and put it as lambdaFunction The error was solved and it deployed correctly "
69654673,69363095,"stackoverflow.com",0,"2021-10-21 18:16:51+03","2024-05-17 05:28:03.254551+03","Most likely your stage name contains an illegal character Serverless autogenerates a name for your s3 bucket based on your stage name If you look at the generated template file you will see the full export which will look something like the following The way around this assuming you do not want to change your stage name is to explicitly set the output by adding something like this to your serverless config in this case the illegal character was the underscore Unfortunately this has to be done for every export It is a better option to update your stage name to not include illegal characters"
69371062,69363354,"stackoverflow.com",1,"2021-09-29 08:53:23+03","2024-05-17 05:28:04.265872+03","Logs are by Invocation of the lambda and log group links are by concurrent executions If you look at your lambda metrics you will see a stat called ConcurrentExecution this is the total number of simultaneous serverless lambda containers you have running at any given moment but that does NOT equal the same as Invocations The headless project im on is doing about 5k invocations an hour and we have never been above 5 concurrent executions of any of our 25ish lambdas helps that they all run after start up at about 300ms So if you have 100 invocations in 10 seconds but they all take less than a second to run once a given lambda container is spun up it will be reused as long as it is continually receiving events This is how AWS works around the cold start problem as much as possible where a given lambda may take 1015 or more seconds to start up By trying to predict traffic flow and you can manipulate these settings as well AWS is attempting to have a warm lambda ready to go for you whenever you need it These concurrent executions are slowly shut down as their volume drops off their calls brought back in to other ones that are still active What this means for Log Group logs is two fold you may see large gaps in the times but if you look closely any given log group will have multiple invocations in it log groups are delayed by several seconds to several minutes depending on the server load so at any given time you may not actually be seeing all the logs of a given moment The other possibility is that you logging is not set up correctly Python lambdas in particular have difficulty in logging properly to cloudwatch the default Logging Handler does not play nice with the way lambda boots up a handler to attach it to the logGroup or what you are getting is a ton of hits that are not actually doing anything only pingskeep alive events that do not actually trigger any of your log statement at which you will generally only see the concurrent start upshutdown log statements as stated above they are far fewer "
69371182,69363354,"stackoverflow.com",1,"2021-09-29 09:07:13+03","2024-05-17 05:28:04.26899+03","What do you mean with gaps in log groups A log group gets its log by log streams and one of the same lambda container use the same log stream So it may not be the most recent log stream in your log group that have the latest log entry Here you can read more about it httpsdashbird iobloghowtosavehundredshoursdebugginglambda"
69379124,69363354,"stackoverflow.com",0,"2021-09-29 20:11:15+03","2024-05-17 05:28:04.270993+03","While trying to edit my question with screenshots and tallies of the data I came upon the answer I thought it would be helpful for this to be a separate answer as it is extremely specific and enlightening The crux of the problem is that I did not expect such huge gaps between invocation times and log write times 12 minutes is an eternity compared to the work I have done in the past Consider this graph 1259 UTC should be 759AM CST Counting the invocations between 1259 and 1308 I get roughly 110 Cloudwatch shows these log streams Looking at these log streams there seems to be a large gap The timestamp on the log stream is the file close time The logstream for 80837 includes events from 12 minutes before So the timestamps on the log streams are not very useful for finding debug data The search all has not been very helpful up until now either Slow and very limited I will look into some other method for crunching logs "
69396755,69377835,"stackoverflow.com",0,"2021-09-30 20:46:50+03","2024-05-17 05:28:04.946754+03","The issue was a typo in my api js I was concentrating on the components when I should have been looking at how the api was interacting since all worked locally but not when deployed on ghpages "
69962579,69500335,"stackoverflow.com",2,"2021-11-14 13:35:20+02","2024-05-17 05:28:05.659667+03","My fix was to uninstall serverless and install again with a different version of Node Look for npm warnings when you install serverless For example The following version worked for me node v15 0 1 You can download and install specific versions of Node with Node Version Manager Alternatively try a different install method httpsserverless cominstall"
69565784,69527970,"stackoverflow.com",0,"2021-10-14 09:06:42+03","2024-05-17 05:28:07.643656+03","I would say there is 2 option how to do it at least what I tried and aware of "
69540375,69538195,"stackoverflow.com",2,"2021-10-12 15:15:18+03","2024-05-17 05:28:08.484285+03","Forgot to add to the end of the resource"
69538980,69538195,"stackoverflow.com",2,"2021-10-12 15:15:47+03","2024-05-17 05:28:08.485285+03","Your 403 Access Denied error is masking a 404 Not Found error as your code Serverless config looks perfectly fine should work as expected provided you have specified the resource correctly If you do not have correct s3ListBucket permissions the S3 endpoint will not return a 404 Not Found error if the object does not exist for the specified key GetObject s API reference highlights this nuance If you have the s3ListBucket permission on the bucket Amazon S3 will return an HTTP status code 404 no such key error If you dont have the s3ListBucket permission Amazon S3 will return an HTTP status code 403 access denied error This is to prevent attackers from enumerating public buckets knowing what objects actually exist in the bucket The absence of a 404 in this case is not allowing information to be leaked on if the object exists or not just like an Invalid Credentials message on a login page as opposed to Invalid Password which indicates a user with the provided username exists Provide the Lambda with permission to carry out the s3ListBucket action to unmask the 404 error andor ultimately doublecheck your GetObjectRequest to make sure the key is being specified correctly for an object that does exist"
69546044,69540580,"stackoverflow.com",2,"2021-10-12 22:29:53+03","2024-05-17 05:28:09.551195+03","4kb for all environment variables is still the limit httpsaws amazon compremiumsupportknowledgecenterlambdaenvironmentvariablesize"
76004809,69540580,"stackoverflow.com",1,"2023-04-13 14:18:49+03","2024-05-17 05:28:09.55219+03","As per the official document limit is 4 KB But there is a catch here you can send data even more than 4 KB let me tell you how I have achieved this I used Zlib httpsdocs python org3libraryzlib html library which compresses the string into a very short size Then again inside the lambda I have decompressed the string This is how I have sent almost 512 KB of data into lambda environment variables "
75324004,69540580,"stackoverflow.com",0,"2023-02-02 15:38:33+02","2024-05-17 05:28:09.55419+03","The documentation specifies a maximum combined size of 4 kb for environment variables However from my experience the real limit is 5 kb or 5120 bytes "
69679500,69591115,"stackoverflow.com",2,"2021-10-22 18:30:41+03","2024-05-17 05:28:12.520165+03","yes you can use already existing roles instead In order to do that you need to specify it e g this way Role specified this way will be used by all your functions You can read more about it here httpswww serverless comframeworkdocsprovidersawsguideiam"
69679478,69591115,"stackoverflow.com",1,"2021-10-22 18:29:14+03","2024-05-17 05:28:12.521658+03","Of course you can Just set role ARN while declaring your function in the serrveless yml file"
69670284,69614217,"stackoverflow.com",0,"2021-10-22 03:55:44+03","2024-05-17 05:28:13.353245+03","my serverless yml file look like this and working Please make sure you have defined YYYYLambdaLayer it exists or we will share the relevant part of your yaml file "
69715392,69707599,"stackoverflow.com",1,"2021-10-26 01:18:58+03","2024-05-17 05:28:14.532697+03","Here is what I what do You have references in your template to the DynamoDB table that will fail if you simply delete the DynamoDB table resource from your template so I would"
69730263,69723242,"stackoverflow.com",2,"2021-10-29 14:35:43+03","2024-05-17 05:28:15.059429+03","Try send email to Amazon tech support Your account may be suspected you need approve billing info for example This was helpful in my case"
69732977,69724457,"stackoverflow.com",0,"2021-10-27 07:59:00+03","2024-05-17 05:28:15.912161+03","Network issues can happen due to various reasons In your case what you can try doing is to reduce the limit e g limit 30 and set your client library to retry the connection again by setting maxNetworkRetries 3 or number that fits your application When this is set Stripe will retry the connection when the timeout error occurs "
69733289,69724457,"stackoverflow.com",0,"2021-10-27 08:37:19+03","2024-05-17 05:28:15.914042+03","This is a perfect match for Step functions use cases It will allow you to orchestrate the steps of getting the invoices and processing them and easily design a retry mechanism in case of errors "
72029860,69724457,"stackoverflow.com",0,"2022-04-27 16:42:02+03","2024-05-17 05:28:15.915039+03","It is not really a solution but what is causing the issue is that Stripe take a very long time to return less than 100 results We found workaround in order to not fetch this list "
69731129,69730569,"stackoverflow.com",0,"2021-10-27 02:46:59+03","2024-05-17 05:28:16.500424+03","You can use Serverless Stages Set your stage to your name and your teammate can set the stage to his name Production and Dev can also be separate stages httpsserverlessstack comchaptersstagesinserverlessframework html"
69770033,69757417,"stackoverflow.com",1,"2021-10-29 16:46:25+03","2024-05-17 05:28:17.362406+03","There is definitely a deployed API Gateway You have deployed your stack in useast1 my best guess is that you are using the AWS console in another region When logged in to the AWS console use the region switcher on the topright to switch to useast1 from there you should see your API Gateway lambda function and cloudwatch logs "
77353886,69757417,"stackoverflow.com",0,"2023-10-24 19:32:26+03","2024-05-17 05:28:17.363406+03","If anyone is finding this question in 2023 I encountered this problem as well and found that the API gateway is not created if you only define the function just define the handler The API gateway will be created when you add the keys events http path method to your function definition "
69785128,69784746,"stackoverflow.com",1,"2021-10-31 10:29:48+02","2024-05-17 05:28:18.425785+03","In order to have a step function being invoked synchronously you might need to use a Step Function with an Express workflow On the API Gateway side you need an Integration Request with the Action set to StartSyncExecution This answer goes indepth how to setup the integration between the API Gateway and the Express Step Function source"
69787242,69787127,"stackoverflow.com",7,"2021-10-31 15:14:08+02","2024-05-17 05:28:19.339779+03","If you check for the serverless frameworks releases its listed under 2 61 0 So you need to update the dependency version to get support for it in serverless framework"
69802707,69800948,"stackoverflow.com",2,"2021-11-01 22:59:08+02","2024-05-17 05:28:20.898087+03","Serverless Cloud is a SaaS product that Serverless Inc created and owns It is in public beta right now unclear about pricing It helps you build deploy and run Serverless applications without worrying about AWS Serverless Framework is an open source project that Serverless Inc also maintains It is free to use but requires your own AWS account to run You are looking at two different products from the same company one is free and open source the other is not That is the difference It is totally possible to use Serverless Framework without using Serverless Cloud disclosure I previously worked at Serverless Inc and have worked on both things "
69834302,69806386,"stackoverflow.com",0,"2021-11-04 05:41:16+02","2024-05-17 05:28:21.574951+03","After checking on their slack I figured out that ESBuild is only used in building the backend SST Functions But in the materialui examples there is an example on how to do the aliasing for createreactapp httpsgithub commuiorgmaterialuitreenextexamplescreatereactappwithstyledcomponentstypescript "
69926384,69810751,"stackoverflow.com",5,"2021-11-11 12:48:36+02","2024-05-17 05:28:22.31191+03","As indicated in the different comments of your question there are several things that can motivate your problem Please be sure that you are providing the necessary configuration about the different content types that should be considered binary The AWS documentation provides great detail about it this related SO question can be valuable as well Due to the fact you are using the serverless framework as indicated in the links 1 2 you cited please provide the necessary configuration there as well In any way it seems that even with this configuration you are still facing the problem You told you were able to successfully upload text files but your images get corrupted increasing their size as indicated in the comments it seems a clear indication that in some place the information is being converted to a different encoding from binary to text something like that In fact according to your dependencies this seems to be the actual problem as reported in this issue of the serverlesshttp library and especially in this others 1 and 2 of the serverlessoffline library I think the issue is only local and that it will probably work without further problems in AWS In any way as you can see in first of the above mentioned issues the one related to serverlesshttp the library has the following code So as a workaround submitting your information as base 64 encoded can solve the issue it is not a straightforward task if you are using form submission in your HTML see for instance this great example for some ideas although it can do the trick if you interact directly with your API from code The only necessary change is in your params variable Please note the use of Buffer from base64 and the inclusion of ContentEncoding base64 In any way if the code works in AWS I think the way to go would be waiting for the serverlessoffline issues resolution "
69925324,69810751,"stackoverflow.com",3,"2021-11-11 10:38:14+02","2024-05-17 05:28:22.315911+03","I am not sure what the problem is but it seems like serverless offline is having trouble processing the data coming in as multipartformdata The easiest solution would be to encode the file as base64 and send the payload as applicationjson To encode the file as base64 use this website httpsbase64 guruconverterencodefile It is trivial to do this locally and programatically but the website is crossplatform and should be good enough for testing Request Payload productimage js s3Functions js Heres the codebase you provided on the other thread with the included changes I have not cleaned it up but just in case you run into any issues "
69920903,69810751,"stackoverflow.com",2,"2021-11-11 00:20:48+02","2024-05-17 05:28:22.317911+03","You need to send Body as a Buffer or ReadableStream so you need to read the request files image as a Buffer before send it to s3 api Also add content type to request headers I am not using your same stack but this is my working code CURL testing command S3 api call I am not specifying ContentType httpsdocs aws amazon comAWSJavaScriptSDKlatestAWSS3 html"
71730463,69810751,"stackoverflow.com",0,"2022-04-04 02:01:40+03","2024-05-17 05:28:22.31956+03","Hi I would suggest you to use binary option while uploading the pic instead of Multipart Then try downloading it That will solve your issue See postman example snapshot here If You want to use multipart for image upload on s3 then you will have to create presigned url for each part then only you can use put to upload your file Here is a reference article where you could find multipart implementation httpsdev totraindexmultipartuploadforlargefilesusingpresignedurlsaws4hg4"
69867746,69866414,"stackoverflow.com",1,"2021-11-06 22:41:19+02","2024-05-17 05:28:23.346862+03","Unless you have editedredacted TABLE_NAME in the error message my guess is that you are inadvertently attempting to write to a table which probably does not exist TABLE_NAME You have not posted your handler code but I would check your code and verify that your actual table name is being setinterpolated correctly before your handler code attempts to insert an item with the DynamoDB API "
69876905,69875775,"stackoverflow.com",0,"2021-11-08 00:08:24+02","2024-05-17 05:28:24.385102+03","I noticed I can reconstruct the endpoint in the lambda and grab the id like so and then reconstruct"
71563529,69875775,"stackoverflow.com",0,"2022-03-21 23:00:23+02","2024-05-17 05:28:24.385969+03","With a bit of cloudformation you can inject it directly That way you do not need to compute it everytime in your lambda handler this is an example for a websocket API "
73314434,69875775,"stackoverflow.com",0,"2022-08-11 04:46:16+03","2024-05-17 05:28:24.386966+03","Is your API created through the sameanother Cloudformation Stack If so you can reference is directly same stack or through a CloudFormation variable export httpscarova iosnippetsserverlessawscloudformationoutputstackvariables httpscarova iosnippetsserverlessawsreferenceothercloudformationstackvariables If you created it outside of CloudFormation ie in the aws console then you will need to add the ids into the template Most likely by creating different environment variables based on the stage "
69891827,69878886,"stackoverflow.com",2,"2021-11-09 03:40:07+02","2024-05-17 05:28:25.233023+03","There is a good article here that explains the steps you need In order for your Lambda to have access to your AWS resources it needs to be inside the same VPC and its execution role needs to have the appropriate permissions through IAM RolesGroups You also want to avoid having your RDS open to the world so you should be creating all of this inside a VPC You can attach your lambda function to the VPC then allow access to the RDS only to the VPC subnets via a security group That will get you steps 1 and 2 of your requirements In this same security group you can allow access to the external IP address of your computer to get step 3 You can configure this through the CLI so if you do not have a static IP it only takes a second to add PowerShell example below"
69945189,69917305,"stackoverflow.com",1,"2021-11-12 17:21:49+02","2024-05-17 05:28:26.89138+03","I found a fix that worked I had to update my cfn sqs yml to include permissions for S3 buckets to send events to the SQS queue as below As for my cfn s3 yml the correct way to reference the queue was I believe the problem was that AWS checks that the notification will be possible at deployment time rather than letting your service fail at runtime as explained in this answer A lot of AWS configuration allows you to connect services and they fail at runtime if they do not have permission however S3 notification configuration does check some destinations for access This would mean that since I had not configured my SQS queue to allow notifications from the S3 bucket AWS noticed this misconfiguration and stopped the deployment with an error "
69933888,69933340,"stackoverflow.com",2,"2021-11-11 21:30:27+02","2024-05-17 05:28:29.024491+03","this works"
69953013,69952799,"stackoverflow.com",1,"2021-11-13 11:27:07+02","2024-05-17 05:28:29.903871+03","Looks like adding externals all to custom bundle in serverless yml fixed the issue "
69989398,69962856,"stackoverflow.com",1,"2021-11-16 14:43:56+02","2024-05-17 05:28:30.87687+03","To Configure AWS API Gateway integration to step function you have to follow these steps"
69970121,69968460,"stackoverflow.com",7,"2021-11-15 08:18:22+02","2024-05-17 05:28:31.616708+03","By default NextJS parses the the request body based upon the incoming ContentType in the headers You would want to disable this [0] and then consume it as a stream using buffer The below code works for me [0] httpsnextjs orgdocsapiroutesapimiddlewarescustomconfig"
74926701,69968460,"stackoverflow.com",2,"2022-12-27 09:31:27+02","2024-05-17 05:28:31.617707+03","To solve this I disabled the NextJS body parser by exporting the config object in the same api route file Then I imported the rawbody package so I can convert the request stream into a raw buffer Finally I used the rawbody package via promise interface This worked for me Hope it can help you out "
76485368,69968460,"stackoverflow.com",1,"2023-06-15 23:39:14+03","2024-05-17 05:28:31.619283+03","I understand that this question may be old but for anyone who comes across it in the future I want to share that I have successfully integrated Stripe into my project using webhooks and the new app router I have documented the entire stepbystep tutorial in the README md file Feel free to check out the repository at the following link httpsgithub comBastidaNicolasnextauthprismastripe"
77109219,69968460,"stackoverflow.com",0,"2023-09-15 04:56:17+03","2024-05-17 05:28:31.621286+03","I gotchu bro doing this for an hour and finally got it to work just use this for your endpoint and it will log the event"
77708371,69968460,"stackoverflow.com",0,"2023-12-23 18:54:02+02","2024-05-17 05:28:31.622281+03","If you are using NextJS 14 you can also do"
70019734,69983401,"stackoverflow.com",1,"2021-11-18 14:19:39+02","2024-05-17 05:28:32.602059+03","I believe it is possible to address that by setting NODE_EXTRA_CA_CERTS at least some users in the past were successful with that approach httpsgithub comserverlessserverlessissues9548issuecomment857882498"
70016561,70013886,"stackoverflow.com",7,"2021-11-18 10:19:37+02","2024-05-17 05:28:33.641775+03","There is documentation at serverless com that describes this All you do is add the cron schedule to the EventBridge event as if it was a schedule event For example You can find the documentation for EventBridge here httpswww serverless comframeworkdocsprovidersawseventseventbridge And for the Schedule event with an example of a cron schedule here httpswww serverless comframeworkdocsprovidersawseventsschedule"
72954767,70013886,"stackoverflow.com",1,"2022-07-12 18:24:52+03","2024-05-17 05:28:33.643776+03","In addition to Gareth McCumskey answer If you are planing to hook an existing event bus you could simply pass it is arn to eventBus key Here is an example from serverless docs Another option is to use CloudFormations intrinsic functions "
70018281,70018020,"stackoverflow.com",5,"2021-11-18 12:33:42+02","2024-05-17 05:28:34.2784+03","you can fix it by setting the name of the service directly to service property like this"
70357398,70018020,"stackoverflow.com",0,"2021-12-15 03:09:56+02","2024-05-17 05:28:34.2794+03","httpsstackoverflow coma7001828116620939 This seems to result in the following error When setting"
70494432,70018020,"stackoverflow.com",0,"2021-12-27 12:42:30+02","2024-05-17 05:28:34.2804+03","Took me a while but if you are running a recent version of serverless instead of using selfservice name use selfservice to reference the name of the service "
70368356,70348461,"stackoverflow.com",1,"2021-12-15 20:03:31+02","2024-05-17 05:28:36.436083+03","Figured it out Nothing wrong with plugin Though I added lambda function handler for SQS events I forgot to include the function in serverless ts under functions resource Including it in functions autoCreated the queue as expected "
70383802,70383416,"stackoverflow.com",0,"2021-12-16 20:15:48+02","2024-05-17 05:28:38.258222+03","Serverless framework does not support conditional statements and properties to resources but you can try and use this ifelse plugin "
70392978,70391775,"stackoverflow.com",2,"2021-12-17 14:12:00+02","2024-05-17 05:28:39.404684+03","I found the solution while browsing the serverless npm package source code"
70393828,70392701,"stackoverflow.com",1,"2021-12-17 15:19:48+02","2024-05-17 05:28:39.702383+03","I think that for your use case you might be better of with considering using JSTS configuration file format instead of YAML That allows you to use regular JS to define your config which makes importing such parts of configurations much easier See the TS template for example on how to use it httpsgithub comserverlessserverlessblobmasterlibpluginscreatetemplatesawsnodejstypescriptserverless ts"
70405374,70394323,"stackoverflow.com",1,"2021-12-18 19:01:18+02","2024-05-17 05:28:40.925108+03","Aarons tip really helped Turns out that I had these two functions In one of them I was using the id as parameter and in the other I was using the email They should be both id or both email You can only use different parameters for different paths So it should look like this Thank you so much Aaron it is working now "
70397308,70394323,"stackoverflow.com",0,"2021-12-17 20:18:17+02","2024-05-17 05:28:40.927109+03","Without seeing your function configuration or understanding what you are trying to do this is hard to debug However I can see this error commonly occurs if you are moving an API Gateway Path Variable IE to Unfortunately this is a limitation in CloudFormation as it creates the new resource before removing the old one which causes this issue source and thus this is still an unsolved issue You will need to create a new function and temporarily route requests to that function then remove the old route then add the new route and finally route requests to the old function "
70422237,70409022,"stackoverflow.com",0,"2021-12-20 15:02:57+02","2024-05-17 05:28:41.376166+03","it is a little bit hard without seeing the example files that you are trying to import but by just looking at your configuration it seems like you are mixing array notation and object notation in yaml configuration I believe the Outputs should also be an item of an array so something like this"
70417054,70409022,"stackoverflow.com",0,"2021-12-20 05:12:16+02","2024-05-17 05:28:41.377167+03","Actually I was looking for the mixed approach of configuring the resources section I have found the guidelines here under Multiple Configuration Files section "
74738969,70409022,"stackoverflow.com",0,"2022-12-09 06:39:16+02","2024-05-17 05:28:41.379167+03","I have a similar issue but I did not face an issue as cited in the examples above With envConfig yml containing The value of stage prod passed in from another envSuffix yml Worked for me However my lambda developed in java uses log4j MDC and so I wish to pass in environment variable called LOGGIN_PATTERN which as a value of LOG_PATTERN X AWSRequestId TxnX transaction id 5p c 1 L mn and I am getting the mentioned error because of the AWSRequestId thanks"
71510951,70415690,"stackoverflow.com",1,"2022-03-17 12:42:17+02","2024-05-17 05:28:42.286351+03","Solved it by setting PYTHONPATH as environment variable pointing to the local packages folder "
70432512,70432261,"stackoverflow.com",2,"2021-12-21 10:30:23+02","2024-05-17 05:28:43.044545+03","This a new approach to get data from another file"
70433219,70433027,"stackoverflow.com",1,"2021-12-21 11:31:21+02","2024-05-17 05:28:43.77066+03","You can usually infer the the entire structure of ARNs inside AWS For example this is the ARN of a DynamoDB table in my AWS account the Xs are my account ID arnawsdynamodbuseast1XXXXXXXXXXXtableawesomemytabledev These ARNs are the same structure every time arnaws[service name][region name or blank][accountID][entity within the service][name of entity] Try manually creating a certificate then seeing how the ARN is structured then just build the ARN you need to match the same structure Recent versions of the Serverless Framework even allow you to add in your AWS information as variables using awsregion or awsaccountId "
71316117,70457231,"stackoverflow.com",0,"2022-03-02 02:25:21+02","2024-05-17 05:28:45.848715+03","I am getting same error Got it working though by creating bucket before the publish command"
70461672,70460275,"stackoverflow.com",1,"2021-12-23 13:40:57+02","2024-05-17 05:28:46.933126+03","I believe such approach might be more effective in your case It feels at least in my opinion a bit more cleaner to separate whole configurations of VPC and just resolve it based on stage "
73420582,70466347,"stackoverflow.com",9,"2022-08-27 03:50:54+03","2024-05-17 05:28:47.682277+03","Yesterday I had the same error Well the error comes from serverlesss version I was using In my local environment I had the version 2 3 and it works fine with the flag true after my secret_name ID but in my CD I had version 3 and that was generating the error After reading the docs I searched Note The method described below works by default in Serverless v3 but it requires the variablesResolutionMode 20210326 option in v2 So if you are using v3 use or instead"
70506845,70469922,"stackoverflow.com",1,"2021-12-29 10:40:28+02","2024-05-17 05:28:48.829858+03","Could be because cloud formation is trying to create your lambda before your dynamodb table To fix this add dependsOn to your lambda function e g This will deploy the function once the the EntityTable has been deployed GetAtt EntityTable StreamArn should then resolve correctly "
74725654,70486773,"stackoverflow.com",5,"2022-12-08 06:50:25+02","2024-05-17 05:28:50.456237+03","Making Changes as follows works Basically we need pass async true promisify true to the annotation onEvent "
70527439,70527438,"stackoverflow.com",3,"2021-12-30 06:19:28+02","2024-05-17 05:28:50.673999+03","I was applying the PutObject and PutObjectAcl permissions to the galleryBucket instead of the objects within it You must apply the PutObject permission to the objects inside the bucket not the bucket itself I updated my permissions to this and the PUT requests succeeded Note the at the end of the resource identifier"
73311848,70543586,"stackoverflow.com",0,"2022-08-10 22:38:27+03","2024-05-17 05:28:51.801774+03","I had this problem when I did not configure the Environment variables that hold my credentials Have you checked that yet"
70552858,70552783,"stackoverflow.com",0,"2022-01-02 02:30:37+02","2024-05-17 05:28:52.66913+03","If you are already using Cognito you can secure your API Gateway method the specific REST API method with a Cognito User Pool That means you would have the following flow App HTTP request with Cognito Authorization Header API Gateway API Gateway method with Authorization setup Lambda S3 API Gateways Authorization settings would take care of securing the endpoint This is the guide for setting it up httpsdocs aws amazon comapigatewaylatestdeveloperguideapigatewayintegratewithcognito html"
70574047,70557361,"stackoverflow.com",0,"2022-02-11 00:40:12+02","2024-05-17 05:28:53.331819+03","You could try the GitHub Action jakejarviss3syncaction which uses the vanilla AWS CLI to sync a directory either from your repository or generated during your workflow with a remote S3 bucket It is based on aws s3 sync which should enable an incremental backup instead of copyingmodifying every files Add as source_dir the migration folder However taseenb comments This does not work as intended like an incremental backup S3 sync cli command will copy all files every time when run inside a GitHub Action I believe this happens when we clone the repository inside a Docker image to execute the operation this is what jakejarviss3syncaction does I do not think there is a perfect solution using S3 sync But if you are sure that your files always change size you can use sizeonly in the args It will ignore files with the same size so probably not safe in most cases "
70605838,70605017,"stackoverflow.com",1,"2022-01-06 12:37:23+02","2024-05-17 05:28:54.294342+03","In your AWSRDSDBCluster it should be"
70607347,70605017,"stackoverflow.com",1,"2022-01-06 14:48:23+02","2024-05-17 05:28:54.295727+03","In your example you are passing DBSubnetGroupName as an array not a string In order to pass it as a string you should use this notation"
70615907,70615788,"stackoverflow.com",2,"2022-01-07 04:25:01+02","2024-05-17 05:28:55.82764+03","Just provide one value Something like this untested code "
70616219,70615788,"stackoverflow.com",1,"2022-01-07 08:44:15+02","2024-05-17 05:28:55.82864+03","This worked as I wanted Thanks "
70629463,70622311,"stackoverflow.com",1,"2022-01-08 05:45:04+02","2024-05-17 05:28:57.016103+03","You need to use SET As the docs show here httpsdocs aws amazon comamazondynamodblatestdeveloperguideExpressions UpdateExpressions htmlExpressions UpdateExpressions SET AddingListElements The docs for ADD say The ADD action supports only number and set data types httpsdocs aws amazon comamazondynamodblatestdeveloperguideExpressions UpdateExpressions htmlExpressions UpdateExpressions ADD"
71146228,70634729,"stackoverflow.com",2,"2022-02-16 19:07:13+02","2024-05-17 05:28:58.052296+03","Heres how to add a user with roles and permissions set up Setting up roles and permissions is important since usually you do not create a user with AccessSecret key in a vacuum the user needs to have some accesses Notes File serverlesswithmyuser yml And heres how to access this users access key and secret key in other parts of serverless resources e g lambda File serverlesswithmyservice yml If user key and service are in a single file there is no need to use output variables and you can reference the values directly "
70664620,70634729,"stackoverflow.com",0,"2022-01-15 01:35:11+02","2024-05-17 05:28:58.054075+03","you can do that by writing raw CloudFormation in resources section httpswww serverless comframeworkdocsprovidersawsguideresources However one note it is not recommended to pass AWS Access Keys directly to Lambda it is better to use IAM Role that is assigned to Lambda to grant it proper permissions EDIT Example on how to access it I want to reiterate once again that it is a really bad idea to do something like the above you should rely on temporary credentials from assumed role instead "
70636068,70635905,"stackoverflow.com",1,"2022-01-08 22:15:41+02","2024-05-17 05:28:59.092095+03","The maximum request payload size for API Gateway is 10MB If the payload is larger than that API Gateway directly returns the error message you are seeing without invoking the integration AWS Lambda in your case You can find more information about this and other quotas in the documentation httpsdocs aws amazon comapigatewaylatestdeveloperguidelimits html"
78479973,70635905,"stackoverflow.com",0,"2024-05-14 21:13:07+03","2024-05-17 05:28:59.094089+03","Go to Gateway responses for the API Gateway Edit Default RXX and set AccessControlAllowOrigin to a specific value or Save and then deploy the API "
70711862,70636476,"stackoverflow.com",1,"2022-01-14 16:08:59+02","2024-05-17 05:29:00.063883+03","On your yml file you can declare the table name as opttablenameDEFAULT This line means that you are going to give the name as a parameter from the terminal command like this serverless deploy tablename NAME_OF_THE_TABLE if you do not give it as a parameter it takes the default name you can give it This can generate a name on the fly "
70656418,70655867,"stackoverflow.com",3,"2022-01-11 11:44:18+02","2024-05-17 05:29:00.949733+03","Iam running into a problem where the saveObject function of Algolia does not seem to call or it returns before finishing what it is doing That is the issue You are not waiting for the saveObject function to finish return the result To do this you need to await for your async functions return the concept is called asyncawait So try changing this line to This should fix the issue Furthermore you are using an async handler with a callback That might also break something You either use an async handler OR a callback not both Try the following handler signature Relevant documentation httpsdocs aws amazon comlambdalatestdgnodejshandler html"
70663654,70655867,"stackoverflow.com",2,"2022-01-11 10:23:06+02","2024-05-17 05:29:00.951729+03","using a try catch block in the updateIndex function combined with Jens s answer solved the issue for me search js"
70841388,70660038,"stackoverflow.com",0,"2022-01-25 00:44:42+02","2024-05-17 05:29:01.890483+03","During serverless deploy it can get config information from awscredentials file There is an explanation about it in the document httpswww serverless comframeworkdocsprovidersawsguidecredentials"
70698652,70697201,"stackoverflow.com",0,"2022-01-13 16:56:51+02","2024-05-17 05:29:02.850281+03","It should be possible in theory but it is not an easy thing to do There are products like LocalStack that offer exactly this But I would not recommend going that route Ultimately by design this will always be a huge cat and mouse game AWS introduces a new feature or changes some minor detail of their implementation and products like LocalStack need to catch up Furthermore you will always only get an approximation of the actual cloud It never will not be a 100 match I would think there is a lot of work involved to get products like LocalStack working properly with your setup and have it running well Therefore I would propose to invest the same time into proper developer experience within the actual cloud That is what we do every developer deploys their version of the project to AWS This is also not trivial but the end result is not a fake version of the cloud that might or might not reflect the real cloud The key to achieve this is Infrastructure as code and as much automation as possible We use Terraform and Makefiles which works very well for us If done properly we only ever build and deploy what we changed The result is that changes can be deployed in seconds to AWS and the developer can test the result either through the Makefile itself or using the AWS console And another upside of this is that in theory you need to do all the same work anyway for your continuous deployment so ultimately you are reducing work by not having to maintain local deployments and cloud deployments "
70713153,70711552,"stackoverflow.com",2,"2022-01-14 17:46:47+02","2024-05-17 05:29:03.683293+03","it is possible to some extent The first way is using invoke local command to invoke individual functions The other way is to use serverlessoffline plugin httpsgithub comdheraultserverlessoffline"
70713853,70711552,"stackoverflow.com",0,"2022-01-14 18:43:05+02","2024-05-17 05:29:03.685293+03","Running Lambda locally First you need to be able to run Lambda locally Enable PyCharm to run bash scripts Now we want to run pythonlambdalocal from PyCharm rather than the terminal Create a bash script Edit Run Configurations in PyCharm Enjoy "
70717540,70717414,"stackoverflow.com",0,"2022-01-15 01:34:43+02","2024-05-17 05:29:04.411431+03","This was a huge pain to figure out Resources should not be an array in this case In addition the iam role statement should not have a resources toplevel item in the yml either"
70804890,70726176,"stackoverflow.com",0,"2022-01-21 18:57:31+02","2024-05-17 05:29:05.356018+03","Here is a syntax example for only allowing access to users from one specific Arn while allowing everyone to call subscribe You need to add a resourcePolicy to the provider apiGateway section specify the allowed Arn under Principal and allow invocation of the specific API pathresource You also want to add an authorizer of type aws_iam to the path in question "
70877821,70877441,"stackoverflow.com",0,"2022-01-27 13:29:55+02","2024-05-17 05:29:06.78456+03","Its a very bad idea to disable OAI as it allows you to make sure that your static assets are only accessible via CloudFront This maybe the reason why this is not doable in serverless framework Do try AWS CDK or CloudFormation if you really want to proceed with this approach "
70889839,70886809,"stackoverflow.com",0,"2022-02-02 20:53:56+02","2024-05-17 05:29:07.081704+03","the issue turns out to be this plugin messing it up after removing it all the packaging directives work fine Removing the usage of serverlesspluginincludedependencies and everything works fine "
71492510,70886809,"stackoverflow.com",0,"2022-03-16 08:15:00+02","2024-05-17 05:29:07.083704+03","I was also facing the same issue node_modules were being included even after excluding it and using plugins such as the serverless_ignore plugin also did not help in the end this config worked for me try this serverless yml the major change is I am using include now and then blob pattern sign to ignore node_modules"
70926441,70924816,"stackoverflow.com",0,"2022-01-31 15:04:17+02","2024-05-17 05:29:07.888582+03","The issue occurred due to improperly calling dotenv using env instead of env"
70940701,70933886,"stackoverflow.com",0,"2022-02-01 21:56:18+02","2024-05-17 05:29:08.669894+03","You have to follow some rules to upgrade your versions to v3 Serverless Framework v3 contains breaking changes that may impact projects In my opinion will be impacted a lot of projects some of the issues are I really recommend you to check this httpswww serverless comframeworkdocsguidesupgradingv3 has the full guide to support the issues you may have "
71782085,70960652,"stackoverflow.com",1,"2022-04-07 15:19:55+03","2024-05-17 05:29:10.35914+03","Whilst it is possible to specify a service account for a Cloud Function it would seem that the Serverless Framework does not yet support specifying it in their configuration There is an open issue for this feature on GitHub as well as an open pull request Unfortunately neither have moved forward in quite a while "
70964462,70964358,"stackoverflow.com",4,"2022-02-03 02:07:19+02","2024-05-17 05:29:11.464579+03","It can take several hours for a bucket name to become available again So either choose a different bucket name or wait a little longer until it becomes available again "
70964458,70964358,"stackoverflow.com",2,"2022-02-03 02:07:05+02","2024-05-17 05:29:11.466579+03","Bucket names are globally unique You can read about it here and about deleting a bucket here If the name has not been taken already by someone else you need to wait for a while to reuse the same name The time taken is unknown afaik "
75827270,70964358,"stackoverflow.com",1,"2023-03-23 21:28:27+02","2024-05-17 05:29:11.467579+03","I have lost some time with a similar error but the name was not the reason the reason was adding an event triggered by s3 and having the s3 bucket definition in the same serverless yml as pointed here thread with detailed explanation"
70968740,70968439,"stackoverflow.com",2,"2022-02-03 11:29:05+02","2024-05-17 05:29:12.486277+03","at the moment such functionality is not supported in serverlessoffline plugin There is an issue open where the discussion started around supporting this use case httpsgithub comdheraultserverlessofflineissues1324"
76826396,70968439,"stackoverflow.com",2,"2023-08-03 11:52:11+03","2024-05-17 05:29:12.487277+03","I had the same issue Fixed adding host 0 0 0 0 when running serverless offline in the docker container sls offline stage dev host 0 0 0 0"
70994556,70994280,"stackoverflow.com",2,"2022-02-05 03:14:56+02","2024-05-17 05:29:13.387959+03","By default serverless framework creates a bucket with a generated name like service nameserverlessdeploymentbuck1x6jug5lzfnl7 to store your services stack state Each successive version of your serverless app is bundled and uploaded by sls to the deployment bucket and deployed from there I think you have some control over how sls does this if you use the serverlessdeploymentbucket plugin "
70997874,70994280,"stackoverflow.com",1,"2022-02-05 14:09:31+02","2024-05-17 05:29:13.38996+03","By default the Serverless Framework creates a number of things on your machine in order to deploy what you have configured in your serverles yml It then needs to make use of a service inside AWS called CloudFormation to actually create the resources you configured like your S3 bucket The best way to do this is to take the things it created on your machine and upload them to AWS to ensure that the deployment continues without interruption or issue and the best place to do that is S3 So the Serverless Framework will always by default create its own S3 bucket entirely unrelated to what you configured as a location to store the files it generated on your AWS account then point CloudFormation at it to build the things you configured to get built While you have some control over this deployment bucket there always needs to be one And it is completely unrelated to the bucket you configured "
71017927,70998665,"stackoverflow.com",1,"2022-02-07 13:38:08+02","2024-05-17 05:29:14.469301+03","You do not need to deploy your stack Things you need to have to use serverless offline in your serverless yml file then run the command sls offline start then you can call your lambda functions with sls invoke local"
71020548,71019685,"stackoverflow.com",1,"2022-02-07 17:05:40+02","2024-05-17 05:29:16.175329+03","In Serverless Framework you can use a switch based on your environment to toggle configuration options within the context of your YAML file For example if you wanted to setup a normal HTTP endpoint via API Gateway you would add something like the following to your functions section If you want this http event to be available for only a certain stage you can define a block elsewhere and reference it based on the current stage name It is commonplace to use the custom block in your serverless yml file for this To isolate between the stages prod and dev with dev being your staging area you would do this The above would only expose the endpoint via HTTP when you run sls deploy stage dev to release your application any other stage e g prod will have it disabled Do note that in this example if you want to support stages outside of dev and prod you will need to add a new block under custom pingEvents "
76435754,71020734,"stackoverflow.com",2,"2023-06-09 00:19:09+03","2024-05-17 05:29:17.18256+03","I had the same question and this is how it worked for me"
73841563,71020734,"stackoverflow.com",2,"2022-09-25 04:34:06+03","2024-05-17 05:29:17.183462+03","This works I am still curious as per my comment about why this does not seem to be a common thing to do And will investigate that and packaging in general "
71216504,71028291,"stackoverflow.com",6,"2022-03-17 07:51:16+02","2024-05-17 05:29:17.831952+03","To solve this issue you need to update your serverless yml file with these changes in the custom block I also face the same issue my issue was with dockerizePip it was set to either remove this entry from serverless yml file or just set it to false"
71071740,71028291,"stackoverflow.com",5,"2022-02-10 22:17:06+02","2024-05-17 05:29:17.83298+03","To be able to deploy your project with serverlesspythonrequirements you need to have docker on your machine if you are on windows consider using docker desktop or a linux vm Why do I need Docker When you do a sls deploy serverlesspythonrequirements launch a docker container to install all the dependencies you have put in your requirements txt file that will be used during the deployement process You are getting this error because your container is not launch correctly"
71034292,71032995,"stackoverflow.com",1,"2022-02-08 14:55:50+02","2024-05-17 05:29:18.413874+03","Thanks to eli6 for the tip about serverless print I do not know the root cause of the problem but after restarting vscode serverless print and serverless deploy then worked"
74703829,71032995,"stackoverflow.com",0,"2022-12-06 16:02:30+02","2024-05-17 05:29:18.414874+03","Little too late on subject but similar issue here I think If someone was strugling on this like I was maybe is a good ideia to check it out httpsgithub comserverlessexamplesissues689"
71037369,71035349,"stackoverflow.com",1,"2022-02-08 18:29:41+02","2024-05-17 05:29:19.355857+03","It is a YAML syntax problem FnGetAtt[pullSqlSvr Arn] is being parsed as a string not a keyvalue pair Add a space after the last colon or use the GetAtt shortcut "
71035784,71035349,"stackoverflow.com",0,"2022-02-08 16:41:15+02","2024-05-17 05:29:19.357857+03","In my experience serverlessstepfunctions fails to deploy properly if any keys in the Steps block begin with a lowercase letter Changing it to the seemingly casesensitive equivalents like the following and redeploying may do the trick I just converted the pullSqlSvr and sendToDataLake to their PascalCase equivalents PullSqlSvr and SendToDataLake "
71070906,71036524,"stackoverflow.com",0,"2022-02-10 21:06:29+02","2024-05-17 05:29:20.391915+03","I know that in serverless yml file you can import env variables from a external file in one line like this After if you use node js you can have a serverless ts or serverless js and import files very easily with Object assign exemple httpsgithub comTheSmartMonkeyserverlessblobmainreportsserviceserverless ts"
71059288,71057678,"stackoverflow.com",0,"2022-02-10 05:02:27+02","2024-05-17 05:29:21.366246+03","Fixed my Kinesis data stream ARN was incorrect "
71080151,71068564,"stackoverflow.com",0,"2022-02-11 14:31:14+02","2024-05-17 05:29:22.435415+03","Problem was due to yaml formatting Line 192 FnGetAtt [PullSqlSvr Arn] This needed an extra tab to indent below Resource"
71129601,71102547,"stackoverflow.com",0,"2022-02-15 18:09:07+02","2024-05-17 05:29:24.144754+03","I think I have figured this out Provisioned Concurrency alarms have a setting on what to do when there is Insufficient data i e no data There is a setting way down on the metric open the Advanced Configuration section I had this set to Treat missing data as missing and this needs to be set to Treat missing data as bad breaching threshold for the Lambda Provisioned concurrency to be scaled down when there are no users In my case I have opened up a ticket with the serverlessprovisionedconcurrencyautoscaling plugin to get this fixed since I do not control it myself "
71119145,71118928,"stackoverflow.com",4,"2022-02-15 06:29:23+02","2024-05-17 05:29:27.280379+03","The path you are using logoswhite png is an absolute path and that means that your code is looking in the wrong place I am not 100 sure but one of the following two options should solve your issue Documentation httpsdocs aws amazon comlambdalatestdgconfigurationenvvars htmlconfigurationenvvarsruntime Another option is split the application code and the assets like the image up The image would go into a Lambda layer which you would be able find in another directory on the Lambdas disk httpsdocs aws amazon comlambdalatestdgconfigurationlayers htmlconfigurationlayerspath"
71179034,71120452,"stackoverflow.com",0,"2022-02-18 21:41:47+02","2024-05-17 05:29:28.339366+03","The key to the problem was realizing that SLS is as often stated mainly a wrapper to CloudFormation CFN More than I would thought CFN scripting is executed during CFN deployment not SLS interpretation So any association between a DynDB SLS resource and a custom expression is purely syntactic Inserting FnGetAtt [ TunesTable Arn ] in our IAM resource is not in a context that is aware of a TuneTable in SLS it does not calculate and substitute it literally sticks the string into the CFN template to be executed later for good or ill Knowing that narrowed the problem to what expressions in CFN will work and can SLS generate them which led to a plugin solution from httpswww npmjs compackageserverlesspluginifelse I hope the functionality is included in a future base SLS release but with a simpler syntax "
71150737,71141495,"stackoverflow.com",1,"2022-02-17 01:48:57+02","2024-05-17 05:29:29.127659+03","The serverlesspythonrequirements plugin is used to bundle your dependencies and package them for deployment This only comes to effect when you run sls deploy From the plugin page The plugin will now bundle your python dependencies specified in your requirements txt or Pipfile when you run sls deploy Read more about python packaging here httpswww serverless comblogserverlesspythonpackaging Since you are running your service locally this plugin will not be used Your dependencies need to be installed locally perform the below steps to make it work Create a virtual environment in you serverless directory install the plugin serverless plugin install n serverlessoffline install pandas using pip run sls offline start"
71142759,71141495,"stackoverflow.com",0,"2022-02-16 15:29:22+02","2024-05-17 05:29:29.129659+03","Your lambda function do not have the panda module installed You need to use the serverlesspythonrequirements plugin httpswww serverless compluginsserverlesspythonrequirements To use it you need docker on your machine and to create a requirement txt file in your service with the packages you need in your lambda"
71171642,71170896,"stackoverflow.com",1,"2022-02-18 12:07:03+02","2024-05-17 05:29:30.104161+03","You need to allow the bucket and resources Try to add the resources permission on the following way"
71177318,71171513,"stackoverflow.com",0,"2022-02-18 19:05:57+02","2024-05-17 05:29:30.739428+03","If the the exclude function inside custom datadog is not working please open a ticket in the repository and I will address it for you httpsgithub comDataDogserverlessplugindatadog Thanks "
71258812,71219386,"stackoverflow.com",1,"2022-02-25 00:15:30+02","2024-05-17 05:29:33.260316+03","Generally speaking using Lambda Layers is a good approach to store shared code as you can reduce redundancy and keep the size of deployment packages small Layers can be created using the Serverless framework as well so you can include them into your existing templates and have them managed by ServerlessCloudFormation Here is a link to the documentation with configuration examples and explanations Here is one example of how to add a layer to your serverless yaml file And then from each Lambda function in your serverlessfunctions yaml file you can refer to the layer as follows"
77398022,71227085,"stackoverflow.com",1,"2023-10-31 19:34:56+02","2024-05-17 05:29:35.023286+03","Do not know if you are still experiencing the issue but while upgrading to serverless v3 I found that I had to use useInProcess in the sls offline command so nyc would see it "
71344183,71272921,"stackoverflow.com",4,"2022-03-03 23:46:50+02","2024-05-17 05:29:37.017688+03","I found the solution to this problem It was required to change the property s3ForcePathStyle instead forcePathStyle because of the v3 of awssdk If you want to see more details about this problem the solution was resolved here httpsgithub comar90nserverlesss3localissues492"
72819515,71505048,"stackoverflow.com",3,"2022-06-30 20:00:12+03","2024-05-17 05:29:38.965073+03","So the solution I was able to come up with was to add a specific permission to the generate API Gateway Cloudformation template The AWS docs outline what the Cloudformation template should look like to add a permission for API Gateway to access a lambda httpsaws amazon compremiumsupportknowledgecenterapigatewayrestapilambdaintegrationsTo_add_Lambda_invoke_permission_to_a_REST_API_with_a_Lambda_integration_using_a_CloudFormation_template So if you adapt that and add this block to the bottom of your serverless yaml you should be able to access the Authorizer referenced by the FunctionName field Hope this helps another lost soul and I smashed my head up against this for a good long while "
71530935,71505751,"stackoverflow.com",1,"2022-03-18 19:24:00+02","2024-05-17 05:29:39.665053+03","Parameters can be passed to runscript commands using You can try npm run sls config credentials provider aws key XXXXXX secret XXXXXXXXXXXXXXXXXXX For more details checkout this answer httpsstackoverflow coma447438715798816"
71633792,71505751,"stackoverflow.com",1,"2022-03-27 07:57:30+03","2024-05-17 05:29:39.667054+03","An alternative way to do this is to set the AWS environment variables used by the AWS CLI and AWS SDK since the serverless framework uses the AWS SDK under the hood More information can be found here The process that I did was"
71557622,71557547,"stackoverflow.com",0,"2022-03-21 14:26:37+02","2024-05-17 05:29:41.923753+03","I would recommend you to use esbuildloader for webpack as it is more effective and provides really good treeshaking bundling only code which is really used by your application rather than bundling everything As an alternative you may use the corresponding serverless plugin but since you already have webpack configured it may be more convenient to use the first option "
71611463,71566505,"stackoverflow.com",0,"2022-03-25 04:19:51+02","2024-05-17 05:29:42.418231+03","I was able to solve this issue by using webpack If you are facing the same issue try using serverlesswebpack plugin "
71607848,71605617,"stackoverflow.com",1,"2022-03-24 20:41:37+02","2024-05-17 05:29:43.501939+03","The answer was that I had put the value in the env but not the env alpha file I am going to go put my head through a wall now "
74227140,71607891,"stackoverflow.com",0,"2022-10-27 22:09:11+03","2024-05-17 05:29:44.321234+03","I believe you can split the elements by a delimiter serverless yml config json"
71670224,71621207,"stackoverflow.com",0,"2022-03-30 03:29:47+03","2024-05-17 05:29:45.021472+03","The solution was to add an additional event handler explicitly for the root in serverless yml I also added app UseDefaultFiles before app UseStaticFiles in Startup cs "
71629771,71629383,"stackoverflow.com",2,"2022-03-26 18:19:03+02","2024-05-17 05:29:46.663973+03","After some digging I found a reference from serverless docs in the IAM permissions section so the solution would be like this"
71638102,71638064,"stackoverflow.com",4,"2024-02-23 23:05:29+02","2024-05-17 05:29:48.626837+03","Yes build folder should be included in gitignore because when the developer pulls the changes after the compilation he will get the build folder back if the tsconfig json file is well written and i do not think that it is a good idea to have node_modules inside the build folder"
71640346,71638816,"stackoverflow.com",0,"2022-03-28 00:11:49+03","2024-05-17 05:29:49.730062+03","The warning means that the deploymentBucket property is not recognized and as such it is not doing what you think it should be doing According to serverless docs deploymentBucket should be a property under provider not a root property "
71657727,71638816,"stackoverflow.com",0,"2022-03-29 10:00:55+03","2024-05-17 05:29:49.732063+03","I was able to get rid of this warning by moving the deploymentBucket property under provider instead of registering it as a root property The modified serverless yml file is attached below Also read up the serverless documentation for more clarity Thanks again to NoelLlevares for the tip "
73469344,71638816,"stackoverflow.com",0,"2022-08-24 10:50:00+03","2024-05-17 05:29:49.733063+03","Also try to update to latest version of serverless in my case some keys were unrecognized in old version"
71697570,71671365,"stackoverflow.com",0,"2022-03-31 21:36:10+03","2024-05-17 05:29:50.611981+03","Try This Note you will need to add the name AuctionsTable to the imported file "
71700301,71699894,"stackoverflow.com",0,"2022-04-01 02:42:32+03","2024-05-17 05:29:53.059115+03","Maybe this can help How do I delay processing of AWS Kinesis messages"
71878584,71756561,"stackoverflow.com",0,"2022-04-15 02:26:17+03","2024-05-17 05:29:53.694174+03","often the default configuration is set up in a way that you can access the database only from a ip inside the VPC or subnet Lambda functions are not running on dedicated infrastructure and have therefore some other random ip In google cloud a concept of vpcconnector exists to route the traffic from functions into your VPC to the database I guess there is something similar in AWS "
72994771,71756561,"stackoverflow.com",0,"2022-07-15 16:27:49+03","2024-05-17 05:29:53.696175+03","I had the same problem when I updated my Node to a version higher than 12 It is solved for me by adding the dialectModule to the Sequelize options My dependencies"
73971864,71758178,"stackoverflow.com",0,"2022-10-06 13:07:57+03","2024-05-17 05:29:54.864238+03","As far as I know serverless does not allow having functions in yaml file since that is just a declaration You could have a custom plugin httpswww serverless comframeworkdocsguidespluginscustomvariables It would looks something similar"
71781123,71771566,"stackoverflow.com",0,"2022-04-07 14:13:14+03","2024-05-17 05:29:55.545019+03","You need to set the origins to serverless yml Also wild card origin is not allowed if AccessControlAllowCredentials is set True httpswww serverless comframeworkdocsprovidersawseventsapigatewayenablingcors"
71793502,71792717,"stackoverflow.com",0,"2022-04-08 10:52:23+03","2024-05-17 05:29:56.135502+03","You can refer to the RDS documentation for policy examples They clearly provide sample values for Action and Resource "
71801142,71799147,"stackoverflow.com",9,"2023-04-28 10:57:10+03","2024-05-17 05:29:57.055214+03","So turned out it was a problem with my node version I was running v17 3 1 After switching to v16 4 0 it works like a charm From EdwinWong comment In node 18 in your serverless custom dynamodb start host declare 127 0 0 1 and it should work I did not test this "
71829406,71829093,"stackoverflow.com",0,"2022-04-11 17:16:50+03","2024-05-17 05:29:59.759233+03","You are not waiting for the dynamodb put operation to finish Additionally you are wrapping the call in a setTimeout Your lambda function is returning before the network operation can be made Make sure the put operation succeeds before returning a result from your lambda "
71846287,71846219,"stackoverflow.com",1,"2022-04-12 19:36:11+03","2024-05-17 05:30:00.767831+03","Connection to the DB is not something infrarelated but functional So you will have in the code the connection to the DB and call to cursors Exception If you use appsync for your API with direct calls not lambda you can declare the DB and make calls to it directly "
78441736,71849290,"stackoverflow.com",0,"2024-05-07 13:13:48+03","2024-05-17 05:30:02.350558+03","I had a similar issue In my case it turned out it was because the sent request had the header ContentType applicationjson Despite the request it was a GET with no body content Removing the header ContentType helped"
71909998,71861938,"stackoverflow.com",1,"2022-04-18 12:30:42+03","2024-05-17 05:30:03.35799+03","To reference the resource correctly you need to use the Ref function Two more notes regarding your template Template"
71866522,71865327,"stackoverflow.com",2,"2022-04-14 07:28:49+03","2024-05-17 05:30:04.412226+03","hard one to spot but since i was using step functions should be"
74608163,71874980,"stackoverflow.com",0,"2022-11-29 03:21:56+02","2024-05-17 05:30:05.405796+03","I have the same problem The problem happend when I update serverless framework v3 from v2 If I do not use basePath or path and move env env prod to root folder it works but I hope to have better solution "
75423445,71874980,"stackoverflow.com",0,"2023-02-11 23:34:37+02","2024-05-17 05:30:05.406796+03","Here"
71887079,71886945,"stackoverflow.com",0,"2022-04-15 20:28:02+03","2024-05-17 05:30:06.479004+03","You need to upgrade Spring Boot as well You are using 2 3 0 and ApplicationContextFactory was added in 2 4 but 2 4 x is no longer supported You should upgrade to Spring Boot 2 5 12 or 2 6 6 "
71919908,71887380,"stackoverflow.com",0,"2022-04-19 08:03:13+03","2024-05-17 05:30:07.48484+03","Apart from Release SQL views from memory and Release data frames from memory everything looks fine If your application required to query the data frequently and there is requirement to create VIEWS you can create the EXTERNAL TABLE in Dedicated SQL Pool and save the VIEWS for the tables using Synapse SQL This would be more efficient and there will not be need to drop VIEWS and release dataframes every time you need the data You can also create and use native external tables using SQL pools in Azure Synapse Analytics as Native external tables have better performance when compared to external tables with TYPEHADOOP in their external data source definition This is because native external tables use native code to access external data You can also refer Best practices for serverless SQL pool in Azure Synapse Analytics to get more details regarding performance optimization "
71929037,71918148,"stackoverflow.com",2,"2022-04-19 20:44:48+03","2024-05-17 05:30:08.243628+03","See Issue 9313 on GitHub httpsgithub comserverlessserverlessissues9813 Problem The latest version of the serverless framework is no longer working for AWS Lambda deployments and is throwing the following error Discussion with the new resolver such definition is not supported In general it is discouraged to configure stage behind env variables for example as at the point where stage is going to be resolved not whole env might be available e g loading env vars from env stage needs to resolve stage first in order to properly load variables from file which might introduce bugs that are hard to debug Also the provider stage serves more as a default stage and stage flag via CLI is the preferred way of setting it In your configuration file you explicitly optin to use new resolver via variablesResolutionMode 20210326 variable We are not discouraging the use of env variables quite the contrary we have been promoting them as a replacement for custom CLI options for example and it is generally a great practice to use them As for env source for stage specifically this has been introduced as a fix as stage should be already resolved before we attempt env variables resolution as loading env files can depend on stage property medikoo I know we have talked about it today do you think it could be safe to resolve stage from env source in specific circumstances e g when dotenv is not used See also"
72156920,72143903,"stackoverflow.com",0,"2022-05-08 01:57:09+03","2024-05-17 05:30:09.207914+03","sls deploy v will only output the version information and exit It is a greedy match Change the v flag to verbose and this will work Example"
72152885,72151314,"stackoverflow.com",0,"2022-05-07 16:15:51+03","2024-05-17 05:30:10.441612+03","path and httpMethod are for invoking an API Gateway route not a Lambda function A Lambda function invocation mostly takes a function name invocation type a payload If you must go via API Gateway take a look at the official Call API Gateway with Step Functions guide on how to do this otherwise just invoke your Lambda manually "
72220436,72215893,"stackoverflow.com",1,"2022-05-12 21:22:45+03","2024-05-17 05:30:11.773756+03","This appears to be a backend error coming from the Serverless coms API My guess is you can set console false in your serverless yml file and then redeploy your service to remove the instrumentation but I have not confirmed that it seems absent from their documentation"
72301031,72288699,"stackoverflow.com",1,"2022-05-19 11:33:19+03","2024-05-17 05:30:14.903166+03","To access env variables in serverless configuration use envVARIABLE_NAME For the stage for me optstage works when running sls stage STAGE_NAME In your case"
72347655,72316180,"stackoverflow.com",3,"2022-05-23 14:18:59+03","2024-05-17 05:30:15.987213+03","Use property awsprofile instead profile Reference httpswww serverless comframeworkdocsprovidersawsguidecredentials Looks like they have changed the variables naming in Serverless v3 "
72324627,72321465,"stackoverflow.com",0,"2022-05-20 23:49:17+03","2024-05-17 05:30:16.480508+03","Turns out this required a weird fix but it is best to remove the following Once these are removed modify the Install serverlesscompose step to run npm install and install all the packages Then run npx serverless deploy instead of serverless deploy This fixed the problem for me "
75417033,72338973,"stackoverflow.com",1,"2023-02-11 01:24:43+02","2024-05-17 05:30:18.176225+03","Your directives do not define any inclusive patterns Perhaps you want to list the files directories you need packaged Each directive builds on the next Something like See httpswww serverless comframeworkdocsprovidersawsguidepackagingpatterns"
72361577,72360699,"stackoverflow.com",0,"2022-05-24 13:50:21+03","2024-05-17 05:30:20.850801+03","Yes you can do this if you develop your own macro for that in CloudFormation Otherwise you have to refactor your template to use nested stacks "
76271256,72360699,"stackoverflow.com",0,"2023-05-17 13:46:18+03","2024-05-17 05:30:20.851801+03","Try Rain It is a development workflow tool for working with AWS CloudFormation rain merge command can merge two or more CloudFormation templates "
72392800,72374104,"stackoverflow.com",2,"2022-05-26 16:40:47+03","2024-05-17 05:30:20.988624+03","If you are using a machine provided by an employer they may have installed selfsigned SSL certificates in order to connect to internal systems or run fleet management software You can override strict checking in Node by either prepending the command with NODE_TLS_REJECT_UNAUTHORIZED0 serverless or permanently with npm config set strictssl false "
73670910,72374104,"stackoverflow.com",1,"2022-09-10 13:01:58+03","2024-05-17 05:30:20.990625+03","I had checked with Aarons answer but it did not work for my case Issue on my side was with my companys VPN as it was blocking sendingreceiving the data I went with the below steps that solved my issue Solution Also make sure that your IAM user has necessary privileges to perform this by attaching necessary policy in the AWSs IAM User page "
72392593,72374926,"stackoverflow.com",0,"2022-05-26 16:26:57+03","2024-05-17 05:30:21.856463+03","I would recommend using a console log statement to print out the value of variables It looks to me as if the body is probably not being extracted correctly and so there is no value for the primary key You can see the value of the variable you console log in your logs in CloudWatch or in the Serverless Framework dashboard if you use it "
72964273,72422079,"stackoverflow.com",2,"2022-07-27 15:40:33+03","2024-05-17 05:30:22.910264+03","EDIT allowCache is now default behavior on 9 0 0 Old Answer From the docs serverlessoffline is a plugin that emulates lambda and apigateway Meaning not the execution environment it will act as if you are calling a cold function everytime You need to add an option to get it to work as a real function "
72442173,72433089,"stackoverflow.com",5,"2022-05-31 07:06:58+03","2024-05-17 05:30:23.548867+03","I have found the error Instead of using ApiGatewayRestApi if I use HttpApi I get the desired result "
72646835,72452103,"stackoverflow.com",1,"2022-06-16 16:38:42+03","2024-05-17 05:30:24.311527+03","Let us first clarify a few things The httpApi event is using HTTP API not REST API from AWS Api Gateway You can set externally created HTTP API by specifying it in the following way If you would like to use REST API then you would need to use http event type and set it like this"
72454449,72453410,"stackoverflow.com",1,"2022-06-01 00:50:58+03","2024-05-17 05:30:25.291815+03","Well I do not have an actuall example for the serverless framework but i can tell what you should do "
72507530,72491238,"stackoverflow.com",2,"2022-06-05 16:00:54+03","2024-05-17 05:30:25.819824+03","It seems that while the above method does not work I am able to use the pseudo variable directly without using Ref "
72507821,72507118,"stackoverflow.com",1,"2022-06-05 16:42:17+03","2024-05-17 05:30:26.800376+03","Please read the documentation on the expected function signature of the JavaScriptNode Lambda handler httpsdocs aws amazon comlambdalatestdgnodejshandler htmlnodejshandlersync The second argument is not the callback function It is the context The callback function is supposed to be the third parameter "
72646769,72510334,"stackoverflow.com",0,"2022-06-16 16:34:26+03","2024-05-17 05:30:27.59443+03","by default all functions deployed with Serverless Framework are versioned You can also disable it or turn it on explicitly by setting Please keep in mind that the old versions are not removed automatically so if you want to keep e g only a few previously deployed versions you might need to use a plugin as httpsgithub comclaygregoryserverlesspruneplugin"
72515684,72513427,"stackoverflow.com",3,"2022-06-06 12:32:33+03","2024-05-17 05:30:28.419124+03","Your lambda functions need to be in the same VPC as the database specifically in a private subnet You would then adjust the security group rules to allow connectivity from the functions to the DB using something like myFynction connections allowToDefaultPort myDatabaseInstance The VPC needs to have a NAT gateway for the lambda functions to be able to access the internet To clarify the functions cannot be in an isolated subnet because isolated subnets do not have Internet connectivity Placing the functions in a public subnet will not work either refer to this for an explanation Relevant documentation httpsaws amazon compremiumsupportknowledgecenterconnectlambdatoanrdsinstance"
72579139,72541865,"stackoverflow.com",10,"2022-06-10 22:41:25+03","2024-05-17 05:30:29.42024+03","As per the documentation in this page httpswww serverless comframeworkdocsprovidersawsguideserverless yml you can specify this under the function ephemeralStorageSize value"
72646640,72640312,"stackoverflow.com",0,"2022-06-16 16:25:27+03","2024-05-17 05:30:32.602506+03","Serverless Framework does things a bit differently and instead of using stages of APIGW it creates a totally new APIGW for each stage that is why you do not see the prefix in your path with the stage name but if you observe the url you will see that the base url will be different across stages That is how you can differentiate between them "
72650781,72645324,"stackoverflow.com",0,"2022-06-16 22:08:11+03","2024-05-17 05:30:34.405515+03","Is it a typo in your serverless yml file you have I imagine it is meant to be test_api handler"
74306368,72645324,"stackoverflow.com",0,"2022-11-03 18:20:16+02","2024-05-17 05:30:34.406989+03","Looking at the structure of your project it seems you missed an important step mentioned in the documentation no package json You need to run the following command in your project directory This will automatically add the plugin to your projects package json and the plugins section of its serverless yml If you want to start from the beginning then run the following commands in your active venv of your project directory then I would add the following to your serverless yml Then deploy while your venv is active Please note serverlesspythonrequirements requires Docker for it to work "
72654814,72654757,"stackoverflow.com",1,"2022-06-17 08:45:42+03","2024-05-17 05:30:35.428783+03","as i understand correctly you want to add this in yaml template for CloudFormation and set all this invocation settings Check this httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawsresourcelambdapermission html"
72654839,72654757,"stackoverflow.com",0,"2022-06-17 08:50:11+03","2024-05-17 05:30:35.430295+03","In this blog we will show you how to set up a Lambda function to trigger on an S3 PUT object event from a different AWS account This can be useful if you have multiple AWS accounts and want to move data between them or if you want to trigger a Lambda function from an S3 event in another AWS account First you will need to create an IAM role in the account where the S3 bucket resides This IAM role will be used by Lambda to assume a crossaccount role in order to trigger the Lambda function Next you will need to create a Lambda function in the account where you want to trigger the event This Lambda function will assume the IAM role created in the previous step Finally you will need to add a bucket policy to the S3 bucket in the account where the S3 bucket resides This bucket policy will allow the Lambda function in the other account to trigger on PUT events Assuming you have two AWS accounts account A and account B heres how you would set this up In account A create an IAM role This IAM role will be used by Lambda in account B to assume a crossaccount role In account B create a Lambda function This Lambda function will assume the IAM role created in step 1 In account A add a bucket policy to the S3 bucket This bucket policy will allow the Lambda function in account B to trigger on PUT events Assuming you have two AWS accounts account A and account B heres how you would set this up In account A create an IAM role This IAM role will be used by Lambda in account B to assume a crossaccount role In account B create a Lambda function This Lambda function will assume the IAM role created in step 1 In account A add a bucket policy to the S3 bucket This bucket policy will allow the Lambda function in account B to trigger on PUT events Heres the IAM role you will need to create in account A Version 20121017 Statement [ Sid Effect Allow Principal Service lambda amazonaws com Action stsAssumeRole ] And heres the Lambda function you will need to create in account B import boto3 def lambda_handler event context TODO implement print event Finally heres the bucket policy you will need to add to the S3 bucket in account A Version20121017 Statement[ Sid EffectAllow Principal AWSarnawsiamaccountbidroot Actions3PutObject Resourcearnawss3bucketname ] Replace accountbid with the ID of account B and bucketname with the name of the S3 bucket in account A Now when you PUT an object into the S3 bucket in account A the Lambda function in account B will trigger and print the event to the logs "
72707894,72668675,"stackoverflow.com",0,"2022-06-22 01:51:45+03","2024-05-17 05:30:37.143584+03","Looks like a classic Lambda callback issue Your Lambda is finishing before a callback is completed Try this or this Also make sure your Lambda configuration is setup so that it does not timeout The default is 3s which is often too short "
72726655,72668675,"stackoverflow.com",0,"2022-06-23 11:06:36+03","2024-05-17 05:30:37.145584+03","PostConfirmation Lambda is triggered when the user is confirmed not only signup so make sure that the user verifies his email or phone number after signup by setting autoVerify email using console Or using serverless"
74493669,72694596,"stackoverflow.com",0,"2022-11-18 19:57:46+02","2024-05-17 05:30:38.126906+03","Have not tested but according to the following you can use what looks like cloudformation fn backreferences httpsforum serverless comthowdoigettheurlforafunctioninmyserverlessymlfile13864"
74513693,72694596,"stackoverflow.com",0,"2022-11-21 04:44:18+02","2024-05-17 05:30:38.127906+03","You need to know how Serverless is internally referencing the function URLs which should be accessable in this format You can check this by viewing the resource names in AWS "
72711322,72695038,"stackoverflow.com",4,"2022-06-22 10:30:06+03","2024-05-17 05:30:39.181697+03","sorry this issue has been resolved It was a serverlessappsyncsimulator and cfnresolverlib dependency issue Downgrade cfnresolverlib Thanks guys "
72942999,72932217,"stackoverflow.com",0,"2022-07-11 21:19:56+03","2024-05-17 05:30:40.081688+03","Branches should be a list and looks like you have an extra in your first ResultPath"
72952169,72945122,"stackoverflow.com",0,"2022-07-12 15:16:50+03","2024-05-17 05:30:42.223021+03","It appears that my bucket access policy had incorrect syntax The correct one is"
74013332,72949313,"stackoverflow.com",0,"2022-10-10 13:17:04+03","2024-05-17 05:30:43.200049+03","You can still use the same function signature for get request Just write it this way for get request PS When initiating a get request there is no need for a body request property which from my understanding is validated against the schema so you can just specify undefined for the schema type "
74088405,72950837,"stackoverflow.com",5,"2022-10-16 18:35:29+03","2024-05-17 05:30:44.146175+03","you can solve this by setting the location of Prisma Client It works for both local and lambda prismaschema prisma serverless ts some function folder structure"
72953312,72950837,"stackoverflow.com",1,"2022-10-06 23:40:18+03","2024-05-17 05:30:44.147176+03","Looks like you are attempting to import the prisma schema file from your node_modules file which is not where it normally goes Where is your prisma schema file located relative to the root of your project If it is not in prisma schema you will need to configure its location by using this option httpswww prisma iodocsconceptscomponentsprismaschemaprismaschemafilelocation"
72952425,72951908,"stackoverflow.com",2,"2022-07-12 15:35:14+03","2024-05-17 05:30:44.829625+03","I found answers here here and here Basically Serverless Framework has no particular support for this feature However it supports extended YAML syntax which has anchor and dictionary merging capabilities So first I unpack the env vars from YAML config file at the top of the file and anchor it with env_vars like a variable for referencing but in YAML And then I use it unpacking this dictionary"
78309463,72951908,"stackoverflow.com",0,"2024-04-11 12:46:41+03","2024-05-17 05:30:44.831626+03","I tried qaliss answer but getting in BitBucket build pipeline Cannot parse serverless yml cannot merge mappings the provided source object is unacceptable in optatlassianpipelinesagentbuildserverless yml 125 "
73945412,72953973,"stackoverflow.com",0,"2022-10-04 11:59:07+03","2024-05-17 05:30:45.432259+03","It would work if you had different topic names but to make it work with the same topic name different region you will have to give topicName parameter which will make cloudformation create another subscription for it Do not worry as the lambda will subscribe to topicnamedev the other parameter will be used to create a unique subscription for the topics last example here httpswww serverless comframeworkdocsprovidersawseventssnsusingapreexistingtopic"
72956679,72956678,"stackoverflow.com",1,"2022-07-12 21:09:51+03","2024-05-17 05:30:46.513494+03","Based in the guides instructions for vscode I was able to migrate that configuration to pycharm and had success in debugging with it I am new to the nodejs world so my first insight was to realize that the command being used is actually npm This is my working configuration Additional notes"
72977717,72967634,"stackoverflow.com",0,"2022-07-14 11:44:28+03","2024-05-17 05:30:47.150703+03","In the end we tried numerous implementations and the issue seemed to boil down to trying to both retrieve the ssm value for securityGroupId and also parse and default the second variable within it The solution ended up being as follows where we removed the parsingdefault variable from within ssm step Additionally we had to remove some of the double quotes on the custom vars"
72968823,72968777,"stackoverflow.com",0,"2022-07-13 18:26:31+03","2024-05-17 05:30:48.016045+03","Seems that the correct syntax is"
72974236,72974219,"stackoverflow.com",1,"2022-07-14 04:26:30+03","2024-05-17 05:30:49.10282+03","It is just a regular JSON to YAML conversion Plenty of such tools online But it should be"
72994031,72993007,"stackoverflow.com",1,"2022-07-15 15:29:59+03","2024-05-17 05:30:51.180032+03","If you write const testnumber 1 typescript assumes that test is of type number In your case it defines the type of the arrow function not return type Like this example const test eunknown number eunknown 1 This type after the colon defines that the arrow method has one argument of type unknown and returns a number Serverless Framework provides types for all of their methods that you handler matches the definition of their type "
73036265,73005973,"stackoverflow.com",1,"2022-07-19 14:42:36+03","2024-05-17 05:30:54.244844+03","Yeah the framework probably could create a package json for you when you create a new nodejs project However there are several cases where a user would not need any node modules at all so it makes sense to not provide a package json file unless the user installs other dependencies The typescript template is a bit different because the Serverless Framework has a plugin system which is based on npm so adding a plugin will require a package json file to track the dependency So when you run sls create t awsnodejstypescript it automatically generates a serverless yml file which includes the serverlessesbuild plugin and thus creates a package json file tracking it The typescript template also includes a bunch of dev dependencies which also are tracked via npm "
73933743,73933564,"stackoverflow.com",1,"2022-10-03 12:51:39+03","2024-05-17 05:31:30.1389+03","Just compile your go code to binary files then run the serverless offline command You can create a Makefile to create some alias command ex Then you can run make deploylocal to start serverless offline server"
73028197,73024580,"stackoverflow.com",1,"2022-07-18 23:24:01+03","2024-05-17 05:30:55.331275+03","No the Serverless Framework is really only useful if you are deploying to a cloud and mostly it is just AWS serverless offline will run a small nodejs server but it is emulating AWS Lambda So if you will never use Lambda there is no real point to emulating it In your case just run a regular nodejs server "
73227446,73024580,"stackoverflow.com",1,"2022-08-03 23:26:51+03","2024-05-17 05:30:55.332275+03","Serverless framework is not designed for such a use case this tool is used for deployments to cloud environment Offline mode is just a simulation which is supposed to be run in local environment not production Another option would be to use a process manager like PM2 To deploy to the virtual machine you can use tools like ansible and then PM2 will take care of the runtime It also has some neat features for example if your server crashes it can automatically revive it cluster mode can run multiple node instances in the virtual machine to utilize all the cores of the CPU which can boost your performance if you run a stateless cluster It covers more than serverless offline is designed for and if you run without containers this would be my next best bet "
75259501,73058439,"stackoverflow.com",0,"2023-01-27 16:32:19+02","2024-05-17 05:30:56.160018+03","I solved it 2 ways serverlesspluginifelse allowed me to exclude resources based on certain parameters Later I realized this may be problematic in the long run So I created separate serverless config file for my offline use case to include all necessary resources ProdStaging environments used default serverless ts file Offline uses offlineserverless ts file Though there is some repetition of the resources config this option ensures prodstaging config is not polluted with offline content I could start offline using sls offline start stage local reloadHandler config offlineserverless ts And the offline config reuses some code from main config Sample offlineserverless ts content is below"
75349610,73083043,"stackoverflow.com",1,"2023-02-05 03:43:44+02","2024-05-17 05:30:56.978897+03","I had the same problem and had to add the following"
73153606,73095546,"stackoverflow.com",0,"2022-07-28 16:15:24+03","2024-05-17 05:30:58.09491+03","Try Sub function httpsdocs aws amazon comAWSCloudFormationlatestUserGuideintrinsicfunctionreferencesub html In this case"
73110749,73106244,"stackoverflow.com",0,"2022-07-25 17:15:11+03","2024-05-17 05:30:59.13344+03","As Martin Costello explained in comments Lambda Test Tool 3 1 cannot identify Environment variables set by the serverless yml even it cannot recognize the local environment variables set in your local device If you deploy it to AWS and then serverless yml can apply Environment Variables to the Lambda Runtime as same as normal NodeJS runtime If you need to test with Environment variables to the local environment as well without writing any other code by utilizing the same implementation So in my case I am using VSCode as debugging environment So I can set Environment variables by adding this part to your launch json file Full launch json will look like this "
73122502,73112435,"stackoverflow.com",0,"2022-07-26 14:23:19+03","2024-05-17 05:30:59.831479+03","Background jobs are typically implemented by tying together other AWS serverless services to manage retries or job state as Lambda functions are stateless As an example it is typical to use API Gateway Lambda to create API endpoints and then implement background jobs by publishing jobs to SQS and consuming them with another Lambda function which can run longer than 30s as it is not being invoked by API Gateway Heres an example of a pattern you might explore "
73158536,73145662,"stackoverflow.com",0,"2022-07-28 22:57:09+03","2024-05-17 05:31:00.811322+03","Add a graphql alias to webpacks resolve "
73821305,73171018,"stackoverflow.com",0,"2022-09-23 03:50:40+03","2024-05-17 05:31:01.675027+03","Try something like export STEP_FUNCTIONS_ENDPOINThttplocalhost8083 serverless offline start that should cause step function local to use itself for the step function service integration "
73196974,73182940,"stackoverflow.com",2,"2022-08-01 19:42:51+03","2024-05-17 05:31:02.559828+03","You are missing a space between the hypen and schedule in the events block This causes the array of events to instead be interpreted as an object which breaks the framework in a nonobvious way Try"
76691234,73237852,"stackoverflow.com",0,"2023-07-15 00:33:31+03","2024-05-17 05:31:06.178302+03","Okay so here is a rough answer At least if you are not concerned with sharing the layers and if your not going for a minimal solution For a more robust solution start by reading the above then do the following I have a whole project with multiple layers inside it here is a screenshot of a single layer layout I think the thing that is most confusing in getting this to work right is everywhere talks about zipping the layer and while you can do that the process of zipping will only help you if you are intending to manually upload the layer The linked blog post is partiallymostly correct but all that is actually needed is the buildLayer task This will copy the dependencies to the build and make them available to serverless To build the layer run gradlew clean build this will result in build gradle serverless yml Now let serverless do its thing Map the directory that has all the dependencies in it from gradle to a layer in the serverless yml file Now when you run sls deploy awsprofile default verbose When you run the deploy command it will automatically zip the dependencies into the correct format for the lambda Once the serverless deploy is done it will return something like the following You can then place that arn in your primary application serverless yml file as follows and now your primary application can use the layer you have built Note There is no change required to your build gradle dependencies block This means you can still run your tests and everything else as normal"
73264242,73256762,"stackoverflow.com",0,"2022-08-07 04:02:16+03","2024-05-17 05:31:07.283033+03","Unfortunately this is not possible with only Lambda and API Gateway connected via a websocket integration In this context API Gateway is responsible for holding the websocket connection the Lambda function handler runs once in response to a websocket message and then exits If you want to maintain state between Lambda function invocations you will need to persist it externally using something like DynamoDB or S3 although Step Functions might be a good choice for your use case As your example should demonstrate it is possible to memoize a variable outside of the handler between serial Lambda invocations to the same container this is a popular way to reuse a database connection between invocations But there is no guarantee that a user will be routed to the same Lambda container over the course of multiple serial invocations "
77336582,73266209,"stackoverflow.com",4,"2023-11-13 20:06:00+02","2024-05-17 05:31:08.006294+03","Why yes yes you can Inside your serverless yml add the following From the docs 6 seconds is the default timeout and the max timeout for is 30 seconds so just add your timeout under provider chunk and you should be all set You were close NOTE Keep in mind that API Gateway is the service that actually times out Your lambda can still be running and compete successfully as nascentediskreta mentioned for a long time after that The response will appear to timed out in the request cycle I ran into this recently where jobs would return a failed response But the lambda successfully completed It just could not tell the server it had completed using API Gateway "
73933572,73273184,"stackoverflow.com",0,"2022-10-03 12:34:29+03","2024-05-17 05:31:08.993449+03","This issue happens most probably because your form object has methods in it like your append method and methods cannot be serialised You have two options here"
73295075,73285029,"stackoverflow.com",18,"2022-08-09 19:19:54+03","2024-05-17 05:31:11.06626+03","I have found the issue the useDotenv attribute simply includes the content of env file to the serverless yml file So I still need to declare the environment variable inside the template The env file is not included in the final package deployed to AWS "
73615679,73574926,"stackoverflow.com",0,"2022-09-06 03:36:23+03","2024-05-17 05:31:12.340095+03","I was ultimately able to do what I want using the serverlessopenapi plugin I am actually using the pvdlg fork which uses a customdocumentation style very similar to the OAS2 serverlessawsdocumentation plugin from serverless com With that plugin installed I can then do sls openapi generate f json o oas3api json to output swagger that is pretty close to fully documented though I did have to write a small script to ingest the swagger add in perpath security sections which the plugin does not seem to support and an info block for things like a contact name and TOS and spit it back out again But at least it means no more waiting for AWS to update the CloudFormation just so I can update my docs "
73619799,73619339,"stackoverflow.com",1,"2022-09-06 12:35:46+03","2024-05-17 05:31:13.774229+03","Looks like it has to be an array and then you can add an object with the intrinsic ref "
76693611,73629340,"stackoverflow.com",2,"2023-07-15 14:52:47+03","2024-05-17 05:31:14.877822+03","I had this error in ESM project and solved it with running npx mikroormesm migrationcreate instead of npx mikroorm migrationcreate"
73662232,73659136,"stackoverflow.com",0,"2022-09-09 15:35:24+03","2024-05-17 05:31:15.979334+03","You need at allow for the role to be assumed by the RDS service so you have to add a Trusted Policy to your role like the following finally if you are behind a VPC make sure to add a rule to your VPCs SG that allows inbound connections from port 443 i e HTTPS port and as a source add your VPCs CIDR "
73740260,73718753,"stackoverflow.com",7,"2022-09-16 08:30:31+03","2024-05-17 05:31:16.893259+03","Finally I figured out the solution with some help This works when I change the offline endpoint to be httplocalhost3002 where 3002 is the default lambdaPort Explanation In serverless offline there are CLI options to set the lambdaPort and httpPort The default httpPort is 3000 and the default lambdaPort is 3002 In my project I have set the httpPort to be 6002 without custom lambdaPort which defaulted to 3002 I was invoking the lambda function using 6002 which was incorrect Using 3002 solved it "
73754335,73753975,"stackoverflow.com",0,"2022-09-22 18:07:58+03","2024-05-17 05:31:19.697255+03","I already found my solution Simply include additionalProperties false so that it wont accept any properties that is not included in json file"
73967946,73776368,"stackoverflow.com",0,"2022-10-06 04:54:56+03","2024-05-17 05:31:20.473572+03","Based on a comment I just read on Github this is caused by an API that consists of more than 200 resources in total CloudFormation which is used to deploy serverless resources only retrieves up to 8 pages of 25 resources to locate the root resource id so if it happens to be on page 9 it will throw that error I am not quite sure what the order of these resources is In my own tests sometimes it works and other times it fails Original text from AWS copied here in case the Github issue disappears CloudFormation is making an API call with action apiatewayGetResources in order to get the root resource ID This corresponds to the following API call [1] This is a paginated response meaning that it only returns resources 25 at a time by default Furthermore to prevent throttling and time out issues this is only checked a maximum of 8 pages deep This means that this root resource ID must be in the first 200 responses in order to get the root resource ID Otherwise it will fail to get this value The service team is aware of this issue and are working on a way to fix this However there is no ETA for this to be available Source httpsgithub comserverlessserverlessissues9036issuecomment1047240189"
73914437,73793840,"stackoverflow.com",1,"2022-10-01 01:19:24+03","2024-05-17 05:31:22.691168+03","if you want to use credentials configured as a part of Serverless Dashboard you need to set corresponding app and org in your configuration and then expose SERVERLESS_ACCESS_KEY as env var in case of CICD setup So in your case adding configured org and app to config should resolve the problem "
74079230,73821950,"stackoverflow.com",0,"2022-10-15 14:57:05+03","2024-05-17 05:31:23.686175+03","You need to install the actual serverless package itself as a dev dependency Try this npm i D serverless Noted remember to check the documentation of serverlessoffline to know find the supported version of serverless package "
74529012,73821950,"stackoverflow.com",0,"2022-11-22 10:33:59+02","2024-05-17 05:31:23.688177+03","I fount in too node 14 17 3 framework 3 25 0 local plugin 6 2 2 SDK 4 3 2 I can workaround fixing by change require to import in UserFunction js img png"
75377894,73821950,"stackoverflow.com",0,"2023-02-07 21:06:18+02","2024-05-17 05:31:23.689177+03","set your node version to 16 17 0 or higher and install serverless as a dev dependency"
76629511,73821950,"stackoverflow.com",0,"2023-07-06 16:44:10+03","2024-05-17 05:31:23.690177+03","This happens because your version of Node 14 17 2 does not support nodeurl package anymore as nodeurl was deprecated and removed Either downgrade your Node version to 12 x or downgrade your serverlessoffline package to ^8 8 1"
73828930,73825022,"stackoverflow.com",1,"2022-09-23 17:00:32+03","2024-05-17 05:31:24.775642+03","There is not a way in cron to define a pattern that is daily but has a 10 day wait built in From your comments it seems like you are deploying to multiple accounts and do not want to touch it twice My recommendation would be to write a deployer lambda that takes in some configuration like account number date etc and deploys the lambda when needed on the day it is needed with a daily cron Then you just update the configuration to reference when a specific account needs the lambda to be deployed "
73893569,73870944,"stackoverflow.com",0,"2022-09-29 12:54:02+03","2024-05-17 05:31:26.378665+03","the problem in your case is that you are using a heavily outdated version of the Framework The param s functionality was introduced in v3 release and you are on v1 54 0 which is no longer supported Upgrading should do the trick but you need to be careful and first upgrade to v2 and then to v3 "
73871510,73870944,"stackoverflow.com",0,"2022-09-27 20:19:54+03","2024-05-17 05:31:26.380666+03","Bit of a shot in the dark but according to the documentation you shared the expected call should be Note the additional around the param provided"
73871500,73871216,"stackoverflow.com",0,"2022-09-27 20:46:40+03","2024-05-17 05:31:27.39665+03","Okay after hours I finally figured this out If you are curious for more information about this here was my thought process Once I saw that it was trigged by CloudWatch to have manual JSON that got me thinking that I need to have a value of json or string or map in one of the AWS Lambda GoLang Event handlers The problem is that none of the cloudwatch events correctly unmarshalled the value that I needed Which made me think why not try my own value for the handler function That is one of the approved Handler invocations in the AWS Lambda GoLang Handler documentation So I created my own object and it worked perfectly Heres the final implementation for a nightly and monthly CRON scheduled Serverless Framework AWS Lambda GoLang function serverless yaml main go And on invocation this prints nightly by the minutely schedule and monthly by the hourly schedule "
73874998,73874712,"stackoverflow.com",1,"2022-09-28 03:18:49+03","2024-05-17 05:31:28.047734+03","Deployments default to useast1 region and used the default profile set on the machine where the serverless command is run Perhaps you dont have permission to deploy is that region or serverless is using a different profile than intended e g If i run serverless from an EC2 and login separately it would still use the default profile i e the EC2 instance Profile Can you update your serverless yml file to include the region as well "
73914568,73874712,"stackoverflow.com",1,"2022-10-01 01:41:17+03","2024-05-17 05:31:28.049735+03","When I tried to create a lambda function manually from the AWS website I found that I have no permission to view or create any lambda function And after that I found that my account was suspended due to a behavior I have done that is not acceptable in AWS policy I have followed the steps the support has sent me and then my account was back and everything worked fine"
74739296,73962939,"stackoverflow.com",2,"2022-12-09 07:41:07+02","2024-05-17 05:31:31.050471+03","I am doing a serverless deploy through github actions and ran into the same error message Hopefully this will help you and if not I think it will help others It looks like the serverlesspythonrequirements plugin will attempt to spawn whatever is in provider runtime so in your case and mine python3 8 You can override this in your serverless yml as such If your image has a python executable it will use this and it should work I realised that despite the fact I had installed the correct target python version in my case with actionssetuppython the plugin would not find it so I am thinking it is running somewhere separate Some people on SOgithub issues say that they fixed the issue by modifying their provider runtime to python3 7 or python3 9 which makes sense if their image has the corresponding executable "
73971385,73971157,"stackoverflow.com",1,"2022-10-06 12:18:41+03","2024-05-17 05:31:32.552701+03","Yes but then the Lambda will finish executing before waiting for DB call finishing It will not guarantee that the call to DB will reach the DB so it is not recommended to do so If you absolutely need this drop the await statement in this line From To Yes you need to initialize your pool outside of Lambda handler Check out this article for more details httpsdashbird ioblogleveraginglambdacacheforserverlesscostefficiency Basically you want to do instead of Do this"
73989094,73985623,"stackoverflow.com",5,"2022-10-07 17:57:33+03","2024-05-17 05:31:33.926127+03","I found a pretty ugly solution If anyone can think of a more elegant one I am still very open to it and happy to give you the points for a correct answer To solve this I just took a very naive approach and manually copied over the schema prisma file from the prisma directory into the srcfunctionsusers Heres the file structure I now had This is obviously a horrible way to solve this because I now have two Prisma schema files in different locations and have to make sure I always update the one in srcfunctionsusersschema prisma after changing the original one in prismaschema prisma to keep them in sync Once I copied this file and redeployed the schema prisma file was in place in the right location in the AWS Lambda and the error went away and PrismaClient could be instantiated I then added a simple Prisma Client query into the handler and encountered a new error this time about the query engine I am familiar enough with Prisma to know that Prisma Client depends on a query engine binary that has to be built specifically for the platform Prisma Client will be running on This can be configured via the binaryTargets field on the generator block in my Prisma schema The target for AWS Lamda is rhelopenssl1 0 x So I adjusted the schema prisma file in both locations accordingly After that I ran npx prisma generate to update the generated Prisma Client in node_modules However this had not resolved the error yet the problem still was the Prisma Client could not find the query engine binary So I followed the same approach as for the schema prisma file when it was missing After I redeployed and tested the function another error occured This time it was pretty straightforward and I went into the AWS Console at httpsuseast1 console aws amazon comlambdahomeregionuseast1functionsawsnodetypescriptdevuserstabconfigure and added a DATABASE_URL env var via the Console UI pointing to my Postgres instance on Railway"
75146425,73985623,"stackoverflow.com",4,"2023-01-17 14:29:01+02","2024-05-17 05:31:33.931128+03","We ran into the same issue recently But our context is slightly different the path to the schema prisma file was vartasknode_modules prismaclientschema prisma We solved this issue by using Serverless Package Configuration serverless yml This way only the src folder containing the lambda functions and the node_modules folder containing these two Prisma files were packaged and uploaded to AWS Although the use of serverless package include and serverless package exclude is deprecated in favor of serverless package patterns this was the only way to get it to work "
74781262,73985623,"stackoverflow.com",2,"2022-12-13 09:27:23+02","2024-05-17 05:31:33.932647+03","I usually lurk but the answer above lead me to come up with a reasonably elegant solution I felt I should share for the next poor sap to come along and try to integrate serverless and prisma in Typescript though I bet this solution and process would work in other build systems I was using the example awsnodejstypescript template which is plagued with a bug which required me to apply the fix here by patching the local node_modules serverless package I then had to essentially walk through nburks answer to get myself up and running which is as stated inelegant In my travels trying to understand prismas behavior requirement of a platformspecific binary and how to fix it figured that if I could manually sideload the binary into the build folder postcompile I could get the serverless bundler to zip it up I came across the serverlesspluginscripts plugin which allows us to do exactly this via serverless lifecycle hooks I put this in my serverless ts plugins [serverlessesbuild serverlesspluginscripts] I put the following in package json and this also in my serverless ts This causes the serverlesspluginscripts plugin to call my postbuild yarn script and fixup the build folder that esbuild creates I imagine that if your build system such as webpack or something creates the build dir under a different name such as lib this process could be modified accordingly I will have to create a yarn script to do this for each function that is individually packaged however this is dynamic precludes the need to keep multiple copies of schema prisma in source and copies the files from the dynamically generated prisma folder in node_modules Note I am using yarn workspaces here so the location of your node_modules folder will vary based on your repo setup Also I did run into this error Please make sure your database server is running at which was remedied by making sure the proper security groups were whitelisted outbound for the lambda function and inbound for RDS Also make sure to check your subnet ACLs and routetables "
74647201,73985623,"stackoverflow.com",0,"2022-12-01 21:24:17+02","2024-05-17 05:31:33.936184+03","An option is to use webpack with the copywebpackplugin and change the structure of your application put all handlers inside the handlers folder Folders structure webpack config js If you need to run the npx prisma generate before assembling the package you can use the plugin serverlessscriptableplugin put before webpack Dependences"
73998732,73989647,"stackoverflow.com",0,"2022-10-08 19:37:09+03","2024-05-17 05:31:34.971161+03","Set the include option in the package section of your sls configuration Also update the handler value to one that is compatible for the importlib importmodule When you run the following command to package your functions sls package You should see the python sources included in the archive "
74016974,74008254,"stackoverflow.com",1,"2022-10-10 18:07:52+03","2024-05-17 05:31:36.042035+03","Unfortunately rabbitmq event is only available for aws provider and you cannot use it with Azure "
74010121,74008781,"stackoverflow.com",1,"2022-10-10 07:06:41+03","2024-05-17 05:31:37.094978+03","According to the serverless docs you can specify the Dockerfile to use with file Set path to root of your repo and file to the specified Dockerfile "
74107732,74008781,"stackoverflow.com",1,"2022-10-18 11:10:18+03","2024-05-17 05:31:37.096979+03","Can you try COPY src LAMBDA_TASK_ROOT to COPY src LAMBDA_TASK_ROOT btw these steps I think should locate in local instead of curl to download for better performance after that use cp instead of mv"
74017050,74015707,"stackoverflow.com",2,"2022-10-10 18:14:42+03","2024-05-17 05:31:38.664994+03","It looks like you are using an externally configured HTTP API I am guessing from the id being set In such a situation you cannot configure authorizers in this manner you can only do so when you are provisioning HTTP API as a part of your serverless service What you can do there is to setup a shared authorizer in a more manual way as described in docs here httpswww serverless comframeworkdocsprovidersawseventshttpapisharedauthorizer"
75302832,74015707,"stackoverflow.com",0,"2023-01-31 22:33:40+02","2024-05-17 05:31:38.666989+03","I came across this post when researching how to use API gateway authorizers and serverless framework I was terraforming the API gateway therefore needed to terraform the authorizer as well When created I stashed the authorizer ID in a parameter store entry This is a 6 character alphanumeric value such as tw9qgj I then referenced the parameter as follows Then added the following block to each API e g "
74025828,74024848,"stackoverflow.com",0,"2022-10-11 12:23:14+03","2024-05-17 05:31:38.829139+03","First of all you will need to setup different group with dedicated security groups granting rights to your users Heres a Cloudformation template for instance it references default OrganizationAccountAccessRole but you mightshould create your own with minimal access Then MYUSER might use this Role through his awscredentials like Once again you might update OrganizationAccountAccessRole to your very own Role Finally during deployment you might use this profile with Which I recommend to set in package json directly Hope this helps and clarifies how you should grant rights and access through the whole deployment process "
74046694,74035195,"stackoverflow.com",2,"2022-10-12 21:49:11+03","2024-05-17 05:31:41.433488+03","It turns out the response I quoted actually works for me In the same serverless yml file put the GatewayResponses you want to configure At least for version 3 20 of serverless framework the created API name is ApiGatewayRestApi this will only work of course if you are only creating one API Gw in that file and I believe the API Gateway has to be already deployed The correct way to do this is as the official serverless documentation states to create the API Gateway with Cloudformation code "
74050446,74039449,"stackoverflow.com",0,"2022-10-13 07:19:58+03","2024-05-17 05:31:42.406674+03","With Cloudformation you can declare a parameter at the top of the Cloudformation File Make the parameter value default to the environment variable According to the Docs Define the Type as a number From the AWS Cloudformation Docs Number An integer or float AWS CloudFormation validates the parameter value as a number however when you use the parameter elsewhere in your template for example by using the Ref intrinsic function the parameter value becomes a string For example users could specify 8888 List An array of integers or floats that are separated by commas AWS CloudFormation validates the parameter value as numbers however when you use the parameter elsewhere in your template for example by using the Ref intrinsic function the parameter value becomes a list of strings For example users could specify 8020 and a Ref would result in [8020] "
74057468,74057313,"stackoverflow.com",2,"2022-10-13 17:32:48+03","2024-05-17 05:31:43.23044+03","I already fixed it Just by changing my DateTimeZone from Hongkong to Singapore UTC8 and It worked for me "
74057401,74057313,"stackoverflow.com",0,"2022-10-13 17:21:58+03","2024-05-17 05:31:43.23244+03","Any chance you are located in China serverless Getting Started guide makes explicit mention of this as a feature Note users based in China get a setup centered around the chinese Tencent provider To use AWS instead set the following environment variable SERVERLESS_PLATFORM_VENDORaws As it mentions simply set the environment variable as per their instructions to bypass this "
74375532,74312759,"stackoverflow.com",2,"2022-11-09 15:33:14+02","2024-05-17 05:31:44.994843+03","It is quite hard to understand where is the problem with given context We have no idea which image format you are uploading no idea how you store this image to S3 My answer will try to cover these missing informations as it is a common mistake on S3 uploads S3 files are stored and returned with given ContentType You might check your S3 files ContentType on AWS console Console S3 Select object image Metadata ContentType I will suppose that image format is PNG and image data is correct and might be posted to S3 as is from result S3Service ts index ts A common mistake which I assume might be the cause of your problem is to forget contenttype resulting in incorrect download format "
74350048,74314531,"stackoverflow.com",0,"2022-11-10 00:03:59+02","2024-05-17 05:31:45.984783+03","The resolution to this question was to use promises in the code Something like this"
74350892,74325421,"stackoverflow.com",0,"2022-11-07 19:55:22+02","2024-05-17 05:31:46.96073+03","You do not need to use ECR but docker daemon has to be running on the machine Docker container will be launched to actually build your dependencies with serverlesspythonrequirements plugin You can also try specifying dockerizePip nonlinux as it might not be needed to dockerize packaging when running on linux machine but I would advise to test it first on nonprod environment "
74339016,74327538,"stackoverflow.com",0,"2022-11-06 21:22:29+02","2024-05-17 05:31:48.095912+03","it is possible to retrieve them using sls param list or sls param get name nameofparam commands You will have to parse the output of these commands to set the environment variables as that is not done automatically "
74365682,74335452,"stackoverflow.com",3,"2022-11-08 20:56:23+02","2024-05-17 05:31:49.101853+03","Issue was in the serverlesscompose ts Configuration in path clientservice there is a single inverted comma which is a syntactical error causing this The correct snippet is as follows"
74418201,74349790,"stackoverflow.com",1,"2022-11-13 05:12:36+02","2024-05-17 05:31:50.267588+03","I also stumbled across this issue when I was working with serverless I initiated the project using serverless and then selected the template as Node js HTTP API Everything is working as expected in postman but not in my FE which is built using Angular 10 But then I came across the Node js Express template provided by serverless It contains bare minimum Express js code with availability of configuring CORS as dependency just like classical Node js server I have tried it and it worked like a charm But Before using it keep in mind"
74406370,74349790,"stackoverflow.com",1,"2022-11-11 19:50:31+02","2024-05-17 05:31:50.26959+03","If you want default setting adding the following snippet should do the trick For more detailed cors settings and also for reference what the shortcut above will do please refer to httpswww serverless comframeworkdocsprovidersawseventshttpapicorssetup"
74425978,74349790,"stackoverflow.com",0,"2022-11-14 02:57:56+02","2024-05-17 05:31:50.270756+03","The settings that you have provided is not compliant with CORS specifications In order to allow your client to send credentials AccessControlAllowCredentials header has to be set to true You have done this step correctly But if you are setting it to true your AccessControlAllowOrigin cannot be wildcard It has to be the specific domain One way to solve your issue is to Refer to this SO for steps involved You can refer to this SO post to implement the header setting and caveats on doing it this way Why OPTIONS is needed Depending upon the API call browser will trigger a preflight request before making the actual API call This comes as an OPTIONS request to your API gateway and it should return correct headers for your browser to make the actual call For understanding this process better refer to this MDN article"
74350846,74350371,"stackoverflow.com",1,"2022-11-07 19:51:06+02","2024-05-17 05:31:51.222235+03","Serverless Framework has support for stages out of the box You do not need a separate configuration you can just specify stage nameofstage when running e g sls deploy and it will automatically use that stage All resources created by the Framework under the hood are including stage in it is names or identifiers If you are defining some extra resources in resources section you need to change them or make sure they include stage in their names You can get the current stage in configuration with slsstage and use that to construct names that are e g prefixed with stage "
74366129,74357690,"stackoverflow.com",1,"2022-11-08 21:23:18+02","2024-05-17 05:31:52.057114+03","Passing param flags will not upload the parameters to DashboardConsole it will only expose them in your configuration so you can access them with paramparamname To my best knowledge it is not possible to set Dashboard parameters with CLI you need to set them manually via UI "
74370757,74357690,"stackoverflow.com",0,"2022-11-09 08:33:18+02","2024-05-17 05:31:52.059114+03","It was a permissions problem The owner of the account updated the permissions and I was able to update the inputs "
75449566,75340442,"stackoverflow.com",0,"2023-02-14 16:52:25+02","2024-05-17 05:32:36.788097+03","This was resolved our naming we were using for both services was the same for some reason and so when one deployed it overwrote the other in cloudformation "
74387789,74387788,"stackoverflow.com",0,"2022-11-10 12:52:09+02","2024-05-17 05:31:52.766475+03","httpswww serverless compluginsserverlesspluginawsalerts It is described here under Custom Naming You can define a custom naming template for the alarms nameTemplate property under alerts configures naming template for all the alarms while placing nameTemplate under alarm definition configures overwrites it for that specific alarm only Naming template provides interpolation capabilities where supported placeholders are Note All the alarm names are prefixed with stack name e g fooservicedev So"
74401165,74389281,"stackoverflow.com",0,"2022-11-11 12:30:02+02","2024-05-17 05:31:53.728839+03","I got it working though I do not really know what I am doing differently Successful methodology That should get it working You can trouble shoot by going over to your Cloudwatch Console Click All Alarms You should be able to see that your alarms have been created here like this The names follow the instruction in the above serverless yml file nameTemplate [functionName][metricName]Alarm Click the alarm name and in the top right of the next screen click ActionsEdit You can see that the FunctionName refers to your Lambda function Scroll down and click the Next button You can see at the bottom of the Notifications section of the next screen which Topic the Alert is sending to It says mypdevalerts So we know that this matches the Topic I created "
74405653,74402748,"stackoverflow.com",1,"2022-11-11 18:42:18+02","2024-05-17 05:31:54.501646+03","Turns out event type kafka is not supported in SeverlessFramework version 1 I had to update to version 3 and update the serverless yml to refere to FrameworkVersion 3 And was able to Deploy Lambda with Apache Kafka as Tigger "
74465851,74463455,"stackoverflow.com",2,"2022-11-16 21:02:32+02","2024-05-17 05:31:57.26222+03","DynamoDB writes are all serialized Meaning that if you have 2 concurrent writes which update a value the end state of that value will consider all updates that happened Consider an item which has a counter with a value of 4 count4 Not two concurrent processes update this value to increase it by the value of 2 each count2 The outcome would be that the value of count would be 6 when all processes have completed count6 However do be aware that this is not idempotent meaning that if your lambda happened to retry for any given reason your counter could even up over value To overcome this you can either use conditional checks on the count attribute or implement versioning httpsdocs aws amazon comamazondynamodblatestdeveloperguideDynamoDBMapper OptimisticLocking html"
75434986,74475122,"stackoverflow.com",0,"2023-02-13 13:05:24+02","2024-05-17 05:31:57.790252+03","You have to write the swagger configurations under custom in serverless yml file As a example Check the official documentation httpswww npmjs compackageserverlessautoswagger"
74476470,74475776,"stackoverflow.com",2,"2022-11-17 15:40:20+02","2024-05-17 05:31:58.613723+03","SQS is not a source for EventBridge Also EventBridge it is not working on message content but more on management events You can find a list of sources for EventBridge httpsdocs aws amazon comeventbridgelatestuserguideebserviceevent html"
74503004,74499839,"stackoverflow.com",0,"2022-11-19 21:26:05+02","2024-05-17 05:31:59.513301+03","Middy uses JSONSchema so you can use anything that is compatible with JSONSchema there You could use length but then you would need to switch the type to string from number as length is not supported for number rightfully so in my opinion If you want to keep it as number then probably using rangebased validation is your best bet httpsjsonschema orgunderstandingjsonschemareferencenumeric htmlrange"
74511942,74506308,"stackoverflow.com",0,"2022-11-20 22:57:35+02","2024-05-17 05:32:00.63658+03","It is a valid approach however not recommended in the long run for production systems as it has a few potential issues Though if it is just your personal project that approach should work just fine for you and will be relatively safe The recommended way to do it though is to fetch and decrypt the secrets at runtime as described in 4 of the cited article "
74518305,74518121,"stackoverflow.com",2,"2022-11-21 13:37:47+02","2024-05-17 05:32:01.907173+03","Serverless Framework has builtin support for this exact case please see how to use Stage Parameters the example in docs refers exactly to setting an environment variable that differs between stages httpswww serverless comframeworkdocsguidesparametersstageparameters"
74561744,74561049,"stackoverflow.com",0,"2022-11-24 15:43:36+02","2024-05-17 05:32:02.695372+03","Not sure if I understood your question but Since you are requesting data associated with a primary key you will get either 0 or 1 element in Item So if you aim to know if you have found something or not you can use Number response Item null and you will get 1 in case of something and 0 in case of nothing If instead your data contains a count attribute then await pullone sessionId count should work Otherwise you have to query your DB but you will get Items plural in your response and use the length function of the Items array you will get in the response "
74576360,74576246,"stackoverflow.com",1,"2022-11-25 19:57:36+02","2024-05-17 05:32:03.811676+03","Cloudformation is a service provided by AWS while Serverless is a thirdparty productsolution that makes infrastructureascode IaC possible There are others too for example Terraform When deploying resources in AWS Serverless almost always employs Cloudformation The main differentiator being Serverless is more polished has associated ecosystem of tools to build manage and monitor the infrastructure Cloudformation does the deployment part while Cloudwatch does the monitoring AWS SAM is another option compared to writing vanilla Cloudformation code which is more closer to Serverless where a lot of boilerplate code is abstracted away at the end like Serverless AWS SAM also builds Cloudformation stack "
75236387,74577243,"stackoverflow.com",0,"2023-01-25 17:44:46+02","2024-05-17 05:32:04.894771+03","I needed to reinstall serverlesss for that I deleted the folder serverless in the directory and reinstalled using homebrew httpsformulae brew shformulaserverlessdefault"
74622021,74592647,"stackoverflow.com",0,"2022-11-30 03:40:48+02","2024-05-17 05:32:06.029322+03","If you want to kick off a Processing Job from Lambda you can use boto3 to make the CreateProcessingJob API call httpsboto3 amazonaws comv1documentationapilatestreferenceservicessagemaker htmlSageMaker Client create_processing_job I would suggest creating the Job as you have been doing using the SageMaker SDK Once created you can describe the Job using the DescribeProcessingJob API call httpsboto3 amazonaws comv1documentationapilatestreferenceservicessagemaker htmlSageMaker Client describe_processing_job You can then use the information from the DescribeProcessingJob API call output to fill out the CreateProcessingJob in Lambda "
74615940,74606640,"stackoverflow.com",1,"2022-11-29 16:52:37+02","2024-05-17 05:32:07.083799+03","Since v3 release of Serverless Framework v is no longer a shortcut for verbose flag In order to keep the old behavior use sls deploy verbose You can see that change on the list of breaking changes for v3 release httpsgithub comserverlessserverlessblobmainCHANGELOG mdbreakingchanges"
74615916,74615308,"stackoverflow.com",2,"2022-11-29 16:51:05+02","2024-05-17 05:32:08.323765+03","No the dependencies are a part of the deployment artifact e g a ZIP file or container image in the case of AWS Lambda so they do not have to be installed on each invocation I understand that serverfull is like my computer running code How can I describe serverless using the same analogy That is not going to be a perfect explanation but hopefully it fits your analogy Imagine that your computer is sleeping but there is another computer that can receive requests and wake up your computer whenever it receives a new one so it can be run on your computer After it finishes running it goes back to sleep But instead of a single computer there are many of them that can be brought from sleep in a matter of milliseconds Hope that makes sense "
74793121,74628409,"stackoverflow.com",0,"2022-12-16 23:54:56+02","2024-05-17 05:32:09.146873+03","I was able to resolve this issue I did exactly what you did above but with the following configuration Note I also ran dynamodb locally Please follow the instructions to run dynamodb locally here Deploying DynamoDB locally on your computer"
75599238,74628409,"stackoverflow.com",0,"2023-03-01 05:07:03+02","2024-05-17 05:32:09.148873+03","To use tyoeDORM with nestJS you can try out my package here httpsgithub comnestdynamodbtypedorm In this way you can inject TypeDORM as a singleton provider in your app First install it or Your app module ts will look like this inject the typeDorm as an imported module to use the typeDorm connection in your service or controller the typeDorm module is a global module so that you can inject it into your app module and use it everywhere Please feel free to open issues or PRs if there is any problem "
74648185,74634151,"stackoverflow.com",3,"2022-12-01 23:07:59+02","2024-05-17 05:32:09.922165+03","A few possible causes here To verify this all works normally I set up a simple test with both FIFO and nonFIFO queues and configured the queues to trigger a Lambda function that simply logged the SQS message and then threw an exception As expected I saw the same SQS message delivered to the Lambda function every 2 minutes which is the queues message visibility timeout That continued until it hit the max receive count on the SQS redrive policy defaults to 10 attempts at which point the failed message was correctly moved to the associated DLQ "
74643617,74640748,"stackoverflow.com",0,"2022-12-01 16:38:19+02","2024-05-17 05:32:11.140884+03","The Docker does not running while you are using orbs awscli and serverlessframework So the solution will be to build it by your own this is circleciconfig yml And serverless yml file is"
77320478,74640748,"stackoverflow.com",0,"2023-10-19 04:43:10+03","2024-05-17 05:32:11.142885+03","To solve your problem of being able to run Docker in your CircleCI job you need to add setup_remote_docker to your config yml file httpscircleci comdocsbuildingdockerimages The next problem you might hit could be that it cannot find the docker image since it has the architecture of x86_64 which may not be what your CircleCI job is using To avoid this add this to the serverlesspipelineplugin options which removes the architecture from the image name "
74661351,74639793,"stackoverflow.com",1,"2022-12-02 22:55:57+02","2024-05-17 05:32:11.221524+03","SnapStart is only available for published versions of a Lambda function It cannot be used with LATEST Using Versions is pretty hard for Serverless Framework SAM CDK and basically any other IaC tool today because by default they will all use LATEST to integrate with API Gateway SNS SQS DynamoDB EventBridge etc You need to update the integration with API Gateway or whatever service you are using to point to the Lambda Version you publish after that Lambda deployment has completed This is not easy to do using Serverless Framework and other tools You may be able to achieve this using this trafficshifting plugin "
74719820,74639793,"stackoverflow.com",1,"2022-12-07 18:22:46+02","2024-05-17 05:32:11.22345+03","In case you use stepFuntions to call your lambda function you can set useExactVersion true in This will reference the latest version of your function you just deployed"
74737838,74639793,"stackoverflow.com",0,"2022-12-10 03:25:20+02","2024-05-17 05:32:11.224447+03","This has got to be one of the worst feature launches that I have seen in a long time How the AWS team could put in all the time and effort required to bring this feature to market while simultaneously rendering it useless because we cannot script the thing is beyond me We were ready to jump on this and start migrating apps to lambda but now we are back in limbo Even knowing there was a fix coming down the line would be something Hopefully somebody from the AWS lambda team can provide some insights Here is a working POC of a serverless plugin that updates the lambda references to use the most recent version This fixed the resulting cloud formation code and was tested with both SQS and APIGateway "
74899917,74639793,"stackoverflow.com",0,"2022-12-23 14:59:38+02","2024-05-17 05:32:11.226873+03","I was able to achieve this by updating my serverless version to 3 26 0 and adding the property snapStart true to the functions that i have created currently serverless creates version numbers and as soon as the new version is published the SnapStart gets enabled to latest version "
74724556,74724422,"stackoverflow.com",0,"2022-12-08 03:24:01+02","2024-05-17 05:32:12.102577+03","You have defined SQS_URL variable Error message says it cannot find SQS_QUEUE_URL If it is still working after fixing variable names Try with a given environment to narrow it down "
75167787,74727619,"stackoverflow.com",1,"2023-01-19 06:33:08+02","2024-05-17 05:32:13.940461+03","It looks as though there is no handling for resources of type awssdksfnstartSyncExecution in the serverless step function plugin defined here It may be possible to add handling to the plugin something like this pull request "
74830160,74728661,"stackoverflow.com",1,"2023-07-28 00:05:27+03","2024-05-17 05:32:15.023614+03","You can enable changesets with deploymentMethod changesets so that serverless deploy does not actually execute the changes but instead creates a changeset inside CloudFormation which you can inspect inside the console and then initiate from there Edit This is unfortunately no longer supported in the latest versions of Serverless as of 2023 "
74741719,74737868,"stackoverflow.com",1,"2022-12-09 12:18:49+02","2024-05-17 05:32:15.953965+03","Found the issue PayloadFormatVersion should be in quotes This solved the issue for me "
75065972,75054129,"stackoverflow.com",2,"2023-01-10 07:51:58+02","2024-05-17 05:32:17.037697+03","you will have to enable CORS on API Gateway as well When click on the resource endpoint on API Gateway on actions there is Enable CORS That will add Options method also for your resource If you want some customization you will have to add OPTIONS method manually"
75098571,75097659,"stackoverflow.com",1,"2023-01-12 17:18:31+02","2024-05-17 05:32:17.781913+03","The solution was to use the noAuth option"
75098476,75097872,"stackoverflow.com",0,"2023-01-12 17:11:37+02","2024-05-17 05:32:18.742533+03","The solution was to use noPrependStageInUrl like this"
77139274,75101030,"stackoverflow.com",0,"2023-09-20 07:23:30+03","2024-05-17 05:32:19.822395+03","I am facing a similar scenario In my case I created a bastion host in the same VPC public subnet that can be accessed from my CICD tool From the runner of the CICD tool I created an SSH tunnel to access the RDS Serverless Aurora in the same VPC a private subnet and NOT publicly accessible and from there I am able to execute Prisma commands Hope this helps"
75103674,75103380,"stackoverflow.com",0,"2023-01-13 02:26:40+02","2024-05-17 05:32:20.930379+03","I recently encountered something similar in a react native app I was trying to send a local file to an api but it was not working turns out you need to convert the blob file into a base64 string before sending it What I had in my app took in a local file path converted that into a blob went through a blobToBase64 function and then I called the api with that string That ended up working for me I have this code snippet to help you but this is tsx so I do not know if it will work for angular Hope this helps "
75127411,75103380,"stackoverflow.com",0,"2023-01-15 20:24:28+02","2024-05-17 05:32:20.932272+03","You can convert your Blob to a File using new File [blob] filename and then you should be able pass that file to your existing uploadImage method "
75127525,75103380,"stackoverflow.com",0,"2023-01-16 00:07:13+02","2024-05-17 05:32:20.933597+03","Looks like you are passing Blob instead of File based on your console log So you should convert Blob to a File before calling the server You can change your frontend code like this Note For more info about converting Blob to File you can check this StackOverflow question "
76636868,75342712,"stackoverflow.com",0,"2023-07-07 15:09:49+03","2024-05-17 05:32:37.535719+03","I used this serverless plugin for link a cloudfront distribution resource with a lambda function Serverless Plugin Support CloudFront Lambda It worked for me"
75129572,75103380,"stackoverflow.com",0,"2023-01-16 17:02:50+02","2024-05-17 05:32:20.934892+03","The thing that got it working for me was this article There might be something different about using Express through Serverless Framework so things like mutler and expressfileupload might not work Or could be because it is an AWS Lambda function I do not know this for sure though I just know I never got it working This article was the only thing that worked for Serverless Framework Express I also had to install version 0 0 3 of busboy ie npm i [email protected] The newer version did not work for busboy Newer version was saying Busboy is not a constructor Since I am sending the file to discord and not S3 like this article does I had to tweak the parser event part in this part of the article for the handler ts comes in as a Buffer which I was able to send as a file like this I hope this helps someone "
75120410,75113327,"stackoverflow.com",0,"2023-01-14 20:39:12+02","2024-05-17 05:32:21.857747+03","Courtesy of jarmod Using the singleton approach does not necessarily make multiple invocations of the Lambda function use the same client instance If you trigger 500 invocations of your Lambda function simultaneously for example and you have sufficient configured concurrency then the Lambda service could create 500 execution environments hence no client reuse But if the Lambda service can reuse prior execution environments because they have completed the processing of prior requests then you will get client reuse One should balance the concurrency settings with the volume of requests they have Static Initialization or Singleton design pattern as to the constructor will optimize when the execution environment is being reused Whether that could throttle depends on the service used your configuration their rate limits and the volume of requests "
75153552,75143941,"stackoverflow.com",3,"2023-01-18 02:20:05+02","2024-05-17 05:32:22.799802+03","The Serverless Google Cloud Functions plugin is actually poorly documented NO You do not need to write your own plugin for that Serverless Google Cloud Functions does accept references to Secrets Manager out of the box and secrets are exposed as environment variables to your Cloud Functions If you just look into the code itself httpsgithub comserverlessserverlessgooglecloudfunctionsblob4e59429ad2857cbc8d95ce70db6b41bed76b67adprovidergoogleProvider jsL160 Notice the functions schema accepts a property named secrets The implementation would look something like this Hope this is what you were looking for "
75188359,75174454,"stackoverflow.com",0,"2023-01-20 21:02:10+02","2024-05-17 05:32:23.604771+03","Fixed it Problem was that I was only allowing access to mongoDB cluster to requests sent only from my IP Changed cluster network access settings and it works now as it should "
75188846,75188028,"stackoverflow.com",0,"2023-01-20 22:08:12+02","2024-05-17 05:32:25.712342+03","I believe this because you are returning a promise I think it can be solved by inputting a placeholder and then exchanging that with what you actually want there once the promise has been resolved You could do this with states If you are using TypeScript replace the useState for You could also do the same with a separate component for handling AWS stuff App sx getBlog sx I have no experience using AWS but I believe you should be able to handle your promise doing something like"
75214183,75193068,"stackoverflow.com",0,"2023-01-24 22:19:30+02","2024-05-17 05:32:26.932989+03","It happened here with the new version of serverlessoffline v12 0 4 My solution was to use httpswww serverless compluginsserverlesspluginifelse See the example below You can change it for your use case "
75221942,75219463,"stackoverflow.com",0,"2023-01-24 15:08:37+02","2024-05-17 05:32:27.676416+03","It is due to AWS limitations You can find it on GlueJob Command properties here An easy way to fix this would be to upload file to s3 during deployment and use the uploaded file through your serverless config "
75269501,75263232,"stackoverflow.com",0,"2023-01-28 20:51:59+02","2024-05-17 05:32:31.404476+03","You can see an example of the Dockerfile at httpsbref shdocswebappsdocker html mine is the same as the example except it also adds Chrome and Chromedriver Heres the handler PHP Testing this Same connection ID both times As far as I understand and that is not too far this is probably just a warm start of Lambda but that is sufficient for my purposes "
75318587,75292317,"stackoverflow.com",3,"2023-02-04 04:55:55+02","2024-05-17 05:32:31.868647+03","I managed to resolve this with a rather niche Django settings option FORCE_SCRIPT_NAME While this does not explain why it is resolving the path to default it does mitigate this issue If you are using this for a different use case than mine and your path is in a subdirectory e g the opposite issue to mine then you would add your path in FORCE_SCRIPT_NAME instead See the link I have provided for more information Add the following to your apps settings py or any file you are using as your DJANGO_SETTINGS_MODULE "
75297561,75297437,"stackoverflow.com",0,"2023-01-31 15:00:49+02","2024-05-17 05:32:32.814571+03","as for this error You get this error because the Framework cannot find SLS_AWS_REGION environment variable The env variable source does not read from env files but from environment variables from the process As for this syntax This does not work because env is a correct variable source not env You have two options here"
75308352,75297437,"stackoverflow.com",0,"2023-02-01 11:54:23+02","2024-05-17 05:32:32.815571+03","According to the plugin documentation you should run sls deploy with stage or env deprecated in Serverless 3 0 0 or NODE_ENV corresponding to the env file name If you run it without any of those flags it will default to development and the plugin will look for the plugin will look for files named env env development env development local Note that Serverless Framework env files resolution works differently to the plugin The framework looks for env and env stage files in service directory and then tries to load them using dotenv If env stage is found env will not be loaded If stage is not explicitly defined it defaults to dev I believe the plugin takes precedence here "
75542229,75297437,"stackoverflow.com",0,"2023-02-23 10:25:11+02","2024-05-17 05:32:32.818573+03","Essentially what I had to do was resolve my own env variables manually I used this link as a reference and the explanation in the dotenv plugin of serverless TLDR If you are using a serverless version beyond serverless2 26 0 then you will most likely need to wire up the envs on your own if you want to relocate your env files"
75310299,75302391,"stackoverflow.com",0,"2023-02-01 14:37:51+02","2024-05-17 05:32:33.723086+03","Solution You do not have to switch the region while creating the cloudformation stack just specify the region while deploying the application using sls deploy region cacentral1 "
75320242,75310700,"stackoverflow.com",1,"2023-02-08 10:16:32+02","2024-05-17 05:32:34.654804+03","The api key values for both stages need to be different"
75326641,75326640,"stackoverflow.com",1,"2023-02-02 19:09:17+02","2024-05-17 05:32:35.812686+03","It seems this was related to my npm version I had 7 24 2 locally but on GHA I got 6 x due to that being the default npm version in Node 14 Possibly this is related to a difference in how optionalDependencies are handled The solution was to add a step that upgrades npm on GHA There is a request on actionssetupnode to support npm_version but for now it is not supported and the suggested workaround is the above "
76884321,75326640,"stackoverflow.com",0,"2023-08-11 17:11:03+03","2024-05-17 05:32:35.814686+03","For me it was the node version used in the jenkins file in the project so if you go to jenkins Global Tool Configuration NodeJS installations you can see all your node versions used for different projects The name which we give for the node version it can be anything is used in our jenkins files Below name you can see the node version Locally I was using node 16 4 and on jenkins it was 14 8 Issue pop up once I delete and recreate the packagelock json file after updating some packages "
75492910,75376221,"stackoverflow.com",1,"2023-02-18 13:07:22+02","2024-05-17 05:32:38.479455+03","I have taken a look at the code and saw some issues You are missing a path to the uri field in xamazonapigatewayintegration Also missing request mapping template in the OpenAPI schema I believe you need this to map JSON payload from POST request to input for the step function Something like this"
75393328,75393064,"stackoverflow.com",0,"2023-02-09 03:45:51+02","2024-05-17 05:32:39.484181+03","After much searching I went to AWSs community and found this post This worked and I no longer get the 404 Only issue is I have to do this manually and cannot do this via Serverless Framework I will post a comment here if I figure it out Edit To set this automatically up with serverless framework you need to use the default path which can only be done as such Now you are custom domain API mapping will work without having to deploy and then set the default route for each API "
75495593,75396702,"stackoverflow.com",2,"2023-02-18 20:45:21+02","2024-05-17 05:32:40.504719+03","I am guessing from those UPDATE_FAILED s you are using the same serverless file for both dev and prod deployment Based on this assumption you may have to provide separate service names for both of your deployments If you have deployed to the dev environment already with service name comebyschedulerapi the next deployment for prod stage with the same service name will try to override the previous deployment In my case I tackled this using 2 separate serverless configuration files one for dev and the other for prod For dev deployment my config file serverlessdev yml looks like the following Whereas for the prod the serverlessprod yml file is My deployment commands for these separate stages are "
77126081,75396702,"stackoverflow.com",2,"2023-09-18 12:39:12+03","2024-05-17 05:32:40.50672+03","This error also occured for me because I had previously setup another unrelated service with the same name and environment Serverless tried to update the existing cloudformation template but failed with the above message because the previously created resource and updated resources were not related in any way The solution for me was to find and delete the old cloud formation template because I did not actually need it anymore but an alternative would have been to change environment name"
76531301,75398850,"stackoverflow.com",2,"2023-06-22 14:11:56+03","2024-05-17 05:32:41.420957+03","I was facing the same problem after some research I found out that I needed to install the cryptography library by running pip install cryptography I hope it will work on your end too "
75420398,75401917,"stackoverflow.com",0,"2023-02-11 15:05:08+02","2024-05-17 05:32:42.114986+03","I found type definitions here which extends the AWS type from serverlesstypescript"
75451444,75422633,"stackoverflow.com",2,"2023-02-14 19:43:40+02","2024-05-17 05:32:43.216256+03","To answering my own question The way I resolve it via two ways 1 We can write whole Resolver code in YML Cloudformation in Code property like below Make sure your resolver code should be inside of your Code property and use special character Multiline code after Code property 2 If you want to keep your business logic out of YML file and keep it separate then you can use CodeS3Location property in your javascript resolver like below first create bucket in S3 and store your javascript resolver file with your resolver code in bucket make sure you give enough IAM permission to your appsync to access your S3 bucket After above step you can rewrite your YML Cloudformation like below Hope this help others and will contribute more about Javascript Resolver so it will be easier for other to find more complex solutions and get as much as resources about Javascript Resolver Thanks to Graham Hesketh for your suggestions "
75428607,75428372,"stackoverflow.com",1,"2023-02-12 18:51:32+02","2024-05-17 05:32:43.961326+03","You can perform a SNS publish operation within a Step Function See AWS Docs Connect to SNS from Step Functions"
75443448,75438699,"stackoverflow.com",2,"2023-02-15 00:18:54+02","2024-05-17 05:32:44.97808+03","The error is not finding a value at file double check the file path Make sure the path you are using in file thispathhere provider It is pointing to the correct file The following works Alternatively you can remove the provider from the file provider The following works For resources the same applies This works Alternatively you can do You can check the GitHub code for a working example"
75450351,75439910,"stackoverflow.com",0,"2023-02-14 18:01:24+02","2024-05-17 05:32:45.703048+03","So Serverless is not intelligent enough to know you have moved it you need to first tear down the original and then spin up the new one "
75639984,75639503,"stackoverflow.com",3,"2023-03-05 05:41:06+02","2024-05-17 05:32:47.478718+03","To understand what can be returned by intrinsic function references we need to inspect the CloudFormation resource you are using In your case when using httpApi it generates AWSApiGatewayV2Api and it returns only two things httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawsresourceapigatewayv2api htmlawsresourceapigatewayv2apireturnvalues Regardless of the resource name autogenerated by Serverless Framework you must build the ARN manually The ARN pattern is httpsdocs aws amazon comapigatewaylatestdeveloperguidearnformatreference html httpsdocs aws amazon comapigatewaylatestdeveloperguideapigatewaycontrolaccessusingiampoliciestoinvokeapi html You can check the name convention list of Serverless Framework in this page httpswww serverless comframeworkdocsprovidersawsguideresources Because we need to reference CloudFormation resources generated by Serverless Framework we need to use CloudFormation intrinsic function to transform the ARN above into And going back to your question There is no dynamic way to reference this resource ARN You need to build the ARN manually "
75645754,75645700,"stackoverflow.com",2,"2023-03-06 00:30:05+02","2024-05-17 05:32:48.483785+03","You are over complicating things Firstly a number type cannot be null as DynamoDB is schemaless you simply omit any value Secondly indexes can be sparse As you are not interested in items which are 0 then simply do not set a value for that which in turn will mean the item will not exist in the index Then you simply Scan the index as you know that all values that are there are not null and are not 0 In this case Scan is efficient as it is reading exactly what you want "
75663978,75661063,"stackoverflow.com",0,"2023-03-07 17:30:05+02","2024-05-17 05:32:50.035283+03","I still do not understand why but the error was not RedrivePolicy but the lambda events Replace this By that Would appreciate if someone explain it to me "
75739283,75712473,"stackoverflow.com",1,"2023-03-15 01:30:22+02","2024-05-17 05:32:50.50107+03","CodeArtifact aims to create managed artifact repository service You can use CodeArtifact with popular build tools and package managers such as the NuGet CLI Maven Gradle npm yarn pip and twine With that in mind sls package does not not create a valid package format for any of these build tools and pacakge managers The Serverless Framework CLI generates deployment artifacts for AWS These artifacts are uploaded to an Amazon S3 bucket and the deployment via AWS CloudFormation will reference these artifacts The use case you described is not the correct one for CodeArtifact If you want you can make it work Run sls package and modify the output folder to be a valid twinepypi package httpswww geeksforgeeks orghowtopublishpythonpackageatpypiusingtwinemodule but that is not what sls CLI was built for "
76767838,75724468,"stackoverflow.com",1,"2023-07-26 06:27:58+03","2024-05-17 05:32:51.636343+03","This has been raised as a bug and raised to P2 reported Mar 27 and still Open Workaround from comments As a workaround we just commented out the sns subscription resource so it can get deleted and then redeployed with the correct subscription and filter to recreate it Avoids a stack deletion as suggested above "
77088147,75954437,"stackoverflow.com",0,"2023-09-12 17:11:58+03","2024-05-17 05:33:10.862481+03","I got the same error even when trying with fetch and custom agent to disable handshake faillure I am using node v18 17 1 LTS and next 13 and this occurs only while using Host header"
76536024,75724468,"stackoverflow.com",0,"2023-06-23 01:24:26+03","2024-05-17 05:32:51.638343+03","When I encounter these quirks I take a tour of the AWS docs to see if something has been updated in one place like the CDK or API docs and not updated in the CloudFormation docs httpsdocs aws amazon comAWSCloudFormationlatestUserGuideawsresourcesnssubscription htmlcfnsnssubscriptionfilterpolicy aka CloudFormation does seem to say that it is not required and there is a default value httpsdocs aws amazon comsnslatestdgmessagefilteringapply htmlmessagefilteringapplycli aka the CLI seems to require it even though CF has a default value httpsdocs aws amazon comsnslatestdgmessagefilteringapply htmlmessagefilteringapplysdks aka the SDK does not seem to require it httpsdocs aws amazon comsnslatestapiAPI_SetSubscriptionAttributes html the API does not seem to require it as it also has a default value of MessageAttributes httpsgithub comawssamplesawssnssamplesblobmastertemplatesSNSSubscriptionAttributesTutorialCloudFormation template is an example CF template and it sets FilterPolicy but not FilterPolicyScope I have no idea what is going wrong I would guess this is a bug "
76048295,75771300,"stackoverflow.com",2,"2023-04-18 22:14:48+03","2024-05-17 05:32:53.53999+03","With some help from a colleague I got this working I can now do breakpoint debugging on Serverless Offline and with Serverless Invoke Local on Windows We also know how to do Mac and Linux I will share that as well Here is the launch json file The first config is for Serverless Offline the second config is for Invoke Local You may have to update the cwd to reach the directory with your serverless yml file Add this to your package json For MacLinux it is exactly the same except one thing In your package json runtimeExecutable must be the full path to your npm Use which npm to get the value "
76898961,75771300,"stackoverflow.com",1,"2023-08-14 15:32:44+03","2024-05-17 05:32:53.54199+03","As a followup to dcyprianrw make sure the outDir of your tsconfig matches the outDir used by serverlessplugintypescript Looks like the vscode debugger will search for sourcemaps in the outDir of your tsconfig json even if you point it in the right direction with outFiles in launch json From the serverlessplugintypescript docs If you want to use a custom tsconfig you will have to specify the path in your serverlesslocal yml"
75802366,75791104,"stackoverflow.com",1,"2023-03-21 17:44:02+02","2024-05-17 05:32:54.439649+03","Yes you can Just build image with your code and dependencies push to ECR and then deploy it as a containerized lambda "
75798443,75798348,"stackoverflow.com",0,"2023-03-21 09:44:06+02","2024-05-17 05:32:55.467394+03","Yes you can overrideextend the CloudFormation resources generated by Serverless Framework using resources extensions httpswww serverless comframeworkdocsprovidersawsguideresourcesoverrideawscloudformationresource Pseudocode You need to match the logical id generated by Serverless Framework In the page above we have a list of it but I often run sls package and verify in my output folder serverless the correct logical id"
75886721,75804593,"stackoverflow.com",0,"2023-03-30 12:57:39+03","2024-05-17 05:32:56.349859+03","You can try removing the responseType configuration from the serverless yml file as you are returning a json response Also try with JSON stringify your response before going with jsonstringifysafe to confirm whether the issue is related to the dependency "
75809276,75808616,"stackoverflow.com",0,"2023-03-22 09:09:47+02","2024-05-17 05:32:57.291452+03","And how to attach VPC on those lambdas You can apply VPC on 2 levels Documentation Pseudocode What I am looking for is how to add tags on those auto generated lambda You have 3 level of tags in Serverless Framework and they are passed to CloudFormation Documentation Pseudocode Important In both cases VPC or Tags configuration the inheritance and overwrites are at play You can define a value in a higher level provider vpc and overwrite it in the resource level functions LambdaName vpc "
75810169,75809319,"stackoverflow.com",0,"2023-03-22 10:59:13+02","2024-05-17 05:32:57.849987+03","There is a way to add alias to your node js project You need to do three things Add a jsconfig json file at te root of your project add _moduleAliases option and the modulealias lib inside your package json and finally import the lib at the very top of your index js Run the following command to install the lib Add a jsconfig json will help your IDE to detect that you are using aliases Here is an example of jsconfig json file It is here that the magic happened In your package json file add the following lines It is important to be synchronised with the jsconfig json file in order to get fully working aliases At the very top of your index js file add the following code This normally should do the trick If needed see the documentation here "
77810300,75833359,"stackoverflow.com",0,"2024-01-13 06:28:26+02","2024-05-17 05:33:00.935291+03","I believe it is a little late for an answer but maybe this can help someone The only way I found that allows nested routes without declaring them manually was this one"
75876224,75875119,"stackoverflow.com",0,"2023-03-29 13:49:05+03","2024-05-17 05:33:02.780263+03","The stage should be under provider tag You can try this by default this will be deployed to dev stage You can use this to deploy to other stages e g prod Reference"
76025331,75884529,"stackoverflow.com",1,"2023-04-16 03:12:15+03","2024-05-17 05:33:03.494171+03","Use the serverlesspluginifelse plugin the If condition can range but example assuming your function name is func3"
75887541,75886752,"stackoverflow.com",2,"2023-03-30 14:17:52+03","2024-05-17 05:33:04.439631+03","This is likely an indication that you do not have permission to retrieve the region of the bucket s3GetBucketLocation permission is required to return the Region that an Amazon S3 bucket resides in See Actions resources and condition keys for Amazon S3 Service Authorization Reference"
77339040,75886752,"stackoverflow.com",0,"2023-10-22 10:29:32+03","2024-05-17 05:33:04.440916+03","You may be experiencing some lag Amazon S3 is built on eventual consistency model it can take some time for changes to propagate across all AWS Regions This is why you might temporarily see your bucket in the console or in a response to a ListBuckets API request even after you delete the bucket Until the bucket is completely removed by Amazon S3 you might still see the bucket but you cannot perform any other actions on the bucket As it states in this answer here"
75956518,75926696,"stackoverflow.com",0,"2023-04-07 10:54:13+03","2024-05-17 05:33:05.405079+03","I finally found the solution after going through a lot of blogs docs and different web pages As everyone is suggesting I have added the following code in pom xml but the most important point that folks missed highlighting is instead of deploying the normal jar file deploy your aws jar file "
76025323,75931608,"stackoverflow.com",0,"2023-04-16 03:09:37+03","2024-05-17 05:33:06.382356+03","It sounds like you are trying to run serverless locally but connect to your live RDSProxy in AWS serverlessoffline is for local testing i e sls offline start then disconnect your internet then interact with your locally running app and it still works Of course you could just attempt to connect to live services from your local running environment But a better pattern is to create a connection builder that knows how to connect to a local DB or the live DB based on serverless stage With all that said you can always run an EC2 instance in the same VPC with permission to connect to the RDS Proxy then create an SSH tunnel to the EC2 instance and bridge a localhost port to the rds proxy ha "
75931787,75931625,"stackoverflow.com",0,"2023-04-04 19:35:03+03","2024-05-17 05:33:07.131175+03","You need to use the data from the output field returned to API Gateway by the Step Functions StartSyncExecution API Action You can find a more complete example using SAM not Serverless but it should be analogous here Trigger via Amazon API Gateway and wait for response "
75954383,75953129,"stackoverflow.com",1,"2023-04-07 02:07:29+03","2024-05-17 05:33:09.852861+03","Well I found the solution Adding ResultPath to the step does the trick"
76769508,75996518,"stackoverflow.com",0,"2023-07-26 11:52:23+03","2024-05-17 05:33:11.465889+03","There was a role that got added in the SMS configuration before this role could not be updated or deleted it had to be manually edited to assign the said permission from the console "
76001176,76000691,"stackoverflow.com",0,"2023-04-13 05:28:15+03","2024-05-17 05:33:12.525901+03","Your patterns list basically includes all the most common directories and files to keep out of the packaging Keep in mind that everything in the root folder will be packed in the deployable zip so I would suggest you to run the du command in the root folder and also look at the hidden files Another suggestion would be to do a git clean xdfn to see what is not being tracked by git if nothing of the list is important to you then do a git clean xdf and try the severless deploy as if you were in a CI environment with a fresh copy of your repositorybranch first then finally"
76001642,76000691,"stackoverflow.com",0,"2023-04-13 07:32:12+03","2024-05-17 05:33:12.527901+03","Answer I am an idiot See if you can spot the issue I was having issues previously with a protected branch that was not allowing me to retrieve Gitlab CICD environment variables for AWS As part of running down that issue I implemented a few extra steps to the pipeline to download extract and install the aws cli so that I could validate that I was getting and assuming an identity Of course the CICD pipeline drops you into a freshly checked out directory with your repo and I was downloading and extracting IN that directory So those things were being repackaged back up with my serverless deploy bloating the package significantly After commenting out AWS CLI related actions Uploading service azurePhishSimReportFunction zip file to S3 14 82 MB It is always fun to find out the problem was you Thanks to the folks who chimed in I appreciate you taking a little bit of time out "
76008065,76005339,"stackoverflow.com",0,"2023-04-13 20:01:34+03","2024-05-17 05:33:13.370833+03","There are at least two ways to achieve that As you can see here both fields should be provided or it must be an empty object which means none of them is accepted Here we use a custom handler to get the full control in our hands helpers state ancestors gives us the parent received object I leveraged Joi number positive to specify positive and greater than zero numbers Please adjust the pageRange custom method as your business I just tried my best to show the way Maybe it needs some improvements The first solution does not provide a good error message So I courage using the second one and implement your custom function You may not need to set the custom value for pageSize Please test it too "
76027278,76025159,"stackoverflow.com",1,"2023-04-16 13:50:17+03","2024-05-17 05:33:14.02458+03","When an AWS Lambda function is NOT connected to a VPC it has direct access to the Internet The Amazon SQS API endpoint is on the Internet so the Lambda function is able to send API requests to the SQS service When the function IS connected to a VPC it does not have a public IP address and is unable to communicate with the Internet Therefore it is not able to send a request to the SQS endpoint There are two options From Amazon SQS now Supports Amazon VPC Endpoints using AWS PrivateLink Amazon Web Services AWS customers can now access Amazon Simple Queue Service Amazon SQS from their Amazon Virtual Private Cloud Amazon VPC using VPC endpoints without using public IPs and without needing to traverse the public internet This allows the Lambda function to send the request to the Amazon SQS endpoint without requiring Internet connectivity I would recommend this option "
76029243,76028728,"stackoverflow.com",1,"2023-04-16 20:14:11+03","2024-05-17 05:33:15.014551+03","The PolicyDocument version needed to be in quotes"
76067220,76066822,"stackoverflow.com",0,"2023-04-20 21:29:29+03","2024-05-17 05:33:16.528046+03","Looks similar to this question Cannot use ES6 on AWS Lambda function how to import ES6 module within Lambda TLDR AWS Lambda needs the handler to be configured in the form of filename methodname it will always use the js extension which makes it not possible to use a different file extension for your entry point "
76326302,76260621,"stackoverflow.com",0,"2023-05-24 21:03:41+03","2024-05-17 05:33:17.641801+03","Consider creating that policy outside of serverless and assign it to a role that you can reference on each service using the role arn just like it is shown here httpswww serverless comframeworkdocsprovidersawsguideiamcustomiamroles "
76291915,76288724,"stackoverflow.com",0,"2023-05-19 22:19:57+03","2024-05-17 05:33:19.507202+03","The call stack was a red herring as it was not showing the call stack of where the error actually occurred The issue was with an npm package we had created using esm imports but was being imported into the project that is using commonjs The fix was to rebuild the package with babel to change it to commonjs "
76307648,76297120,"stackoverflow.com",0,"2023-05-22 18:16:56+03","2024-05-17 05:33:20.377019+03","We got in touch with Prisma team and the answer is You would need to set the pool_timeout and the connection_limit in the database connection string during environment creation for example DATABASE_URLpostgresqlaaa [email protected] 5432dddconnection_limit10pool_timeout100sslmodeprefer Prisma would then use that to generate a connection string to use with the Data Proxy I hope this could solve the issue to anyone encountering the same problem "
76326177,76318531,"stackoverflow.com",0,"2023-05-24 20:45:21+03","2024-05-17 05:33:21.245947+03","To reference the CloudFront distribution created in the resources section of your serverless yml file in the Lambda trigger you can use the CloudFormation intrinsic function FnImportValue Heres an updated example In this example the distributionId property of the Lambda trigger is set to FnImportValue MyCloudFrontDistributionId You will need to replace yours3bucketurl with the URL of your S3 bucket Before deploying this configuration you will need to export the CloudFront distribution ID value in your CloudFormation stack For example By exporting the distribution ID you can import it in the Lambda trigger using FnImportValue This way you can reference the CloudFront distribution created in the resources section Hope this helps or gives you idea on the right direction"
77083849,76345522,"stackoverflow.com",0,"2023-09-11 20:56:19+03","2024-05-17 05:33:23.3306+03","You are right Serverless Framework creates minimal roles for each Lambda You should simply extend them with additional permissions that are needed to all Lambdas Source httpswww serverless comframeworkdocsprovidersawsguidefunctionspermissions It is not a perfect solution because it will be applied to all functions so if you want to have the least privilege applied to each function you can use this plugin httpswww npmjs compackageserverlessiamrolesperfunction"
76366603,76365874,"stackoverflow.com",1,"2023-05-30 18:55:23+03","2024-05-17 05:33:24.333028+03","That is one of the most basic use cases for AWS Lambda and Serverless Framework Just add an events key like this You can find more detailed examples on Serverless coms SQS Queues documentation here httpswww serverless comframeworkdocsprovidersawseventssqs Additionally you mentioned in your original post that you created the SQS Queue outside of your serverless yaml I suggest that you should create your SQS Queue from within your serverless config There are may benefits to doing so If you choose to proceed Serverless com recommends using serverlesslift plugin to create and manage the queue for you see bottom of doc linked above "
76388554,76387147,"stackoverflow.com",1,"2023-06-02 11:43:52+03","2024-05-17 05:33:29.309001+03","You have basically two options First one is using parameters in the CloudFormation template and passing the ARN as a parameter in the call through parameters The second one is assuming the SNS topic has been also created from a CloudFormation template recommended for IaC advantages to store the ARN of the SNS topic which is a CloudFormation Output into the Parameter Store or even directly exporting it And then retrieving its value directly from the CloudFormation template you mention However this creates a dependency between stacks "
76390959,76389362,"stackoverflow.com",1,"2023-06-02 17:16:09+03","2024-05-17 05:33:31.436911+03","I understand you want to update the MasterUsername property on an existing RDS instance that you created with Serverless Framework Serverless Framework uses AWS CloudFormation According to the AWS CF docs the MasterUsername property cannot be updated on an existing RDS instance Any attempt will result in a resource replacement If you still want to update the MasterUsername your only option is to destroy the RDS instance and create a new one "
76453133,76398799,"stackoverflow.com",1,"2023-07-27 07:12:38+03","2024-05-17 05:33:32.169569+03","I found the cause serverless v2 is deprecated httpsgithub comserverlessserverlessissues11400 Need to upgrade to v3 When the parameter existing true is set on bucket events serverless creates a lambda function Custom S3 Resources which creates the event on the original lambda See using existing buckets This lambda which is created runs on a deprecated runtime Node 12 in Serverless 2 "
76539737,76454240,"stackoverflow.com",0,"2023-06-23 14:38:26+03","2024-05-17 05:33:34.535825+03","You do not need to do anything with these files manually Whenever serverless deploy is ran it will handle all uploading for you If you want to deploy with prebuild package you can use package flag to serverless deploy "
76464111,76463745,"stackoverflow.com",0,"2023-06-13 13:48:50+03","2024-05-17 05:33:35.452995+03","The initial challenge requires you to first set a new password before the user is logged in That is why the response says ChallengeName NEW_PASSWORD_REQUIRED From the docs [AuthenticationResult] is only returned if the caller doesnt need to pass another challenge If the caller does need to pass another challenge before it gets tokens ChallengeName ChallengeParameters and Session are returned httpsdocs aws amazon comAWSJavaScriptSDKlatestAWSCognitoIdentityServiceProvider htmladminInitiateAuthproperty You can use the adminRespondToAuthChallenge operation to set a new password httpsdocs aws amazon comAWSJavaScriptSDKlatestAWSCognitoIdentityServiceProvider htmladminRespondToAuthChallengeproperty"
76517289,76465092,"stackoverflow.com",0,"2023-06-21 00:18:51+03","2024-05-17 05:33:36.435559+03","In the Snapshots window in the console you see a set of tabs Manual System Shared with me Public Backup service Exports in Amazon S3 The retention interval applies to the autogenerated snapshots in the System tab If you pick one of the snapshots in there do Actions Copy and keep the AWS Region the same the new snapshot will appear in the Manual tab Those manual snapshots do not expire You do get charged for the storage once the retention interval expires I like the Export Snapshot Data to S3 feature for bringing the data into some other type of system that can read Parquet files But because that type of exported data cannot easily be reimported back into Aurora I do not consider it a way to keep a permanent version of the actual snapshot The snapshot represents all the views indexes column types etc so it has everything from the original cluster It is not a thing though that you can get in the form of a file If you want to have something to store in textual format longterm independent of AWS that would be a logical backup via pg_dump or mysqldump "
76496790,76496379,"stackoverflow.com",0,"2023-06-17 18:03:22+03","2024-05-17 05:33:38.187051+03","Yes it supports binary formats Solved by changing code to this"
76539720,76500742,"stackoverflow.com",1,"2023-06-23 17:08:00+03","2024-05-17 05:33:39.305939+03","The way you are trying to add statements is not supported by Serverless Framework If you would like to add those statements only to a single Lambda function you will need httpsgithub comfunctionaloneserverlessiamrolesperfunction plugin and use iamRoleStatements property If you want to add those statements to the default role you can do it as presented in docs here httpswww serverless comframeworkdocsprovidersawsguideiamthedefaultiamrole"
76662341,76517605,"stackoverflow.com",0,"2023-07-11 16:10:44+03","2024-05-17 05:33:40.148318+03","There are several possibilities when Lambda returns a 502 error It could be due to the Lambda functions response being incorrectly formed or containing invalid content The best way to troubleshoot this error is by utilizing CloudWatch Additionally if necessary you can enable XRay for further debugging If you using API Gateway to access the lambda function you can turn on API Gateway logging and you will see more information in CloudWatch This related documentation hopefully can help you "
76533777,76533551,"stackoverflow.com",0,"2023-06-22 19:08:13+03","2024-05-17 05:33:42.844321+03","If you deploy one JAR the entire JAR is being loaded into the runtime This is not much of an issue if it is rather small or has only a few dependencies However it can become an issue once the JAR becomes larger because that not only increases startup time but also its memory footprint As an example It is highly recommended to resolve resourceintensive dependencies e g establishing a database connection outside the handler function so that it can be reused between Lambda function invocations If some of your function calls do not require these resource they will still be impacted at least in terms of memory and CPU requirement Like everything it essentially comes down to a tradeoff between manageability and performance For very small functions that all use the same resources I might package them into one JAR If they have different dependencies I would definitely break them each into their own For a deep dive into this topic check out this great article by AWS Serverless Hero Yan Cui AWS Lambda should you have few monolithic functions or many singlepurposed functions"
76541549,76541128,"stackoverflow.com",1,"2023-06-23 18:32:01+03","2024-05-17 05:33:43.810501+03","So from my understanding you want to transform postman data to your desired format but you got recursive invocation You cannot filter it by using principalId only prefix or suffix filter are available to s3 events So currently I can suggest a few options including your ideal solution Write back the transformed data to different folder inside the s3 bucket from your lambda Means if your invocation happens postmancollectionprod then write back to processedpostmancollectionprod or any name you want Use two S3 bucket with the same exact configuration one to upload the other to write back the transformed If you want to still use folder name processedpostmancollectionprod because you got other configuration Which AWS itself recommended way to do this kind of things Check on your Lambda if the event is what you want to process or not But here you are adding a cost because of the lambda will run and it will cost you I know there is additional cost to all of the above solution because of the model AWS use There is only two option to folder which are prefix and suffix So try using something like yourfile unprocessed json then add that in the suffix Or create something to identify from the file by either suffix or prefix Because Prefixes and suffixes do not support wildcards so the strings provided should be literal "
77143770,76566941,"stackoverflow.com",1,"2023-09-20 18:09:36+03","2024-05-17 05:33:45.704505+03","I agree with you but it is not possible I also wish to build one immutable artifact for my entire pipeline This is a limitation of Serverless Framework or perhaps the Lambda ecosystem itself The full answer is best documented in post over at Seed run httpsseed runblogwhyserverlessdeploymentartifactscannotbereusedacrossstages html"
76015076,76009557,"stackoverflow.com",1,"2023-04-14 15:26:28+03","2024-05-17 05:42:01.380001+03","You need to set the flag in the configfeatures ConfigMap That ConfigMap should already have one key named _example You will need to add your key above or below the example The _example key is ignored by Knative but provides a place for documentation for system administrators "
74042402,74038692,"stackoverflow.com",1,"2022-10-12 16:10:33+03","2024-05-17 05:44:30.653319+03","The uri can be an absolute URL with a nonempty scheme and nonempty host that points to the target or a relative URI From the docs So you should be able to specify the protocol when using only the uri"
76586171,76586135,"stackoverflow.com",0,"2023-06-30 10:49:27+03","2024-05-17 05:33:46.702251+03","This maybe happening because the runtime incompatibility between the binaries generated on your machine and the runtime used by AWS Lambda Amazon Linux 2 Since you are working on a newer NodeJS environment if you start utilizing import and export ES6 syntax then you have to configure the type module property of package json files Another approach that may help you fix the problem is by making sure that the libraries you have used on the existing application contains libraries that were compatible with NodeJS 16 Another approach that you can try is to delete your node_modules folder and packagelock json files and run a npm install command on a machine compatible with lambdas runtime "
76612702,76611251,"stackoverflow.com",1,"2023-07-04 15:34:35+03","2024-05-17 05:33:47.478908+03","The root cause is a breaking change in v3 httpswww serverless comframeworkdocsguidesupgradingv3 Free form parameter are no longer allowed and have to be wrapped inside the param options now v2 v3 if you need to reference them in your config do not use the opt syntax Instead use param now"
76843576,76839809,"stackoverflow.com",1,"2023-08-06 00:21:43+03","2024-05-17 05:33:52.664413+03","We would need to see your serverless yml file to check HTTP option configuration Typically you will see missing authentication token when the route exists but the HTTP method is not valid for that route You will likely want to set method ANY like this example If you also wanted any path from that API Gateway instance to route to your function you can use the proxy configuration"
76861305,76859556,"stackoverflow.com",0,"2024-04-18 12:53:55+03","2024-05-17 05:33:55.7634+03","As per the Documentation util urlDecode and passing each query element into this solved the problem "
76861233,76860721,"stackoverflow.com",0,"2023-08-08 19:34:21+03","2024-05-17 05:33:55.901378+03","The Serverless Framework does not know which functions in a stack require which dependencies unless you explicitly declare it Serverless does not perform any bundlingtreeshaking does not create an abstract syntax tree or do any kind of compiling of your function code By default it creates one bundle per stack and uploads it to each function You can also package individually and then each function can declare its own includeexclude packaging configuration This is covered in the documentation and some strategies to handle this have been discussed on the forum I would suggest specifically including or excluding dependencies in your verySmallFunction You may also consider separating these functions into different stacks entirely This will not cost additional money as AWS does not price based on number of Lambda functions deployed nor on the number of CloudFormation Stacks "
76882846,76867539,"stackoverflow.com",0,"2023-08-11 13:47:19+03","2024-05-17 05:33:56.876945+03","That is because the ARTIFACTORY_USER variable evals to admin and it is configured as a secret See httpsstackoverflow coma7368063311715259 Note now everyone in the internet knows your secret If that was truly a secret for you you should rotate that value ASAP You should avoid such weak secrets Using dictionary words that can legitimately show up in the logs will cause this security feature to expose the value of your secret so that it could be inferred even if it was never deliberately printed "
76906917,76876663,"stackoverflow.com",0,"2023-08-15 17:52:49+03","2024-05-17 05:33:58.882371+03","Judging by the CloudFormation error it looks like there is a preexisting or once existing stack that was spun up with the same Lambda name at some point on your account The easiest way to fix this is to completely delete the resource in questionin this case the S3 bucket which is causing the conflict and redeploy The reason it is failing to deploy lambda2 even though its lambda1 which has the conflict is that CloudFormation stacks require all resources to be able to be stood up else it will initiate a rollback process to reverse the operation "
77251182,76878320,"stackoverflow.com",1,"2023-10-07 22:14:18+03","2024-05-17 05:33:59.658309+03","I believe your IAM statement that grants access is incomplete It only grants access to the bucket resource but not to individual objects within bucket Please consult the docs here on how it should be defined httpsdocs aws amazon comAmazonS3latestuserguideusingwiths3actions html tldr You need to specify resource with at the end In works locally probably because the IAM checks are simply not checked there or you are credentials with appropriate permissions"
76911018,76910264,"stackoverflow.com",0,"2023-08-16 09:30:35+03","2024-05-17 05:34:01.109019+03","I need create a folder inside the bucket is possible do it Its not possible without a custom resource that you have to develop for creating those folders "
77083729,76915857,"stackoverflow.com",0,"2023-09-11 20:33:03+03","2024-05-17 05:34:02.287992+03","You can simply define that value in custom section of yaml file and then pick it up from there However I see that basing on this [ envMY_ENV_VARIABLE false true] your variable might be not set in some cases If it depends on the stage you can simply use params section Then basing on the stage name that you pass during deployment a correct value will be picked In that example for prod the value of MY_ENV_VARIABLE will be different If nonstandard stage will be provided then it will fallback to default value "
78281003,76962729,"stackoverflow.com",0,"2024-04-05 18:42:47+03","2024-05-17 05:34:06.255917+03","I suggest using serverlesspythonrequirements plugin and configure it as follow which will resolve your issue but note that you need to have docker installed and running in order for layer to get built I have answered this in detail in the following article Hope this helps "
76973967,76968030,"stackoverflow.com",6,"2023-08-25 05:51:16+03","2024-05-17 05:34:07.162476+03","Nodejs18 x has been supported since version 3 25 0 so there are a couple things to check"
76974752,76968030,"stackoverflow.com",2,"2023-08-25 09:28:43+03","2024-05-17 05:34:07.164477+03","so i found the answer so i updated the serverless library but i did not updated the serverlessoffline library due to that we are getting this error latest serverless versions are supporting nodejs18 x in order to use serverless please update all the packages related to serverless "
77423104,76968030,"stackoverflow.com",1,"2023-11-04 19:24:56+02","2024-05-17 05:34:07.165477+03",""
77056337,76989182,"stackoverflow.com",1,"2023-09-15 04:00:52+03","2024-05-17 05:34:07.333253+03","Interesting I am trying to achieve the same What I did in my scenario with turborepo was installing serverlessnestmonorepo which does pretty much what you want if you check the docs Then you can just integrate the command in your CI to deploy the lambda or deploy on your machine However It does not seem to be working with nestjs 16 properly by reading the issues There are also other tools such as serverlesspluginmonorepo but I had a bug using it It seems that dependencies of dependencies are not being loaded into the lambda function The major problem that I am having with trying to deploy multiple NestJS applications in a monorepo is that all their dependencies are inside the root node_modules and this means that using just serverless deploy does not work because it does not add all the dependencies necessary And that is some of the reason why the 2 plugins I listed before exist You can try disabling hoisting that should do the trick but it will make it very slow to build everything You could also try using AWS CDK to manage these deploys it might be easier that handling the serverless npm module to understand your monorepo After some tinkering what worked for me was the following serverless yml file inside each application of my turborepo The most important plugin I guess it the serverlesspluginincludedependencies It decreases the bundle size by a lot specially when having shared dependencies in packages folder And what I like to to do about sharing dependencies is putting the database models in the pacakges folder Also maybe some services there In this scenario each Module becomes it is singular serverless application With this setup test it with serverless offline first locally running it no the folder of each application make some calls to your lambda and then serverless deploy and everything should be all right And the API gateway can handle multilpe lambda functions at the same time Just make sure that their routes do not overlap For example multiple lambdas with the login route "
77610477,76009557,"stackoverflow.com",0,"2023-12-06 05:59:56+02","2024-05-17 05:42:01.382001+03","The E Anderson answer is correct you need to set the flag in the configfeatures ConfigMap However if you installed Knative Serving by using the Knative Operator you will need to set the config using the specific custom resource Otherwise the Operator will reconcile and reset your configmap as soon as you edit it "
77143729,76990708,"stackoverflow.com",0,"2023-09-20 18:03:33+03","2024-05-17 05:34:08.313794+03","Question 1 I have not changed anything on the S3 bucket Should I still provision it I do not want to change the name of my existing S3 bucket But I want to use my existing bucket If your bucket was created outside of serverless yaml then all you need to do is reference the name There is no need to declare an AWSS3Bucket resource if you do not need to create one Question 2 Is my resources yml file out of date and not synchronised with my AWS resources That is unlikely Can you confirm whether or not you have an existing AWSElastiCacheSubnetGroup with the name mmvpccache Go to the Elasticache Dashboard on the left side it says Subnet Groups If it is not listed there then you need to create it "
76993345,76991338,"stackoverflow.com",1,"2023-08-28 16:40:12+03","2024-05-17 05:34:09.586843+03","I think you can use Outputs and export the S3 bucket name from old Cloudformation template and import it to other templates like below Import it to other templates Another solution is using randomstackid in S3 bucket name You will have many S3 buckets "
77225829,77020480,"stackoverflow.com",0,"2023-10-04 01:31:53+03","2024-05-17 05:34:10.23529+03","path login the dev stage is added for you just remove it from your path"
77039418,77029277,"stackoverflow.com",0,"2023-09-04 19:39:45+03","2024-05-17 05:34:11.042948+03","So it turns out I did not deploy my changes Maybe it is something obvious to various blog writers and the official docs but for a novice who works with Serverless Framework this information was totally missing Go to Resources click the Actions button select Deploy API"
77212210,77038165,"stackoverflow.com",3,"2023-10-02 11:13:17+03","2024-05-17 05:34:14.64624+03","Can API Gateway natively filter out requests based on header values Unfortunately Amazon API Gateway does not support natively checking header values The closest feature would be API Gateway request validators which will not work as they can only check if the header exists and not the header value The only other option I can think of is using AWS WAF which only supports REST APIs and has an 8 KB limit You can also create rules that match a specified string or a regular expression pattern in HTTP headers method query string URI and the request body limited to the first 8 KB If possible I would recommend just going with a simple Python Lambda authoriser WAF would probably be more expensive and overkill However for whatever reason you may still want to go ahead with AWS WAF You need to define a web ACL associate it with your API Gateway and create a suitable rule for your web ACL We will do this in AWS CloudFormation CFN as the Serverless framework is basically a wrapper around CFN Anything that can be defined in CloudFormation is supported by the Serverless Framework Two options Configure web ACL to allow by default create a rule with a condition to block any request that does not have the header XSecret set to FOOBAR Configure web ACL to block by default create a rule with a condition to allow any request that does have the header XSecret set to FOOBAR objectively easier logic to visualise and implement We will go with option 2 We need a web ACL first Essentially it is a container for rules The Web ACL as a whole governs how your AWS resource i e your API Gateway responds to web requests based on the rules it contains It needs 4 things to be defined We will just call it MyAcl Note the CloudFormation docs are incorrect in saying it is not required and I have submitted a feedback request to fix this it is needed by the API and thus CloudFormation We will specify REGIONAL as the only other option is CLOUDFRONT which is only for Amazon CloudFront distributions In this case we are going with a single rule that allows anything with the XSecret set to FOOBAR so we want to block everything else We will set this to BlockAction to block requests that do not match our rule by default If you went with the inverse scenario you would need to inverse this In this case we need neither of these features so we will set CloudWatchMetricsEnabled and SampledRequestsEnabled both to false We also need to weirdly still set MetricName to something even if it is disabled forever a mystery so we will set it to RandomMetricName You also will eventually define Rules a list of Rule resources Pretty selfexplanatory in that it will contain your rule s but this is not strictly needed at the time of creation Then we need a WAF rule as we have a container but no stuff A WAF rule lets you precisely target the requests that you want to allow or block by specifying the exact condition s that you want it to watch for It needs 5 things to be defined We will just name it MyHeaderRule` In this case we have one rule so we could set this to any number We will just set this to 0 but note that when you have more than one rule priorities are really important as they determine processing order Our rule statement will define a string match search on the XSecret header The AWS WAF console the developer guide call it a string match rule statement while the API and CloudFormation call it a ByteMatchStatement We will set this to AllowAction to allow on rule match as the ACL will block everything else based on our DefaultAction Note for completeness sake this does not always need to be set depending on if you are using rule groups or not and sometimes you might need to set OverrideAction but in this case we do need it As above Finally we need our actual rule statement a ByteMatchStatement It also has 4 things we need to define We want to check a header in the headers However as per the docs for Headers since we only want to inspect a single header we have a mini shortcut of just setting this to a SingleHeader object with a Name value of XSecret This is easier than defining a more complex Headers object We want an exact match Exactly matches string in the console so we will set this to EXACTLY We will set SearchString to FOOBAR as FOOBAR is a base64unencoded value As per docs this is usually done to reverse unusual formatting that attackers use in web requests in an effort to bypass detection In this case we do not want any transformations so we will just set our Type to NONE and Priority to 0 Our rule in JSON would look like this Bringing all of the above together with the serverlessassociatewaf plugin we get something like this"
77106358,77104323,"stackoverflow.com",1,"2023-09-14 18:24:44+03","2024-05-17 05:34:15.45265+03","The problem is solved by updating the version of the serverless framework and related plugins used "
77136697,77136439,"stackoverflow.com",6,"2023-09-19 20:24:22+03","2024-05-17 05:34:18.014084+03","There is no requirement to have any readreplica instances in an Aurora cluster serverless or not You can create an Aurora cluster with a single primary writer instance and nothing else You can even create a cluster with no instances if you want although that certainly has limited functionality "
77380968,77380860,"stackoverflow.com",1,"2023-10-28 22:39:01+03","2024-05-17 05:34:19.942121+03","I found the answer when searching through repost It is mentioned here that the issue is not the ResourceArn of the apigateway but the WebAclArn The error messages being thrown are wrong The WebAclArn cannot be referenced like Ref MyWafWebACL because this seems to be an object with multiple values The correct reference is GetAtt MyWafWebACL Arn which points to the arn directly This solved my issues "
77471504,77411767,"stackoverflow.com",1,"2023-11-13 04:41:03+02","2024-05-17 05:34:22.482904+03","Thanks for trying xray Please refer to xray daemon config to ensure the segments are sent to to correct accountregion httpsgithub comawsawsxraydaemoncredentialconfiguration Also you can enable debug log for daemon to get more information "
77436692,77431695,"stackoverflow.com",1,"2023-11-07 10:46:31+02","2024-05-17 05:34:24.609084+03","It looks like your configuration might be missing org and app settings to actually link your application to Serverless Dashboard Please see the setup guide here httpswww serverless comframeworkdocsguidesdashboardsetup"
77456237,77455733,"stackoverflow.com",1,"2023-11-09 22:48:01+02","2024-05-17 05:34:26.042324+03","Double check that arnawsiamxxxxxxxxusersai has the correct IAM permissions specifically GetRole From the exception is explicitly states you do not have that permission on the role Make sure you look at the conditions in your role etc "
77787836,77473488,"stackoverflow.com",0,"2024-01-09 17:18:30+02","2024-05-17 05:34:26.838553+03","The solution was hardcoded the ARN its the most correct way to do it simply i had a typo error"
77479222,77476358,"stackoverflow.com",0,"2023-11-14 10:44:03+02","2024-05-17 05:34:27.909641+03","It is very likely you are updating something that is not supported such as changing partition or sort key of a table or index This is not a limitation on serverless but rather that of DynamoDB CloudFormation cannot update a stack when a customnamed resource requires replacing Rename Test Dynamo Table and update the stack again As such the exception you receive is telling you that if you want to update the table it must be replaced And that is why you see success while creating a new one "
77494394,77486817,"stackoverflow.com",0,"2023-11-16 13:22:16+02","2024-05-17 05:34:29.013525+03","The solution was quite simple in fact as all af the elements seems to be merged"
78088018,77492929,"stackoverflow.com",1,"2024-03-01 15:16:42+02","2024-05-17 05:34:30.009216+03","Do"
77496653,77494019,"stackoverflow.com",1,"2023-11-19 15:29:41+02","2024-05-17 05:34:30.865603+03","You need to explicitly specify that you want to receive compressed response httpsdocs aws amazon comapigatewaylatestdeveloperguideapigatewayreceiveresponsewithcompressedpayload html Also a note if you want to enable compression for all requestsresponses you can set the value to 0 Ref httpsdocs aws amazon comapigatewaylatestapiAPI_RestApi htmlminimumCompressionSize EDIT After our conversation I have realised that you are using httpApi event type while apiGateway settings on provided only apply to http event type and compression setting is only available for http event type If your case you could use the http event instead of httpApi like below For differences between httpApi and http events please see the docs"
78080806,77494927,"stackoverflow.com",1,"2024-02-29 12:54:31+02","2024-05-17 05:34:31.892414+03","Do"
77578082,77553765,"stackoverflow.com",0,"2023-11-30 13:29:52+02","2024-05-17 05:34:33.564779+03","I resolved by issue by using the httpresponseserializer from middy Upon lambda JSON response not string the validator will validate the response body and then the middy httpresponseserializer middleware serializes the lambda response The validator did not seem to serialize the response under the hood Lambda function Middleware"
77578187,77562832,"stackoverflow.com",0,"2023-11-30 13:48:14+02","2024-05-17 05:34:35.180857+03","After digging around a bit finally found the solution This can be achieved using serverlessdotenvplugin httpsgithub comneverendingqsserverlessdotenvplugin All i had to do was install this plugin and use it in my serverless ts file like this It read the environment variables from my env dev or env prod based on the stage value in serverless deploy command Hope someone else also finds this helpful "
77577808,77566986,"stackoverflow.com",0,"2023-11-30 12:49:47+02","2024-05-17 05:34:36.238364+03","To use paths with jest you need to specify them in the jest config"
77883929,77588004,"stackoverflow.com",1,"2024-01-26 04:33:30+02","2024-05-17 05:34:36.969953+03","Have you tried the stackTags property All the tags specified under provider stackTags section will be Applied to the stack which contains all the resources that the framework will create for a service Applied to most of the resources created directly or indirectly by the framework The reason its most and not all is detailed in the last section of this article Reference httpsmojitocoder medium comawsresourcestaggingusingserverlessframeworkfbfb32122cde"
77771151,77662902,"stackoverflow.com",0,"2024-01-07 06:30:49+02","2024-05-17 05:34:40.768223+03","You need a QueuePolicy that will allows SNS to send a message to the queue This is an example from one of my projects hope it helps "
77710365,77710336,"stackoverflow.com",1,"2023-12-24 12:49:22+02","2024-05-17 05:34:42.889066+03","I think the only way to solve it is instead of running npmp install choose pnpm install shamefullyhoist instead "
76063414,76061776,"stackoverflow.com",1,"2023-04-20 17:39:16+03","2024-05-17 05:42:02.457952+03","finally after some research and expermients To make the HPA work for knative service we also need to specify the target as annotations as we can see here This if condition for hpa will only work if we specify some target for metric httpsgithub comknativeservingblob91ac3b335131565cb9304ed9f6259c959f71b996pkgreconcilerautoscalinghparesourceshpa goL62 now it works by adding one more annotation now the hpa will be of metric type memory"
77731253,77715669,"stackoverflow.com",0,"2023-12-29 11:46:38+02","2024-05-17 05:34:43.916061+03","The most common way to use outputs of a different stack as a part of config is to use cf variable source as documented here httpswww serverless comframeworkdocsprovidersawsguidevariablesreferencecloudformationoutputs The errors you are seeing are caused by the fact that Serverless Framework seems to not be able to resolve the env variables for subnetId If you run sls print you will see that the values resolved for subnetIds are not the ones that you will expect them to be In general defaulting variable resolution to an object might be a bit tricky which is why I would recommend using cf variable source directly that seems to cover your case pretty nicely "
77809802,77726602,"stackoverflow.com",0,"2024-01-13 01:42:12+02","2024-05-17 05:34:45.707607+03","Couple of things that came to mind that you can try NOTE sorry if this sound condescending it is not intended to "
77770936,77738723,"stackoverflow.com",0,"2024-01-06 23:30:13+02","2024-05-17 05:34:46.603995+03","The following is what I usually do as a practice not sure if it is a best practice or not that is followed by many This is the way I would also do it If I have a shared resource such as an authorizer I would create it in a different service and then reference it in the different services that require such as what you are doing So the way you are doing it IMO is fine If you mean what is the best place to define a shared resource I would also have it in a place in a service that is not being touched modifiedredeployed quite a lot just to be safe that no one make any modifications that might break it Though I would place it as close as possible to it is service that it relates to in the first place if there is any You might want to look into serverlesscompose that can help you deploy multiple services at once instead of running sls deploy in each and orchestrate the deployment all together"
77754875,77753724,"stackoverflow.com",0,"2024-01-04 00:08:21+02","2024-05-17 05:34:49.179463+03","We have both Java Lambdas and Node js Lambdas in the same Serverless yml and the packaging seems to work differently between SLS v2 and now v3 I moved the package artifact to the Java functions instead so the Java code did not end up in the Node js Lambda ZIP and now it is working fine Apparently AWS gets confused when there is Java code in a Node js Lambda and then seems to default to ESM "
78139061,78138671,"stackoverflow.com",1,"2024-03-11 12:54:14+02","2024-05-17 05:34:54.627104+03","Ive faced a similar issue in the past and in my case the issue was with the way the Lambda entry point was configured in my Dockerfile which is quite similar to what youve shown You can read more about working with lambda container images here Usually AWS Lambda base images handle the command and entry point internally and you shouldnt need to specify them explicitly if your handler is written in the supported language and follows the expected naming conventions which youve already done The handler youve specified in the functions definition within the Serverless Framework configuration serverless yml file is good So just modify your Dockerfile to take out the last line After making the change to your Dockerfile rebuild your Docker image and redeploy your service using Serverless Framework With these changes AWS Lambda will automatically use the entry point defined in the base image and it will locate your handler based on the command specified in serverless yml "
78168970,78168789,"stackoverflow.com",0,"2024-03-15 20:26:09+02","2024-05-17 05:34:55.467165+03","Looks like I just had to investigate a bit further I do not completely understand how this works yet But apparently having both functions defined the way I did it created two separate functions in the Lambda console hubspotfetchdevfetchContact and hubspotfetchdevfetchContactBatch It is confusing because each contains both the files hubspotFetchContact js and hubspotFetchContactBatch js So I found the hubspotfetchdevfetchContactBatch function and also set it up with the hubspotAxiosLayer layer Now it seems to be working "
78217000,78198567,"stackoverflow.com",0,"2024-03-25 05:56:13+02","2024-05-17 05:34:57.77476+03","finally I find the answer "
78446352,78442101,"stackoverflow.com",0,"2024-05-08 09:01:17+03","2024-05-17 05:35:01.881988+03","I have found the solution basically I had made a big mistake I created a repo on github which was public and pushed my revamped code over there then I made it private I forgot that I used the twilio auth key as a direct value instead of using it as an env due to which when I pushed my code on the repo the key got publicly exposed and when I pushed my previous code which was live it was using the same key in the env due to which Aws was restricting my deployment "
78479156,78476830,"stackoverflow.com",0,"2024-05-14 18:34:10+03","2024-05-17 05:35:04.651599+03","The Lambda function for custom resources have nodejs16 runtime hardcoded httpsgithub comserverlessserverlessblobd0e3056b77ba295adb87ceeca9a49a26b315f083libpluginsawscustomresourcesindex jsL165L179 I tried to edit the node_modules as this link it works well httpsgithub comserverlessserverlessissues12133issuecomment1958961198"
52012367,52009124,"stackoverflow.com",163,"2022-12-24 03:42:10+02","2024-05-17 05:41:34.814303+03","So it turns out the root cause was that Custom resources with finalizers can deadlock The CustomResource functions kubeless io had a and this is can leave it in a bad state when deleting httpsgithub comkuberneteskubernetesissues60538 I followed the steps mentioned in this workaround and it now gets deleted "
61382014,52009124,"stackoverflow.com",50,"2022-09-19 11:31:24+03","2024-05-17 05:41:34.815691+03",""
66438236,52009124,"stackoverflow.com",13,"2021-03-02 13:11:25+02","2024-05-17 05:41:34.816687+03","Try Solved my problem after trying to force delete got stuck "
67726402,52009124,"stackoverflow.com",4,"2021-05-27 19:34:52+03","2024-05-17 05:41:34.817569+03","I had to get rid of a few other things kubectl get mutatingwebhookconfiguration ack consul awk print 1 xargs I kubectl delete mutatingwebhookconfiguration kubectl get clusterrolebinding ack consul awk print 1 xargs I kubectl delete clusterrolebinding kubectl get clusterrolebinding ack consul awk print 1 xargs I kubectl delete clusterrole "
54400138,52009124,"stackoverflow.com",1,"2019-01-28 12:39:02+02","2024-05-17 05:41:34.818566+03","In my case it was an issue that I have deleted a custom resource object but not a custom resource definition CRD I fixed it with kubectl delete f resourcedefinition yaml In that file I defined my CRDs So I think it is the best practice not to delete custom objects manually but by deleting file where you define both object and CRD Reference "
57421125,56575267,"stackoverflow.com",0,"2019-08-09 00:57:18+03","2024-05-17 05:41:36.758763+03","This feature was not developed in that moment It is now possible httpsgithub comkubelessruntimesblobmasterstablenodejsexampleshellomultipart js"
57102479,57077530,"stackoverflow.com",0,"2019-07-18 23:52:53+03","2024-05-17 05:41:37.606158+03","kubeless should not be run from the nodes following the install guide you are deploying k8s resources on the cluster but the kubeless command line should be done from your computer "
60098303,60091934,"stackoverflow.com",0,"2020-02-06 17:21:07+02","2024-05-17 05:41:38.699225+03","As I can see from the kubeless io To debug MISSING Check controller logs kind of issues it is necessary to check what is the error in the controller logs To retrieve these logs execute There are cases in which the validations done in the CLI will not be enough to spot a problem in the given parameters If that is the case the function Deployment will never appear Hope that helps "
57789957,57771790,"stackoverflow.com",1,"2019-09-04 16:53:07+03","2024-05-17 05:42:45.869483+03","Ok I found the problem I tried posting custom images All worked until I change the port inside image to 80 This image not only work as Knative service but also It did not work on Cloud run service as well Bottom line is either pull port number from environment variable or hard code it to any other port than 80 "
60331931,60091934,"stackoverflow.com",0,"2020-02-21 05:47:13+02","2024-05-17 05:41:38.700225+03","From kubeless code this status happens if kubeless cannot get status of the k8s deployment for this function So there are some possible reasons as followed There is a runtime issue for this function for example syntax issue or dependency issue which cause the pod failed to run Check the pod logs can help to figure out This happens for my case not sure whether it is caused by the second reason which cause kubeless cannot get the failure message The kubeless version is not compatible with the k8s cluster version From k8s 1 15 the extensionv1beta1 version for deployment is removed However early version kubeless still uses extensionv1beta1 to get the status of deployment You can check the apiresources of your k8s cluster Check the following change list of kubeless which uses new appsv1 endpoint Use new appsv1 endpoint"
64289439,60091934,"stackoverflow.com",0,"2020-10-10 05:34:32+03","2024-05-17 05:41:38.702225+03","First get the name for the kubelesscontroller pod You can get the logs from the Kubeles controller"
60260251,60249842,"stackoverflow.com",1,"2020-02-17 12:17:53+02","2024-05-17 05:41:39.988288+03","after a long R D now it downloaded the latest localkube version and this solved my problem "
60250784,60249842,"stackoverflow.com",0,"2020-02-16 18:46:20+02","2024-05-17 05:41:39.990289+03","As the comment says you are using a version of Kubernetes that is very old Deployment was not available under appsv1 until Kubernetes 1 9 and you appear to be using 1 8 If you want to solve this problem without upgrading your cluster you will need to replace the apiVersion value in all of your Deployment objects with one of the older paths Note that while this will fix the immediate problem you will still likely run into other compatibility issues and should upgrade your cluster "
60960042,60958094,"stackoverflow.com",17,"2023-09-16 18:03:44+03","2024-05-17 05:41:40.793661+03","This is not possible If you look at the API documentation for PodSpec v1 core you can see that serviceAccountName expects a string not an array or object This is because using a ServiceAccount resource creates a 11 relationship between your pod and authentication against the API server You will either need to Diversify your workload into multiple pods Which with you can apply different service accounts Combine your service account capabilities into a single account and apply it exclusively to this pod I recommend 2 "
62904793,62902735,"stackoverflow.com",2,"2020-07-15 01:17:27+03","2024-05-17 05:41:41.489756+03","I nerded out on this and there is an issue with the Kubeless kafkatrigger Essentially I am getting this log when the message is picked up by the controller and tries to trigger the function This means that the controller cannot find the myhandler service to trigger the function If you see the code you see that it is a call to the K8s API server So my guess is that there is an API version mismatch on the newer K8s versions My server is v1 18 2 I have created this issue to track "
68130994,68037678,"stackoverflow.com",1,"2021-06-25 15:28:21+03","2024-05-17 05:41:43.486066+03","A good start is to set the imagePullPolicy for your PodSpec to IfNotPresent so that you will only have to pull once per version per node Depending on the criticality of the workload you should also consider mirroring the image to a container registry you control You do not want to be hitting rate limits when you need to roll out a hotfix at 3 AM "
70681381,68037678,"stackoverflow.com",0,"2022-01-12 14:13:47+02","2024-05-17 05:41:43.487627+03","This is what worked for EKS AWS K8s Let me know how it goes Note You will need to run the patch script 3 every time you deploy a new workload that depends on dockerhub "
77456707,77370138,"stackoverflow.com",0,"2023-11-10 00:31:01+02","2024-05-17 05:41:46.576097+03","After exploring many options I eventually settled on adding the basic auth layer outside of Knative I stopped exposing the Knative services externally by removing the domain name from KnativeServing CR so they can only be reached from within the cluster with svc cluster local url Then I created Ingress resource for Nginx ingress controller to talk to the internal KNative cluster local url I can easily configure the basic auth by setting Ingress annotations The tricky part is that 1 I need to have the Ingress point to the ExternalName service that is created as part of Knative service 2 I need to manipute the upstreamvhost header to set the host to be the cluster local url But once it is setup this provides the behaviour I want If the user does not pass the authentication then the request terminates on Nginx and Knative does not scale up "
77627494,77622915,"stackoverflow.com",1,"2023-12-08 17:09:04+02","2024-05-17 05:41:47.321195+03","Uninstalling Apache and nginx solved this issue for me Followed the answers here and here to completely remove the services Hope this helps anyone who encounter the same issue "
75628598,75624217,"stackoverflow.com",3,"2023-03-03 16:54:12+02","2024-05-17 05:41:52.519584+03","I believe Knative 1 9 requires Kubernetes 1 23 or newer One of the changes that happened in the last year or so was the removal of the v2beta2 autoscaling API and replacement with v2 API I believe the new API is present in Kubernetes 1 22 but Knative 1 9 was tested with the community supported versions as of January which includes 1 26 1 25 1 24 and possibly 1 23 "
75791276,75787724,"stackoverflow.com",0,"2023-03-20 16:06:36+02","2024-05-17 05:41:53.623797+03","Have you looked at the logs for your KServe service This looks like it could be an unhandled program exit for example "
75850085,75812508,"stackoverflow.com",1,"2023-03-26 22:33:15+03","2024-05-17 05:41:54.833913+03","The current implementation in httpsgithub combosonprojectbuildpacksblobmainbuildpacksgofaasmain goL82 appears to use gorillamux s Router Handle which defines an exact match on Different function implementations may define different HTTP serving semantics some might only allow POST others might allow only specific URLs you could also file an issue at httpGitHub comknativefuncissuesnew requesting a change in the behavior along with your use case "
75834851,75812508,"stackoverflow.com",0,"2023-03-24 16:24:44+02","2024-05-17 05:41:54.835913+03","Looking at the documentation generated alongside the func s go template it looks like a Go HTTP function has access to a standard http Request as the second or third parameter which includes a URL field You could use this to implement your own mux "
75845242,75841240,"stackoverflow.com",0,"2023-03-26 02:13:30+02","2024-05-17 05:41:55.750819+03","Knative traffic flows through an HTTP ingress which can be implemented through multiple pods on the cluster which could run on either controlplane or worker nodes If you use a cloud providers Kubernetes you will not be able to run any pods on the control plane nodes Try scaling your envoy pods beyond replicas 1 and make sure that your load balancer can send traffic to all replicas You should see that there are multiple paths that the traffic flows through You may also need to scale the activator replicas to see the full distributed system "
77558584,75855319,"stackoverflow.com",0,"2023-11-27 18:45:59+02","2024-05-17 05:41:56.636317+03","The issue was the configuration of the service Knative assumed the default port whereas kubernetes service was configured for 8080 "
51617507,51617251,"stackoverflow.com",2,"2018-07-31 18:55:01+03","2024-05-17 05:41:59.813139+03","if dockercompose command is not found it means you did not install dockercompose correctly I do not think you can install dockercompose as a python library using pip See the instructions here httpsdocs docker comcomposeinstallinstallcompose someting like Or take a look at httpsmedium comkhandelwal12nidhidockersetuponawsec2instancec670ff3d5f1b"
75878139,75858480,"stackoverflow.com",0,"2023-03-29 16:50:16+03","2024-05-17 05:42:00.434141+03","Issue was Nginx ingress controller rewriting host added the following annotation and resolved the issue"
57782345,57771790,"stackoverflow.com",0,"2019-09-04 09:03:53+03","2024-05-17 05:42:45.871484+03","Thanks for the precisions When you installed Knative you should see this kind of errors You did not have installed Istio Do it relaunch the knative installation with and without CRD to solve previous errors and enjoy "
76100650,76090599,"stackoverflow.com",2,"2023-04-25 14:27:26+03","2024-05-17 05:42:03.714357+03","Unfortunately the v1 Ingress API in Kubernetes does not have sufficient capabilities to express Knatives routing requirements Knative does support several ingress implementations including Istio Contour and the Gateway API but no one has written a plugin for the Nginx Ingress annotations Some of the capabilities that are missing from the Kubernetes Ingress API which are needed by Knative include If you are willing to use bets software the Gateway API plugin is mostly feature complete and should plug into a variety of ingress providers Unfortunately Nginx does not appear to be on that list "
69530910,69372234,"stackoverflow.com",0,"2021-10-11 21:27:36+03","2024-05-17 05:42:05.266633+03","I have been able to successfully install Knative Serving on a RaspberryPi running ARM64 Ref using the published images on Knative Serving Releases If you are wondering about a specific image using crane You can confirm whether the image has multiarch support already In this example the Knative activator image is already setup for ARM "
69797882,69796337,"stackoverflow.com",2,"2021-11-01 15:50:31+02","2024-05-17 05:42:06.307824+03","Knative should work fine with Kata Containers I know of no incompatibility and the runtime pods are pretty standard but I do not think I have seen it tested or written up before "
69799245,69796337,"stackoverflow.com",1,"2021-11-01 17:40:45+02","2024-05-17 05:42:06.308824+03","I have seen it tested or written up before but it should work fine with Kata"
69807749,69807255,"stackoverflow.com",0,"2021-11-02 11:04:41+02","2024-05-17 05:42:07.430771+03","I would not consider that an issue but a warning That hellodisplay seems to be using the Knative library that you can find here If you want to configure observability without falling back to defaults you should add those items environment variables to that hellodisplay which is usually done by reading a ConfigMap For reference you can find the default config being used at the core eventing components here "
69811315,69807255,"stackoverflow.com",0,"2021-11-02 15:28:34+02","2024-05-17 05:42:07.432771+03","It sounds like kubectl is handing you the logs from the queueproxy container installed by Knative rather than the usercontainer Try this command Alternatively AKS may have a centralized log service which you can use to view the logs for current and past pods which should allow you to select which container you are viewing "
70056125,70017680,"stackoverflow.com",0,"2021-11-21 17:54:50+02","2024-05-17 05:42:08.622933+03","I think your suggestion in the comments is the correct one you will want to manage multiple Kubernetes clusters and direct your individual users to different namespaces in different clusters I do not know of software to do this automatically but you could use separate DNS records for each customer to point them to their cluster once the customer has registered "
70150617,70121503,"stackoverflow.com",0,"2021-11-29 08:37:54+02","2024-05-17 05:42:09.553663+03","I would try a nslookup greeter10 default 10 104 139 27 sslip io to see what that resolves to on your local setup From my laptop using 8 8 8 8 I see that DNS address resolve just fine "
70174084,70172679,"stackoverflow.com",0,"2021-11-30 20:05:14+02","2024-05-17 05:42:10.308423+03","there was some issue with Dead Letter Sinks not being propagated at preGA releases Can you make sure you are using Knative 1 0 This is working for me as expected using the inmemory channel httpsgist github comodacremolbapf6ce029caf4fa6fbb3cc1e829f188788"
70187212,70172679,"stackoverflow.com",0,"2021-12-01 17:47:10+02","2024-05-17 05:42:10.310212+03","I never found an example of this in the docs but the API docs for the SequenceStep does show a delivery property Which when assigned uses the DLQ It seems odd to have to specify a delivery for EVERY step and not just the sequence as a whole "
70274050,70272070,"stackoverflow.com",0,"2021-12-08 13:12:41+02","2024-05-17 05:42:11.052372+03","GRPC uses HTTP2 You may need to explicitly name your port h2c I am assuming that you have tested the container locally without Knative in the path and have been able to make a grpc call in that case "
70280199,70279159,"stackoverflow.com",2,"2021-12-08 20:44:32+02","2024-05-17 05:42:11.76603+03","Knative Serving does not support such a policy as of today However the community thinks that Kubernetes Descheduler should work just fine as is discussed in httpsgithub comknativeservingissues6176 "
70293476,70279159,"stackoverflow.com",0,"2021-12-09 18:36:42+02","2024-05-17 05:42:11.76703+03","Not kn specific but you could try adding activeDeadlineSeconds to the podSpec "
70292510,70285727,"stackoverflow.com",0,"2021-12-09 17:28:55+02","2024-05-17 05:42:12.685828+03","If you have async background or scheduled processes it is likely that Knative is not a good match for your application There has been some investigation into exposing the HPA v2 custom metrics scanning options which would might preclude scale to zero as you note but even with HPA2 scaling you will still run into problems The problem with background processes is that Knative and Kubernetes do not have visibility into which Pods are still doing work so they are equally likely to shut down a Pod doing work as one that is idle One workaround would be to move the async work to be synchronous with a request possibly by using eventing to send a do work event and then processing those events synchronously the eventing Broker will not get upset if your requests take a long time to complete If you are worried about nonuniform processing times you can even run a second copy of the Knative Service just for handling the longrunning requests "
70310955,70310954,"stackoverflow.com",2,"2021-12-13 21:06:24+02","2024-05-17 05:42:13.80713+03","This is because the documentation for adding an environment variable is wrong or confusing at best The env node should be a child of the image and not the containers node as it says here httpscloud google comrundocsconfiguringenvironmentvariablesyaml This is correct"
70316427,70314634,"stackoverflow.com",2,"2021-12-11 17:23:19+02","2024-05-17 05:42:14.800851+03","It looks like your application runs on port 80 based on the EXPOSE 80 line in your Dockerfile but the default for Knative is to run on 8080 exposed as PORT to your container You can override this by setting a single containerPort in your yaml to point Knative at the port your application serves on "
70329696,70329177,"stackoverflow.com",0,"2021-12-13 05:38:56+02","2024-05-17 05:42:15.712599+03","You will need to use GKE 1 21 or later which adds support for mutating admission we hooks It should work on GKE 1 21 or later assuming that autopilot clusters support pods with those resource requirements General installation instructions should work assuming you have GKE 1 21 or later "
70483798,70483327,"stackoverflow.com",1,"2021-12-27 01:33:22+02","2024-05-17 05:42:16.487737+03","I am assuming there is a typo in your error message it doubles the b if not I would look at that first Do you have any sort of controller or mutating webhook operating on deployments The message that the controller cannot update the deployment because there is a newer version suggests that there is some sort of racing update going on "
70607705,70607204,"stackoverflow.com",0,"2022-01-06 15:16:39+02","2024-05-17 05:42:17.204038+03","for the Sequence example I think there might be something wrong with your default channel configuration Sequences create channels to communicate between every step it is expected that three channels are created for that example Can you check what channels are created for you and their status Can you also make sure your default channel is properly setup See httpsknative devdocseventingchannelschanneltypesdefaults Can you also post here the Parallel you are using Status is up to the controller to fill that one sounds like an issue "
56043042,56042511,"stackoverflow.com",3,"2019-05-08 17:26:29+03","2024-05-17 05:42:59.817184+03","In your server code you are not listening on port 443 so this is most likely the reason your example is not working If you want to keep using http and not https then your code is working just fine If you want to get it working with TLS this overview is a pretty good one To get port 80 to redirect to port 443 I highly recommend it if you are using https see this SO post "
70670206,70607204,"stackoverflow.com",0,"2022-01-11 18:31:18+02","2024-05-17 05:42:17.206086+03","At both of your outputs for sequence and parallel an issue with services can be found Although Knative Eventing does not depend on Serving the examples for sequenceparallel requires it to be properly installed because they use serverless services Eventing can use regular kubernetes services instead of knative services but I think the best way to make the examples work for you is making sure Knative Serving works as expected Did you configure a network provider for Knative Serving If you did not can you go through this step httpsknative devdocsinstallservinginstallservingwithyamlinstallanetworkinglayer If you are in doubt about which one to choose I would go for Kourier which is maintained by the Knative project "
70946976,70946255,"stackoverflow.com",3,"2022-02-01 22:13:42+02","2024-05-17 05:42:17.931691+03","It looks like all temporal code runs inside a persistent temporal server That probably makes it a poor fit for an environment like Cloud Run or Knative or AWS Lambda containers Looking further through the doc it also appears that multiple temporal servers end up individually addressing each other through their own clustering protocol From the video at the start it does seem like you could use an Activity to encapsulate a call to a service running on Knative or Cloud Run "
71070327,71066772,"stackoverflow.com",0,"2022-02-10 20:19:27+02","2024-05-17 05:42:19.090661+03","The suggested way is to expose storage as an external service SQL database Redis S3compatible object storage like Minio or Ceph or custom API If you are running a recent Knative build there are feature flags which can enable PersistentVolumeClaims but these shared filesystems can introduce scaling issues and data corruption bugs depending on locking implementation so are not enabled by default "
62723832,62722297,"stackoverflow.com",2,"2020-07-04 02:55:22+03","2024-05-17 05:42:21.107497+03","I do not think there is a great way to expand environment variables inside of an existing yaml but if you do not want to use sed you might be able to use envsubst You would just run this command before you use the yaml to expand the environment variables contained within it Also I think you will need your variables to use curly braces instead of parentheses like this KUBE_NAMESPACE EDIT You might also be able to use this inline like this kubectl apply f envsubst service yaml "
62724275,62722297,"stackoverflow.com",2,"2020-07-08 19:24:48+03","2024-05-17 05:42:21.109498+03","More than a Knative issue this is more of Kubernetes limitation Kubernetes allows some expansion but not in annotations or namespace definitions For example you can do it in container env definitions If this is a CICD system like Gitlab the environment variables should be in a shell environment so a simple shell expansion will do For example You can also use envsubst as a helper like mentioned in the other answer "
62829099,62829052,"stackoverflow.com",2,"2020-07-10 15:54:05+03","2024-05-17 05:42:21.956204+03","After tracing the command using IBMCLOUD_TRACEtrue I found an error indicating that my currently set cloud region is not enabled Setting ibmcloud target r ussouth and then executing the above ibmcloud coligo proj create n HLTest succeeded Lessons learned Listing projects is global creating a project only works when a supported cloud region has been targeted before From vidyas answer Also set the resource group using ibmcloud target g RG with RG being the resource group Each account has a default resource group "
62831985,62829052,"stackoverflow.com",1,"2020-07-10 13:06:30+03","2024-05-17 05:42:21.957205+03","Another best practice is to set the resource group before creating a Code Engine project with the command below ibmcloud target g Default Default is the default resource group automatically created when you create an IBM Cloud account"
63042526,63031461,"stackoverflow.com",0,"2020-07-22 23:17:54+03","2024-05-17 05:42:22.736993+03","Cloud Run for Anthos apps do not work with a GKE Ingress Knative services are exposed through a public gateway service called istioingress on the gkesystem namespace Domain names etc work very differently on Cloud Run for Anthos so make sure to read the docs on that "
63618289,63615721,"stackoverflow.com",0,"2020-08-27 17:31:55+03","2024-05-17 05:42:24.457555+03","I do not think Knative Serving does not support this in the way you wanted There is an open issue for this httpsgithub comknativeservingissues4736 There are some concerns there and no decision yet made What you make with the VirtualService is right configuration but the VirtualService will be reconciled and your changes will be gone So that is not an option "
77775563,63615721,"stackoverflow.com",0,"2024-01-08 02:37:19+02","2024-05-17 05:42:24.459555+03","Knative now supports that see httpsgithub comknativedocstreemaincodesamplesservingtagheaderbasedrouting"
63763088,63618094,"stackoverflow.com",1,"2020-09-06 13:24:22+03","2024-05-17 05:42:25.512892+03","Hope you your load balancer forward the traffic from 443 to the backend target port 3190 in case of Istio Check your Istio gateway file wether you have 443 port mapped with the targets "
63770212,63671125,"stackoverflow.com",0,"2020-09-07 04:01:20+03","2024-05-17 05:42:26.443096+03","as error says yaml has 2 containers multi container which is not supported by knative default thats why your yaml not accepted by knative webhook now v0 17 supports multi container you should try this first httpsgithub comknativeservingblobmasterconfigcoreconfigmapsfeatures yamlL44 httpsknative devdocsservingfeatureflagscontrol In my case I did collect log to ES using just stdout and it works pretty easy so i didnt try your apporach however it seems that you dont need to mount additional volume considering this config file httpsgithub comknativeservingblobmasterconfigmonitoringloggingelasticsearch100fluentdconfigmap yamlL46 I have no better idea anymore hope you find best way "
63928139,63912397,"stackoverflow.com",1,"2020-09-17 00:25:20+03","2024-05-17 05:42:27.700391+03","According to the Cloud Run on Anthos Official Documentation the error read tcp 127 0 0 139130127 0 0 180 read connection reset by peer is caused because the URL you are trying to access is not allowed by your HTTP proxy To mitigate this issue please check that your HTTP proxy allowlist includes the following URLsdomains"
64028340,64025710,"stackoverflow.com",2,"2020-09-23 17:46:50+03","2024-05-17 05:42:28.588059+03","You can change the configdomain config map in the knative namespace you can see the config like this Then you can update it like this more detail here With the new domain name configure your DNS registrar to match your domain name to the load balancer external IP and you website will present the correct host on each request The curl H Host is a cheat to lie to the Istio controller and say to it Yes I come from there If you really come from there your own domain name no need to cheat "
64212482,64210432,"stackoverflow.com",1,"2020-10-05 19:26:40+03","2024-05-17 05:42:29.455764+03","How to create the knative serving using golang You need to use the Goclient for Knative Serving e g the client type corresponding to the corev1 Pod that you used in your code The Go client for Knative v1 Serving is in the Knative repository Instead of CoreV1 in your code you can use ServingV1 from the Knative Go client But I would recommend to use Yaml manifests unless you have custom needs "
64228815,64227749,"stackoverflow.com",3,"2020-10-06 18:27:16+03","2024-05-17 05:42:30.622234+03","Yes code generation for controllers is not the most easy thing And it has changed over the years To start writing a controller with code generation I would recommend to use Kubebuilder and follow the Kubebuilder guide And perhaps do custom things when that is understood The Kubebuilder guide includes chapters on how to generate CRD code using controllergen "
73227253,59747412,"stackoverflow.com",2,"2022-08-03 23:08:31+03","2024-05-17 05:43:11.94247+03","Knative has introduced init containers injection behind a feature flag Please check the documentation on knative dev site httpsknative devdocsservingconfigurationfeatureflagskubernetesinitcontainers"
65684440,65123023,"stackoverflow.com",0,"2021-01-12 14:57:34+02","2024-05-17 05:42:33.05252+03","I had to buy support for Google Cloud to get a good answer to this They told me to make adjustment to my cloud service instances but none to any effect They later admitted that this was due to a problem on their end It is a shame that you as a user do not get any feedback on problems like these when using the Google Cloud Platform a simple notification in the Google Cloud console for affected users would be of great help but I think they may like to cover these things up as to not worsen the service accessibility numbers "
65312665,65307834,"stackoverflow.com",4,"2020-12-16 11:55:16+02","2024-05-17 05:42:33.788701+03","Knative contract as you said does not allow to mount or claim a volume So you cannot achieve this for now on Cloud Run managed On the other hand Pod allow this but Knative is a special version of Pod no persistent volume and you cannot define a list of container it is a pod with only one container the mesh most of the time Istio sidecar injected when you deploy For you additional question Cloud Run implements Knative API And thus you need to present Knative serving YAML file to configure your service If you want to write file you can do it in the tmp in memory partition So at your container starts download the file and store them there However if you update the files and you need to push the update you need to push them manually to Cloud Storage In addition the other running instances that have already downloaded the file and stored them in their tmp directory will not see the file change in Cloud Storage it is only the new instances UPDATE 1 If you want to download the files before the container start you have 2 solutions The previous solution has 2 issues UPDATE 2 For the solution1 it is not a Knative solution it is in your code I do not know your language and framework but at startup you need to use Google Cloud Storage client library to download from your code the file that you need Show me your server startup I could try to provide you an example For the solution 2 the files are not in your git repo but still in your Cloud Storage Your Docker file can look like to this You can also imagine downloading the file before the Docker build command and simply perform a copy in the Dockerfile I do not know your container creation pipeline but it is ideas that you can reuse "
66779351,65340294,"stackoverflow.com",0,"2021-03-24 12:43:09+02","2024-05-17 05:42:34.670416+03","I fixed it by upgrading ambassador to the latest version using Host and TLSContext to configure TLS and adding a Host config to serve cleartext for this host"
56623289,56573188,"stackoverflow.com",2,"2019-06-17 02:19:15+03","2024-05-17 05:42:36.329107+03","The annotation had to be added to the PodAutoscaler object Or you could set the minScale on your yaml configuration file as described in the link"
56576457,56573188,"stackoverflow.com",0,"2019-06-13 11:30:46+03","2024-05-17 05:42:36.329954+03","I think the annotation has to be added to the Revision object but you are annotating the Service object and that is why it will not work Try listing all your Revision objects and annotating the one you are interested in using the same command that you were using for annotating the Service "
56777151,56624532,"stackoverflow.com",1,"2019-06-26 19:15:02+03","2024-05-17 05:42:37.244469+03","The latest release of Knative v0 7 is v1beta1 not alpha and as far as I am aware the APIs are now very close to how they will be in v1 0 We currently use Knative v0 7 and I would suggest developing the apps using that until v1 0 is released R E Knative v1 0 release I would keep an eye on these two github milestones httpsgithub comknativeservingmilestone18 httpsgithub comknativeservingmilestone24 The latter milestone is due on August 06 2019 currently The ambition of 0 8 [this milestone] is to be a release candidate for a v1 of serving "
56651025,56650085,"stackoverflow.com",2,"2019-06-18 17:06:49+03","2024-05-17 05:42:38.229278+03","Most proxies that handle external traffic match requests based on the Host header They use what is inside the Host header to decide which service send the request to Without the Host header they would not know where to send the request Hostbased routing is what enables virtual servers on web servers Its also used by application services like load balancing and ingress controllers to achieve the same thing One IP address many hosts Hostbased routing allows you to send a request for api example com and for web example com to the same endpoint with the certainty it will be delivered to the correct backend application That is typical in proxiesload balancers that are multitenant meaning they handle traffic for totally different tenantsapplications sitting behind the proxy "
56661594,56650085,"stackoverflow.com",1,"2019-06-20 10:14:51+03","2024-05-17 05:42:38.231279+03","All answers given are correct more or less but I would like to post a more concrete description of the situation I came about after some digging As pointed out by other fellows in GKEbased cloud run istio manages routing Therefore by default and unless there is a way to override that behavior istio will create an istio ingress gateway handling your incoming traffic a virtual service with the routing rules for the specific container you spin up via gcloud cloud run deploy So I discovered this resource whose description and the corresponding hostbased routing rules explain the need for passing the specific `Host What is more in case you add a custom domain mapping it turns out GCP takes care the routing by creating an additional virtual service in the default namespace this time"
56653444,56650085,"stackoverflow.com",0,"2019-06-18 19:28:36+03","2024-05-17 05:42:38.232948+03","As mentioned by Jose Armestos answer its simply because Cloud Run on GKE uses Knative which uses Istio Istio ingress gateway receives all traffic to all your Cloud Run services and proxies them to the right place based on registered hostnames of the service If you Map custom domains using Cloud Run and actually set up your domains DNS records to point to the ingress gateway of your Cloud Run on GKE set up you will not need it as you will actually have a domain name thats used in the Host header and recognized by the gateway So the traffic will flow to the right place "
57451527,57313405,"stackoverflow.com",0,"2019-08-11 19:01:21+03","2024-05-17 05:42:40.337318+03","I am able to solve the problem using pistache for REST api I guess any framework should work "
57456760,57451700,"stackoverflow.com",0,"2019-08-12 09:50:00+03","2024-05-17 05:42:41.173528+03","That problem occurs because you are trying to run your binary from bash but scratch has no bash I am normally using alpina instead To build for alpina you need the same environment variables so probably you only need to change a second stage image "
57691264,57652780,"stackoverflow.com",2,"2019-08-28 14:23:00+03","2024-05-17 05:42:42.896238+03","You definitely do not have enough resources Only for Istio on Minikube you need 16384 MB of memory and 4 CPUs Add to this requirements for Knative which are not included in the above and you will see that resources you provide are not enough "
57746431,57730416,"stackoverflow.com",0,"2019-09-01 16:34:48+03","2024-05-17 05:42:44.018051+03","You cannot make changes to this confgiMap as the GKE reconciler will revert the changed you make back Any resource with the addonmanager kunernetes iomode reconcile will have any changes made revert back as part of the managed components of GKE"
57829135,57743492,"stackoverflow.com",0,"2019-09-07 01:24:30+03","2024-05-17 05:42:45.059653+03","I do not think this is the answer to the issue you are facing but knative eventing seems not to have been released for gloo yet httpsgithub comsoloioglooissues753"
60326257,57743492,"stackoverflow.com",0,"2020-02-20 20:17:05+02","2024-05-17 05:42:45.060653+03","as pointed out Eventing does not depend on the network layer it depends solely on Serving which in turn depends on a network layer such as Gloo "
75703834,74393202,"stackoverflow.com",0,"2023-03-11 11:05:17+02","2024-05-17 05:44:34.308833+03","The way to do this without writing any custom code is to create a PingSource KafkaSink that would send a fixed event periodically to a Kafka topic Configure the event source object Configure the event sink object"
57981339,57950399,"stackoverflow.com",1,"2019-09-17 23:04:28+03","2024-05-17 05:42:46.877181+03","Yes you should be able to deploy Cloud Run services from Cloud Run services From there you have several options Use the REST API directly Since run googleapis com behaves like a Kubernetes API server you can directly apply JSON objects of Knative Services You can use gcloud loghttp to learn how deployments are made using REST API requests Use gcloud you can ship your container image with gcloud and invoke it from your process Use Google Cloud Client Libraries You can use the client libraries that are available for Cloud Run for example this Go library to construct inmemory Service objects and send them to the API using a higher level client library recommended approach "
58107549,58106188,"stackoverflow.com",3,"2019-09-26 01:57:29+03","2024-05-17 05:42:47.852583+03","This is configurable via the ConfigMap named confignetwork in the namespace knativeserving See the ConfigMap in the deployment resources Therefore your confignetwork should look like this You can also have a look and customize the configdomain to configure the domain name that is appended to your services "
68788032,58106188,"stackoverflow.com",0,"2021-08-15 03:57:24+03","2024-05-17 05:42:47.854582+03","Assuming you are running knative over an istio service mesh there is an example of how to use an Istio Virtual Service to accomplish this at the service level in the knative docs "
58322685,58115936,"stackoverflow.com",1,"2019-10-10 15:15:20+03","2024-05-17 05:42:48.813954+03","A similar issue was fixed doing that in this GitHub thread httpsgithub comknativeservingissues4868 The main cause is that in a private GKE cluster by default only the GKE master have access to the services at port 443 or 80 Could you try to use the port 8443 instead of the 443 and whitelist the port 8443 httpscloud google comkubernetesenginedocshowtoprivateclustersadd_firewall_rules"
58117853,58115936,"stackoverflow.com",0,"2019-09-26 16:24:54+03","2024-05-17 05:42:48.815954+03","Can you try to deploy Cloud Run service with yaml file Here the file Simply perform a kubectl apply f file from your bastion If it works this means that gcloud command cannot communicate with a private cluster and you can open an issue on this "
58268162,58222766,"stackoverflow.com",2,"2019-10-07 13:56:43+03","2024-05-17 05:42:49.851307+03","I think you are missing the local and it should work But you should be able to drop the ClusterIP entirely as you are making the request from within the cluster Basically execute The DNS helloworldgo default svc cluster local will be resolved by the Kubernetes DNS resolution mechanism It is important to specify the HOST header only on ingress traffic public traffic entering into private traffic Details The Knative Service creates a Route CRD which creates a ClusterIngress which will be picked up by a controller by default istionetwork to configure the Ingress Gateway using another CRD in istios case VirtualService Ingress Gateways routing is based on the HOST header "
58294759,58294347,"stackoverflow.com",2,"2019-10-09 01:22:37+03","2024-05-17 05:42:50.718757+03","Thanks to great community help on solo io slack I have got answers "
53545915,53512182,"stackoverflow.com",1,"2018-11-29 21:05:07+02","2024-05-17 05:42:52.308543+03","There is a bug in recent versions of KNative which has been documented in this issue httpsgithub comknativeservingissues2218 There is already an approved but not yet merged PR about it you can see here httpsgithub comknativeservingpull2560 In short the problem is that fluentd pods use systemnodecritical priority class which is no longer supported outside of kubesystem namespace As a result fluentd pods do not get created and therefore do not send any logs to Elasticsearch and consequently no logstash indexes showup in Kibana As a work around for KNative v0 2 2 you can download and delete line 1909 from the release file here httpsgithub comknativeservingreleasesdownloadv0 2 2release yaml You can then install the patched version kubectl apply f release yaml If you do not want to download and edit you can get an already patched version of release 0 2 2 here which you can install with kubectl apply f httpsgithub comgevouknativeblueprintblobmasterknativeservingrelease0 2 2patched yaml You can do something similar for previous versions of course "
53755675,53512182,"stackoverflow.com",0,"2018-12-13 07:49:32+02","2024-05-17 05:42:52.310544+03","Here are some additional steps which I have to perform to make this completely work Posting here so that this may help someone facing the same issue and looking for an answer Here are the steps Run the below command to apply a patch to fix the fluentdds pods not showing issue Verify that each of your nodes have the beta kubernetes iofluentddsreadytrue label If you receive the No Resources Found response Run the following command to ensure that the Fluentd DaemonSet runs on all your nodes Run the following command to ensure that the fluentdds daemonset is ready on at least one node Wait for a while and run this command Navigate to the Kibana UI It might take a couple of minutes for the proxy to work Within the Configure an index pattern page enter logstash to Index pattern and select timestamp from Time Filter field name and click on Create button To create the second index select Create Index Pattern button on top left of the page Enter zipkin to Index pattern and select timestamp_millis from Time Filter field name and click on Create button If the issue still exists following the suggestions in the comments above should fix the error Added the endtoend findings here"
54086029,54078705,"stackoverflow.com",4,"2019-01-08 08:04:56+02","2024-05-17 05:42:53.034655+03","Knative uses Istio and Istio by default does not allow outbound traffic to external hosts such as httpbin org That is why your request is failing Follow this document to learn how to configure Knative so that it configures Istio correctly to make outbound connections Or you can directly configure the Istio by adding an egress policy httpsistio iodocstaskstrafficmanagementegress"
55666887,55401253,"stackoverflow.com",2,"2019-06-25 20:13:55+03","2024-05-17 05:42:54.088926+03","UPDATED 25619 As per httpsgithub comknativeservingpull4196 in Knative v0 7 you can now specify MaxRevisionTimeoutSeconds which can be any integer timeoutSeconds must be less than or equal to MaxRevisionTimeoutSeconds timeoutSeconds defaults to 300 OLD You can change timeoutSeconds which I believe defaults to 300 seconds "
55666849,55531556,"stackoverflow.com",0,"2019-04-13 18:01:15+03","2024-05-17 05:42:54.962272+03","Currently Kubernetes generates a random string that Knative uses and hence there is no order It is possible to create your own revision names but you have to make sure that each revision name is unique i e you need to change mynextrevision A member of the Knative team has said they are considering a v1beta1 API proposal is to allow users to bring your own revision name Full details all qudos for this information can be found from Matthew Moore in this Google Groups Discussion"
55898416,55861375,"stackoverflow.com",1,"2019-04-29 10:01:13+03","2024-05-17 05:42:57.130246+03","IMHO going with KNativeKubernetes is probably not the way to proceed here You will have to manage a ton of complexity just to get some IP addresses Beanstalk will seem like a walk in the park Depending on how many IPs you need you can just setup a few EC2 instances loaded up with IP addresses One cheap t3 small instance can host 12 IPv4 addresses ref and your JS code can simply send requests from each of the different IP addresses Depending on your JS http client usually there is a localAddress option you can set "
60228652,55911235,"stackoverflow.com",0,"2020-02-14 16:54:18+02","2024-05-17 05:42:57.96189+03","First look at the logs being issued by the cert manager pod kubectl logs n namespace podpodname Cert manager will tell you why the challenge is failing One common reason is the rate limiting by Letsencrypt and you have to wait for 7 days You can also view this same issue on github httpsgithub comjetstackcertmanagerissues1745"
59571182,55971277,"stackoverflow.com",1,"2020-01-03 01:09:35+02","2024-05-17 05:42:58.985929+03","For anyone that stumbles into this it was eventually resolved in the Github Issue httpsgithub comknativeservingissues1996"
56352504,56352181,"stackoverflow.com",2,"2019-05-29 17:25:56+03","2024-05-17 05:43:01.678917+03","[Update This person has a networking problem in his area I tested his endpoint from Seattle with no problems Details in the comments below ] I have worked with Cloud Run constantly for the past several months I have deployed several production applications and dozens of test services I am in Seattle Cloud Run is in uscentral1 I have never noticed a delay Actually I am impressed with how fast a container starts up For one of my services I am seeing cold start time to first byte of 485ms Next invocation 266ms 360ms My container is checking SSL certificates 2 on the Internet The response time is very good For another service which is a PHP website time to first byte on cold start is 312ms then 94ms 112ms What could be factors that are different for you Increase the memory allocated to the container Does this affect performance PythonFlask does not require much memory my containers are typically 128 MB and 256 MB Container images are loaded into memory so if you have a bloated container you might now have enough memory left reducing performance What does Stackdriver logs show you You can see container starts requests and container terminations "
56434734,56352181,"stackoverflow.com",0,"2019-06-04 00:26:23+03","2024-05-17 05:43:01.680918+03"," Cloud Run product manager here We have detected high latency when calling Cloud Run services from some particular regions in the world Sadly Seoul seems to be one of them We will explicitly capture this as a Known issue and we are working on fixing this before General Availability Feel free to open a new issue in our public issue tracker"
58556648,58553294,"stackoverflow.com",0,"2019-10-28 21:18:45+02","2024-05-17 05:43:03.720868+03","When using Knative it is recommended that all your requests are routed through the Ingress Gateway as such you ensure to make use of the additional capabilities provided by Knative like autoscaling if pods are scaled down to 0 The Kubernetes Service underneath is used just as a mechanism for collecting all endpoints of the running pods If there are no running pods it cannot scale up and will return a default response when no backends are present The problem that you are having is exactly this using the Kubernetes Service there are no pods running You need to use the Ingress gateway To do so grab its IP by executing the command below And use this IP in your browser The immediate problem that you will face is that the ingress gateway routes based on the header and you lack the header For this reason install a chrome extension that adds the header on the request I use ModHeader After adding the header in your chrome extension in your case Host helloworld default example com opening the IP you will reach your pod even if it is down "
58749248,58710291,"stackoverflow.com",2,"2019-11-07 14:56:00+02","2024-05-17 05:43:04.618931+03","I had this same problem and I assume it was because I had older Cloud Run deployment that was created before I had ran gcloud components update since some update I was able to fix it by deleting the whole Cloud Run service through the GUI and deploying it from scratch again via terminal I noticed that the ports definition disappeared from the YAML once I did this After this I could do deployments normally "
58766065,58710291,"stackoverflow.com",2,"2019-11-08 13:52:32+02","2024-05-17 05:43:04.620932+03","This was a bug in Cloud Run It has been fixed and deploying with CLI is working for me now Heres the link to the issue I had raised with Google Cloud which has a response from them httpsissuetracker google comissues144069696 "
58863605,58860118,"stackoverflow.com",12,"2019-11-15 09:13:49+02","2024-05-17 05:43:05.747492+03","Knative has a new concept CRD known as the Serverless service which is created for each Knative Service The serverless service creates two Kubernetes Services Serverless Services can be in one of the following modes The serverless service is in Serve mode as long as there are pod instances of your application running As such your Public service is configured with the endpoints from your private service meaning that requests forwarded by the ingress gateway reach your application as shown in the diagram below When the instances of your application are scaled down by the autoscaler the serverless service controller updates the public service to be configured with the IPs discovered by the Activator Service Which triggers autoscaling buffers the request until one service is up and running and forwards the request Proxy mode can be seen in the diagram below As a summary the Serverless controller sets the endpoints of the public service by alternating between the endpoints of the Private Service or if it is scaled down to zero to the endpoints of the Activator Service "
58925430,58923740,"stackoverflow.com",1,"2019-11-19 03:40:42+02","2024-05-17 05:43:06.614389+03","Presumably you should be able to differentiate events by using the different sourcesmethods coming from that event A quick look at the event spec shows you could split into channels based on source for example The main thing I see that is not being utilized here is the context object It seems you could glean the source from that context This can be seen in their hello world example check out the receive function For your example If there are say 3 different sources to be supported you can use the factory pattern to instantiate those different channels and an interface that all of those implement "
58963159,58961512,"stackoverflow.com",0,"2019-11-20 22:49:52+02","2024-05-17 05:43:07.652866+03","Knative uses a shared ingress gateway to serve all incoming traffic within Knative service mesh which is the knativeingressgateway gateway under knativeserving namespace By default it uses Istio gateway service istioingressgateway under istiosystem namespace as its underlying service You can replace the service with that of your own as follows[1]and for more detail steps refer to link[2] [1] httpsknative devdocsservingsettingupcustomingressgateway [2] httpsstarkandwayne comblogpublictrafficintoknativeongke"
59061513,59054617,"stackoverflow.com",1,"2019-11-27 02:52:42+02","2024-05-17 05:43:08.611921+03","Problem was I had installed regular Istio where you specifically need Istio with SDS httpsknative devdocsservingusingautotls Also be sure to use certmanager 0 10 0 as 0 11 0 is currently unsupported httpsgithub comknativeservingissues6011 Once I did these everything works"
59728573,59710798,"stackoverflow.com",1,"2020-01-14 08:31:49+02","2024-05-17 05:43:10.742656+03","The knativeeventing issue 2376 opened I supposed by the OP includes an explanation from Vincent houshengbo Vincent refers to Knative Eventing Sources about how to install the eventing sources GitHubSource is one of them and all the available sources are be found here The repo is httpsgithub comknativeeventingcontrib However there should be instruction on how to install the sources Install Knative Eventing Those instructions also install the default eventing sources including the GitHubSource we will use I have not found any I guess you need to install it by for 0 11 0 "
59747836,59747412,"stackoverflow.com",4,"2020-01-15 10:52:50+02","2024-05-17 05:43:11.939469+03","Init containers are not there in the Knative serving API spec which leaves me to believe that it is not supported "
59797661,59747412,"stackoverflow.com",2,"2020-01-18 06:57:25+02","2024-05-17 05:43:11.94047+03","To add a little more color to Arghyas answer Knative deliberately excludes init containers right now because they can induce large unbounded amounts of additional latency during coldstart Additionally one of the goals for Knative was to simplify much of the Kubernetes API so solutions like init containers which can also be implemented as startup parts of the user container were blocked off One reason for being restrictive here is that it is much easier to add to an API later than to remove from it so the initial API was minimal to see which constraints were substantially onerous "
77260246,60151779,"stackoverflow.com",0,"2023-10-09 18:53:53+03","2024-05-17 05:43:15.600385+03","This is done on purpose and is one of the main advantages of Knative Eventing Knative Eventing adds a layer of abstraction to asynchronous communication in a specific technology e g Kafka simplifying that communication to the familiar HTTP Only from the point of service is the communication synchronous Looking at the whole system communication is asynchronous all the time and managed by a Broker or Channel centrally "
60209009,60197617,"stackoverflow.com",2,"2020-02-13 15:28:46+02","2024-05-17 05:43:16.371864+03","I cannot understand why you want to do it this way Is not cloud function intended to do not care about virtualization containerization servers etc Seems you want to do something that Google Cloud Functions want to avoid As requested I am attaching this It is really worth to watch "
73136490,60197617,"stackoverflow.com",0,"2022-07-27 13:42:21+03","2024-05-17 05:43:16.372859+03","Your use case sounds like GCR Google Cloud Run is the solution you want it runs docker containers "
60435952,60251424,"stackoverflow.com",1,"2020-02-27 16:54:37+02","2024-05-17 05:43:18.42065+03","Do you have cluster local gateway enabled If not then this might be the reason why it is not working Details at the following link httpsknative devdocsinstallinstallingistio"
60523719,60251424,"stackoverflow.com",0,"2020-03-08 13:01:35+02","2024-05-17 05:43:18.421367+03","Your application is accepting GET requests only as you can see here httpsgithub comknativedocsblobmasterdocsservingsampleshelloworldhelloworldpythonapp py As you can see in the cloudevents spec httpsgithub comcloudeventsspecblobmasterhttpwebhook md21deliveryrequest The HTTP method for the delivery request MUST be POST You have to use POST method to trigger your service If you want to fix it you have two simple choices You can change the image of the service to event_display gcr ioknativereleasesgithub comknativeeventingsourcescmdevent_display You can change the code of your application and change L7 from app route to app route methods[GET POST] "
60453922,60416312,"stackoverflow.com",1,"2020-02-28 16:45:53+02","2024-05-17 05:43:19.342286+03","According to Marios reply I solved configuring Docker HUB credentials Here the procedure I think KNative for some reason do not simply pull the image but it does some additional stuff i e verify digest that requests Docker HUB authentication According to the procedure linked if you give the command Then you need to modify the service yaml in this way"
64393376,60429443,"stackoverflow.com",1,"2020-10-16 19:54:11+03","2024-05-17 05:43:19.92883+03","I had the same issue But with version 0 16 0 I fixed it by not using Enabling automatic TLS certificate provisioning with certmanager instead I used the HTTP01 directly provided from knative How to automatically provisioning TLS certificates using Lets Encrypt HTTP01 challenges"
61901508,60452769,"stackoverflow.com",0,"2020-05-20 01:02:05+03","2024-05-17 05:43:21.527891+03","This is a client side timeout most likely Looking at your service it will timeout internally after 500s if no response data is sent at all is usually the giveaway "
60690800,60689880,"stackoverflow.com",3,"2020-03-15 09:56:27+02","2024-05-17 05:43:22.455702+03","Cluster local gateway need to be installed as part of knative installation Since you are using GKE to install istio instead of using helm you need to install it manually here VERSION is istio version i e istio1 5 0 httpsgithub comknativeservingblobmasterthird_partyistio1 5 0istioknativeextras yaml"
60854616,60848031,"stackoverflow.com",2,"2020-03-25 19:56:51+02","2024-05-17 05:43:23.425555+03","There was issue with ambassador CRD cluster role Make sure you have below entry in ambassador"
60969130,60967905,"stackoverflow.com",0,"2020-04-01 13:46:54+03","2024-05-17 05:43:24.522549+03","Your case is about Knative ingress and not Kubernetes From the inbound network connectivity part of the runtime contract of the knative documentation In addition the following base set of HTTP1 1 headers MUST be set on the request Also the following proxyspecific request headers MUST be set Look to the headers inside your request An example for servlet doGet method"
61516057,61384188,"stackoverflow.com",3,"2022-02-08 00:32:31+02","2024-05-17 05:43:26.617706+03","kubernetes client V1Service is a reference to the Kubernetes Service concept which is a selector across pods that appears as a network endpoint rather than the Knative Service concept which is the entire application which provides functionality over the network Based on this example from the kubernetesclientpython repo you need to do something like this to get and use a client for Knative services If you are going to be doing this a lot you might want to make a helper that takes arguments similar to create_namespaced_service and possibly also a wrapper object similar to kubernetes client V1Service to simplify creating Knative Services "
61386844,61384188,"stackoverflow.com",2,"2020-04-23 15:06:29+03","2024-05-17 05:43:26.619706+03","Try using create_namespaced_custom_object Refer httpsgithub comkubernetesclientpythonblobmasterkubernetesdocsCustomObjectsApi mdcreate_namespaced_custom_object Here service is a custom resource specific to Knative "
61741934,61723398,"stackoverflow.com",0,"2020-05-12 03:53:27+03","2024-05-17 05:43:27.700583+03","What version of Knative are you using It looks like your Knative webhook may be generating a certificate with an empty subject Have you tried connecting to the webhook directly on the cluster via curl kvv httpswebhook knativeserving svc443defaulting That should print out the certificate and ignore the validation so that you can test that the service is working "
61790053,61742958,"stackoverflow.com",0,"2020-05-14 08:37:31+03","2024-05-17 05:43:28.969401+03","It is likely that Java will be higher performance than python but since you have not said what concurrency you are targeting it is hard to assess how much hardware you will need to handle the load If suggest choosing the language and told that feel best for you to start with and measure how well the system handles the load before optimizing your language and framework Note that Knative will automatically scale the number of containers based on requestsinflight so you can potentially handle high throughput by running many containers with a small amount of parallelism per container of that makes more sense e g due to Python global interpreter lock For Java a natural choice might be Spring For Python I have been happy with Flask In either case I would recommend choosing a stack that you are comfortable with or one you always wanted to learn if it is a hobby project and measuring to see if it performs well enough "
75115101,61742958,"stackoverflow.com",0,"2023-01-14 00:41:16+02","2024-05-17 05:43:28.971401+03","welcome to 2023 Mid2022 the Knative project launched the func tool which allows you to avoid needing to choose things like web frameworks and instead to simply get started writing your function from a simple template The function tool will then add the necessarily libraries without you needing to get into the details of Flask Spring whatnot "
61894057,61893180,"stackoverflow.com",1,"2020-05-19 17:52:48+03","2024-05-17 05:43:29.960881+03","If you have an existing container which listens on port 80 instead of PORT you can set spec template spec containers[0] ports[0] containerPort to indicate which port the container listens on For example Knative will automatically set the PORT environment variable to the requested container port so setting containerPort should work with any of the Knative samples "
65160279,61893180,"stackoverflow.com",1,"2020-12-05 20:04:29+02","2024-05-17 05:43:29.962371+03","If I understand the question this is similar to an issue I have run into trying to configure Knative with the Istio ingress gateway on nonstandard ports Following the Knative instructions for Istio integration I applied the Knative Istio controller netistio This includes a networking istio io Gateway that selects istio ingressgateway that specifies port 80 Modifying this resource to the correct port 8080 in the question should fix the issue "
61917118,61910325,"stackoverflow.com",0,"2020-05-20 18:37:39+03","2024-05-17 05:43:30.871981+03","You can get the Prometheus metrics exported to Cloud Monitoring using the sidecar built for that purpose Logs are going to be harder you need to figure out a way to reconfigure the plugin to send logs to stdout andor stderr to get the cluster logging agent to pick them up and ingest them into Cloud Logging Another option would be to build a sidecar container explicitly for that purpose "
63525855,62195356,"stackoverflow.com",2,"2020-08-21 22:14:32+03","2024-05-17 05:43:32.364853+03","There is no one way you could approach deploying a Knative Service with Tekton but a couple key things to consider are as follows These instructions assume proper installation of both TektonKnative Serving on a Kubernetes cluster Tekton allows the use of Kubernetes ServiceAccounts to assign permissions e g creating a Knative service so that a TaskRun or PipelineRun i e the execution of a Tekton CICD process is able to create and update certain resources In this case the creation of a Knative Service In order to create a ServiceAccount with permissions to create and update a Knative Service you would need to create a role that is similar to what is shown below The Role above allows for the creation and updating of Knative Services The Role can be associated with a ServiceAccount via a RoleBinding which assumes the creation of a ServiceAccount Once the Role RoleBinding and ServiceAccount are available you can use this ServiceAccount during a TaskRun or PipelineRun depending on how you are deploying a Knative Service The ServiceAccount to use can be specified via the ServiceAccountName property of a TaskRun or PipelineRun If you are using the Tekton cli tkn you can use s flag to specify what ServiceAccount to use when starting a PipelineRun or TaskRun NOTE The Role and RoleBinding examples are for a specific case that only allows deployment of the Knative Service to the same namespace where the TaskRunPipelineRun is executing To deploy the Knative Service via a TaskRun or PipelineRun you will need to create a Task with a Step that deploys the Knative Service There is no one tool that you could use in this case as you mention as far as using kubectl kn or any other tool to deploy to Kubernetes The important things for Tekton to know is the Task definition the image used by the Step and the command to run to deploy the Knative Service An example using kn is shown below In the example above a Task is defined with one Step called kn The Step uses the official kn image specifies to run the kn root command and then passes arguments to the kn command to create a Knative Service named knativeservice use an image for the Knative Service named gcr ioknativesampleshelloworldgo and the force flag that updates the Knative Service if it already exists EDIT Adding kubectl Example The question asks about using kubectl so I am adding in an example of using kubectl in a Tekton Task to deploy a Knative Service from a YAML file There is no exact way to answer this question but hopefully the examples and main points help with considerations around permissions for a PipelineRun or TaskRun as well as how to use certain tools with Tekton "
63563333,62195356,"stackoverflow.com",0,"2020-08-24 17:47:00+03","2024-05-17 05:43:32.369721+03","I wanted to add that a quick start way to get knative service created in a Pipeline is to apply the existing kn Catalog task httpsgithub comtektoncdcatalogtreemastertaskkn0 1 to your cluster and then fashion a TaskRun PipelineRun to run create with the options you require httpsgithub comknativeclientblobmasterdocscmdkn_service_create md The same could be said of kubectl using that Catalog Task "
62373873,62369688,"stackoverflow.com",2,"2020-06-14 17:27:07+03","2024-05-17 05:43:33.32764+03","MaxRevisionTimeoutSeconds is a clusterglobal setting which enforces the max value for TimeoutSeconds on each Revision This value exists so that cluster administrators can set upper bounds on the amount of time a single HTTP request can be in the system Knowing an upper bound can be useful when configuring graceful shutdown settings on the HTTP routing components to prevent dropped requests during upgrades It is possible that Cloud Run on GKE has overridden these configurations so that they can upgrade the underlying Istio and Knative components on a predictable schedule If you have a 10 upgrade budget and it takes 10m to drain a component your minimum upgrade time is probably around 110m taking into account additional scheduling image fetch startup time "
62519819,62518180,"stackoverflow.com",2,"2020-06-22 20:10:08+03","2024-05-17 05:43:34.104845+03","I think you are correct in assessing that current Cloud Run for Anthos set up unintentionally does not let you see the origin IP address of the user As you said the created gateway for IstioKnative in this case is a Cloud Network Load Balancer TCP and this LB doesnt preserve the clients IP address on a connection when the traffic is routed to Kubernetes Pods due to how Kubernetes networking works with iptables etc Thats why you see an xforwardedfor header but it contains internal hops e g 10 x x x I am following up with our team on this It seems that it was not noticed before "
62590952,62574130,"stackoverflow.com",1,"2020-06-26 11:37:47+03","2024-05-17 05:43:35.109339+03","You can use kn CLI httpsgithub comknativeclientblobmasterdocscmdkn_service_update md This would create a new revision for the Knative service though You can clean up old revisions with kn Get kn here httpsknative devdocsinstallinstallkn"
65881346,65365944,"stackoverflow.com",1,"2021-01-25 10:36:02+02","2024-05-17 05:43:36.069582+03","The Knative autoscaler relies on the pod strictly working in a requestresponse fashion As long as the huge amount of data is processed as part of an HTTP request or Websocket session or gRPC session etc the pod will not even be considered for deletion What will not work is sending the request immediately return and then munging the data in the background The autoscaler will think that there is no activity at all and thus shut it down There is a sandbox project that tries to implement such asynchronous semantics though "
65580738,65419195,"stackoverflow.com",0,"2021-01-05 16:24:22+02","2024-05-17 05:43:37.112251+03","This is being addressed in httpsgithub commeteatamelknativetutorialissues84 please follow that issue for updates "
65882940,65854143,"stackoverflow.com",3,"2021-01-25 13:39:32+02","2024-05-17 05:43:38.222208+03","I believe HTTP binary or HTTP structured should be used to transform event to headers and body Edit It might be required to set up bodyparser Also it is better to use cloneWith instead of spreading "
65882746,65854143,"stackoverflow.com",0,"2021-01-25 12:15:02+02","2024-05-17 05:43:38.224209+03","Hi there and sorry for the troubles I am not very familiar with the js sdk but the example we use here httpsgithub comknativedocsblobmasterdocsservingsamplescloudeventscloudeventsnodejsindex jsL64 uses send instead of json As far as configuring the sequence hopefully these examples help get the configuration of the sequence right httpsknative devdocseventingflowssequence Does the sequence Status show any errors And lastly do you see any errors in the logs Depending on which components you have installed this may vary but if you are running the core and with InMemoryChannel the logs would be in knativeeventing namespace and you would see the imcdispatcher pods which would provide a clue if the event is not constructed properly "
66178533,66177640,"stackoverflow.com",2,"2021-02-12 22:21:17+02","2024-05-17 05:43:39.275037+03","I would use KFServing for serverless inference on k8s You can install it as a standalone component or you can run it with the full Kubeflow toolkit You can inject your models from s3 into the serving containers which are optimized for whichever ML framework or language you might use It uses Knative Istio under the hood in order to achieve the scaletozero functionality "
66178765,66177640,"stackoverflow.com",2,"2021-02-12 22:43:15+02","2024-05-17 05:43:39.276334+03","Have you looked at Kubeless httpskubeless io I have not but something I would like to play with at some point Heres a quote from their site Kubeless is a Kubernetesnative serverless framework that lets you deploy small bits of code functions without having to worry about the underlying infrastructure It is designed to be deployed on top of a Kubernetes cluster and take advantage of all the great Kubernetes primitives If you are looking for an open source serverless solution that clones what you can find on AWS Lambda Azure Functions and Google Cloud Functions Kubeless is for you "
66189658,66177640,"stackoverflow.com",1,"2021-02-13 22:35:49+02","2024-05-17 05:43:39.278331+03","Check Knative project It has serverless workload management serving component and also eventing component httpsknative dev It had been adopted by many other projects A few of the commercial project are listed here httpsknative devdocsknativeofferings Knative is widely adopted and also has good open source community active contribution etc "
67009640,66177640,"stackoverflow.com",0,"2021-04-08 21:12:59+03","2024-05-17 05:43:39.279331+03","MicroFunctions is opensource Kubernetes Native Serverless platform bthat lets you deploy small bits of code without having to worry about the underlying infrastructure plumbing It leverages Kubernetes resources to provide autoscaling API routing monitoring troubleshooting and supports every programming language NodejsGopython httpsgithub commicrofunctionsiomicrofunctions"
66280017,66279000,"stackoverflow.com",1,"2021-02-19 17:04:51+02","2024-05-17 05:43:40.307281+03","I am wondering if it is possible is to extend Knative features to offer more capabilities How do you see users using setting capabilities There is no mechanism to allow custom properties in the resources spec without forking our controllers You could have users set labels and annotations on the spec template metadata of the Knative Service and have webhooks mutate the eventual pods that get created I do not have expertise to comment on Istio but if you need to create additional resources to set an auth policy you could write a controller that watches the virtual servicesgateways that knatives istio networking plugin creates If the auth policy modify the virtual service then you will probably encounter trashing as the controller will probably reconcile your changes away This is an area we need to explore whether K8s has facilities help with such coordination of controllerswebhooks Feel free to create an issue with more specifics"
66288171,66279000,"stackoverflow.com",0,"2021-02-20 06:56:43+02","2024-05-17 05:43:40.309562+03","I am glad to hear that you are finding Knative useful Knative is intended to complement rather than replace Kubernetes so I am going to point you to a few useful Kubernetes patterns which will work with Knative rather than suggesting that you fork or extend the Knative controllers to add capabilities like database deployment For features like Cloud Databases I would look into the operator pattern If you are working with cloud providers Crossplane provides a set of custom resources to allow developers to manage cloud resources using custom kubernetes objects For example here is the AWS RDSInstance type for provisioning RDS databases For Istio authorization policy in particular you might simply preconfigure a set of canned authentication policies and then use workload labels to select which ones should apply to a particular workload Note I am not an Istio user so this is based on reading their docs rather than practical experience If you need something more dynamic for example an annotation to indicate which principals can invoke the service you will probably want to write a controller to watch the Knative Services andor Deployments in the cluster and create the appropriate Istio AuthorizationPolicies on the fly It sounds like you are already using OAM so it might make more sense to build this into your application definition there Knative Serving might best be thought of as a better version of Deployment than a full application model "
66485371,66484358,"stackoverflow.com",0,"2021-03-05 03:18:42+02","2024-05-17 05:43:42.258559+03","I am guessing that you do not control the DNS for mydomain com In that case you will need to issue a command like curl H Host helloworldgo default mydomain com INGRESS_IP You may need to use minikube tunnel to get an ingress IP on minikube This is described here under the temporary DNS section of Configure DNS httpsknative devdocsinstallanykubernetescluster"
66503246,66484358,"stackoverflow.com",0,"2021-03-06 09:03:42+02","2024-05-17 05:43:42.259559+03","After applying so many solution this one worked for me If you are getting this error then something is wrong with you GATEWAY To resolve that thing try to deploy demo application from istio httpsistio iolatestdocstaskstrafficmanagementingressingresscontrol It will fix your gateway issue and you can successfully deploy your application using knative [NOTE If you are using minikube you can first try httpsistio iolatestdocssetupgettingstarteddownload may be this could work for you ] All the best"
66823468,66803231,"stackoverflow.com",0,"2021-03-26 21:53:15+02","2024-05-17 05:43:44.256535+03","Knative is a plugin to kubernetes You will manage secrets in knative the same way you would manage secrets in kubernetes httpskubernetes iodocsconceptsconfigurationsecret A Pod is used when you use Knative the same type of Pod if you where to use Kubernetes Deployment or DeamonSet httpskubernetes iodocsconceptsworkloadspods Now in terms of you the 3rd party API token Do not know your use case but what I am familiar is with the concept of using an API Key this you would put in a kubernetes secret then your Pod will have access when you application starts You would use the key to request a token also refresh token then use this token to access the API you will hold to it in memory and when is about to expire you refresh the token using the refresh token So what happens if your Pod shuts down when it scales down this could be in normal kubernetes not neccesarly knative Do you want to store that token some where maybe like a cache What people do in this cases they would save it in memcache or redis so if a new pod comes up it will check the cache for a recent token and if its valid then use it if its not valid or not found then use the API key from the Secret to get a new token httpszapier comengineeringapikeyoauthjwt Without more specifics is hard to answer more precisely "
67113441,66824534,"stackoverflow.com",0,"2021-04-15 20:41:14+03","2024-05-17 05:43:45.246939+03","I ended up sorting through enough docs to find the answer I will share details just for others who might find this or if I google it again The key was to a Set the required headers for the issue key Seting headers examples b Ensure that my properties are set correctly I used a configmap to set my properties and then referenced them as shown below in the URI I believe this should also be possible through DSL but URI was easiest for me to just get working Functional Integration below "
66898216,66897719,"stackoverflow.com",0,"2021-04-01 06:55:16+03","2024-05-17 05:43:46.807502+03","What I suspect is that the kn CLI is sending the request and is not compatible with the version of knative serving Make sure the version of the kn CLI is compatible with the version of knative serving you have installed I suggest it be safe to have the same versions For example I was able to get your example working using the kn CLI v0 21 using knative serving v0 21 you can check with kn version What I did to reproduce use konk and kn CLI Download the kn CLI binary v0 21 from here httpsgithub comknativeclientreleasestagv0 21 0 Then install kind 0 10 from here httpsgithub comkubernetessigskindreleasestagv0 10 0 Then install knative on kind konk httpskonk dev it takes less than 5 minutes konk comes with a serving and eventing sample apps Run the update command for the resource requests You will see the following output"
67710699,66897719,"stackoverflow.com",0,"2021-05-26 21:23:21+03","2024-05-17 05:43:46.810503+03","It looks like you are trying to us a newer version of the kn tool against a Service created by an older version of the kn tool In release 0 21 there was a switch from clientside to serverside naming support As a onetime transition from the clientside to the serverside naming you may need to run You can combine this with an existing update like so if you wish Note that clearing the clientgenerated revision name will create a new Revision and potentially roll it out on the server side since the name is part of the revision template on the server "
67465764,67465576,"stackoverflow.com",2,"2021-05-10 13:23:28+03","2024-05-17 05:43:47.945746+03","If you can prepull all images used in the Knative core then the answer is yes you can install and run Knative offline It is quite a few images but you should be able to find all of them easily by inspecting the installation manifests from the YAMLbased installation section of the Knative documentation Here are a few examples of images that compose the Knative core as of Knative v0 22 Serving Eventing Note although the image attributes I referenced above are in the format ko the release manifests documented in the YAMLbased installation section of the Knative documentation contain actual image URLs "
74051647,74038692,"stackoverflow.com",1,"2022-10-13 10:00:38+03","2024-05-17 05:44:30.654319+03","Correct the URI can be a HTTPS endpoint but the actual implementation depends on the dispatcher code in the broker Generally it does work by providing the custom TLS certificates if any similar to this approach for tag resolution Which broker are you using"
67465787,67465576,"stackoverflow.com",1,"2021-05-10 09:34:20+03","2024-05-17 05:43:47.948747+03","Yes that should absolutely be possible Assuming Knative Serving for now you would fetch the release YAML from httpsknative devdocsinstallinstallservingwithyaml You will need 3 YAML files You can find all necessary images in the respective Deployments in those YAMLs One notable exception is the queueproxy image which you will find in the configdeployment ConfigMap Make sure to prepull all those images and replace the coordinates in the YAML with the coordinates of your private registry Also make sure to grant the necessary pullrights to the service accounts used "
69563900,67465576,"stackoverflow.com",0,"2021-10-14 03:51:14+03","2024-05-17 05:43:47.949906+03","I assume youre likely doing government work or something similar if youre asking this question Youre asking us to provide actual code on what to do Heres the thing You will have to docker save all the images Youll find these by searching each of the kubernetes manifests Each container image will be prefixed in the yaml with image Then you can tar everything up and take it where you need you are embarking on a very long and annoying task "
67496671,67491101,"stackoverflow.com",1,"2021-05-12 05:43:37+03","2024-05-17 05:43:48.84167+03","Run minikube tunnel this will allocate an EXTERNALIP that is reachable from the host for more info check the minkube docs"
67634264,67603293,"stackoverflow.com",1,"2023-02-02 03:35:19+02","2024-05-17 05:43:50.05899+03","I found this solution httpsgithub comKongkubernetesingresscontrollerissues706 But I need the correct host header in my service in a matter of multitenancy "
67618132,67617648,"stackoverflow.com",1,"2021-05-20 12:58:39+03","2024-05-17 05:43:50.871972+03","Does the service you deploy have the matching label As in something like this or As far many changes to the yaml there are some auto generated fields for example the managedFields which decorate the k8s resources httpskubernetes iodocsreferenceusingapiserversideapply"
67619593,67617648,"stackoverflow.com",0,"2021-05-20 14:38:12+03","2024-05-17 05:43:50.872971+03","Be sure when you edit the config map you dont edit the comments and instead move the indentation I would recommend to not change example com and instead add the new new line with key and field I would strongly recommend to edit with CLI like this export KNATIVE_DOMAINkonk dev kubectl patch configmap n knativeserving configdomain p data KNATIVE_DOMAIN You can see how it is used in this tutorial at httpskonk dev"
67945253,67652812,"stackoverflow.com",1,"2021-06-12 05:54:55+03","2024-05-17 05:43:51.939596+03","I notice that you are connecting to httpsmydomain dev but passing a host header for a different domain My guess would be that curl is sending an SNI request for a mydomain dev cert since networkingnscert will acquire wildcard certs for namespace my domain dev it is possible that the server does not have a cert matching the SNI request and closes the TCP connection Try using the kvv options to curl instead of v to print more verbose debugging information and bypass some SSL errors Since you have DNS and certs set up I would try curl kvv httpshelloworldgo default mydomain dev"
67703271,67701721,"stackoverflow.com",3,"2021-05-26 13:49:43+03","2024-05-17 05:43:52.774868+03","The SinkBinding object has a subject configured using a label selector However there is no such label set on the Deployment object The solution here would be to either add the corresponding label s to the Deployments metadata use a subject matching on the Deployments name API documentation "
67721704,67720787,"stackoverflow.com",1,"2021-05-27 15:41:15+03","2024-05-17 05:43:53.314832+03","It is not possible to update your existing containerized running application in pod rather you can replace it with new pod You need to create another updated image and follow the rolling update Knative is Kubernetesbased platform to deploy and manage modern serverless workloads Kubernetes has a feature called Rolling Update Rolling updates Users expect applications to be available all the time and developers are expected to deploy new versions of them several times a day In Kubernetes this is done with rolling updates Rolling updates allow Deployments update to take place with zero downtime by incrementally updating Pods instances with new ones The new Pods will be scheduled on Nodes with available resources Kubernetes commands to update tutorial with steps your application Update App"
67954943,67720835,"stackoverflow.com",0,"2021-06-13 07:44:20+03","2024-05-17 05:43:54.337557+03","I was just blindly following the guide which did not state about broker creation The trigger issue is resolved after creating the broker "
67881044,67766104,"stackoverflow.com",1,"2021-06-08 06:30:04+03","2024-05-17 05:43:55.390505+03","It looks like you are using hostPort networking here if that is the case then Kubernetes will map port 80 and 443 of the hosts IP address to Istios envoy pods This week work as long as your istioingressgateway pods remain scheduled on the same machines for example if you use a daemonset and have 3 nodes in the cluster put all three node IPs in DNS Here are a few places where this will break down and a Kubernetes LoadBalancer service will work better If possible you might look at MetalLB as a liecost software load balancer of any of the above concern you If none of the above are major concerns nodePort services are a bit simpler than LoadBalancer ones "
67778979,67766104,"stackoverflow.com",1,"2021-05-31 21:53:29+03","2024-05-17 05:43:55.392505+03","Setting up DNS as follows works ok so far for me"
67804273,67800887,"stackoverflow.com",1,"2021-06-02 14:30:13+03","2024-05-17 05:43:55.934+03","This is not supported outofthebox but you can use a Kubernetes ingress A minimal example If you want to create the ingress resource in your example namespace and connect it to the kafkabrokeringress service look at Kubernetes Cross Namespace Ingress Network "
67822721,67820250,"stackoverflow.com",2,"2021-06-03 16:49:40+03","2024-05-17 05:43:56.704947+03","containerConcurrency is an argument to the Knative infrastructure indicating how many requests your container can handle at once In AWS Lambda and some other FunctionasaService offerings each instance will only ever process a single request This can be simpler to manage but some languages Java and Golang for example easily support multiple requests concurrently using threaded request models Platforms like Cloud Foundry and App Engine support this larger concurrency but not the function model of code transformation Knative is somewhere between these two since you can bring your own container you can build an application container which is singlethreaded like Lambda expects and set containerConcurrency to 1 or you can create a multithreaded container and set containerConcurrency higher "
67881716,67880759,"stackoverflow.com",2,"2021-06-08 08:06:53+03","2024-05-17 05:43:57.841567+03","You are noticing an optimization added last year in the case of small amounts of traffic basically less than 1015 pods the activator can often perform better requestweighted list balancing than the typical ingress in terms of queueing and managing concurrencyCount for existing pods and routing delayed requests to new pods or existing pods which have become available If your serving scales up to 20 or 30 pods you should see the activator stop being in the traffic path I believe the cutover point is trafficBurstCapacity 1 0targetCapacity concurrencyCount pods but I may be mistaken If I recall correctly this works out to something like 200 0 3 80 8 but I have not looked in a while The way this is implemented in the apiserver is that the Knative autoscaler manages the endpoints for the helloworldgo00001 service directly using metrics from the activator and queueproxy for details "
73770279,73767152,"stackoverflow.com",1,"2022-09-19 11:07:30+03","2024-05-17 05:44:28.425939+03","Knative uses validating admission webhooks to ensure that the resources in the cluster are valid It seems like the Knative webhooks are not running on your cluster but the validatingwebhookconfiguration has been created as has the service in front of the webhook the IP in the error message is a ClusterIP of a Kubernetes service on your cluster I would look at the webhook pods in the knativeserving namespace for more details "
67961200,67958254,"stackoverflow.com",0,"2021-06-13 20:58:00+03","2024-05-17 05:43:58.770067+03","It should be fine to mix Knative and nonKnative workloads in the same cluster Knative runs k8snative containers if you look at the underlying components of Knative Serving you will see Deployments and Pods just like other Kubernetes applications Knative Eventing can deliver events to Kubernetes Services as well as Knative Serving and Eventing so you can deliver events to a StatefulSet if that makes sense Knative should also be compatible with Kubernetes quotas and limits so it is possible to limit the number of pods or cpu resources used by Knative in a particular namespace It should also be possible to mix Knative and other Kubernetes constructs including things like MySQL provisioned by an operator in the same namespace I do this for my blog for example "
68035453,68032029,"stackoverflow.com",1,"2021-06-18 16:07:25+03","2024-05-17 05:43:59.510729+03","If you want to do this serverside rather than with a client script you will probably need to implement a small Kubernetes controller There is are also two halfway solutions each with some tradeoffs If you implement a controller you will basically have a single process PodStatefulSet if running it on Kubernetes which performs a Watch on all Knative Services It could handle both the create and delete aspects if you want or you could keep your current creation code A second option is to add a finalizer to each service and run a periodic job to look for all Services with a deletionTimestamp and your finalizer set For reach of these Services the job can unregister the Service with the API registry then remove the finalizer This is similar to the controller approach but instead of a constantlyrunning Watch the finalizer delays the delete until the cleanup job runs The drawback is that this means it may take up to N minutes the frequency of the job running for deletes to happen The last option is to use a validating admission webhook or an audit webhook to be notified of delete events Each of these has tradeoffs Two more noncontroller solutions that popped to mind Add a heartbeat check on the API registration that includes the URL of the having resource If the registration has a having URL and the object no longer exists remove the registration This may be fairly simple to implement You can use the apiserver source to set up a trigger on Knative Services You would be watching for the dev knative apiserver ref delete or dev knative apiserver resource delete events and the could have a single Knative Service or other URL handle these POSTs and do a cleanup of the resources Unfortunately I am not sure there is a good example of this on the web yet "
68035874,68033749,"stackoverflow.com",0,"2021-06-18 16:16:51+03","2024-05-17 05:44:00.297138+03","Knative uses the underlying container filesystem to store scratch files For an ordinary Kubernetes installation those will be a series of overlay directories on local disk but it is possible that some or all of the overlay directories are mounted on tmpfs Depending on your cluster you may be able to raise your requests and limits to get more RAM On Googles Cloud Run disk storage is backed by RAM Requesting more memory might help here or streaming the file over the network piece by piece If it is always the same file you can build it into your container image "
68258029,68257836,"stackoverflow.com",2,"2021-07-05 17:41:36+03","2024-05-17 05:44:00.906268+03","By default Knative uses a target utilization of 70 diluting the value it uses for concurrency targets by 70 That means the system targets 70 capacity for the current load see httpsknative devdocsservingautoscalingconcurrencytargetutilization You might want to try setting the utilization to 100 to run completely hot "
68318441,68313180,"stackoverflow.com",0,"2021-07-09 17:28:21+03","2024-05-17 05:44:01.520654+03","Have you tried the Domain Mapping API That API is designed to make this task easier and more consistent across the different Knative networking implementations If that API is not available these instructions for Istio might provide a working example I suspect that you may need to either target the Knativeinternal VirtualService or the internal url for the Knative Service in the VirtualService you posted above "
68625376,68615957,"stackoverflow.com",0,"2021-08-02 19:55:50+03","2024-05-17 05:44:02.397413+03","Have you installed Knative as well as Microk8s Installation instructions are here httpsknative devdocsadmininstall there is also a quickstart that uses kind if you are not attached to microk8s You can use kubectl apiresources to see whether you have the Knative resources installed For example I have"
69331548,69330632,"stackoverflow.com",1,"2021-09-26 06:07:57+03","2024-05-17 05:44:05.048041+03","It is likely this may work in Knative open source which uses Kubernetes to execute pods but not on Google Cloud Run fully hosted which runs on a proprietary execution engine "
71128317,71125655,"stackoverflow.com",0,"2022-02-15 16:37:23+02","2024-05-17 05:44:06.06935+03","I am not an Istio expert but you might be able to express the last policy based on either the ingress gateway have one which is listening only on a ClusterIP address or based on the SourceIP being within the cluster For the latter I would want to test that Istio is using the actual SourceIP and not substituting in the Forwarded headers IP address a different reasonable configuration "
71174065,71169155,"stackoverflow.com",1,"2022-02-22 21:16:02+02","2024-05-17 05:44:07.138271+03","The mesh name is a keyword as you guessed That keyword represents the EastWest traffic between Pods in the Kubernetes cluster as managed by the Istio sidecar You can think of those VirtualServices as being programmed onto each sidecar to do the routing and traffic splitting next to the request sender rather than needing to route to a central service gateway "
71170475,71169155,"stackoverflow.com",1,"2022-02-18 10:33:15+02","2024-05-17 05:44:07.139271+03","As you noticed knative uses istio as a service mesh In the Istio context mesh is not an object or resource like for example a Service Istio About page explain what Service Mesh is A service mesh is a dedicated infrastructure layer that you can add to your applications It allows you to transparently add capabilities like observability traffic management and security without adding them to your own code The term service mesh describes both the type of software you use to implement this pattern and the security or network domain that is created when you use that software So mesh is a term that encapsulate all Istio objects istioproxy containers Virtual Services Ingress Gateways etc that work together to allow for traffic management inside cluster A Gateway is a load balancer operating at the edge of the mesh receiving incoming or outgoing HTTPTCP connections "
71256395,71241780,"stackoverflow.com",0,"2022-02-24 20:09:15+02","2024-05-17 05:44:08.092561+03","I had to swap the triggers and the subscriptions namespace and now it works as expected "
71270037,71268823,"stackoverflow.com",4,"2022-02-25 20:08:21+02","2024-05-17 05:44:08.709686+03","Knative uses subdomains rather than URL paths because the underlying container could handle many different URLs and might encode requests with absolute URLs which might point to a different function depending on deployment or relative URLs which would point within the current application If you want to map multiple Knative services under a single domain name you can use an Ingress implementation or API server like Kong Istio or many others You will need an HTTP router which can rewrite the Host header to point to the Knative Services hostname in question the default Kubernetes Ingress resource does not expose this capability If you choose to set this up you will also need to decide on a policy for mapping the URL paths you could either strip the URL paths off when passing them to the Knative Service or leave them present It probably makes more sense to strip the URL paths off since otherwise you will end up needing to have a dependency between your application code and the namespace and name that you have chosen to deploy it at Other gotchas to watch out for"
71397262,71396601,"stackoverflow.com",0,"2022-03-08 17:11:36+02","2024-05-17 05:44:09.558242+03","It is not clear from your post if you are running Knative or if this is a general Istio question It has Knative tags but does not mention Knative I am also assuming you are running Istio on Kubernetes This is not a requirement for Istio but is a common configuration Based on your description it sounds like you have the lifecycles of the two components slightly confused Istio is not responsible for the lifecycle of your application kubelet does this You might need to set terminationGracePeriodSeconds to give your application an early notice before it is due down my kubelet "
71541960,71540077,"stackoverflow.com",1,"2022-03-19 22:49:18+02","2024-05-17 05:44:10.399683+03","I am not sure what you are going to use OPA for here if you are trying to constrain the types of resources which can be created OPA gatekeeper should work fine though kourier only implements an internaltoKnative interface so you might not get as much mileage out of the integration If you are trying to use OPA to govern or restrict actual HTTP requests to the workload kourier does not have that capability out of the box on purpose You could fork it and add the functionality but at that point it might be easier just to run Istio in nonmesh mode "
71590438,71589021,"stackoverflow.com",2,"2022-03-23 18:05:09+02","2024-05-17 05:44:11.1607+03","If you have tasks you want to run for days then Knative is probably not a good baseline for the effort Knative assumes that your application is only active as long as there is at least one HTTP request in flight to your application As you intuit leaving an HTTP connection open for days is probably not a good design practice For your use case it seems like Kubernetes Jobs might be the best approach If you need something to react to the there is work to spin up a job signal and create the Job you could use a Knative Service to talk to Kubernetes to create the Job I have seen that work successfully in other cases Knative also does not provide a hard mechanism for exit after processing one request for users that want this level of isolation I have suggested putting an exit 1 call in their application after they handle one request but I agree that it is not an ideal workaround "
71594353,71589021,"stackoverflow.com",2,"2022-03-23 23:38:29+02","2024-05-17 05:44:11.162701+03","Tekton is pretty much purpose built for this scenario We use it for exactly this I agree with the other answer knative is not a great fit for this use case and I do love knative "
71675004,71659787,"stackoverflow.com",1,"2022-03-30 12:46:44+03","2024-05-17 05:44:12.27443+03","I worked with the IBM Cloud Code Engine CLI and Knative CLI to investigate it First I retrieved information about the app It showed that two revisions were active due to split traffic but not showing details The YAML output had more information It showed that one revision had scale to zero the other a minimum of one instance active I also checked if there was more information available using Knative First get the Kubernetes configuration for the project Then list the revisions again as YAML output This part of the revision metadata shows the minScale as zero causing a cold start and hence the delay "
71679549,71659787,"stackoverflow.com",1,"2022-03-30 17:54:05+03","2024-05-17 05:44:12.276428+03","If you have access to application latency logs they might include which revision returned the slow result which could help explain why you sometimes get different latency results There may also be a Knative header in the response which indicates which revision served a request but there does not seem to be anything consistently documented on that front If you are willing to push new code to your application you could implement your own applicationlevel logging of latency and instance I would which might help pin down host level or startup behaviors "
71673999,71659787,"stackoverflow.com",0,"2022-03-30 11:32:02+03","2024-05-17 05:44:12.278428+03","If you are allowing minimum instances to be 0 then under zero load the apps will be shut down When the next request comes in there will be startup instantiation time "
75379844,71872219,"stackoverflow.com",0,"2023-02-08 01:01:48+02","2024-05-17 05:44:13.378192+03","I ran into a very similar issue After hunting on the hashcorp github issues I realized that the fix was just recently included so you need to update your google provider Here a working configuration that has supported tag inside traffic module"
71926145,71912106,"stackoverflow.com",0,"2022-04-19 16:52:47+03","2024-05-17 05:44:14.464992+03","I am not sure what configuration you are imagining for Apache Spark Knative Knative is intended to provide developers a serverless deployment and management experience but AIUI Spark is an independent project which does not specifically target Kubernetes execution Similarly it looks like Argoflow aims to simplify deployment of Kubeflow ML training applications specifically whereas Spark is a generalpurpose data processing framework Your question is a bit like asking what is better for getting around a Tesla or a Cessna Without knowing where you are going it is a bit hard to make a recommendation "
72194275,72193693,"stackoverflow.com",2,"2022-05-11 03:46:12+03","2024-05-17 05:44:15.567257+03","It worked by referencing fullhostname httpauthservice backend svc cluster local in my code appsettings json file instead of only the external service name which I previously assumed should be sufficient "
72194459,72193693,"stackoverflow.com",1,"2022-05-11 04:26:27+03","2024-05-17 05:44:15.568257+03","You probably want to look at the Knative Service for a URL rather than at the Kubernetes Service If you run kubectl describe ksvc n backend or even kubectl get ksvc n backend you should get a URL you can use to reach the Knative Service Yes having two different things named Service is confusing Yes it is too late to change it "
72302915,72300452,"stackoverflow.com",1,"2022-05-19 13:27:29+03","2024-05-17 05:44:16.769911+03","As listed in the Knative Eventing sources catalog there is a RabbitMQ source being developed and maintained by the community largely contributors from VMware and the RabbitMQ team Documentation and install instructions are here in the case of bugs please report them in the associated repo "
72370240,72300452,"stackoverflow.com",1,"2022-05-25 01:51:19+03","2024-05-17 05:44:16.77183+03","Hey Juan you were absolutely right about our docs here I will leave a PR fixing part of them while it is being reviewed and merged and with a clear example about using external RabbitMQ instances httpsgithub comknativesandboxeventingrabbitmqpull786filesdiff4fdb9e4eb3a1c9da58e4445d94aa5ce4573b5c8d005f20c41c767b07c09a2418 Hope this helps and thanks for the feedback If you find anything wrong you can comment on the PR or leave a reply over here "
72362408,72300452,"stackoverflow.com",0,"2022-05-24 14:51:09+03","2024-05-17 05:44:16.772834+03","thank you sameer for your answer I think I am not understanding you correctly After visiting the link you give me I have launched kubectl apply filename httpsgithub comknativesandboxeventingrabbitmqreleaseslatestdownloadrabbitmqsource yaml To install the rabbitmq source and generated the secret kubectl create secret generic rabbitmqcdefaultuser n knativeeventing fromliteraluserroot fromfilepasswordtmppassword And the following resource But after generating the resources I get these errors knativesourcesrabbitmqcontrollermanager The truth is that I have searched and searched and I have not found information on how to connect it to an external RabbitMQ to Kubernetes maybe I am not understanding the documentation well "
73831228,73825623,"stackoverflow.com",0,"2022-09-23 20:38:49+03","2024-05-17 05:44:29.279034+03","I am not sure how this is implemented in the library but it should be possible to access other object store systems in a similar way You might need to extend the current mechanism to use a more generic API like the S3 API most object stores have this as a compatibility layer If you do need to do this I would recommend contributing it back upstream as it seems like a generallyuseful capability when either storage space is tight or when fast startup is desired "
76188755,72300452,"stackoverflow.com",0,"2023-05-06 15:22:52+03","2024-05-17 05:44:16.77486+03","This error message indicates that xxx rabbitmq com CRD is not installed in the current k8s cluster W0524 114504 109915 1 reflector go324] k8s io [email protected] toolscachereflector go167 failed to list v1beta1 Queue the server could not find the requested resource get queues rabbitmq com You need to install the RabbitQA messagingtopologyoperator in order to add the missing CRDS as mentioned in the docs Note An external RabbitMQ instance can be used but if you want to use the Source without predeclared resources specifically the Exchange and Queue the RabbitMQ Message Topology Operator needs to be installed in the same Kubernetes Cluster as the Source follow this command "
72408178,72355485,"stackoverflow.com",1,"2022-05-27 19:04:58+03","2024-05-17 05:44:17.727146+03","Knative needs more capabilities out of the HTTP routing layer than are exposed via the Kubernetes Ingress resource Percentage splits and header rewrite are two of the big ones Unfortunately so one has written an adapter from Knatives KIngress implementation to the nginx ingress In the future the gateway API aka Ingress V2 may provide these capabilities in the meantime you will need to install one of the other network adapters and ingress implementations Kourier provides the smallest implementation while Contour also provides an Ingress implementation if you want to switch from nginx entirely "
72817563,72446169,"stackoverflow.com",0,"2022-06-30 17:27:40+03","2024-05-17 05:44:18.47062+03","The main problem was pretty simple yet So it is working as expected i only had to perform these modifications after common installation guide "
72941606,72938125,"stackoverflow.com",0,"2022-07-11 19:15:52+03","2024-05-17 05:44:19.881322+03","It looks like you may have requests which are hitting request timeouts in the queue proxy What is your typical request latency and what is the Revision timeoutSeconds set to It is also possible that istio is cancelling resetting some of the TCP connections to the queueproxy but that seems unlikely "
73145649,73097578,"stackoverflow.com",0,"2022-07-28 02:28:41+03","2024-05-17 05:44:20.90544+03","queueproxy will eagerly read aggressively probe the user container during startup particularly when coldstarting a container on the general notion that there is a user caller on the other end of the HTTP request which is waiting for the pod to become ready Once the initial startup probing is complete I expect that queueproxy will probe less aggressively Is the concern about the probes or about the log messages indicating that the probes are failing and filling up your logs "
73164603,73161022,"stackoverflow.com",0,"2022-07-29 12:37:20+03","2024-05-17 05:44:21.726109+03","Knative uses the HTTP Host header to distinguish traffic intended for different services This allows you to use a single IP address and port to support multiple services at the same time If you expose your HTTP load balancer service probably kourier if you are using the quickstart configuration using a node port or load balancer service type you can then change the default DNS domain to match your clients IP The default 127 0 0 1 sslip io domain uses a service from sslip io that lets you make up additional subdomains based on your IP address if you do not have your own DNS server You can also use a wildcard DNS record to point to the exposed host IP "
73599897,73597233,"stackoverflow.com",1,"2022-09-04 16:44:17+03","2024-05-17 05:44:22.507012+03","Knative uses the HTTP Host header to share a single IP address across many services sorta like what you are doing with caddy If you want to use caddy to route these requests you will need to rewrite the Host header using the header_up directive You may be able to use the replacement form andor change the Knative domain prefix to make this easier "
76045613,73619481,"stackoverflow.com",0,"2023-04-18 18:33:48+03","2024-05-17 05:44:23.666091+03","I had the same issue It seems knative does an extra step to resolve the image digest for revision handling so it tries to connect to the registry and it is not able to That is why a normal deployment works I found a workaround which is to send the digest with the image name in the yaml files You can get the digest by executing docker image ls digests Then add the digest to the value of the image field imagelocalhost5001myimagetag 9eb50ab0a4f8a9408a46bb89c18df9a5f5d2bd96724a845401840955e056ced1 I could not find any other solution yet PLG Edit I found another solution Disable digest resolution for your local registry Execute kubectl n knativeserving edit configmap configdeployment And edit the file to add disable tag resolution without adding the port it did not work for me "
73708817,73683932,"stackoverflow.com",1,"2022-09-13 23:23:22+03","2024-05-17 05:44:24.506202+03","Kafka provides ordering within a partition the implementation is a distributed log You may need to change the number of partitions on your Kafka topic to achieve higher parallelism you may be able to also use the spec consumers value to increase the throughput untested I would also encourage filing an issue in the eventingkafka repo with your problem and any additional knobs if there is other behavior you are looking for "
73708776,73688710,"stackoverflow.com",2,"2022-09-13 23:18:15+03","2024-05-17 05:44:25.532186+03","It looks like you are hitting concurrency limits with your Kafka topic and delivery here in particular Kafka delivery parallelism how many events can be delivered at once is probably bounded by partitions eventbatchsize It looks like the number of partitions is controlled by the kafkabrokerconfig ConfigMap and the latter is controlled by max poll records in the configkafkabrokerdataplane ConfigMap Another limiting factor of the Kafka implementation is the number of outgoing HTTP requests from a single dispatcher which appears to be controlled by the maxPoolSize parameter of the data plane You did not see this behavior from the InMemory channel because it does not perform ordered delivery from partitions and simply attempts to deliver all messages in parallel without tracking order or blocking newer messages on older ones "
73691924,73688710,"stackoverflow.com",2,"2022-09-12 18:49:08+03","2024-05-17 05:44:25.534359+03","Is not because you have containerConcurrency1 Broker receives events from SQS and commits the offset of received events xx seconds auto commit interval ms Now with containerConcurrency means that every event which is send to the kservice can be consumed one per whole consumer If you will increase this number you will also increase the numer of events possible to be send If you care about the order perpartition you can play and use kafka eventing knative devdelivery order setting which will make perpartition sequential execution of events one event in the partition is successfully processed a next one will be send Other partitions are processed async "
73701480,73688710,"stackoverflow.com",1,"2022-09-13 13:34:08+03","2024-05-17 05:44:25.536021+03","I confirmed that we can use 2 parameters httpsgithub comknativesandboxeventingkafkabrokerissues2597issuecomment1243665444"
73708495,73706498,"stackoverflow.com",1,"2022-09-13 22:43:54+03","2024-05-17 05:44:26.491247+03","Have you looked in the browser console for error messages My first guess would be that some of the resources served by the application are being served with an incorrect hostname i e httplocalhostpathtomy css which will not work if the application is actually running at httpmyservice default mydomain com "
73738887,73733322,"stackoverflow.com",1,"2022-09-16 04:17:44+03","2024-05-17 05:44:27.759858+03","There is a feature flag kubernetes podspecpersistentvolumeclaim and kubernetes podspecpersistentvolumewrite which an administrator can use to enable mounting persistent volumes including SMB shares These are disabled by default because volume mounts can impose scalability performance and correctness challenges for applications and may restrict portability across clusters There are some guard rails like this in Knative with a goal of defaulting to more portable and less hostlinked configuration "
73974348,73962332,"stackoverflow.com",0,"2022-10-06 16:07:00+03","2024-05-17 05:44:30.130235+03","Ok that is what worked for me I removed this line According to docs it is 0 by default "
74543869,74543499,"stackoverflow.com",0,"2022-11-23 10:40:04+02","2024-05-17 05:44:34.960149+03","The protocol I followed above is correct the problem was because of a bug in the code of the Docker image that Knative is serving I was able to troubleshoot the issue by looking at the logs of the pods as follow First run the following command to get the pod name kubectl n myspacename get pods Example of pod name myservicename00001deployment56595b764fdl7x6 Then get the logs of the pod with the following command kubectl n myspacename logs myservicename00001deployment56595b764fdl7x6"
75111136,75108597,"stackoverflow.com",1,"2023-01-13 17:21:52+02","2024-05-17 05:44:36.059951+03","Since you are chaining two different HTTP routers together you might want to try isolating the behavior for each one Try invoking the Knative service from a container in the cluster using the address of the internal Istio balancer that the Nginx ingress is pointing at i e 172 17 17 19 with the appropriate Host header If that is not working your problem is in the Istio Knative combination Try running a grpc container directly behind the Nginx Ingress and make sure that Nginx is able to pass grpc traffic If both of those are working then there is some difference between your test incluster traffic and the way Nginx is sending the traffic My guess here would be that the forwarded traffic is missing the Host header but I would check the two other debugging steps outlined above first "
76209980,76208256,"stackoverflow.com",0,"2023-05-09 16:38:10+03","2024-05-17 05:44:38.880108+03","I am at assuming you are referring to this story which has been hot for the last week or so It would be possible to build a system like the one described by the Prime Video team using multicontainer support and a controller container much like the one described by the Prime Video team It is even possible to build a multicontainer workflow orchestrator like this so that it works with both Knative Serving and standard Kubernetes Deployments including ones managed by KEDA which really suggests it would make sense as an independent project It is also possible to use Argo Workflows or Tekton for this you do not need Knative to do everything One of the goals of the project is to provide a tool that works well with other Kubernetes tools rather than replacing all other tools "
76406849,76282226,"stackoverflow.com",0,"2023-06-05 16:20:51+03","2024-05-17 05:44:39.874636+03","Did you look at the instructions in httpsgithub comknativeservingblobmainDEVELOPMENT md "
76406917,76334072,"stackoverflow.com",0,"2023-06-05 16:29:08+03","2024-05-17 05:44:40.81773+03","You might need to set either timeoutSeconds or configure readinessProbe s and livenessProbe s which are longer than the Knative defaults You should be able to see the defaults in the applied service spec "
76457143,76378221,"stackoverflow.com",0,"2023-06-12 16:26:56+03","2024-05-17 05:44:41.932074+03","It seems your targetsink URL is not correct The brokeringress service is the entrypoint for multiple logical brokers the ones you can create for example via kn broker create and thus requires additional information namespace broker name in the path to route the request to the correct logical broker instance You can get this URL from the broker object itself too"
76595456,76594587,"stackoverflow.com",1,"2023-07-01 18:50:08+03","2024-05-17 05:44:47.05803+03","I have not looked at your service yaml but I have a hypothesis that this might be related to slow tag to digest resolution Your can try the following Monitor latency for registry operations particularly GET operations Use image digests when referencing images These look like sha256 rather than latest and ensure that the image does not change after deployment Disable tag to digest resolution Note that this can lead to unpredictable behavior if a referenced tag is moved Some instances may pick up the new image while other instances may use an earlier image If this is tag to digest resolution and you are using public Dockerhub images adding pull credentials to the service account that is running the Knative Service might give you higher rate limits "
76629094,76620614,"stackoverflow.com",0,"2023-07-06 15:57:59+03","2024-05-17 05:44:48.004921+03","Readiness probes run from the start of the Pods lifecycle so it is not surprising to see some failures at startup Have you tested running the containers locally with the same requests and limits IIRC native compilation also greatly reduces the RAM footprint especially at startup If you have limits set on your container it is possible that you are getting an OOM at startup "
76718689,76620614,"stackoverflow.com",0,"2023-07-19 09:56:11+03","2024-05-17 05:44:48.006922+03","I finally had the chance to revisit this issue and I am glad to report that I have found a solution Solution The root cause of the problem was the insufficient resources Memory and CPU I had allocated for my minikube local cluster This led to the abrupt termination of the application without any apparent errors To address this you need to restart Minikube with appropriate resource values Alternatively you can set these values permanently in the Minikube configuration ensuring it always starts with the specified resources I hope this information helps someone else encountering a similar issue "
76635035,76629914,"stackoverflow.com",1,"2023-07-07 11:04:01+03","2024-05-17 05:44:48.918286+03","The error message means that your Go tool That indicates the Go tool installed on your system is too old According to the go directive in the go mod file I think you should upgrade to Go1 18 at least It is better to install the latest Go See httpsgo devdocinstall for the installation instructions "
76713776,76710271,"stackoverflow.com",0,"2023-07-18 17:21:13+03","2024-05-17 05:44:52.485127+03","this change in the Broker made a difference for me "
76846450,76840898,"stackoverflow.com",1,"2023-08-06 17:58:11+03","2024-05-17 05:44:53.447171+03","You need to define mounts and volumes httpskind sigs k8s iodocsuserconfigurationextramounts"
77235459,77230385,"stackoverflow.com",0,"2023-10-05 11:32:11+03","2024-05-17 05:44:57.383864+03","Knative service autoscale settings by sending a PATCH request to the Knative services API It is independent of Azure services and works with any Kubernetes cluster with Knative installed The HTTP 422 error code indicates that the server understands the request but cannot process it due to a clientside error Check the URI Authorization header and patch_data For Error Handling I have also crosschecked with managing an Azure AKS cluster using the Azure SDK It enables autoscaling for the agent pool of an AKS cluster and sets the minimum and maximum counts for the nodes in the pool This code is specific to Azure AKS and is used to update AKS cluster settings both leads to the same response "
77974226,77675206,"stackoverflow.com",1,"2024-02-10 20:14:53+02","2024-05-17 05:44:57.974362+03","Your docker is not running Please try to restart the docker desktop "
77696352,77695513,"stackoverflow.com",0,"2023-12-21 09:22:02+02","2024-05-17 05:44:58.748052+03","In this official Istio documentation they have explained how to bypass proxy for a specific IP range global proxy includeIPRanges and global proxy excludeIPRanges options will help you in bypassing the envoy proxy In the documentation they have explained how to bypass proxy for external services and you need to do the reverse of it Check the internal IP range of the cluster and use global proxy excludeIPRanges option and pass the internal IP range to this command As mentioned in the below command"
77789520,77786370,"stackoverflow.com",0,"2024-01-09 22:17:37+02","2024-05-17 05:44:59.910885+03","I would strongly recommend upgrading Knative It sounds like you are running a version from 2020 which will not have fixes for the many discovered vulnerabilities in the Knative dependency tree This includes a number of remote vulnerabilities in the go standard library Knative Serving has made a concerted stability effort since at least 0 20 testing availability while upgrading and downgrading the Knative version on the cluster Personally I have upgraded since around 0 20 to 1 10 with no problems upgrading Knative once the underlying Kubernetes version was in the supported range Upgrading Kubernetes versions on the other hand has been about a 50 initial success rate "
77794625,77786370,"stackoverflow.com",0,"2024-01-10 17:48:09+02","2024-05-17 05:44:59.912886+03","Your other option is to rebuild the Knative 0 23 distribution from source which should still be reproducible if you use the correct go compiler version and then patch that version with the PVC code from the later release "
77812784,77808871,"stackoverflow.com",1,"2024-01-13 21:17:47+02","2024-05-17 05:45:00.762898+03","Knative Uses CloudEvents for message routing in particular CloudEvents over HTTP as described in the Knative Eventing spec If you are looking for a higherlevel library you can use the CloudEvents Java SDK but it will still be HTTP underneath Part of the goal of Knative Eventing is to work well with a large variety of programming languages without needing a specalpurpose SDK so just using HTTP to post events is actually a design feature compared with implementing something like AMQP for sending "
78268194,78156097,"stackoverflow.com",1,"2024-04-03 18:17:15+03","2024-05-17 05:45:02.549094+03","Solving your issue is only possible when using the Broker and Trigger model as currently Kafka channels are not configurable perresource Each Broker can be configured to use a separate configuration referenced in a Brokers spec config In this config you can config your bootstrap servers then see configure a kafka broker So you could use something like the following to configure your Broker to use a dedicated broker config and then your Broker configmap which can be referenced by multiple Brokers "
78428700,78392711,"stackoverflow.com",1,"2024-05-06 16:02:10+03","2024-05-17 05:45:05.076152+03","It is not possible to get this when curling the Broker The Broker allows to decouple the components and send events asynchrnously Also there could be multiple Triggers behind the Broker When you need the response of your Knative Function synchronously you need to call the function directly e g via curl In case you do not want to lose the Broker and simply need the response of the Function somewhere you can make use of the reply event of your BrokerTrigger The response of your Triggers subscriber the Function in your case is sent back as a new event to the Broker So you could add another Trigger which filters on the response event type and send this event somewhere where the responses are handled "
60600349,60599829,"stackoverflow.com",1,"2020-03-09 14:13:16+02","2024-05-17 05:50:13.460941+03","In Oracle RDBMS you can compile a java source Then you can wrap it in an Oracle function Then you can just call it in an normal SQL statement as per any other function The Java function will run on the server but the query can be invoked from any SQL client connected to the server and will return the output to that client "
62299670,61915279,"stackoverflow.com",3,"2020-06-16 14:47:25+03","2024-05-17 05:50:14.200455+03","I think that you have already figured this out but the best way to interact with other OCI services within your Function is to authenticate to that service using the Functions Resource Principal This is an identity given to your Function such that you can write policies to allow it to interact with other OCI resources This saves you handling any credentials yourself since an ephemeral API key representing this identity is passed into your Function by the service To interact with a Kubernetes cluster you need a kubeconfig and as you can see this can be generated by the OKE API The returned kubeconfig uses ephemeral time based tokens to authenticated to the cluster and as you can see for interactive use cases there is an implementation of this token generation in the OCI CLI Unfortunately there is no implementation of this method in the SDKs so you need to one of the following Copy the code from the CLI which generates the token into your function code translating into your chosen language if necessary then pass this generated token into your kubeconfig instead Keep the configuration which execs the OCI CLI to get the token install the OCI CLI and make the OCI CLI generate the token using the resource principal Since you said you would tried the first approach and not succeeded and since the first approach is somewhat fiddly I will present how you would achieve the second method To install the OCI CLI you need to take control of your docker build process so you can modify the contents of the generated function image Each FDK has an implicit Dockerfile You can find the boilerplate for these in the Fn CLI and you can then place that Dockerfile in your function directory and change the function type in func yaml to docker There is a blog post about how to extract the implicit dockerfile here httpsconstructivelaziness blogspot com202005thecaseofvanishingdockerfile html Now you can add to the Dockerfile steps to install the OCI CLI based on the instructions here httpsdocs cloud oracle comenusiaasContentAPISDKDocsclimanualinst htm Since the FDK base images for Go are alpine based the process may need some modifications to accomodate that To make the OCI CLI use the resource principal you need to set an environment variable OCI_CLI_AUTH to resource_principal This can be done in an ENV line in the last stage of your Dockerfile You will also need to make sure you have your functions in a dynamic group and an appropriate policy in place for your function per the docs here httpsdocs cloud oracle comenusiaasContentFunctionsTasksfunctionsaccessingociresources htmHighlightfunctions20resource20principal Disclaimer I work for Oracle on the Functions team and the advice above is correct to the best of my knowledge but does not constitute official support "
62241974,61915279,"stackoverflow.com",0,"2020-06-07 10:03:10+03","2024-05-17 05:50:14.205411+03","You should be able to use the Go SDK to update the Cluster and NodePool inside your OCI Function as mentioned in httpsdocs cloud oracle comenusiaasContentAPISDKDocsgosdk htm Alternatively you should also be able to spin up an HTTP client inside your Go code in your OCI Function and call OKEs UpdateCluster and UpdateNodePool REST APIs see e g httpsdocs cloud oracle comenusiaasapiencontainerengine20180222NodePoolUpdateNodePool"
62875643,62875135,"stackoverflow.com",0,"2020-07-13 15:15:00+03","2024-05-17 05:50:15.117396+03","Once you do from helper import foo inside func py the entire code including the library import will be done in func py Using the command pip freeze requirements txt in your main directory you can list all the dependent libraries of your project inside the requirements txt Hence when you create the docker image it will install everything from requirements txt "
63501590,63501589,"stackoverflow.com",0,"2020-08-20 11:55:10+03","2024-05-17 05:50:17.085515+03",""
64025941,64006637,"stackoverflow.com",5,"2020-09-23 13:28:38+03","2024-05-17 05:50:18.542956+03","In Fn there are two builtin ways of invoking functions These serve different purposes The invoke API is system to system API designed to allow software and systems to invoke functions as part of their code to give an example you might use this if you were writing an application where you wanted to use functions to handle internal application events This API has a narrow contract to make it easier for client applications to handle errors and tell the difference between when a function raised an error or the fn server raised an error This API only accepts POST requests and only returns a limited set of response codes HTTP Triggers on the other hand are a builtin way to expose functions directly on their own HTTP endpoints e g for serving a web page or handling an API that you define Triggers take a raw HTTP request from a client which can have any verb like PUTPOSTHEAD and any HTTP headers and wrap the request in a call to the underlying function which can then extract information about the request and generate an HTTP respond In Oracle Cloud Infrastructure you can do the same thing but you would use an API Gateway rather than a trigger to receive the incoming request see below Internally you can see triggers as a layer on top of the invoke endpoint that wraps an HTTP request into an invocation on the invoke API calls the function with the wrapped request and then unwraps HTTP response information from the function back into an HTTP request So the key difference between the invoke API and a trigger or API gateway is that on the invoke API Fn defines the API to make it easier to build software on top of Fn but on a trigger you define the API you can capture the full request and define the full HTTP response In Fn if you wanted to expose a REST API implemented by a function that handled a GET and returned a web page you would In Oracle Cloud Infrastructure you can do the same thing by creating an API Gateway that binds the function to an HTTP endpoint An example of how do do this is documented here httpsblogs oracle comdeveloperscreatingyourfirstapigatewayintheoraclecloud httpsblogs oracle comdevelopersworkingwithhttpinoraclefunctionsusingthefnprojectpythonfdk"
64565116,64551988,"stackoverflow.com",3,"2020-10-28 03:15:36+02","2024-05-17 05:50:18.902146+03","That is known in serverless as the cold start and it is something that is being worked on to reduce the initial startup time Until then a healthcheck can be used to periodically ping the function Essentially create a case in the function where the URL ends in something like status or healthcheck In that case return response Response ctxresponse_datajson dumps status OK headers ContentType applicationjson In API Gateway create a route making sure to enable anonymous for status or healthcheck that invokes the function Then set up a health check to periodically invoke the API with the status or healthcheck endpoint This both keeps the function active and also monitors the health Your case could perform any needed validation rather than just returning an OK response Another thing to keep in mind is API Gateway will cache responses so depending on your chosen TTL you can adjust your healthcheck timing accordingly "
35906534,33610044,"stackoverflow.com",0,"2020-06-20 12:12:55+03","2024-05-17 05:52:20.530741+03","According to Iron mq php the recommended version of Iron MQ for Laravel 5 1 is 4 There is a commit by Ryantology that adds the reservation_id to deleteMessage method It only involves changing three lines Enjoy "
64557558,64551988,"stackoverflow.com",2,"2020-10-27 17:21:41+02","2024-05-17 05:50:18.904146+03","I doubt if you could keep the FN Container hot without repeatedly invoking it at the first place One of the daft options could be to keep calling it after every sleep interval but this has to be tradedoff with associated FN Invoking costmonth Other options could be based on how long the actual operation runs for For instance this could be split into the two Operations represented by two FNs A FN can call another FN so you should be able to sequence invoking them one by one if that is achievable for your intended task "
74657860,64551988,"stackoverflow.com",0,"2022-12-02 17:11:51+02","2024-05-17 05:50:18.905845+03","This hot start requirement is now covered by Oracle Clouds Provisioned Concurrency feature for Functions httpsdocs oracle comenusiaasContentFunctionsTasksfunctionsusingprovisionedconcurrency htm From the documentation Provisioned concurrency is the ability of OCI Functions to always have available the execution infrastructure for at least a certain minimum number of concurrent function invocations "
77130654,65294545,"stackoverflow.com",1,"2023-09-19 01:08:38+03","2024-05-17 05:50:19.443099+03","If server version is missing then it means your fn server is still not running and you will not be able to deploy and invoke your function "
69811470,69807200,"stackoverflow.com",1,"2021-11-02 15:40:15+02","2024-05-17 05:50:21.830641+03","I found a way to do it I downloaded the source code from the repository changed the max timeout in apimodelsfn go and used the dockerfile to rebuild the image and that is working fine for me "
23235429,23213509,"stackoverflow.com",1,"2014-04-23 08:25:20+03","2024-05-17 05:50:27.097007+03","I am curious how you are implementing the actual queue for processing the images When I have needed a process queue in the past I created a server daemon using PHP that would check a db for new images Each time an image was uploaded I copied the original to a temp location and stored the name and status of the image in the DB Status was new processing and complete As soon as a server grabbed a file to process from the db I updated the status to processing I also mounted an S3 bucket to each of my machines and then symlinked to a local folder so all files were accessible with out having to download the file first The code behaves as if the file is local even though in the background the image is being downloaded However another solution that lives in the AWS service is their SQS simple queuing service Use the S3 API with the SQS API inside your application and you can accomplish what your trying to do with out trying to build a server daemon I would check out this link here httpaws amazon comarticles1602_encodingUTF8jiveRedirect1 They have a pretty good guide on how to do exactly what your wanting to do using the services above They recommend using dynamoDB but you can probably swap out with any DB that you are already using Either route you go you need a DB to track files and process status and to keep track of your files in general If you are worried your sometimes running into errors because the file is not downloaded yet I would check to make sure the file exists first if it does check file size against DB and then determine if the file is ready to be processed You could run a script in laravel by hitting that specific url with a cron job as well Hope this helps "
47676550,46618828,"stackoverflow.com",2,"2017-12-06 16:22:39+02","2024-05-17 05:50:29.339387+03","You need to Disable SecurityEnhanced Linux SELinux Its caused due to this Issue the below command as root for temporaryly disable selinux setenforce 0 OR"
46917549,46803457,"stackoverflow.com",1,"2017-10-24 21:25:34+03","2024-05-17 05:50:29.955528+03","It looks as if there was a problem with version 16 of the UI image Please pull down a new image using docker pull fnprojectui and try again If you are using the newer version should see the following at startup"
47874693,47871139,"stackoverflow.com",1,"2017-12-18 20:59:12+02","2024-05-17 05:50:30.899766+03","The Fn Project httpsfnproject io is meant for people that want to run their own functionsasaservice FaaS This gives you a lot of control and no vendor lockin since you can run it anywhere In terms of the containers for your functions it is correct that you will not be able to run those directly on Lambda But Fn can run Lambda functions And yes the cost will be different since you will actually have to run servers to run Fn "
48529889,48527551,"stackoverflow.com",0,"2018-01-30 22:40:20+02","2024-05-17 05:50:31.940406+03","I think proxy_pass http172 17 0 18081rvivopersonperson should most likely be proxy_pass http172 17 0 18080rvivopersonperson 8080 is the fn server port by default and 8081 is flow It looks like you are trying to cache the whole function call so you should proxy to 8080 fnserver "
50782552,50781354,"stackoverflow.com",0,"2018-06-10 13:02:27+03","2024-05-17 05:50:34.04802+03","One of fn contributors answered me By creating a route you would create a function with an HTTP webhook to that "
51565330,51441703,"stackoverflow.com",0,"2018-07-27 23:45:55+03","2024-05-17 05:50:34.839801+03","There is an example for building an fn with extensions located here httpsgithub comfnprojectfnblobmasterexamplesextensionsmain goL16 for building with a custom driver at the moment it requires using that same process i e there is no way to configure another driver at runtime from fn cores binary without extending it In order to build with an alternative driver such as mocker a user would use the agent WithDockerDriver option to specify a driver when creating the agent documented here httpsgodoc orggithub comfnprojectfnapiagentAgentOption and sample follows we need to tidy up the agent interface to make them easier to create data access stuff is convoluted but is not too bad most of this can be stolen from this file httpsgithub comfnprojectfnblobmasterapiserverserver go we need to name it to WithDriver as well assuming you are looking at using something like rkt or a more robust driver on the backend it is possible to hook this up by implementing the driver interface and in the past we have tried it but we are not maintaining it at present since it was not a viable option performance issues perhaps improved since would be cool to see if you manage to get rkt working gladly take a PR for it and figure out where to put it "
51565448,51532548,"stackoverflow.com",2,"2018-07-27 23:56:34+03","2024-05-17 05:50:35.848319+03","You may set the variable FN_API_URL to change the cluster that a function is deployed to An example would be If you need to manage functions across multiple deployments or want a more robust way of managing this than an env var you can also use context files in various capacities which are documented here httpsgithub comfnprojectcliblobmasterCONTEXT md"
72646762,72621020,"stackoverflow.com",2,"2022-06-16 16:33:57+03","2024-05-17 05:50:53.640734+03","Before discussing these functions it is important to note that MLRun has an automount feature do not confuse with the function which means that every function created will get a default mount applied to it with logic that is described in the documentation linked below This means that unless you have a need to apply a mount with specific parameters or configuration in most cases you should not have to use any of these functions Please refer to httpsdocs mlrun orgenstableruntimesfunctionstorage html for additional details on automount As for these functions both of them are modifiers that can be applied to an MLRun function to modify its execution spec in some way Both should be used through the apply function such as The difference between them is that the mount_v3io function is used solely to create a v3io volume mount The auto_mount modifier may create a PVC mount or a v3io mount based on environment variables it tries to deduce the correct configuration to use Refer to httpsdocs mlrun orgenstableapimlrun platforms html for documentation of these modifiers and others "
64788823,33610044,"stackoverflow.com",0,"2020-11-11 16:58:30+02","2024-05-17 05:52:20.531741+03","Feel free to use this Laravel driver for IronMQ It is designed for different Laravel versions including 5 1 "
54498799,54497126,"stackoverflow.com",2,"2019-02-03 03:56:52+02","2024-05-17 05:50:38.083529+03","You cannot trigger the fulfillment asynchronously In a conversational model it is expected that the fulfillment will perform some logic that determines the response You can however perform an asynchronous operation in the fulfillment that does not complete before you return the result If you are using a sufficiently modern version of node version 8 and up you can do this by declaring a function as an async function but not calling it with the await keyword If you did call it with await it would wait for the asynchronous operation to complete before continuing So something like this should work given your example Update 1 based on your code example It seems odd that you report that the call to request does not appear to be done at all but there are some odd things about it that may be causing it First request itself is not an async function It is using a callback model and async functions do not just automatically wait for those callbacks to be called So your callFlow function calls console log a couple of times calls request and returns before the callbacks are called back You probably should replace request with something like the requestpromisenative package and await the Promise that you get from the call This makes callFlow truly asynchronous and you can log when it finishes the call Second I would point out that the code you showed does not do a POST operation It does a GET by default If you or the API you are calling expect a POST that may be the source of the error However I would have expected the err parameter to be populated and your code does look like it checks for and logs this The one unknown in the whole setup for me is that I do not know how fdk handles async functions and my cursory reading of the documentation has not educated me I have done this with other frameworks and this is not a problem but I do not know if the fdk handler times out or does other things to kill a call once it sends a reply "
11056244,11056243,"stackoverflow.com",7,"2012-06-15 21:26:31+03","2024-05-17 05:50:41.267636+03","The worker needs to make a connection to the database explicitly since it is not running within your application so you need to pass the connection information to your worker You can do this in the worker payload like so Then inside your worker"
30874683,11056243,"stackoverflow.com",2,"2015-06-16 20:50:45+03","2024-05-17 05:50:41.268636+03","I whipped up a blog post on this Hopefully it helps In a nut shell though storing your database configurations in environment variables makes it easy "
21711783,11803497,"stackoverflow.com",1,"2014-02-11 21:59:45+02","2024-05-17 05:50:41.760284+03","According to the documentation you can now specify a gemfile in your worker file I do not think this was available when this question was asked Instead of having a remote_build_command you specify full_remote_build true or just remote to have IronWorker set up your environment for you "
11803499,11803497,"stackoverflow.com",0,"2012-08-04 00:24:02+03","2024-05-17 05:50:41.762284+03","bundle install standalone installs the gems to bundlebundlersetup directory So at the top of my_worker rb add the following line This should load up all your gems "
46450017,46449999,"stackoverflow.com",3,"2017-10-03 23:27:09+03","2024-05-17 05:50:43.425428+03","With the command line tool you can run Or without it"
54071688,46449999,"stackoverflow.com",0,"2019-01-07 11:31:51+02","2024-05-17 05:50:43.426428+03","For Fn CLI V0 5 15 and presumably higher use fn update server"
46553119,46552899,"stackoverflow.com",2,"2017-10-03 23:44:45+03","2024-05-17 05:50:44.266117+03","Some of those warningsmessages are MacOS specific ie you will not see them on Linux But they will not affect regular operations so you can safely ignore them "
72409316,72408785,"stackoverflow.com",2,"2022-05-27 20:51:48+03","2024-05-17 05:50:45.384947+03","MLRun has several different ways to run a piece of code At this time the following runtimes are supported If you are interested in learning more about each runtime see the documentation "
72410520,72410519,"stackoverflow.com",0,"2022-05-27 23:01:58+03","2024-05-17 05:50:46.354737+03","To install the correct version of MLRun for your cluster run the align_mlrun sh script in your home directory in the Jupyter service This will ensure that the server and client versions of MLRun are aligned"
74913854,72410519,"stackoverflow.com",0,"2022-12-25 16:48:27+02","2024-05-17 05:50:46.355737+03","You can install mlrun manually from terminal online installation last version pip install mlrun online installation specific version e g 1 2 0 pip install mlrun1 2 0 online installation with proxy specific version e g 1 2 0 pip install proxy httpproxyrepo prod3128 mlrun1 2 0 offline installation from source v3iobigdatamlrun_dist pip install noindex findlinks v3iobigdatamlrun_dist mlrun"
72454566,72454565,"stackoverflow.com",0,"2022-06-01 01:09:34+03","2024-05-17 05:50:47.449603+03","Yes this is possible A common development pattern with the Iguazio platform is to utilize a local version of MLRun and Nuclio on a laptopworkstation and moveexecute jobs on the cluster at a later point There are two main options for installing MLRun and Nuclio on a local environment Once you have installed MLRun and Nuclio using one of the above options and have created a jobfunction you can test it locally as well as deploy to the Iguazio cluster directly from your local development environment"
72481059,72463927,"stackoverflow.com",0,"2022-06-02 21:53:50+03","2024-05-17 05:50:48.569096+03","You need to deploy the default image to the cluster docker registry There is one image for remote spark and one image for spark operator Those images contain all the necessary dependencies for remote Spark and Spark Operator See the code below Once these images are deployed to the cluster docker registry your function with function spec use_default_image True will be able to pull the image and deploy "
72478526,72478525,"stackoverflow.com",0,"2022-06-02 18:15:21+03","2024-05-17 05:50:49.465458+03","You can implement ML model serving using MLRun serving and Nuclio runtimes Both MLRun and Nuclio are integrated into the Iguazio platform The Iguazio platform includes an API gateway that enables security on the REST endpoints MLRun serving can produce managed realtime serverless pipelines from various tasks including MLRun models or standard model files The pipelines use the Nuclio realtime serverless engine which can be deployed anywhere Nuclio is a highperformance opensource serverless framework thats focused on data IO and computeintensive workloads Here is a snippet of code showing a model deployment For more details click here"
72479832,72479831,"stackoverflow.com",0,"2022-06-02 20:02:19+03","2024-05-17 05:50:50.486357+03","There are two ways of using Spark in Iguazio Where the spark_read_csv py file looks like"
72606145,72480662,"stackoverflow.com",0,"2022-06-13 20:03:20+03","2024-05-17 05:50:51.541732+03","Inside the Iguazio Dashboard navigate to the job summary of the job you want to reproduce In the right corner there is a menu that has a Rerun option See the screenshot below When you select that by default it keeps the same task config params inputs mounts etc but you can also modify them as well This is the easiest and fastest way to reproduce an experiment but it can also be done programmatically using the SDK "
72646531,72606834,"stackoverflow.com",0,"2022-06-16 16:18:18+03","2024-05-17 05:50:52.614412+03","Yes you can run Spark jobs in Iguazio using spot instances To run the job on the Spot instance first you will need to config a new NodeGroup with Spot instances e g addedspotng Then in your job specify the node selection like the code snippet below This way the function Spark job will be scheduled to the Spot instance nodegroup "
72621442,72621334,"stackoverflow.com",0,"2022-06-14 20:55:06+03","2024-05-17 05:50:54.480287+03","The file at v3iobigdatamy_file and v3iobigdatamy_file is exactly the same but is accessed differently A path with v3io is accessed via the Iguazio V3IO protocol This is generally more performant but is not compatible with all libraries A path with v3io is accessed via a mounted filesystem This is not as performant as the V3IO protocol but is compatible with more libraries I recommend using v3io where possible and falling back on v3io when it is not"
72635737,72635736,"stackoverflow.com",2,"2022-06-15 20:47:04+03","2024-05-17 05:50:55.491028+03","MLRun has the ability to automatically log models with metrics and plots generated and attached You will use something like The result is a model logged in the experiment tracking framework with metrics code logs plots etc available per run The MLRun autologger supports standard ML frameworks such as SciKitLearn TensorFlow and Keras PyTorch XGBoost LightGBM and ONNX Alternatively you can log something manually using the MLRun context object that is available during the run This lets you do things like context log_model context log_dataset or context logger info Something happened More info on the MLRun execution context can be found here "
72661168,72648719,"stackoverflow.com",0,"2022-06-17 17:46:48+03","2024-05-17 05:50:56.203732+03","A job stuck in this status is usually a Kubernetes issue The reason there is no logs in the Iguazio dashboard for the job is because the pod never started which is where the logs come from You can navigate to the web shell Jupyter service in Iguazio and use kubectl commands to find out what is going on in Kubernetes Usually I see this when there is an issue with the docker image for the pod it either cant be found or has bugs In a terminal doing kubectl get pods and find your pod It usually has ImagePullBackOff or CrashLoopBackOff or some similar error Check the docker image which is usually the culprit You can kill the pod in Kubernetes which in turn will error the job out You can also abort the job from the menu in the dashboard under that specific job "
72719904,72690551,"stackoverflow.com",0,"2022-06-22 20:45:42+03","2024-05-17 05:50:57.02923+03","You can use Kubernetes secret in an Nuclio function There are several steps to set this up create a Kubernetes secret simple example using kubectl like this kubectl create secret generic dbuserpass fromliteralusernamedevuser fromliteralpasswordAPasswordHere Then create an Nuclio function with set_env_from_secret like this fn mlrun code_to_function nucliowithsecret kindnuclio imagemlrunmlrun handlerhandler fn set_env_from_secret asecretname dbuserpass password fn apply mlrun auto_mount fn deploy In your Nuclio function you can use the secret like this the_secret_inside_nuclio_to_use os getenv asecretname "
72721227,72718114,"stackoverflow.com",0,"2022-06-22 22:47:12+03","2024-05-17 05:50:58.039961+03","There are actually different grafana dashboards being used in Iguazio The grafana dashboard you are referring to is for the platform users and is found on the services page This includes all your model monitoring dashboards as you mentioned There is another grafana service that is used by the Iguazio system which comes preinstalled You can access it by navigating to the Clusters page under the Application tab Status Dashboard column there is an icon that is a link which takes you to that grafana service See this screenshot There you will find about 15 dashboards related to kubernetes NGINX GPUs Nuclio and Iguazio services resources monitoring This constitutes the clusterlevel monitoring that you were referring to "
40778371,40778192,"stackoverflow.com",1,"2016-11-24 07:03:13+02","2024-05-17 05:50:59.064236+03","This is likely due to the application not receiving a correct config path In your init hh file add an args key and provide the application constructor parameters as shown in the below example Without this the Application plugin will try to search for config but eventually give up giving the resulting error We will make the error more obvious in a future release "
41438439,41391859,"stackoverflow.com",0,"2017-01-03 09:26:46+02","2024-05-17 05:50:59.793666+03","Without knowing more about your code I can predict a couple of possible sources of this type of behavior 1 You are not writing the fact that the user is authenticated to your sessioncookie so the second page request is not aware of the result of the first one 2 If the authentication is successful on the first page and you record this in the sessioncookie and the redirection happens but you redirect back to a page already seen by the user e g Homepage Login page Homepage then your browser might be loading it out of it is local cache rather than fetching the new authenticated page from the server Try dumping your session variables to the browser to see if the authentication result is being preserved between requests and try appending a timestamp on the redirection url or using headers to prevent client side caching This will at least allow you to narrow down or eliminate these two options "
41484793,41391859,"stackoverflow.com",0,"2017-01-05 14:11:13+02","2024-05-17 05:50:59.79601+03","The Auth plugin already manages all session control for authentication without any additional effort from the developer The problem you are facing could likely be because the session is not starting for some reason This could be because Nuclio is not detecting that it is being run from a browser Nuclio detects this by checking REMOTE_HOST and HTTP_HOST values in _SERVER If both are null it will not start the session to avoid generating headers on a command line Also make sure that your base application class is extending the Nuclio Application plugin class and NOT overriding the __construct method without calling the parent construct method as this would cause all the initialization to fail and no session will be createdresumed "
57587141,57514959,"stackoverflow.com",1,"2020-04-22 02:47:22+03","2024-05-17 05:51:00.770476+03","I will start by saying that Serverless is more innovative technology than Docker Containers However they both have their advantages and disadvantages Starting from Serverless it is possible to build them for virtually any type of application or backend service and everything required to run and scale your application with high availability is handled for you Deployment simplicity Theres no need to administrate infrastructure just upload your functions and thats all No Dockerfiles or Kubernetes configurations Almost all Serverless solutions support event triggers which means they are great for pipelines and sequenced workflows As you pay per function execution Serverless is cheaper than containers When an application is not being used it shuts down and you dont pay for the idle time you have mentioned that you do not have to pay for autoscaling When the Serverless app grows it is hard to troubleshoot because of the way the FaaS applications are designed to work Serverless always depends on a third party vendor so that changing the cloud provider can be a headache Docker is a containerization platform that packages your application and all its dependencies together in the docker container If you want to reduce application management and dont care about the architecture Serverless is the best option If you want to deploy an application on specified system architecture with having control over it then Docker containers are the best option So when comparing Serverless vs Docker it comes down to choosing what is better for your particular needs I encourage to read interesting article about it "
64289507,57514959,"stackoverflow.com",0,"2020-10-10 05:50:53+03","2024-05-17 05:51:00.773476+03","I was working with the serveless framework created for Kubernetes Kubeless Its great because you want to maintain your architecture agnostic from the Cloud vendor Kubeless functions are triggered when some event HTTP call or some cron job is raised But in your infrastructure you should always have at least one container running to execute your code That is the case for Kubeless it will autoscale based on demand just like other containers Some points to have in mind So if you need something more advanced supporting HTTP methods and with more business logic I recommend to use a traditional API approach easier to maintain and monitor Check this article based on how to implement serverless functions using Kubeless "
34050216,34046646,"stackoverflow.com",0,"2015-12-02 20:23:45+02","2024-05-17 05:52:21.56282+03","Your request is malformed 400 status code means Bad Request You also get an explanation in the error description Code package zip file required if no image specified or using ironimages "
67632811,67632707,"stackoverflow.com",0,"2021-05-21 10:52:19+03","2024-05-17 05:51:01.839433+03","You cannot install the same thing from two different Helm apps You have previously installed this chart and did not fully clean it up or it is just actually in use by someone else You can manually delete the CRD via kubectl delete crd but of course you should be very certain it is safe to do that first "
68288555,68288214,"stackoverflow.com",1,"2021-07-07 18:06:05+03","2024-05-17 05:51:02.879209+03","You should follow these steps"
68309412,68309305,"stackoverflow.com",1,"2021-07-09 23:28:02+03","2024-05-17 05:51:03.784177+03","In MLRun a Feature Set is a group of features that are ingested together A Feature Vector is a selection of features from Feature Sets a few columns here a few columns there etc This is great for joining several data sources together using a common entitykey A full example of creating and querying a feature set from MLRun can be found below You can find more feature store examples in the documentation here httpsdocs mlrun orgenlatestfeaturestorefeaturestore html"
68383621,68382785,"stackoverflow.com",1,"2021-07-14 21:46:20+03","2024-05-17 05:51:04.756024+03","One of the core concepts of MLRun is creating a serverless function out of a piece of code You can specify a Python file entrypoint function Docker image K8s resources and more code_to_function is how this is accomplished See this page in the docs for more info You can create and run these functions on their own or use something like KubeFlow to orchestrate a pipeline with multiple functions set_project is part of that workflow You can use the function created via code_to_function or just specify some of the parameters within set_project Then you will be able to use this function as a part of a larger KubeFlow pipeline See this page in the docs for more info "
76790358,68446927,"stackoverflow.com",1,"2023-07-28 21:45:58+03","2024-05-17 05:51:05.811571+03","If you are using an older version of CVAT you might want to refer to this solution posted in response to a similar issue For newer versions of CVAT you can follow the steps below Let us assume you need the timeout to be 175 seconds "
68873376,68869992,"stackoverflow.com",0,"2021-08-21 16:15:49+03","2024-05-17 05:51:07.296771+03","I suggest you explore connecting the Service Buss to Azure Event Hub Although your idea of initiating the connection in init_context is a good start you will have the complexity of managing the state and configuration of the ESB connection Nuclio includes an Azure Event Hub trigger Not only will it simplify your deployment but you will take advantage of Nuclios autoscaling and recovery options I found this article that seems to guide you through integrating ESB with Hub httpstechcommunity microsoft comt5azurepaasbloghowtosendmessagestoorreceivefromservicebuseventhubbap2136244"
72393621,72393620,"stackoverflow.com",0,"2022-05-26 17:40:28+03","2024-05-17 05:51:08.693434+03","The Active Directory configuration is under the IDP tab Identity Provider Please follow these instructions httpswww iguazio comdocslatestreleaseusersandsecurityusersidp When you click on IDP you will see the option to configure Active Directory"
72409920,72408377,"stackoverflow.com",2,"2022-05-27 21:53:13+03","2024-05-17 05:51:09.797427+03","In the Iguazio when you create a Dask cluster you do not need to worry about lowerlevel dask_kubernetes related stuff You just need to specify the min and max number of workers like below Depending on your workload the cluster will scale up and down between the min and max number of workers We bake in the adaptive deployments of the Dask cluster so it results in both faster analyses that give users much more power but with much less pressure on computational resources "
72750390,72746425,"stackoverflow.com",0,"2022-06-25 03:00:00+03","2024-05-17 05:51:10.491523+03","You can define a secret in Iguazio You can either define a secret at the project level or at the cluster level For define a secret at the project level here is an example code snippet MLRun provides facilities to map k8s secrets that were created externally to jobs that are executed Then you will use the secret in a function like this"
12788565,12777865,"stackoverflow.com",1,"2012-10-08 22:55:31+03","2024-05-17 05:51:11.484983+03","Right now our API does not support creating new access tokens I think it is something that could be really powerful though generate a token per server as part of the build process for example I have created an issue in our global issue tracker If you like you can follow that to get a notification when something on this front changes We have a lot of stuff we are working on right now so I cannot even guess at a timeline unfortunately Hope that helps "
14278696,13285901,"stackoverflow.com",13,"2013-01-11 14:58:12+02","2024-05-17 05:51:12.398356+03","Newer iron_worker version has native support of pip command So you need"
13296163,13285901,"stackoverflow.com",6,"2017-05-23 15:04:35+03","2024-05-17 05:51:12.399554+03","[edit]We have worked on our toolset a bit since this answer was written and accepted The answer from my colleague below is the recommended course moving forward [edit] I wrote the Python client library for IronWorker I am also employed by Iron io If you are using the Python client library the easiest and recommended way to do this is to just copy over the librarys installed folder and include it when uploading the package That is what the Python Loggly sample is doing above As you said that does not specify a version or where to download the library from because it does not care It just takes the one installed on your system and uses it Whatever you get when you enter import boto on your local machine is what would be uploaded The other option is using our CLI to upload your worker with a worker file To do this heres what you would need to do Create a botoworker worker file That second line is the pip command that will be run to install the dependency You can modify it like you would any pip command run from the command line It is going to execute that command on the worker during the build phase so it is only executed once instead of every time you run a task The third line should be changed to the Python file you want to runit is your Python worker file Heres the one we used to test this If you save that as botoworker py the above should work without any modification The fourth line is a shell script that is going to actually run your worker I have included the one we used below Just save it as botoworker sh and you will not have to worry about modifying the worker file above You will notice it refers to your Python fileif you do not name your Python file botoworker py remember to change it here too All this does is set your PYTHONPATH to include the installed library and then runs your Python file To upload this just make sure you have the CLI installed gem install iron_worker_ng making sure your Ruby version is 1 9 3 or higher and then run iron_worker upload botoworker in your shell from the same directory your botoworker worker file is in Hope this helps "
13339914,13305823,"stackoverflow.com",3,"2012-11-12 09:44:45+02","2024-05-17 05:51:13.528571+03","If your unpublished gem itself has dependancies you need to do a little massaging to get things going Here is a technique that works for me mygem worker install_dependancies sh"
13311330,13305823,"stackoverflow.com",2,"2012-11-09 17:51:06+02","2024-05-17 05:51:13.529571+03","As far as i know git and local paths unsupported right now Here is way to manually include local gem Add these lines to worker file"
13315069,13305866,"stackoverflow.com",4,"2012-11-09 21:58:24+02","2024-05-17 05:51:14.637628+03","I would recommend using our next generation gem iron_worker_ng httpsgithub comironioiron_worker_ruby_ng The iron_worker gem is deprecated And if you want to keep it similar style to what you have your child_worker rb might look like this And in a child_worker worker file Then to upload it to IronWorker Then you can start queuing jobs for it"
18464520,13305866,"stackoverflow.com",1,"2013-08-27 14:47:13+03","2024-05-17 05:51:14.638628+03","If you use iron_worker_ng it also possible to define a run method This method will be called when the IronWorker runs You have to specify the Class within the worker file And the child_worker worker file"
13470269,13445796,"stackoverflow.com",3,"2012-11-20 11:37:33+02","2024-05-17 05:51:15.559358+03","From our side cloudControl there rule is that latency critical Addons have to be in Europe and optimally in the same EUWest AWS region when they leave beta status Usually you can always buy services directly and still use them in combination with our platform The only difference is you will not get a consolidated invoice and cloudControl first level support mostly cannot really answer questions regarding third party services not bought via the addon market due to lack of information "
35471073,34046646,"stackoverflow.com",0,"2016-02-18 03:23:17+02","2024-05-17 05:52:21.564821+03","The final solution was to change my PHP Version I moved from 5 6 to 5 4 and that did the difference and everything worked as expected "
13448070,13445796,"stackoverflow.com",2,"2012-11-19 07:18:14+02","2024-05-17 05:51:15.560359+03","As of this writing IronMQ is hosted on awsuseast and rackspacedfw IronWorker is hosted at awsuseast We have plans for other zones and clouds but cannot provide a definitive timeline You can find information on cloud locations at the following pages in our dev center You can subscribe directly to Iron io although we are grateful for all our platform partners Should you have any questions feel free to connect with us via our realtime chat room or our support channel "
13504390,13445796,"stackoverflow.com",0,"2012-11-22 02:49:11+02","2024-05-17 05:51:15.562359+03","I am the OP Interestingly the answers from cloudControl and iron io make me realise that I have been making a wrong assumption which is that the cloudControl Addons had to be running locally on AWS EUWest The fact that the iron io and cloudant addons are running on separate data centres is something I did not expect Now that this is clear it makes perfect sense that buying them separately or as addon does not change their hosting location and the argument for addons is indeed support and common billing I reckon it would make sense for PaaS in general to make clear whether their addons are running locally or not for it is an important criterion of choice For now IronWorkerCloudant via cloudControl is probably not the ideal combination for data lifting workers in my app they would be perfect if they were all in the same place so I will host the workers differently "
13492250,13491897,"stackoverflow.com",6,"2012-11-21 13:24:20+02","2024-05-17 05:51:16.563024+03","Add this line to worker file"
13691557,13630393,"stackoverflow.com",1,"2012-12-03 22:57:28+02","2024-05-17 05:51:17.687001+03","There is currently no idiomatic way to update a noninteger Cache item without provoking the race condition gods There are a lot of different hacks to get around the limitation but your MQ solution assuming only one worker is writing the changes is probably your best bet We are aware of the shortcoming and we are working on a fix but we have nothing to announce at this time "
13792106,13630393,"stackoverflow.com",1,"2012-12-09 23:34:30+02","2024-05-17 05:51:17.689002+03","One way to do this would be to split up your value into multiple cache entries Say you have your json hierarchy Change it to Then in cache_key_a And do the same for cache_key_b and so on Would that solve your problem"
14006772,14006544,"stackoverflow.com",4,"2012-12-22 23:36:34+02","2024-05-17 05:51:18.763424+03","Queue task obtain task_id as result of operation But there are plenty of different ways to get result read task log via api you need project_id token task_id Store data to Amazon S3 push information to some kind of queue touch own api send info to webhook write information to database etc"
14148931,14124086,"stackoverflow.com",1,"2013-01-04 01:22:56+02","2024-05-17 05:51:20.36488+03","I am under the impression that IronWorker can use the databases for Heroku I may be mistaken about this however I cannot claim a great deal of experience with cron tasks on Heroku so forgive me if I get its limitations wrong However I am under the impression that scaling from one dyno to many dynos is a little bit of a process Where IronWorker shines is really in enabling the number of worker servers to fluctuate based on demandyou have the capacity to scale at a moments notice but are only paying for the scale you are actually using "
14491953,14491952,"stackoverflow.com",2,"2017-05-23 15:27:06+03","2024-05-17 05:51:21.049637+03","Ok I have found both the reason of my problem and its solution So to answer my own question Problem 1 I am receiving POST messages from an IronMQ push queue httpdev iron iomqreferencepush_queues their content type is textplain 2 I am using connect js middleware express connect and it parses only applicationjsonapplicationxwwwformurlencoded and multipartformdata httpwww senchalabs orgconnectbodyParser html So the body gets parsed and as its content type is not supported the result is In order to get the body of my textplain request I had to parse it by myself as in httpsstackoverflow coma9920700"
22212668,14491952,"stackoverflow.com",1,"2014-03-06 02:51:49+02","2024-05-17 05:51:21.051637+03","IronMQ have now updated their push queues to send custom headers If you set the headers to ContentType applicationjson in the list of subscribers when creating the queue then the body gets parsed correctly eg Heres the relevant change on github"
14615394,14615046,"stackoverflow.com",3,"2013-01-31 00:50:56+02","2024-05-17 05:51:22.042184+03","You really need only statically compiled x64 binaries find them anywhere and you are done Possible solutions worker file example including custom version of imagemagick "
17382642,17382019,"stackoverflow.com",2,"2013-06-29 20:04:29+03","2024-05-17 05:51:23.163521+03","If you added all those endpoints subscribers to your queue it is possible that IronMQ sends multiple requests Check your queues subscribers list If it contains multiple endpoints and its type is multicast this is the reason of multiple requests on your side In this case remove all odd subscribers or setup new queue In other case contact support More information at httpdev iron iomq"
17383013,17382019,"stackoverflow.com",1,"2013-06-29 20:44:25+03","2024-05-17 05:51:23.165521+03","IronMQ will not send any unknown requests If your endpoint does not return a 200 the push queue will keep retrying the message until it either a receives a 200 or b fails the max_retries number of times Also per Featilions answer check the multicastunicastsubscriber setup as well If you are getting requests to those other endpoints then there is something up with your subscriber setup Feel free to jump into live chat if you do not figure out your answer rather quickly "
17613898,17607616,"stackoverflow.com",1,"2013-07-12 14:25:46+03","2024-05-17 05:51:24.043743+03","I am not sure but looks like params is nil could you try to inspect it before using in It could happened if you are using in your worker separate namespaces like worker body inside class "
18455875,18449084,"stackoverflow.com",2,"2013-08-27 05:12:56+03","2024-05-17 05:51:25.142981+03","in run sh"
18516640,18511223,"stackoverflow.com",2,"2013-08-29 19:45:07+03","2024-05-17 05:51:26.212958+03","Generally messages are for background processing like Antonio said but you can poll or push using websockets to get the results after the background process is completed Basically anytime you see a progress bar or spinner on a website after you have clicked something that is what is happening So the process is Heres a good article on various polling options httptechoctave comc7posts60simplelongpollingexamplewithjavascriptandjquery"
18742399,18727938,"stackoverflow.com",0,"2017-05-23 13:27:58+03","2024-05-17 05:51:26.876391+03","As Joseph pointed out in the comments you will need to The popular methods are"
18863953,18863893,"stackoverflow.com",4,"2013-09-18 18:20:06+03","2024-05-17 05:51:27.434137+03","ah sometimes writing out the question on stackoverflow makes you see the problem I needed without the 3 2 8 version it was using an old old version of action mailer "
19101282,19101281,"stackoverflow.com",3,"2013-09-30 21:32:58+03","2024-05-17 05:51:28.130313+03","curl sh curl worker"
19263467,19262461,"stackoverflow.com",1,"2013-10-09 07:56:04+03","2024-05-17 05:51:28.748343+03","1 Merge config file into package 2 upload worker with workerconfig flag e g iron_worker upload hello worker workerconfig cfg json and use config helper inside worker 3 Pass connection data via payload"
19277605,19262461,"stackoverflow.com",1,"2013-10-10 01:39:47+03","2024-05-17 05:51:28.750344+03","Funny you should ask an IronCast just went out yesterday on our blog about connecting to databases from your IronWorkers httpblog iron io201310ironcast4howtoconnecttoyour html To pass it in the payload Then to use it in your worker"
76654191,76653956,"stackoverflow.com",0,"2023-07-10 16:22:44+03","2024-05-17 05:51:45.575878+03","I got it The worker uses separate worker scope This means that each worker has a copy of all variables and all changes are kept within the worker change by worker x do not affect worker y It means it is useful to increase the requestlimit resources at least for memory in level of podreplica You can setup amount of workers for http trigger based on that fn with_http workersn more information see I updated code based on source tuning"
19288552,19277327,"stackoverflow.com",3,"2014-05-10 01:00:03+03","2024-05-17 05:51:29.712851+03","[edit] You can now use multiple different language packs using our Stacks functionality as specified above Also heres more info in the Iron io dev center httpdev iron ioworkerreferenceenvironmentdefault_language_versions Currently no there is no way to use Ruby 2 We are working on a way to give users the ability to choose their language versions much easier but there is no ETA at this point If you need any help or support do not hesitate to jump into our live chat httpget iron iochat"
22793015,19277327,"stackoverflow.com",2,"2014-04-01 20:40:56+03","2024-05-17 05:51:29.714851+03","Iron Worker is finally supporting custom environments one of which is Ruby 2 1 httpblog iron io201403ironiolaunchescustomruntime html"
64822470,19331325,"stackoverflow.com",0,"2020-11-13 16:14:41+02","2024-05-17 05:51:30.831373+03","Looks like you are using the old docker image to run your IronWorker tasks Try to use ironmono image for that This guide will walk you through HelloWorld NET example "
64806432,19412410,"stackoverflow.com",0,"2020-11-12 17:04:09+02","2024-05-17 05:51:31.331259+03","IronWorker already announced a Docker Workflow and you do not need the worker file anymore You just need to install all dependencies locally in docker image reproducing the same environment as running on IronWorker servers Here you can find a ruby HelloWorld example "
19633780,19631129,"stackoverflow.com",1,"2013-10-28 13:36:36+02","2024-05-17 05:51:33.207621+03","By analysis of the iron_worker source it looks to use the HOME variable environment and is blocking where no HOME env variable is set I suggest you to set the variable with putenv HOMEhomemy_user before running the script by PHP "
19631225,19631129,"stackoverflow.com",0,"2013-10-28 11:33:21+02","2024-05-17 05:51:33.209622+03","This program seems to need a HOME directory to run from You have to add this to sudoers file where http is the webserver and foo is a user on the machine Oh i forgot the system part I would suggest to put the program in homefoo I hope this works for you "
20290117,20288202,"stackoverflow.com",16,"2015-09-15 16:23:46+03","2024-05-17 05:51:34.013806+03","I think I have understand the problem there are two problems all in all If data is passed to the view with an associative array You should access the values in views with And there MUST NOT be any message keys in the data array because the closures message variable is also passed to the view "
20694456,20693415,"stackoverflow.com",0,"2013-12-20 02:44:03+02","2024-05-17 05:51:34.930605+03","handle the 400 errorcode and have it rerun the post again after a backoff period "
20694805,20693415,"stackoverflow.com",0,"2013-12-20 03:26:49+02","2024-05-17 05:51:34.931605+03","HTTP 400 response code means application sent wrong data Possibly JSONified message is greater than 64kB "
20802837,20693415,"stackoverflow.com",0,"2013-12-27 17:01:28+02","2024-05-17 05:51:34.932606+03","I ended up just moving to use Sidekiq to handle photo uploading It has retry logic built into it and is easy to configure "
21710593,21709724,"stackoverflow.com",2,"2014-02-11 20:53:41+02","2024-05-17 05:51:35.671736+03","Shortterm you could likely just use the scheduling capabilities in IronWorker and have the worker hit an endpoint in your application The endpoint will then trigger the operations to run within your app environment Longerterm we do suggest you look at more of a serviceoriented approach whereby you break your application up to be more loosecoupled and distributed Heres a post on the subject The advantages are many especially around scalability and development agility httpsblog heroku comarchives2013123end_monolithic_app You can also take a look at this YII addition httpwww yiiframework comextensionyiiron Certainly do not want you rewrite your app unnecessarily but there are likely areas where you can look to decouple Suggest creating a worker directory and making efforts to write the workers to be selfcontained In that way you could run them in a different environment and just pass payloads to the worker Push queues can also be used to push to these workers Once you get used to distributed async processing it is a pretty easy process to manage Note I work at Iron io "
21804915,21801705,"stackoverflow.com",0,"2014-02-16 01:07:25+02","2024-05-17 05:51:36.728258+03","How about touching iron_mqclearQueue queue_name httpsgithub comironioiron_mq_phpblobmasterIronMQ class phpL235 No idea how Laravel exposes it though"
22153804,22148917,"stackoverflow.com",2,"2014-03-03 19:54:15+02","2024-05-17 05:51:37.37929+03","I figured out that it was a bad Ruby install No idea why but reinstalling it worked "
22153710,22148917,"stackoverflow.com",0,"2014-03-03 19:49:53+02","2024-05-17 05:51:37.38029+03","It is difficult to know for sure what is happening without being able to see a traceback Do you get anything like that which could be used to help figure out what is going on"
22179868,22175721,"stackoverflow.com",1,"2014-03-04 20:21:31+02","2024-05-17 05:51:38.188727+03","As cmancre said you can use HUD to set the error queue or you could use the API to set it httpdev iron iomqreferenceapiupdate_a_message_queue"
22178884,22175721,"stackoverflow.com",0,"2014-03-04 19:30:57+02","2024-05-17 05:51:38.189518+03","Iron guys just rolled out an UI allowing us to set a error_error via iron admin panel "
22180188,22175721,"stackoverflow.com",0,"2014-03-04 20:37:26+02","2024-05-17 05:51:38.190518+03","In case your error_queue is already firing to complete the cycle you need to know which message failed To grab the error message information in the error_queue route just do Reference httpwww codingswag com201307getrawpostdatainlaravel"
56695070,56673536,"stackoverflow.com",3,"2019-06-21 02:31:12+03","2024-05-17 05:51:39.550502+03","Try this FnIf takes three parameters The first one is the condition name the second is the value if true and the third is the value if false You passed a map instead "
56759199,56673536,"stackoverflow.com",0,"2019-06-25 20:26:29+03","2024-05-17 05:51:39.551502+03","Ok that makes sense thank you But I keep getting this error now"
74823262,74700311,"stackoverflow.com",0,"2022-12-16 12:39:17+02","2024-05-17 05:51:42.195158+03","I see two steps how to solve the issue 1 Relevant installation The MLRun Community Edition in desktop docker has to be install under relevant HOST_IP not with localhost or 127 0 0 1 but with stable IP address see ipconfig and with relevant SHARED_DIR See relevant command line from OS windows BTW YAML file see httpsdocs mlrun orgenlatestinstalllocaldocker html 2 Access to the port In case of call serving_fn invoke you have to open relevant port from deploy_function on your IP address based on setting of HOST_IP see the first point Typically this port can be blocked based on your firewall policy or your local antivirus It means you have to open access to this port before invoke call BTW You can see focus on the issue httpsgithub commlrunmlrunissues2102"
76308615,75151612,"stackoverflow.com",1,"2023-05-22 20:26:34+03","2024-05-17 05:51:43.131525+03","This might have to do with Nuclio version before 1 11 16 supported HPA version was v2beta1 which is not supported from K8s 1 24 11 version"
76642766,76546895,"stackoverflow.com",1,"2023-07-08 14:41:50+03","2024-05-17 05:51:44.017911+03","UPDATE I managed to do this using the nuclios dashboard "
77510777,76546895,"stackoverflow.com",1,"2023-11-19 14:54:19+02","2024-05-17 05:51:44.018911+03","You can delete this function by nucleo by run this comand It works well with CVAT for me Run this command to get all function running by nucleo We get this responce run this command to delete pthfacebookresearchsamvith Then run this command get all function running by nucleo again And we see that there is no functions running by nucleo If it does not help you can do it by nucleo UI like in this answer in this issue link "
78005062,76546895,"stackoverflow.com",1,"2024-02-16 05:36:53+02","2024-05-17 05:51:44.020916+03","If you get a delete error do the following"
76648507,76648444,"stackoverflow.com",0,"2023-07-09 22:18:36+03","2024-05-17 05:51:45.112348+03","I solved the same problem the issue is that you use a invalid unit 500m you have to use these quantity suffixes E P T G M k or you can also use the poweroftwo equivalents Ei Pi Ti Gi Mi Ki See official documentation k8s meaningofmemory Relevant value is 500M x1 000 K or 500Mi x 1 024 Ki updated code see"
76787268,76781280,"stackoverflow.com",1,"2023-07-28 14:11:46+03","2024-05-17 05:51:47.362109+03","To scrape metrics from Nuclio functions and make them available to Prometheus you will need to set up a custom Prometheus scrape configuration for the Nuclio functions in your monitoring namespace This involves defining a new ServiceMonitor resource that specifies the target endpoints to scrape Heres how you can proceed 1 Create a ServiceMonitor resource You need to define a ServiceMonitor resource in the monitoring namespace to tell Prometheus what to scrape This resource should include the labels that match your Nuclio functions services For example create a file named nucliofunctionmonitor yaml with the following content In this example the selector field selects all services with the label app kubernetes ioinstance nuclio you might need to adjust this label depending on how your Nuclio functions are labeled The endpoints field specifies that Prometheus should scrape the httpmetrics port of these services 2 Apply the ServiceMonitor resource Apply the nucliofunctionmonitor yaml file to create the ServiceMonitor resource in the monitoring namespace 3 Verify Ensure that the ServiceMonitor is created successfully You should see the nucliofunctionmonitor resource in the output 4 Configure Nuclio to expose metrics You also need to ensure that your Nuclio functions are configured to expose metrics on the httpmetrics endpoint It looks like you have already done this in your nuclioplatformconfig ConfigMap 5 Access the metrics Now Prometheus should automatically start scraping metrics from the Nuclio functions exposed on the httpmetrics endpoint You can check if Prometheus is scraping the targets by navigating to the Prometheus web UI if you have installed Prometheus with kubeprometheusstack and checking the Targets page You should see the Nuclio function endpoints listed there "
76890388,76890325,"stackoverflow.com",0,"2023-08-12 20:21:33+03","2024-05-17 05:51:48.202102+03","It seems that your total CPU requestslimits on your K8s are out of available sources You have two options"
76894482,76893925,"stackoverflow.com",0,"2023-08-13 20:25:51+03","2024-05-17 05:51:49.157972+03","You have to use igztop 0 1 4 parameters are little different than in the version 0 1 3 check igztop h You can see mlrun name of functions in case of command igztop o mlrun see the outputs"
77339920,77339653,"stackoverflow.com",1,"2023-10-22 15:27:26+03","2024-05-17 05:51:50.154718+03","Kafka data is always serialized Therefore must be deserialized and any Kafka consumer must set deserializer settings Any framework you are using should document how to configure the consumer If you are restricted to bytestring deserialization then yes you would configure for bytes or otherwise get a base64 string for example then need to use your own decode deserialize function using builtin Kafka deserializers or external ones such as from Confluent in a handler httpsgithub comnuclionuclioissues916"
78145799,78139425,"stackoverflow.com",1,"2024-03-12 11:03:48+02","2024-05-17 05:51:52.315194+03","The Nuclio doc site got updated recently and can now be found here Specifically refer to installing Nuclio on GCP The RBAC configuration roles and rolebindings in the Github repository is available in the templates directory but installing Nuclio using Helm as in the above documentation should handle it all by itself "
23492203,23492054,"stackoverflow.com",7,"2014-05-06 13:30:07+03","2024-05-17 05:51:55.089719+03","Is not valid json object You need double quotes everywhere"
23600013,23599790,"stackoverflow.com",3,"2014-05-12 05:03:46+03","2024-05-17 05:51:56.00945+03","So after digging around in the source code a bit I found that if you add public delete true to the class that contains your fire method Laravel will automatically delete completed jobs This is referenced from Illuminate\Queue\Worker process where it checks for jobautoDelete after calling jobfire In my case this was not set and I was unable to reliably reproduce the issue I was trying to fix I am settling for just setting delete since Laravel will return an Exceptionthrowing job to the queue regardless "
24293193,24289317,"stackoverflow.com",1,"2014-06-18 22:01:57+03","2024-05-17 05:51:57.674902+03","Architecturally there are a number of approaches but it sounds as if you are making the right choices Using a queue to decouple the producer from the notification process makes sense This enables a more proper SOA architecture where you can changeupdateevolve various parts of the app independently without worrying too much about tightly coupled code That said your question is specifically around offloading to third parties There are third parties that can abstract the notification part out of your code I am not super familiar with them but there are many options PubNub Pusher Twilio SendGrid Mailgun AWS SNS etc I work for Iron io We have many customers doing exactly what you are trying to accomplish creating workers that become little miniservices and calling them from either push events scheduled tasks or ondemand This frees you up from having to deal with the queuing routing scheduling and workerbackground server capacity We are happy to help you architect things right from the beginning just reach out to [email protected] "
24174436,24167739,"stackoverflow.com",0,"2014-06-12 02:52:30+03","2024-05-17 05:51:57.721352+03","64KB is the maximum total message size allowed on that platform You could"
24644940,24642358,"stackoverflow.com",3,"2014-07-09 06:28:24+03","2024-05-17 05:51:58.363056+03","You can use pushRaw function pushRaw payload queue null array options array Example QueuepushRaw This is Hello World payload email "
64776517,24861036,"stackoverflow.com",1,"2020-11-10 22:44:53+02","2024-05-17 05:51:59.108411+03","Since the answer to this question IronWorker has released a Docker workflow Feel free to use our official ironnode Docker Image httpsgithub comironiodockerworkertreemasternode"
24897327,24861036,"stackoverflow.com",0,"2014-07-24 06:00:46+03","2024-05-17 05:51:59.110411+03","Ahh this is definitely not a problem with Iron io but a problem with your post to the Facebook v1 0 API call do you really want to omit response on success Which Facebook endpoint are you sending a Post to edit IronWorker is currently set to 0 10 25 as of 07222014 Use if your node version is 0 10 25 you may receive this error fix load your own version of node in your worker file add the following You can install other missing or updated versions of binaries in a similar manner if there is a deb for it Example in practice here on github tldr use latest version of node possibly openssl also "
25418289,25321050,"stackoverflow.com",1,"2014-08-21 07:34:23+03","2024-05-17 05:52:00.16178+03","Currently there are no alerts for when a message times out and goes back on the queue but that does seem like it would be a good idea I assume this is a pretty inactive queue I made a feature request for this here httpstrello comcXcHi0NdN35firealertwhenamessagetimesoutgoesbackonqueue And regarding messages that are causing issues your best bet would be to add them to a different queue an error queue and delete them off the original queue Then you can go through the error queue to figure out why certain messages are causing you problems This is known as a dead letter queue btw and we have a feature request for it here please give it a vote httpstrello comcbGnJcNa926deadletterqueue"
25928409,25927437,"stackoverflow.com",0,"2014-09-19 10:17:21+03","2024-05-17 05:52:01.138216+03","You can probably include the linux x64 binary distribution along with your worker by including the dir command in your worker file Something like dir apacheant1 9 4 That will include the entire directory in your worker package Hit us up at get iron iochat for help "
26581583,26569970,"stackoverflow.com",0,"2014-10-27 07:21:34+02","2024-05-17 05:52:02.240822+03","If you are using restclient gem here is the issue Solution from github ticket or"
34125958,34125672,"stackoverflow.com",1,"2015-12-07 05:27:34+02","2024-05-17 05:52:22.351195+03","bundle install without development test"
64895118,34488676,"stackoverflow.com",0,"2020-11-18 16:31:13+02","2024-05-17 05:52:23.410734+03","Try to use the port 80 instead of 11211 i e replace with"
26978874,26947367,"stackoverflow.com",1,"2014-11-17 19:57:39+02","2024-05-17 05:52:03.140677+03","There is actually a special subscriber format just for IronWorker as specified in the Push Queue documentation here httpdev iron iomqreferencepush_queuessubscribers Eg That will kick off a worker task whenever something hits your queue Or you can use the workers webhook URL And you do not need to deal with the response as thousandsofthem said IronWorker will return a 200 which acknowledges the pushed message "
26948592,26947367,"stackoverflow.com",0,"2014-11-15 19:27:24+02","2024-05-17 05:52:03.141678+03","IronWorker API will respond immediately for a post request with HTTP 200 OK status and queue a task after that it is too late to respond something from running task You could find exact webhook value on Code page httpshud iron io Screenshot httpsi sstatic netkf5D4 png Just use it as is"
26982808,26958507,"stackoverflow.com",3,"2017-05-23 13:26:27+03","2024-05-17 05:52:04.104879+03","This is possibly related to mono version on the IronWorker service I found related question on StackOverflow Try to change the stack in your worker file to mono3 6 or mono3 0 See all possible runtime environments on Iron ios dev site mono runtime is default and it is alias for mono2 10 "
27602703,27601485,"stackoverflow.com",2,"2017-05-23 13:26:07+03","2024-05-17 05:52:05.129621+03","Webhook endpoint receives unmodified POST content as payload just send json and you are done Code borrowed from httpsstackoverflow coma65872491758892"
28032514,28014005,"stackoverflow.com",2,"2015-01-19 21:54:37+02","2024-05-17 05:52:07.482889+03","Looks like you are trying to use Ruby gems in your worker file with a python script You should be using pip for Python libraries Try changing your worker file to this"
28037817,28028138,"stackoverflow.com",1,"2017-03-26 11:12:59+03","2024-05-17 05:52:08.024554+03","Go to app\config\queue php and check the default key If the latter is set to sync then the above behaviour is expected sync driver runs your task immediately which means Queuelater is in truth Queuepush Try converting your Queuelater into Queuepush and the code will run without errors As for explicit deletion of task in the queue it is for most part unnecessary However you have to account for queue services which do not provide for automatic deletion of the job For my part AWS SQS Message Queue Service automatically deletes the job after it is pulled from the queue tube "
28220527,28219251,"stackoverflow.com",0,"2015-01-29 19:08:50+02","2024-05-17 05:52:08.993717+03","You could add almost any deb file to a worker package via keyword deb working example Also you could find some other helpful examples there httpsgithub comironioiron_worker_examplestreemasterdebpackages"
28630651,28629511,"stackoverflow.com",3,"2015-02-20 15:55:45+02","2024-05-17 05:52:09.965656+03","99 9 it is wrong iron_mq version Please try to set version 1 "
28860054,28859633,"stackoverflow.com",1,"2015-03-04 18:45:06+02","2024-05-17 05:52:10.913674+03","There is a feature built in to IronMQ called Error Queues that will collect messages that could not be delivered into a separate pull queue so you can deal with them later Heres a diagram showing how this works You can read more about it on the Iron io blog here httpblog iron io201401pushqueueserrorqueuesbetterqueue html or in the docs here httpdev iron iomqreferencepush_queueserror_queues"
29090958,29078571,"stackoverflow.com",1,"2017-05-23 15:28:18+03","2024-05-17 05:52:12.049364+03","If you do not have Scrapy executable available you can run Scrapy via cmdline You can also run Scrapy from script Here is a very detailed answer "
29110026,29078571,"stackoverflow.com",0,"2015-03-17 23:26:17+02","2024-05-17 05:52:12.051365+03","You would probably be better off using Scrapy from your IronWorker code rather than calling it from the command line just like it has on the front page of httpscrapy org or in the tutorial httpdoc scrapy orgen0 24introtutorial html To use this in IronWorker after you have done the pip install be sure to add to your worker file Then in your worker script you would import it Then use it like it says in the tutorial link above "
29573997,29572908,"stackoverflow.com",0,"2015-04-11 07:06:57+03","2024-05-17 05:52:13.107829+03","you cannot update your ruby cartridge on openshift You will have to make a new app and use ruby 1 9 from very beginning You can use something like"
29775810,29760597,"stackoverflow.com",0,"2015-04-21 17:55:27+03","2024-05-17 05:52:14.182549+03","I have been using iron io just for the message queue I ask iron io to call specific URL in my Laravel app and process the request in the Laravel app not in the remote iron io server "
29906019,29760597,"stackoverflow.com",0,"2015-04-28 00:12:47+03","2024-05-17 05:52:14.18455+03","To run code on IronWorker you must include all your dependencies along with your worker Whichever dependency has the DB class you must include that Heres an example worker with dependencies httpsgithub comironiodockerworkerblobmasterphp You can use Laravels database module ORM called Eloquent ORM outside of a Laravel app heres some info on how to do that httpwww edzynda comuselaravelseloquentormoutsideoflaravel Another option is to use Push Queues which uses IronMQ to deliver messages to an endpoint on your application httplaravel comdocs5 0queuespushqueues Heres a video showing this in action httpsvimeo com64703617 Hope that helps "
64806171,30793682,"stackoverflow.com",0,"2020-11-12 16:49:38+02","2024-05-17 05:52:14.879327+03","Most likely you forgot to update ironmq version after switching to Laravel 5 1 Only IronMQ v4 is compatible with Laravel 5 1 Specify 4 as IronMQ version in composer json "
31008747,30997430,"stackoverflow.com",0,"2015-06-23 19:47:49+03","2024-05-17 05:52:15.828071+03","Laravel is using IronMQs Push Queues and since push queues are delivered immediately they do not stick around upon successful delivery Although you can create an error queue to inspect messages that cannot be delivered successfully "
31009211,30999660,"stackoverflow.com",1,"2015-06-23 20:38:29+03","2024-05-17 05:52:16.528474+03","I am not sure Mailqueue goes out to IronMQ but if it does I think you would want to set it up in a way that the body does not get sent with it Instead of queuing the mail queue up the metadata for the email eg Then when the push comes back to your app SendMail can call the synchronous send mail I am not a PHP person so the code may be a bit off but it should convey the idea "
32193200,32192823,"stackoverflow.com",2,"2015-08-25 02:17:24+03","2024-05-17 05:52:17.368862+03","Workers that postfixed with builder are special They build worker code package remotely on IronWorker for you It happens when you put remote directive to your worker file Actual worker name is helloworldworker So that your webhook URL must looks like the following Replace TOKEN with your actual token As you shared your current token I suggest you to delete it and generate new You can do that by clicking on your name at the top right corner of the HUD and then on My Tokens menu item "
42443478,33109234,"stackoverflow.com",4,"2017-02-24 18:21:55+02","2024-05-17 05:52:17.877852+03","I see that this question is old but I was just having a similar issue In my case I discovered that for some reason if the current time zone is UTC when I parse a string like 20170223T065400Z the resulting datetime has tzinfotzlocal while for other timezones the resulting datetime has tzinfotzutc as expected "
33227938,33227315,"stackoverflow.com",0,"2015-10-20 07:19:48+03","2024-05-17 05:52:18.782755+03","Do you upload 1 file or entire project You have to upload entire project Also take a look at this packet "
33308995,33228002,"stackoverflow.com",0,"2015-10-23 21:18:50+03","2024-05-17 05:52:19.690628+03","So this is just for another poor soul who is facing the same issue After a lot of searching I resorted to a separate Job class that sends email and takes data for view template data and details specifying email template to email address and subject in its constructor Then I Mailsend in handle method I fire this job using Controllers thisdispatch method just as they said in Laravel Documentation"
64857005,33228002,"stackoverflow.com",0,"2020-11-16 13:17:41+02","2024-05-17 05:52:19.692632+03","After updating Laravel from 4 2 to 5 1 you need to update the IronMQ version to 4 in composer json Laravel 5 1 is only compatible with IronMQ v4 "
35255138,34703094,"stackoverflow.com",0,"2016-02-07 17:09:11+02","2024-05-17 05:52:24.389107+03","I had the same trouble trying to run pip install on my Mac with the Iron docker images After trying a bunch of things to fix it I tracked the problem down to Docker Toolbox and vboxfs They were not syncing host filesystem changes properly To fix it in the docker toolbox VM I ran sync echo 3 procsysvmdrop_caches The sync call syncs any pending writes to disk The second command tells the kernel to clear the filesystem caches Once I did that my pip install worked fine "
64776370,34703094,"stackoverflow.com",0,"2020-11-10 22:32:18+02","2024-05-17 05:52:24.390854+03","On Windows and Mac OS Docker Daemon runs on a virtual machine Try to enter the virtual machine and run the Docker commands there A possible work around for this issue is to avoid running Docker commands on host machine As a reminder Iron ios support is available via phone email or chat for additional questions "
34769852,34769159,"stackoverflow.com",2,"2016-01-13 16:57:06+02","2024-05-17 05:52:25.338572+03","There is a couple things here 1 you are using full_remote_build with a worker file but then uploading the zip Those two things do not go together it is one or the other 2 Be sure you are vendoring the gems so they are included when you zip them To vendor docker run rm v PWDworker w worker ironrubydev bundle install standalone clean Then at the top of your script add require_relative bundlebundlersetup See the documentation here for more details httpsgithub comironiodockerworkertreemasterruby"
35016306,35015391,"stackoverflow.com",2,"2016-01-26 16:33:40+02","2024-05-17 05:52:26.381663+03","Try to add phppdo package into your docker image Example of installing phppdo package via Dockerfile FROM ironphp RUN apk add phppdo "
35583335,35498096,"stackoverflow.com",0,"2016-02-23 18:48:20+02","2024-05-17 05:52:27.413515+03","There could be issues at 2 stages that would need to be debugged 1 If the version of ironmq client and server are same if not if there a change in way they communicate I would say this is most important to debug 2 If calling latter triggers a queue in ironmq is the call received in ironmq and it is taking longer to respond "
64857979,35498096,"stackoverflow.com",0,"2020-11-16 14:23:56+02","2024-05-17 05:52:27.415516+03","It could be related to network issues while connecting to IronMQ clusters I checked the latency for two public clusters useast1 and euwest1 and it takes less than 1 sec now "
38405623,35514401,"stackoverflow.com",3,"2016-07-16 01:05:28+03","2024-05-17 05:52:28.551452+03","For those who may be in the same situation as I was this happens because Iron io now integrates Docker and we have to specify an image we use when running our code but ironphp image does not include any additional middlewares they used to support So we have to write our own DockerFile to install MongoDB or MySQL or whatever necessary middleware software we need As for MongoDB this is an example of a Dockerfile Please note that this is not optimized at all in terms of an image size I think it is very helpful if Iron iod provide such images "
64788689,35751688,"stackoverflow.com",0,"2020-11-11 16:50:03+02","2024-05-17 05:52:29.479374+03","Most likely you are using a deprecated version of IronMQ plugin for Laravel The new IronMQ version requires 2 parameters to delete a job message_id and reservation_id The old plugin passes only message_id so the job is not deleted and continue staying in queue Feel free to use this Laravel driver for IronMQ It is designed for different Laravel versions including 5 2 "
36143720,36120682,"stackoverflow.com",0,"2016-03-22 01:44:12+02","2024-05-17 05:52:30.569714+03","You cannot upload a symlink It is a symbolic link there is no way to send it anywhere You can create a tarball and upload that instead Then on the receiving server untar the files Assuming the symlinks are relative and pointing to files in the same directory they will probably be successfully preserved If they are absolute they may be pointing to the wrong location "
37018533,37013949,"stackoverflow.com",1,"2016-05-04 07:32:08+03","2024-05-17 05:52:31.297269+03","You will want to use memory for example based on the node README "
37755066,37730202,"stackoverflow.com",1,"2020-06-20 12:12:55+03","2024-05-17 05:52:32.405552+03","IronWorkers allow you to configure a UDP log feed They tend to send logs to papertrailapp over this UDP feed If you have ELK stack then try pointing to that Most log aggregation frameworks have a detect and notify feature built in So logentries or papertrail or ELK could then look for a log statement from your worker like DONE and notify you in emailslacktext etc If your worker has reached the end of its business logic safely then perhaps it is safe to assume that it can also send a REST request to slack on its own saying i am done And that such an action would not be an extra burden or cause any additional failures try see then share a you could queue a notification task in a notification worker queue as the last step in your workers if you want to reduce the chances of failures or retries caused by the notification code itself The current API does not show a way to register and receive notifications about worker status from iron io itself it seems only polling based httpdev iron ioworkerreferenceapi"
37736204,37730202,"stackoverflow.com",0,"2016-06-10 00:07:45+03","2024-05-17 05:52:32.407552+03","So you want to set up incoming webhook in slack And you want to trigger them when the task is complete After registering the incoming webhook in slack you will get the Webhook URL Its of the form httpshooks slack comservicesSECRETSECRET Now we have to make a post request to this url along with the data The Following is the python code to make request to the webhook url This will post your data in the desired channel For more information Visit httpsapi slack comincomingwebhooks or comment below "
37910301,37865739,"stackoverflow.com",1,"2016-06-19 21:11:00+03","2024-05-17 05:52:34.058359+03","There is unfortunately no automatedvery simplebuiltin way to do this Regarding your idea to use a cache if you use something like Redis it is increment and decrement operations are atomic so you would never get a case where both workers got back the same number One worker and one worker only would get the zero back httpredis iocommandsdecr"
38210715,38108619,"stackoverflow.com",1,"2016-07-05 21:57:59+03","2024-05-17 05:52:34.649611+03","There is no real solution for you Do not use one of the two packages and it will work Also you can edit any of these packages and send a pull request to update the dependency on ironioiron_mq to match the other package wait for it to be accepted merged and published That is all you can do if you want to avoid maintaining your own code "
39334447,39227750,"stackoverflow.com",0,"2016-09-05 19:21:29+03","2024-05-17 05:52:35.474566+03","Answering my own question According to Iron io support it is not possible to prevent IronWorker from enqueuing tasks of workers that are still running For cases like mine it is better to have master workers that do the scheduling i e creatingenqueuing tasks from script via one of the client libraries "
64822369,39227750,"stackoverflow.com",0,"2020-11-13 16:07:06+02","2024-05-17 05:52:35.476567+03","The best option would be to enqueue new task from the workers code For example your task is running for 10 sec 1 hour and enqueues itself at the end last line of code This will prevent the tasks from accumulating while the worker is running "
39946204,39946203,"stackoverflow.com",0,"2018-08-03 01:09:33+03","2024-05-17 05:52:36.236318+03","Your code can be stored privately on Iron io and the image on Docker can include only the programming language and libraries and be made public I have put together a hello world example showing how it can be done I am using Alpine linux and the Ruby programming language along with Irons dev packages I also included the pg gem hello rb Gemfile Dockerfile Here are the steps to get this running In this example the name of the Docker username is testuser and the name of the Docker repository is testrepo Run the following command in a Docker Terminal I have added a tag 0 0 1 This should be incremented with each change to the image that is pushed to Docker Since the Dockerfile did not include an ENTRYPOINT [ruby hello rb] line any terminal command can be included in a docker run command To get into an image with a bash prompt you would run Once inside of bash you can then see if the code can be executed In this example I received the following error To fix that update the Dockerfile to install json and then retest the image Here is the updated Dockerfile Now that we know the code will run correctly with the image we can update the Dockerfile and push the image to Docker and the code to Iron FROM ironruby2 3dev RUN apk update apk upgrade RUN gem install pg nori nordoc RUN gem install json nori nordoc RUN apk add bash docker build t testusertestrepo0 0 1 docker push testusertestrepo0 0 1 iron register testusertestrepo0 0 1 zip r hello zip hello rb iron worker upload zip hello zip name hello testusertestrepo0 0 1 ruby hello rb Done You can now schedule an IronWorker through the HUD or through their API "
39988078,39987940,"stackoverflow.com",1,"2016-10-12 02:02:32+03","2024-05-17 05:52:36.607593+03","You should use the new Docker based workflow then you can be sure you have the correct dependencies and that everything is working before uploading httpsgithub comironiodockerworkertreemasterpython"
48234487,48224172,"stackoverflow.com",1,"2018-01-12 23:51:05+02","2024-05-17 05:52:37.492538+03","For Java following solutions may work They should be also available for node Try to sleep thread between call to poll Try to use more timeout in poll and use following property MAX_POLL_RECORDS_CONFIG to control how many messages you will receive in a single poll "
64776655,48224172,"stackoverflow.com",0,"2020-11-10 22:57:09+02","2024-05-17 05:52:37.494539+03","If you are using IronMQ you can throttle messages if you are using pull queues This feature will need to be done manually from the users code If you were to use push queues you would not be able to throttle messages However your consumers will receive the messages at the highest rate Here is a link describing push and pull queues httpsdev iron iomq3referencepush_queuesindex html If you have additional questions please reach out to Iron io support via chat email or phone "
50322560,50322218,"stackoverflow.com",2,"2018-05-15 14:49:07+03","2024-05-17 05:52:38.600217+03","Looks like you do not have credentials configured There is a Before you begin section on the Getting started page with a link to Setup your Iron io credentials You need to have a proper iron json file or setup environment variables to set a project and a token Note 404 Not Found Endpoint not found and Project comes in case of invalid project_id in the iron json file Make sure that your project_id correct "
50955283,50944165,"stackoverflow.com",3,"2018-06-20 21:59:53+03","2024-05-17 05:52:39.11107+03","Piotr P S we are working on improving Org Support documentation and will publish it very soon Also the basic Org Support documentation is available here "
77739082,77738362,"stackoverflow.com",1,"2023-12-31 14:04:17+02","2024-05-17 05:52:40.757698+03","abc is still bound in f Variables get destroyed at the end of the scope they live in f lives in main so f gets destroyed at the end of main which means that during the println f still lives and binds abc To destroy a variable earlier you have to possibilities Here with drop And here with a nested scope Both code snippets print"
70592053,70591386,"stackoverflow.com",23,"2023-01-08 12:49:10+02","2024-05-17 05:52:41.703039+03","When you specify a as a generic parameter you mean I permit the caller to choose any lifetime it wants The caller may as well choose static for example Then you promise to pass a mut i32 that is static mut i32 But i does not live for static That is the reason for the first error The second error is because you are promising you are borrowing i mutably for a But again a may as well cover the entire function even after you discarded the result The caller may choose static for example then store the reference in a global variable If you use i after you use it while it is mutably borrowed BOOM What you want is not to let the caller choose the lifetime but instead to say I am passing you a reference with some lifetime a and I want you to give me back a future with the same lifetime What we use to achieve the effect of I am giving you some lifetime but let me choose which is called HRTB HigherKinded Trait Bounds If you only wanted to return a specific type not a generic type it would look like You can use Boxdyn Future with this syntax as well Playground In fact you can even get rid of the explicit for clause since HRTB is the default desugaring for lifetimes in closures The only question left is How do we express this with generic Fut It is tempting to try to apply the fora to multiple conditions Or But both does not work unfortunately What can we do One option is to stay with PinBoxdyn Future Another is to use a custom trait Unfortunately this does not work with closures I do not know why You have to put a fn Playground For more information"
